[
  "OPS01-BP01 Evaluate external customer needs\n    Involve key stakeholders, including business, development, and\n    operations teams, to determine where to focus efforts on external\n    customer needs. This verifies that you have a thorough understanding\n    of the operations support that is required to achieve your desired\n    business outcomes.\n  \n    Desired outcome: \n  \n     \n     \n     \n     \n  \n      \n        You work\n        backwards from customer outcomes.\n      \n    \n      \n        You understand how your operational practices support business\n        outcomes and objectives.\n      \n    \n      \n        You engage all relevant parties.\n      \n    \n      \n        You have mechanisms to capture external customer needs.\n      \n    \n    Common anti-patterns:\n  \n     \n     \n  \n      \n        You have decided not to have customer support outside of core\n        business hours, but you haven't reviewed historical support\n        request data. You do not know whether this will have an impact\n        on your customers.\n      \n    \n      \n        You are developing a new feature but have not engaged your\n        customers to find out if it is desired, if desired in what form,\n        and without experimentation to validate the need and method of\n        delivery.\n      \n    \n    Benefits of establishing this best\n      practice: Customers whose needs are satisfied are much\n    more likely to remain customers. Evaluating and understanding\n    external customer needs will inform how you prioritize your efforts\n    to deliver business value.\n  \n    Level of risk exposed if this best practice\n      is not established: High\n  \n    \n    Implementation guidance\n    \n    \n    \n      Understand business needs: Business success is created by shared\n      goals and understanding across stakeholders, including business,\n      development, and operations teams.\n    \n    \n      Review business goals, needs, and\n        priorities of external customers: Engage key\n      stakeholders, including business, development, and operations\n      teams, to discuss goals, needs, and priorities of external\n      customers. This ensures that you have a thorough understanding of\n      the operational support that is required to achieve business and\n      customer outcomes.\n    \n    \n      Establish a shared\n        understanding: Establish a shared understanding of the\n      business functions of the workload, the roles of each of the teams\n      in operating the workload, and how these factors support your\n      shared business goals across internal and external customers.\n    \n   \n    \n    Resources\n    \n      Related best practices:\n    \n      \n         \n      \n          \n            OPS11-BP03\n              Implement feedback loops\n          \n        \n  \u003cdiv\u003e\u003cdiv\u003e\u003cdiv\u003e\u003cdiv id=\"js_error_message\"\u003e\u003cp\u003e\u003cimg src=\"https://d1ge0kk1l5kms0.cloudfront.net/images/G/01/webservices/console/warning.png\" alt=\"Warning\" /\u003e \u003cstrong\u003eJavascript is disabled or is unavailable in your browser.\u003c/strong\u003e\u003c/p\u003e\u003cp\u003eTo use the Amazon Web Services Documentation, Javascript must be enabled. Please refer to your browser's Help pages for instructions.\u003c/p\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003eDocument ConventionsOPS 1. How do you determine what your priorities are?OPS01-BP02 Evaluate internal customer needs",
  "OPS01-BP02 Evaluate internal customer needs\n    Involve key stakeholders, including business, development, and\n    operations teams, when determining where to focus efforts on\n    internal customer needs. This will ensure that you have a thorough\n    understanding of the operations support that is required to achieve\n    business outcomes.\n  \n    Desired outcome: \n  \n     \n     \n  \n      \n        You use your\n        established priorities to focus your improvement efforts where they\n        will have the greatest impact (for example, developing team skills,\n        improving workload performance, reducing costs, automating runbooks,\n        or enhancing monitoring).\n      \n    \n      \n        You update your priorities as needs change.\n      \n    \n    Common anti-patterns:\n  \n     \n     \n     \n  \n      \n        You have\n        decided to change IP address allocations for your product teams,\n        without consulting them, to make managing your network easier. You\n        do not know the impact this will have on your product teams.\n      \n    \n      \n        You are implementing a new development tool but have not engaged\n        your internal customers to find out if it is needed or if it is\n        compatible with their existing practices.\n      \n    \n      \n        You are implementing a new monitoring system but have not\n        contacted your internal customers to find out if they have\n        monitoring or reporting needs that should be considered.\n      \n    \n    Benefits of establishing this best\n      practice: Evaluating and understanding internal customer\n    needs informs how you prioritize your efforts to deliver business\n    value.\n  \n    Level of risk exposed if this best practice\n      is not established: High\n  \n    \n    Implementation guidance\n    \n    \n    \n       \n       \n       \n    \n        \n          Understand business needs: Business success is created by\n          shared goals and understanding across stakeholders including\n          business, development, and operations teams.\n        \n      \n        \n          Review business goals, needs, and priorities of internal\n          customers: Engage key stakeholders, including business,\n          development, and operations teams, to discuss goals, needs,\n          and priorities of internal customers. This ensures that you\n          have a thorough understanding of the operational support that\n          is required to achieve business and customer outcomes.\n        \n      \n        \n          Establish shared understanding: Establish shared understanding\n          of the business functions of the workload, the roles of each\n          of the teams in operating the workload, and how these factors\n          support shared business goals across internal and external\n          customers.\n        \n      \n   \n    \n    Resources\n    \n    \n    \n      Related best practices:\n    \n       \n    \n        \n          OPS11-BP03\n            Implement feedback loops\n        \n      \n  \u003cdiv\u003e\u003cdiv\u003e\u003cdiv\u003e\u003cdiv id=\"js_error_message\"\u003e\u003cp\u003e\u003cimg src=\"https://d1ge0kk1l5kms0.cloudfront.net/images/G/01/webservices/console/warning.png\" alt=\"Warning\" /\u003e \u003cstrong\u003eJavascript is disabled or is unavailable in your browser.\u003c/strong\u003e\u003c/p\u003e\u003cp\u003eTo use the Amazon Web Services Documentation, Javascript must be enabled. Please refer to your browser's Help pages for instructions.\u003c/p\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003eDocument ConventionsOPS01-BP01 Evaluate external customer needsOPS01-BP03 Evaluate governance requirements",
  "OPS01-BP03 Evaluate governance requirements\n    Governance is the set of policies, rules, or frameworks that a company uses to \n    achieve its business goals. Governance requirements are generated from within your \n    organization. They can affect the types of technologies you choose or influence the \n    way you operate your workload. Incorporate organizational governance requirements \n    into your workload. Conformance is the ability to demonstrate that you have implemented \n    governance requirements. \n  \n    Desired outcome:\n  \n     \n     \n     \n  \n      \n        Governance requirements are incorporated into the architectural design and operation of your workload.\n      \n    \n      \n        You can provide proof that you have followed governance requirements.\n      \n    \n      \n        Governance requirements are regularly reviewed and updated.\n      \n    \n    Common anti-patterns:\n  \n     \n     \n     \n     \n  \n      Your organization mandates that the root account has multi-factor authentication. You failed to implement this requirement and the root account is compromised.\n    \n      During the design of your workload, you choose an instance type that is not approved by the IT department. You are unable to launch your workload and must conduct a redesign.\n    \n      You are required to have a disaster recovery plan. You did not create one and your workload suffers an extended outage.\n    \n      \n        Your team wants to use new instances but your governance requirements have not been updated to allow them.\n      \n    \n    Benefits of establishing this best\n    practice: \n  \n     \n     \n  \n      \n        Following governance requirements aligns your workload with larger organization policies.\n      \n    \n      \n        Governance requirements reflect industry standards and best practices for your organization.\n      \n    \n    Level of risk exposed if this best practice\n    is not established: High\n  \n\n  Implementation guidance\n    Identify governance requirement by working with stakeholders and \n      governance organizations. Include governance requirements into your \n      workload. Be able to demonstrate proof that you’ve followed governance \n      requirements.\n    \n      Customer example\n    \n    \n      At AnyCompany Retail, the cloud operations team works with stakeholders \n      across the organization to develop governance requirements. For example, \n      they prohibit SSH access into Amazon EC2 instances. If teams need system access, \n      they are required to use AWS Systems Manager Session Manager. The cloud \n      operations team regularly updates governance requirements as new services \n      become available. \n    \n    \n      Implementation steps\n    \n    \n       \n       \n       \n       \n    \n        \n          Identify the stakeholders for your workload, including any centralized teams. \n        \n      \n        \n          Work with stakeholders to identify governance requirements. \n        \n      \n        \n          Once you’ve generated a list, prioritize the improvement items, and begin implementing them into your workload. \n        \n        \n           \n           \n        \n            \n              Use services like AWS Config to create governance-as-code and validate that governance requirements are followed.\n            \n          \n            \n              If you use AWS Organizations, you can leverage Service Control Policies to implement governance requirements.\n            \n          \n      \n        \n          Provide documentation that validates the implementation.\n        \n      \n    \n      Level of effort for the implementation plan: Medium. Implementing missing governance requirements\n      may result in rework of your workload.\n    \n   \n\n  Resources\n    \n      Related best practices:\n    \n    \n       \n    \n        \n          OPS01-BP04 Evaluate compliance requirements - Compliance is like governance but comes from outside an organization.\n        \n      \n\n      \n    \n      Related documents:\n    \n    \n       \n       \n       \n       \n    \n        AWS Management and Governance Cloud Environment Guide\n        \n      \n        \n          Best Practices for AWS Organizations Service Control Policies in a Multi-Account Environment\n        \n      \n        \n          Governance in the AWS Cloud: The Right Balance Between Agility and Safety\n        \n      \n        \n          What is Governance, Risk, And Compliance (GRC)?\n        \n      \n    \n      Related videos:\n    \n    \n       \n       \n       \n       \n    \n        AWS Management and Governance: Configuration, Compliance, and Audit - AWS Online Tech Talks\n        \n      \n        AWS re:Inforce 2019: Governance for the Cloud Age (DEM12-R1)\n        \n      \n        AWS re:Invent 2020: Achieve compliance as code using AWS Config\n      \n        AWS re:Invent 2020: Agile governance on AWS GovCloud (US)\n      \n    \n      Related examples:\n    \n    \n       \n    \n        AWS Config Conformance Pack Samples\n        \n      \n    \n      Related services:\n    \n    \n       \n       \n    \n        AWS Config\n      \n        AWS Organizations - Service Control Policies\n        \n      \n  \u003cdiv\u003e\u003cdiv\u003e\u003cdiv\u003e\u003cdiv id=\"js_error_message\"\u003e\u003cp\u003e\u003cimg src=\"https://d1ge0kk1l5kms0.cloudfront.net/images/G/01/webservices/console/warning.png\" alt=\"Warning\" /\u003e \u003cstrong\u003eJavascript is disabled or is unavailable in your browser.\u003c/strong\u003e\u003c/p\u003e\u003cp\u003eTo use the Amazon Web Services Documentation, Javascript must be enabled. Please refer to your browser's Help pages for instructions.\u003c/p\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003eDocument ConventionsOPS01-BP02 Evaluate internal customer needsOPS01-BP04 Evaluate compliance requirements",
  "OPS01-BP04 Evaluate compliance requirementsRegulatory, industry, and internal compliance requirements are an important driver for defining \n    your organization’s priorities. Your compliance framework may preclude you from using specific \n    technologies or geographic locations. Apply due diligence if no external compliance frameworks are \n    identified. Generate audits or reports that validate compliance.\n    If you advertise that your product meets specific compliance standards, you must have an internal process for ensuring continuous compliance. Examples of compliance standards include PCI DSS, FedRAMP, and HIPAA. Applicable compliance standards are determined by various factors, such as what types of data the solution stores or transmits and which geographic regions the solution supports.\n  \n    Desired outcome:\n  \n     \n     \n  \n      \n        Regulatory, industry, and internal compliance requirements are incorporated into architectural selection.\n      \n    \n      \n        You can validate compliance and generate audit reports.\n      \n    \n    Common anti-patterns:\n  \n     \n     \n     \n  \n      Parts of your workload fall under the Payment Card Industry Data Security Standard (PCI-DSS) framework but your workload stores credit cards data unencrypted.\n    \n      Your software developers and architects are unaware of the compliance framework that your organization must adhere to.\n    \n      \n        The yearly Systems and Organizations Control (SOC2) Type II audit is happening soon and you are unable to verify that controls are in place.\n      \n    \n    Benefits of establishing this best\n    practice: \n  \n     \n     \n     \n  \n      \n        Evaluating and understanding the compliance requirements that apply to your workload will inform how you prioritize your efforts to deliver business value.\n      \n    \n      \n        You choose the right locations and technologies that are congruent with your compliance framework.\n      \n    \n      \n        Designing your workload for auditability helps you to prove you are adhering to your compliance framework.\n      \n    \n    Level of risk exposed if this best practice\n    is not established: High\n  \n\n  Implementation guidance\n    \n      Implementing this best practice means that you incorporate compliance requirements into your architecture design process. Your team members are aware of the required compliance framework. You validate compliance in line with the framework.\n    \n    \n      Customer example\n    \n    \n      AnyCompany Retail stores credit card information for customers. Developers on the card storage team understand that they need to comply with the PCI-DSS framework. They’ve taken steps to verify that credit card information is stored and accessed securely in line with the PCI-DSS framework. Every year they work with their security team to validate compliance.\n    \n    \n      Implementation steps\n    \n    \n       \n       \n       \n    \n        \n          Work with your security and governance teams to determine what industry, regulatory, or internal compliance frameworks that your workload must adhere to. Incorporate the compliance frameworks into your workload.\n        \n        \n           \n        \n            \n              Validate continual compliance of AWS resources with services like \n              AWS Compute Optimizer \n              and AWS Security Hub. \n            \n          \n      \n        \n          Educate your team members on the compliance requirements so they can operate and evolve the workload in line with them. Compliance requirements should be included in architectural and technological choices.\n        \n      \n        \n          Depending on the compliance framework, you may be required to generate an audit or compliance report. Work with your organization to automate this process as much as possible.\n        \n        \n           \n           \n        \n            \n              Use services like AWS Audit Manager to validate compliance and generate audit reports.\n            \n          \n            \n              You can download AWS security and compliance documents with AWS Artifact.\n            \n          \n      \n    \n      Level of effort for the implementation plan: Medium. Implementing compliance frameworks can be challenging. Generating audit reports or compliance documents adds additional complexity. \n    \n   \n\n  Resources\n    \n      Related best practices:\n    \n    \n       \n       \n       \n       \n    \n        \n          SEC01-BP03 Identify and validate control objectives - Security control objectives are an important part of overall compliance.\n        \n      \n        \n          SEC01-BP06 Automate testing and validation of security controls in pipelines - As part of your pipelines, validate security controls. You can also generate compliance documentation for new changes.\n        \n      \n        \n          SEC07-BP02 Define data protection controls - Many compliance frameworks have data handling and storage policies based.\n        \n      \n        \n          SEC10-BP03 Prepare forensic capabilities - Forensic capabilities can sometimes be used in auditing compliance.\n        \n      \n\n      \n    \n      Related documents:\n    \n    \n       \n       \n       \n       \n       \n    \n        AWS Compliance Center\n        \n      \n        AWS Compliance Resources\n        \n      \n        AWS Risk and Compliance Whitepaper\n        \n      \n        AWS Shared Responsibility Model\n        \n      \n        AWS services in scope by compliance programs\n        \n      \n    \n      Related videos:\n    \n    \n       \n       \n       \n    \n        AWS re:Invent 2020: Achieve compliance as code using AWS Compute Optimizer\n      \n        AWS re:Invent 2021 - Cloud compliance, assurance, and auditing\n        \n      \n        AWS Summit ATL 2022 - Implementing compliance, assurance, and auditing on AWS (COP202)\n        \n      \n    \n      Related examples:\n    \n    \n       \n    \n        \n          PCI DSS and AWS Foundational Security Best Practices on AWS\n      \n    \n      Related services:\n    \n    \n       \n       \n       \n       \n    \n        AWS Artifact\n      \n        AWS Audit Manager\n      \n        AWS Compute Optimizer\n      \n        AWS Security Hub\n      \n  \u003cdiv\u003e\u003cdiv\u003e\u003cdiv\u003e\u003cdiv id=\"js_error_message\"\u003e\u003cp\u003e\u003cimg src=\"https://d1ge0kk1l5kms0.cloudfront.net/images/G/01/webservices/console/warning.png\" alt=\"Warning\" /\u003e \u003cstrong\u003eJavascript is disabled or is unavailable in your browser.\u003c/strong\u003e\u003c/p\u003e\u003cp\u003eTo use the Amazon Web Services Documentation, Javascript must be enabled. Please refer to your browser's Help pages for instructions.\u003c/p\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003eDocument ConventionsOPS01-BP03 Evaluate governance requirementsOPS01-BP05 Evaluate threat landscape",
  "OPS01-BP05 Evaluate threat landscape\n    Evaluate threats to the business (for example, competition, business\n    risk and liabilities, operational risks, and information security\n    threats) and maintain current information in a risk registry.\n    Include the impact of risks when determining where to focus efforts.\n  \n    The\n    Well-Architected\n      Framework emphasizes learning, measuring, and improving. It\n    provides a consistent approach for you to evaluate architectures,\n    and implement designs that will scale over time. AWS provides the\n    AWS Well-Architected Tool to help you review your approach prior\n    to development, the state of your workloads prior to production, and\n    the state of your workloads in production. You can compare them to\n    the latest AWS architectural best practices, monitor the overall\n    status of your workloads, and gain insight to potential risks.\n  \n    AWS customers are eligible for a guided Well-Architected Review of\n    their mission-critical workloads to\n    measure\n      their architectures against AWS best practices. Enterprise\n    Support customers are eligible for an\n    Operations\n      Review, designed to help them to identify gaps in their\n    approach to operating in the cloud.\n  \n    The cross-team engagement of these reviews helps to establish common\n    understanding of your workloads and how team roles contribute to\n    success. The needs identified through the review can help shape your\n    priorities.\n  \n    AWS Trusted Advisor is a tool that provides access to a core set\n    of checks that recommend optimizations that may help shape your\n    priorities.\n    Business\n      and Enterprise Support customers receive access to additional\n    checks focusing on security, reliability, performance, and\n    cost-optimization that can further help shape their priorities.\n  \n    Desired outcome:\n  \n     \n     \n     \n     \n     \n  \n      \n        You regularly\n        review and act on Well-Architected and Trusted Advisor outputs\n      \n    \n      \n        You are aware of the latest patch status of your services\n      \n    \n      \n        You understand the risk and impact of known threats and act\n        accordingly\n      \n    \n      \n        You implement mitigations as necessary\n      \n    \n      \n        You communicate actions and context\n      \n    \n    Common anti-patterns: \n  \n     \n     \n     \n  \n      \n        You are\n        using an old version of a software library in your product. You are\n        unaware of security updates to the library for issues that may have\n        unintended impact on your workload.\n      \n    \n      \n        Your competitor just released a version of their product that\n        addresses many of your customers' complaints about your product.\n        You have not prioritized addressing any of these known issues.\n      \n    \n      \n        Regulators have been pursuing companies like yours that are not\n        compliant with legal regulatory compliance requirements. You\n        have not prioritized addressing any of your outstanding\n        compliance requirements.\n      \n    \n    Benefits of establishing this best practice:\n     You identify and understand the threats to your\n    organization and workload, which helps your determination of which\n    threats to address, their priority, and the resources necessary to\n    do so.\n  \n    Level of risk exposed if this best practice\n      is not established: Medium\n  \n    \n    Implementation guidance\n    \n    \n    \n       \n       \n    \n        \n          Evaluate threat landscape:\n          Evaluate threats to the business (for example, competition,\n          business risk and liabilities, operational risks, and\n          information security threats), so that you can include their\n          impact when determining where to focus efforts.\n        \n        \n           \n           \n        \n            \n              AWS               Latest Security Bulletins\n            \n          \n            \n              AWS Trusted Advisor\n            \n          \n      \n        \n          Maintain a threat model:\n          Establish and maintain a threat model identifying potential\n          threats, planned and in place mitigations, and their priority.\n          Review the probability of threats manifesting as incidents,\n          the cost to recover from those incidents and the expected harm\n          caused, and the cost to prevent those incidents. Revise\n          priorities as the contents of the threat model change.\n        \n      \n   \n    \n    Resources\n    \n    \n    \n      Related best practice:\n    \n    \n       \n    \n        \n          SEC01-BP07\n            Identify threats and prioritize mitigations using a threat\n            model\n        \n      \n      \n \n    \n      Related documents:\n    \n    \n       \n       \n       \n    \n        \n          AWS Cloud\n            Compliance\n        \n      \n        \n          AWS           Latest Security Bulletins\n        \n      \n        \n          AWS Trusted Advisor\n        \n      \n    \n      Related videos:\n      \n    \n       \n    \n        \n          AWS       re:Inforce 2023 - A tool to help improve your threat\n            modeling\n        \n      \n  \u003cdiv\u003e\u003cdiv\u003e\u003cdiv\u003e\u003cdiv id=\"js_error_message\"\u003e\u003cp\u003e\u003cimg src=\"https://d1ge0kk1l5kms0.cloudfront.net/images/G/01/webservices/console/warning.png\" alt=\"Warning\" /\u003e \u003cstrong\u003eJavascript is disabled or is unavailable in your browser.\u003c/strong\u003e\u003c/p\u003e\u003cp\u003eTo use the Amazon Web Services Documentation, Javascript must be enabled. Please refer to your browser's Help pages for instructions.\u003c/p\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003eDocument ConventionsOPS01-BP04 Evaluate compliance requirementsOPS01-BP06 Evaluate tradeoffs while managing benefits and risks",
  "OPS01-BP06 Evaluate tradeoffs while managing benefits and risks\n    Competing interests from multiple parties can make it challenging to\n    prioritize efforts, build capabilities, and deliver outcomes aligned\n    with business strategies. For example, you may be asked to\n    accelerate speed-to-market for new features over optimizing IT\n    infrastructure costs. This can put two interested parties in\n    conflict with one another. In these situations, decisions need to be\n    brought to a higher authority to resolve conflict. Data is required\n    to remove emotional attachment from the decision-making process.\n  \n    The same challenge may occur at a tactical level. For example, the\n    choice between using relational or non-relational database\n    technologies can have a significant impact on the operation of an\n    application. It's critical to understand the predictable results of\n    various choices.\n  \n    AWS can help you educate your teams about AWS and its services to\n    increase their understanding of how their choices can have an impact\n    on your workload. Use the resources provided by\n    Support\n    (AWS     Knowledge Center,\n    AWS     Discussion Forums, and\n    Support Center) and\n    AWS     Documentation to educate your teams. For further questions,\n    reach out to Support.\n  \n    AWS also shares operational best practices and patterns in\n    The\n      Amazon Builders' Library. A wide variety of other useful\n    information is available through the\n    AWS Blog and\n    The\n      Official AWS Podcast.\n  \n    Desired outcome: You have a\n    clearly defined decision-making governance framework to facilitate\n    important decisions at every level within your cloud delivery\n    organization. This framework includes features like a risk register,\n    defined roles that are authorized to make decisions, and a defined\n    models for each level of decision that can be made. This framework\n    defines in advance how conflicts are resolved, what data needs to be\n    presented, and how options are prioritized, so that once decisions\n    are made you can commit without delay. The decision-making framework\n    includes a standardized approach to reviewing and weighing the\n    benefits and risks of every decision to understand the tradeoffs.\n    This may include external factors, such as adherence to regulatory\n    compliance requirements.\n  \n    Common anti-patterns: \n  \n     \n     \n     \n  \n      \n        Your\n        investors request that you demonstrate compliance with Payment Card\n        Industry Data Security Standards (PCI DSS). You do not consider the\n        tradeoffs between satisfying their request and continuing with your\n        current development efforts. Instead, you proceed with your\n        development efforts without demonstrating compliance. Your investors\n        stop their support of your company over concerns about the security\n        of your platform and their investments.\n      \n    \n      \n        You have decided to include a library that one of your\n        developers found on the internet. You have not evaluated the\n        risks of adopting this library from an unknown source and do not\n        know if it contains vulnerabilities or malicious code.\n      \n    \n      \n        The original business justification for your migration was based\n        upon the modernization of 60% of your application workloads.\n        However, due to technical difficulties, a decision was made to\n        modernize only 20%, leading to a reduction in planned benefits\n        long-term, increased operator toil for infrastructure teams to\n        manually support legacy systems, and greater reliance on\n        developing new skillsets in your infrastructure teams that were\n        not planning for this change.\n      \n    \n    Benefits of establishing this best\n      practice: Fully aligning and supporting board-level\n    business priorities, understanding the risks to achieving success,\n    making informed decisions, and acting appropriately when risks\n    impede chances for success. Understanding the implications and\n    consequences of your decisions helps you to prioritize your options\n    and bring leaders into agreement faster, leading to improved\n    business outcomes. Identifying the available benefits of your\n    choices and being aware of the risks to your organization helps you\n    make data-driven decisions, rather than relying on anecdotes.\n  \n    Level of risk exposed if this best practice\n      is not established: Medium\n  \n    \n    Implementation guidance\n    \n    \n    \n      Managing benefits and risks should be defined by a governing body\n      that drives the requirements for key decision-making. You want\n      decisions to be made and prioritized based on how they benefit the\n      organization, with an understanding of the risks involved.\n      Accurate information is critical for making the organizational\n      decisions. This should be based on solid measurements and defined\n      by common industry practices of cost benefit analysis. To make\n      these types of decisions, strike a balance between centralized and\n      decentralized authority. There is always a tradeoff, and it's\n      important to understand how each choice impacts defined strategies\n      and desired business outcomes.\n    \n     \n      \n      Implementation steps\n      \n      \n      \n         \n         \n         \n         \n         \n      \n          \n            Formalize benefits measurement practices within a holistic\n            cloud governance framework.\n          \n          \n             \n             \n             \n          \n              \n                Balance central control of decision-making with\n                decentralized authority for some decisions.\n              \n            \n              \n                Understand that burdensome decision-making processes\n                imposed on every decision can slow you down.\n              \n            \n              \n                Incorporate external factors into your decision making\n                process (like compliance requirements).\n              \n            \n        \n          \n            Establish an agreed-upon decision-making framework for\n            various levels of decisions, which includes who is required\n            to unblock decisions that are subject to conflicted\n            interests.\n          \n          \n             \n             \n          \n              \n                Centralize one-way door decisions that could be\n                irreversible.\n              \n            \n              \n                Allow two-way door decisions to be made by lower level\n                organizational leaders.\n              \n            \n        \n          \n            Understand and manage benefits and risks. Balance the\n            benefits of decisions against the risks involved.\n          \n          \n             \n             \n             \n          \n              \n                Identify benefits:\n                Identify benefits based on business goals, needs, and\n                priorities. Examples include business case impact,\n                time-to-market, security, reliability, performance, and\n                cost.\n              \n            \n              \n                Identify risks:\n                Identify risks based on business goals, needs, and\n                priorities. Examples include time-to-market, security,\n                reliability, performance, and cost.\n              \n            \n              \n                Assess benefits against risks\n                  and make informed decisions: Determine the\n                impact of benefits and risks based on goals, needs, and\n                priorities of your key stakeholders, including business,\n                development, and operations. Evaluate the value of the\n                benefit against the probability of the risk being\n                realized and the cost of its impact. For example,\n                emphasizing speed-to-market over reliability might\n                provide competitive advantage. However, it may result in\n                reduced uptime if there are reliability issues.\n              \n            \n        \n          \n            Programatically enforce key decisions that automate your\n            adherence to compliance requirements.\n          \n        \n          \n            Leverage known industry frameworks and capabilities, such as\n            Value Stream Analysis and LEAN, to baseline current state\n            performance, business metrics, and define iterations of\n            progress towards improvements to these metrics.\n          \n        \n     \n    \n      Level of effort for the implementation\n        plan: Medium-High\n    \n   \n    \n    Resources\n    \n    \n    \n      Related best practices:\n      \n    \n       \n    \n        \n          OPS01-BP05\n            Evaluate threat landscape\n        \n      \n    \n    \n      Related documents:\n      \n    \n       \n       \n       \n       \n    \n        \n          Elements\n            of Amazon's Day 1 Culture | Make high quality, high velocity\n            decisions\n        \n      \n        \n          Cloud\n            Governance\n        \n      \n        \n          Management\n            \u0026 Governance Cloud Environment\n        \n      \n        \n          Governance\n            in the Cloud and in the Digital Age: Parts One \u0026\n            Two\n        \n      \n    \n      Related videos:\n    \n    \n       \n    \n        \n          Podcast\n            | Jeff Bezos | On how to make decisions\n        \n      \n    \n      Related examples:\n    \n    \n       \n       \n    \n        \n          Make\n            informed decisions using data (The DevOps Sagas)\n        \n      \n        \n          Using\n            development value stream mapping to identify constraints to\n            DevOps outcomes\n        \n      \n  \u003cdiv\u003e\u003cdiv\u003e\u003cdiv\u003e\u003cdiv id=\"js_error_message\"\u003e\u003cp\u003e\u003cimg src=\"https://d1ge0kk1l5kms0.cloudfront.net/images/G/01/webservices/console/warning.png\" alt=\"Warning\" /\u003e \u003cstrong\u003eJavascript is disabled or is unavailable in your browser.\u003c/strong\u003e\u003c/p\u003e\u003cp\u003eTo use the Amazon Web Services Documentation, Javascript must be enabled. Please refer to your browser's Help pages for instructions.\u003c/p\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003eDocument ConventionsOPS01-BP05 Evaluate threat landscape OPS 2. How do you structure your organization to support your business\n                  outcomes? ",
  "OPS02-BP01 Resources have identified owners\n    Resources for your workload must have identified owners for change\n    control, troubleshooting, and other functions. Owners are assigned\n    for workloads, accounts, infrastructure, platforms, and\n    applications. Ownership is recorded using tools like a central\n    register or metadata attached to resources. The business value of\n    components informs the processes and procedures applied to them.\n  \n    Desired outcome:\n  \n     \n     \n     \n  \n      \n        Resources have identified owners using metadata or a central\n        register.\n      \n    \n      \n        Team members can identify who owns resources.\n      \n    \n      \n        Accounts have a single owner where possible.\n      \n    \n    Common anti-patterns:\n  \n     \n     \n     \n     \n  \n      \n        The alternate contacts for your AWS accounts are not populated.\n      \n    \n      \n        Resources lack tags that identify what teams own them.\n      \n    \n      \n        You have an ITSM queue without an email mapping.\n      \n    \n      \n        Two teams have overlapping ownership of a critical piece of\n        infrastructure.\n      \n    \n    Benefits of establishing this best\n    practice:\n  \n     \n     \n  \n      \n        Change control for resources is straightforward with assigned\n        ownership.\n      \n    \n      \n        You can involve the right owners when troubleshooting issues.\n      \n    \n    Level of risk exposed if this best practice\n    is not established: High\n  \n\n  Implementation guidance\n\n      \n    \n      Define what ownership means for the resource use cases in your\n      environment. Ownership can mean who oversees changes to the\n      resource, supports the resource during troubleshooting, or who is\n      financially accountable. Specify and record owners for resources,\n      including name, contact information, organization, and team.\n    \n    \n      Customer example\n    \n    \n      AnyCompany Retail defines ownership as the team or individual that\n      owns changes and support for resources. They leverage AWS Organizations to manage their AWS accounts. Alternate account\n      contacts are configuring using group inboxes. Each ITSM queue maps\n      to an email alias. Tags identify who own AWS resources. For other\n      platforms and infrastructure, they have a wiki page that\n      identifies ownership and contact information.\n    \n     \n\n  Implementation steps\n\n      \n      \n         \n         \n         \n         \n         \n      \n          \n            Start by defining ownership for your organization. Ownership\n            can imply who owns the risk for the resource, who owns\n            changes to the resource, or who supports the resource when\n            troubleshooting. Ownership could also imply financial or\n            administrative ownership of the resource.\n          \n        \n          \n            Use\n            AWS Organizations to manage accounts. You can manage the\n            alternate contacts for your accounts centrally.\n          \n          \n             \n             \n          \n              \n                Using company owned email addresses and phone numbers\n                for contact information helps you to access them even if\n                the individuals whom they belong to are no longer with\n                your organization. For example, create separate email\n                distribution lists for billing, operations, and security\n                and configure these as Billing, Security, and Operations\n                contacts in each active AWS account. Multiple people\n                will receive AWS notifications and be able to respond,\n                even if someone is on vacation, changes roles, or leaves\n                the company.\n              \n            \n              \n                If an account is not managed by\n                AWS Organizations, alternate account contacts help\n                AWS get in contact with the appropriate personnel if\n                needed. Configure the account's alternate contacts to\n                point to a group rather than an individual.\n              \n            \n        \n          \n            Use tags to identify owners for AWS resources. You can\n            specify both owners and their contact information in\n            separate tags.\n          \n          \n             \n             \n          \n              \n                You can use\n                AWS Config rules to enforce that resources have the\n                required ownership tags.\n              \n            \n              \n                For in-depth guidance on how to build a tagging strategy\n                for your organization, see\n                AWS                 Tagging Best Practices whitepaper.\n              \n            \n        \n          \n            Use\n            Amazon Q Business, a conversational assistant that uses\n            generative AI to enhance workforce productivity, answer\n            questions, and complete tasks based on information in your\n            enterprise systems.\n          \n          \n             \n          \n              \n                Connect Amazon Q Business to your company's data source.\n                Amazon Q Business offers prebuilt connectors to over 40\n                supported data sources, including Amazon Simple Storage Service (Amazon S3), Microsoft SharePoint, Salesforce,\n                and Atlassian Confluence. For more information, see\n                Amazon Q Business connectors.\n              \n            \n        \n          \n            For other resources, platforms, and infrastructure, create\n            documentation that identifies ownership. This should be\n            accessible to all team members.\n          \n        \n      \n        Level of effort for the implementation\n        plan: Low. Leverage account contact information and\n        tags to assign ownership of AWS resources. For other resources\n        you can use something as simple as a table in a wiki to record\n        ownership and contact information, or use an ITSM tool to map\n        ownership.\n      \n     \n   \n\n  Resources\n\n      \n    \n      Related best practices:\n    \n    \n       \n       \n    \n        \n          OPS02-BP02\n          Processes and procedures have identified owners\n        \n      \n        \n          OPS02-BP04\n          Mechanisms exist to manage responsibilities and\n          ownership\n        \n      \n    \n      Related documents:\n    \n    \n       \n       \n       \n       \n       \n       \n       \n       \n    \n        \n          AWS           Account Management - Updating contact information\n        \n      \n        \n          AWS Organizations - Updating alternative contacts in your\n          organization\n        \n      \n        \n          AWS           Tagging Best Practices whitepaper\n        \n      \n        \n          Build\n          private and secure enterprise generative AI apps with Amazon Q\n          Business and AWS IAM Identity Center\n        \n      \n        \n          Amazon Q Business, now generally available, helps boost workforce\n          productivity with generative AI\n        \n      \n        \n          AWS Cloud Operations \u0026 Migrations Blog - Implementing\n          automated and centralized tagging controls with AWS Config and\n          AWS Organizations\n        \n      \n        \n          AWS           Security Blog - Extend your pre-commit hooks with AWS CloudFormation Guard\n        \n      \n        \n          AWS           DevOps Blog - Integrating AWS CloudFormation Guard into CI/CD\n          pipelines\n        \n      \n    \n      Related workshops:\n    \n    \n       \n    \n        \n          AWS           Workshop - Tagging\n        \n      \n    \n      Related examples:\n    \n    \n       \n    \n        \n          AWS Config Rules - Amazon EC2 with required tags and valid\n          values\n        \n      \n    \n      Related services:\n    \n    \n       \n       \n    \n        \n          AWS Config Rules - required-tags\n        \n      \n        \n          AWS Organizations\n        \n      \n  \u003cdiv\u003e\u003cdiv\u003e\u003cdiv\u003e\u003cdiv id=\"js_error_message\"\u003e\u003cp\u003e\u003cimg src=\"https://d1ge0kk1l5kms0.cloudfront.net/images/G/01/webservices/console/warning.png\" alt=\"Warning\" /\u003e \u003cstrong\u003eJavascript is disabled or is unavailable in your browser.\u003c/strong\u003e\u003c/p\u003e\u003cp\u003eTo use the Amazon Web Services Documentation, Javascript must be enabled. Please refer to your browser's Help pages for instructions.\u003c/p\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003eDocument Conventions OPS 2. How do you structure your organization to support your business\n                  outcomes? OPS02-BP02 Processes and procedures have identified\n  owners",
  "OPS02-BP02 Processes and procedures have identified\n  owners\n    Understand who has ownership of the definition of individual\n    processes and procedures, why those specific process and procedures\n    are used, and why that ownership exists. Understanding the reasons\n    that specific processes and procedures are used aids in\n    identification of improvement opportunities.\n  \n    Desired outcome: Your\n    organization has a well defined and maintained set of process and\n    procedures for operational tasks. The process and procedures are\n    stored in a central location and available to your team members.\n    Process and procedures are updated frequently, by clearly assigned\n    ownership. Where possible, scripts, templates, and automation\n    documents are implemented as code.\n  \n    Common anti-patterns:\n  \n     \n     \n     \n     \n  \n      \n        Processes are not documented. Fragmented scripts may exist on\n        isolated operator workstations.\n      \n    \n      \n        Knowledge of how to use scripts is held by a few individuals or\n        informally as team knowledge.\n      \n    \n      \n        A legacy process is due for an update, but ownership of the\n        update is unclear, and the original author is no longer part of\n        the organization.\n      \n    \n      \n        Processes and scripts are not discoverable, so they are not\n        readily available when required (for example, in response to an\n        incident).\n      \n    \n    Benefits of establishing this best\n    practice:\n  \n     \n     \n     \n     \n     \n     \n  \n      \n        Processes and procedures boost your efforts to operate your\n        workloads.\n      \n    \n      \n        New team members become effective more quickly.\n      \n    \n      \n        Reduced time to mitigate incidents.\n      \n    \n      \n        Different team members (and teams) can use the same processes\n        and procedures in a consistent manner.\n      \n    \n      \n        Teams can scale their processes with repeatable processes.\n      \n    \n      \n        Standardized processes and procedures help mitigate the impact\n        of transferring workload responsibilities between teams.\n      \n    \n    Level of risk exposed if this best practice\n    is not established: High\n  \n\n  Implementation guidance\n\n      \n    \n       \n       \n    \n        \n          Processes and procedures have identified owners who are\n          responsible for their definition.\n        \n        \n           \n           \n           \n        \n            \n              Identify the operations activities conducted in support of\n              your workloads. Document these activities in a\n              discoverable location.\n            \n          \n            \n              Uniquely identify the individual or team responsible for\n              the specification of an activity. They are responsible to\n              verify that it can be successfully performed by an\n              adequately skilled team member with the correct\n              permissions, access, and tools. If there are issues with\n              performing that activity, the team members performing it\n              are responsible for providing the detailed feedback\n              necessary for the activity to be improved.\n            \n          \n            \n              Capture ownership in the metadata of the activity artifact\n              through services like AWS Systems Manager, through\n              documents, and AWS Lambda. Capture resource ownership\n              using tags or resource groups, specifying ownership and\n              contact information. Use AWS Organizations to create\n              tagging polices and capture ownership and contact\n              information.\n            \n          \n      \n        \n          Over time, these procedures should be evolved to be runnable\n          as code, reducing the need for human intervention.\n        \n        \n           \n           \n           \n        \n            \n              For example, consider AWS Lambda functions, CloudFormation\n              templates, or AWS Systems Manager automation docs.\n            \n          \n            \n              Perform version control in appropriate repositories.\n            \n          \n            \n              Include suitable resource tagging so owners and\n              documentation can readily be identified.\n            \n          \n      \n    \n      Customer example\n    \n    \n      AnyCompany Retail defines ownership as the team or individual that\n      owns processes for an application or groups of applications (that\n      share common architetural practices and technologies). Initially,\n      the process and procedures are documented as step-by-step guides\n      in the document management system, discoverable using tags on the\n      AWS account that hosts the application and on specific groups of\n      resources within the account. They leverage AWS Organizations to\n      manage their AWS accounts. Over time, these processes are\n      converted to code, and resources are defined using infrastructure\n      as code (such as CloudFormation or AWS Cloud Development Kit (AWS CDK)\n      templates). The operational processes become automation documents\n      in AWS Systems Manager or AWS Lambda functions, which can be\n      initiated as scheduled tasks, in response to events such as AWS\n      CloudWatch alarms or AWS EventBridge events, or started by\n      requests within an IT service management (ITSM) platform. All\n      process have tags to identify ownership. Documentation for the\n      automation and process is maintained within the wiki pages\n      generated by the code repository for the process.\n    \n     \n\n  Implementation steps\n\n      \n      \n         \n         \n         \n         \n         \n      \n          \n            Document the existing processes and procedures.\n          \n          \n             \n             \n             \n             \n          \n              \n                Review and keep them up-to-date.\n              \n            \n              \n                Identify an owner for each process or procedure.\n              \n            \n              \n                Place them under version control.\n              \n            \n              \n                Where possible, share processes and procedures across\n                workloads and environments that share architectural\n                designs.\n              \n            \n        \n          \n            Establish mechanisms for feedback and improvement.\n          \n          \n             \n             \n             \n             \n          \n              \n                Define policies for how frequently processes should be\n                reviewed.\n              \n            \n              \n                Define processes for reviewers and approvers.\n              \n            \n              \n                Implement issues or a ticketing queue for feedback to be\n                provided and tracked.\n              \n            \n              \n                Whereever possible, processes and procedures should have\n                pre-approval and risk classification from a change\n                approval board (CAB).\n              \n            \n        \n          \n            Verify that processes and procedures are accessible and\n            discoverable by those who need to run them.\n          \n          \n             \n             \n             \n          \n              \n                Use tags to indicate where the process and procedures\n                can be accessed for the workload.\n              \n            \n              \n                Use meaningful error and event messaging to indicate the\n                appropriate processes or procedures to address an issue.\n              \n            \n              \n                Use wikis and document management, and make processes\n                and procedures searchable consistently accross the\n                organization.\n              \n            \n        \n          \n            Use Amazon Q Business, a conversational assistant that uses generative AI to enhance workforce productivity, answer questions, and complete tasks based on information in your enterprise systems.\n          \n          \n             \n          \n              \n                Connect Amazon Q Business to your company's data source. Amazon Q Business offers prebuilt connectors to over 40 supported data sources, including Amazon S3, Microsoft SharePoint, Salesforce, and Atlassian Confluence. For more information, see Amazon Q connectors.\n              \n            \n        \n          \n            Automate when appropriate.\n          \n          \n             \n             \n             \n          \n              \n                Automations should be developed when services and\n                technologies provide an API.\n              \n            \n              \n                Educate adequately on processes. Develop the user\n                stories and requirements to automate those processes.\n              \n            \n              \n                Measure the use of your processes and procedures\n                successfully, and create issues or tickets to support iterative\n                improvement.\n              \n            \n        \n      \n        Level of effort for the implementation\n        plan: Medium\n      \n     \n   \n\n  Resources\n\n      \n    \n      Related best practices:\n    \n    \n       \n       \n       \n    \n        \n          OPS02-BP01\n          Resources have identified owners\n        \n      \n        \n          OPS02-BP04\n          Mechanisms exist to manage responsibilities and\n          ownership\n        \n      \n        \n          OPS11-BP04\n          Perform knowledge management\n        \n      \n    \n      Related documents:\n    \n    \n       \n       \n       \n       \n       \n       \n       \n       \n    \n        \n          AWS           Whitepaper - Introduction to DevOps on AWS\n        \n      \n        \n          AWS           Whitepaper - Best Practices for Tagging AWS Resources\n        \n      \n        \n          AWS           Whitepaper - Organizing Your AWS Environment Using Multiple\n          Accounts\n        \n      \n        AWS Cloud Operations and Migrations Blog - Using Amazon Q Business to streamline your operations\n        \n      \n        \n          AWS Cloud Operations \u0026 Migrations Blog - Build a Cloud\n          Automation Practice for Operational Excellence: Best Practices\n          from AWS Managed Services\n        \n      \n        \n          AWS Cloud Operations \u0026 Migrations Blog - Implementing\n          automated and centralized tagging controls with AWS Config and\n          AWS Organizations\n        \n      \n        \n          AWS           Security Blog - Extend your pre-commit hooks with AWS CloudFormation Guard\n        \n      \n        \n          AWS           DevOps Blog - Integrating AWS CloudFormation Guard into CI/CD\n          pipelines\n        \n      \n    \n      Related workshops:\n    \n    \n       \n       \n    \n        \n          AWS           Well-Architected Operational Excellence Workshop\n        \n      \n        \n          AWS           Workshop - Tagging\n        \n      \n    \n      Related videos:\n    \n    \n       \n       \n       \n       \n    \n        \n          How\n          to automate IT Operations on AWS\n        \n      \n        \n          AWS           re:Invent 2020 - Automate anything with AWS Systems Manager\n        \n      \n        \n          AWS           re:Inforce 2022 - Automating patch management and compliance\n          using AWS (NIS306)\n        \n      \n        \n          Supports You - Diving Deep into AWS Systems Manager\n        \n      \n    \n      Related services:\n    \n    \n       \n       \n    \n        \n          AWS Systems Manager - Automation\n        \n      \n        \n          AWS Service Management Connector\n        \n      \n  \u003cdiv\u003e\u003cdiv\u003e\u003cdiv\u003e\u003cdiv id=\"js_error_message\"\u003e\u003cp\u003e\u003cimg src=\"https://d1ge0kk1l5kms0.cloudfront.net/images/G/01/webservices/console/warning.png\" alt=\"Warning\" /\u003e \u003cstrong\u003eJavascript is disabled or is unavailable in your browser.\u003c/strong\u003e\u003c/p\u003e\u003cp\u003eTo use the Amazon Web Services Documentation, Javascript must be enabled. Please refer to your browser's Help pages for instructions.\u003c/p\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003eDocument ConventionsOPS02-BP01 Resources have identified ownersOPS02-BP03 Operations activities have identified owners\n  responsible for their performance",
  "OPS02-BP03 Operations activities have identified owners\n  responsible for their performance\n    Understand who has responsibility to perform specific activities on\n    defined workloads and why that responsibility exists. Understanding\n    who has responsibility to perform activities informs who will\n    conduct the activity, validate the result, and provide feedback to\n    the owner of the activity.\n  \n    Desired outcome:\n  \n    Your organization clearly defines responsibilities to perform\n    specific activities on defined workloads and respond to events\n    generated by the workload. The organization documents ownership of\n    processes and fulfillment and makes this information discoverable.\n    You review and update responsibilities when organizational changes\n    take place, and teams track and measure the performance of defect\n    and inefficiency identification activities. You implement feedback\n    mechanisms to track defects and improvements and support iterative\n    improvement.\n  \n    Common anti-patterns:\n  \n     \n     \n     \n     \n  \n      \n        You do not document responsibilities.\n      \n    \n      \n        Fragmented scripts exist on isolated operator workstations.\n        Only a few individuals know how to use them or informally refer\n        to them as team knowledge.\n      \n    \n      \n        A legacy process is due for update, but no one knows who owns\n        the process, and the original author is no longer part of the\n        organization.\n      \n    \n      \n        Processes and scripts can't be discovered, and they are not\n        readily available when required (for example, in response to an\n        incident).\n      \n    \n    Benefits of establishing this best\n      practice:\n  \n     \n     \n     \n     \n     \n     \n     \n  \n      \n        You understand who is responsible to perform an activity, who to\n        notify when action is needed, and who performs the action,\n        validates the result, and provides feedback to the owner of the\n        activity.\n      \n    \n      \n        Processes and procedures boost your efforts to operate your\n        workloads.\n      \n    \n      \n        New team members become effective more quickly.\n      \n    \n      \n        You reduce the time it takes to mitigate incidents.\n      \n    \n      \n        Different teams use the same processes and procedures to perform\n        tasks in a consistent manner.\n      \n    \n      \n        Teams can scale their processes with repeatable processes.\n      \n    \n      \n        Standardized processes and procedures help mitigate the impact\n        of transferring workload responsibilties between teams.\n      \n    \n    Level of risk exposed if this best practice\n    is not established: High\n  \n\n  Implementation guidance\n\n      \n    \n      To begin to define responsibilities, start with existing\n      documentation, like responsibility matrices, processes and\n      procedures, roles and responsibilities, and tools and automation.\n      Review and host discussions on the responsibilities for documented\n      processes. Review with teams to identify misalignments between\n      document responsibilities and processes. Discuss services offered\n      with internal customers of that team to identify expectations gaps\n      between teams.\n    \n    \n      Analyze and address the discrepancies. Identify opportunities to\n      improvement, and look for frequently requested, resource-intensive\n      activities, which are typically strong candidates for improvement.\n      Explore best practices, patterns, and prescriptive guidance to\n      simplify and standardize improvements. Record improvement\n      opportunities, and track the improvements to completion.\n    \n    \n      Over time, these procedures should be evolved to be run as code,\n      reducing the need for human intervention. For example, procedures\n      can be initiated as AWS Lambda functions, AWS CloudFormation\n      templates, or AWS Systems Manager Automation documents. Verify\n      that these procedures are version-controlled in appropriate\n      repositories, and include suitable resource tagging so that teams\n      can readily identify owners and documentation. Document the\n      responsibility for carrying out the activities, and then monitor\n      the automations for successful initiation and operation, as well\n      as performance of the desired outcomes.\n    \n    \n      Customer example\n    \n    \n      AnyCompany Retail defines ownership as the team or individual that\n      owns processes for an application or groups of applications that\n      share common architectural practices and technologies. Initially,\n      the company documents the processes and procedures as step-by-step\n      guides in the document management system. They make the procedures\n      discoverable using tags on the AWS account that hosts the\n      application and on specific groups of resources within the\n      account, using AWS Organizations to manage their AWS accounts.\n      Over time, AnyCompany Retail converts these processes to code and\n      defines resources using infrastructure as code (through services\n      like CloudFormation or AWS Cloud Development Kit (AWS CDK) templates). The\n      operational processes become Automation documents in AWS Systems Manager or AWS Lambda functions, which can be initiated as\n      scheduled tasks in response to events such as Amazon CloudWatch\n      alarms or Amazon EventBridge events or by requests within an IT\n      service management (ITSM) platform. All processes have tags to\n      identify who owns them. Teams manage documentation for the\n      automation and process within the wiki pages generated by the code\n      repository for the process.\n    \n     \n\n  Implementation steps\n\n      \n      \n         \n         \n         \n         \n      \n          \n            Document the existing processes and procedures.\n          \n          \n             \n             \n             \n             \n          \n              \n                Review and verify that they are up-to-date.\n              \n            \n              \n                Verify that each process or procedure has an owner.\n              \n            \n              \n                Place the procedures under version control.\n              \n            \n              \n                Where possible, share processes and procedures across\n                workloads and environments that share architectural\n                designs.\n              \n            \n        \n          \n            Establish mechanisms for feedback and improvement.\n          \n          \n             \n             \n             \n             \n          \n              \n                Define policies for how frequently processes should be\n                reviewed.\n              \n            \n              \n                Define processes for reviewers and approvers.\n              \n            \n              \n                Implement issues or a ticketing queue to provide and\n                track feedback.\n              \n            \n              \n                Wherever possible, provide pre-approval and risk\n                classification for processes and procedures from a\n                change approval board (CAB).\n              \n            \n        \n          \n            Make process and procedures accessible and discoverable by\n            users who need to run them.\n          \n          \n             \n             \n             \n          \n              \n                Use tags to indicate where the process and procedures\n                can be accessed for the workload.\n              \n            \n              \n                Use meaningful error and event messaging to indicate the\n                appropriate process or proceedure to address the issue.\n              \n            \n              \n                Use wikis or document management to make processes and\n                procedures consistently searchable across the\n                organization.\n              \n            \n        \n          \n            Automate when it is appropriate to do so.\n          \n          \n             \n             \n             \n          \n              \n                Where services and technologies provide an API, develop\n                automations.\n              \n            \n              \n                Verify that processes are well-understood, and develop\n                the user stories and requirements to automate those\n                processes.\n              \n            \n              \n                Measure the successful use of processes and procedures,\n                with issue tracking to support iterative improvement.\n              \n            \n        \n      \n        Level of effort for the implementation\n        plan: Medium\n      \n     \n   \n\n  Resources\n\n      \n    \n      Related best practices:\n    \n    \n       \n       \n       \n       \n       \n    \n        \n          OPS02-BP01\n          Resources have identified owners\n        \n      \n        \n          OPS02-BP02\n          Processes and procedures have identified owners\n        \n      \n        \n          OPS02-BP04 Mechanisms exist to manage responsibilities and\n            ownership\n        \n      \n        \n          OPS02-BP05\n          Mechanisms exist to identify responsibility and\n          ownership\n        \n      \n        \n          OPS11-BP04\n          Perform knowledge management\n        \n      \n    \n      Related documents:\n    \n    \n       \n       \n       \n       \n       \n       \n    \n        \n          AWS           Whitepaper | Introduction to DevOps on AWS\n        \n      \n        \n          AWS           Whitepaper | Best Practices for Tagging AWS Resources\n        \n      \n        \n          AWS           Whitepaper | Organizing Your AWS Environment Using Multiple\n          Accounts\n        \n      \n        \n          AWS Cloud Operations \u0026 Migrations Blog | Build a Cloud\n          Automation Practice for Operational Excellence: Best Practices\n          from AWS Managed Services\n        \n      \n        \n          AWS           Workshop - Tagging\n        \n      \n        \n          AWS           Service Management Connector\n        \n      \n    \n      Related videos:\n    \n    \n       \n       \n       \n       \n    \n        \n          AWS           Knowledge Center Live | Tagging AWS Resources\n        \n      \n        \n          AWS           re:Invent 2020 | Automate anything with AWS Systems Manager\n        \n      \n        \n          AWS           re:Inforce 2022 | Automating patch management and compliance\n          using AWS (NIS306)\n        \n      \n        \n          Supports You | Diving Deep into AWS Systems Manager\n        \n      \n  \u003cdiv\u003e\u003cdiv\u003e\u003cdiv\u003e\u003cdiv id=\"js_error_message\"\u003e\u003cp\u003e\u003cimg src=\"https://d1ge0kk1l5kms0.cloudfront.net/images/G/01/webservices/console/warning.png\" alt=\"Warning\" /\u003e \u003cstrong\u003eJavascript is disabled or is unavailable in your browser.\u003c/strong\u003e\u003c/p\u003e\u003cp\u003eTo use the Amazon Web Services Documentation, Javascript must be enabled. Please refer to your browser's Help pages for instructions.\u003c/p\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003eDocument ConventionsOPS02-BP02 Processes and procedures have identified\n  ownersOPS02-BP04 Mechanisms exist to manage responsibilities and\n  ownership",
  "OPS02-BP04 Mechanisms exist to manage responsibilities and\n  ownership\n    Understand the responsibilities of your role and how you contribute\n    to business outcomes, as this understanding informs the\n    prioritization of your tasks and why your role is important. This\n    helps team members recognize needs and respond appropriately. When\n    team members know their role, they can establish ownership, identify\n    improvement opportunities, and understand how to influence or make\n    appropriate changes.\n  \n    Occasionally, a responsibility might not have a clear owner. In\n    these situations, design a mechanism to resolve this gap. Create a\n    well-defined escalation path to someone with the authority to assign\n    ownership or plan to address the need.\n  \n    Desired outcome: Teams within\n    your organization have clearly-defined responsibilities that include\n    how they are related to resources, actions to be performed,\n    processes, and procedures. These responsibilities align to the\n    team's responsibilities and goals, as well as the responsibilities\n    of other teams. You document the routes of escalation in a\n    consistent and discoverable manner and feed these decisions into\n    documentation artifacts, such as responsibility matrices, team\n    definitions, or wiki pages.\n  \n    Common anti-patterns:\n  \n     \n     \n     \n     \n     \n     \n     \n     \n  \n      \n        The responsibilities of the team are ambiguous or\n        poorly-defined.\n      \n    \n      \n        The team does not align roles with responsibilities.\n      \n    \n      \n        The team does not align its goals and objectives its\n        responsibilities, which makes it difficult to measure success.\n      \n    \n      \n        Team member responsibilities do not align with the team and the\n        wider organization.\n      \n    \n      \n        Your team does not keep responsibilities up-to-date, which makes\n        them inconsistent with the tasks performed by the team.\n      \n    \n      \n        Escalation paths for determining responsibilities aren't defined\n        or are unclear.\n      \n    \n      \n        Escalation paths have no single thread owner to ensure timely\n        reponse.\n      \n    \n      \n        Roles, responsibilities, and escalation paths are not\n        discoverable, and they are not readily available when required\n        (for example, in response to an incident).\n      \n    \n    Benefits of establishing this best\n    practice:\n  \n     \n     \n     \n     \n     \n     \n  \n      \n        When you understand who has responsibility or ownership, you can\n        contact the proper team or team member to make a request or\n        transition a task.\n      \n    \n      \n        To reduce the risk of inaction and unaddressed needs, you have\n        identified a person who has the authority to assign\n        responsibility or ownership.\n      \n    \n      \n        When you clearly define the scope of a responsibility, your team\n        members gain autonomy and ownership.\n      \n    \n      \n        Your responsibilities inform the decisions you make, the actions\n        you take, and your handoff activities to their proper owners.\n      \n    \n      \n        It's easy to identify abandoned responsibilities because you\n        have a clear understanding of what falls outside of your team's\n        responsibility, which helps you escalate for clarification.\n      \n    \n      \n        Teams avoid confusion and tension, and they can more adequately\n        manage their workloads and resources.\n      \n    \n    Level of risk exposed if this best practice\n    is not established: High\n  \n\n  Implementation guidance\n\n      \n    \n      Identify team members roles and responsibilities, and verify that\n      they understand the expectations of their role. Make this\n      information discoverable so that members of your organization can\n      identify who they need to contact for specific needs, whether it's\n      a team or individual. As organizations seek to capitalize on the\n      opportunities to migrate and modernize on AWS, roles and\n      responsibilities might also change. Keep your teams and their\n      members aware of their responsibilities, and train them\n      appropriately to carry out their tasks during this change.\n    \n    \n      Determine the role or team that should receive escalations to\n      identify responsibility and ownership. This team can engage with\n      various stakeholders to come to a decision. However, they should\n      own the management of the decision making process.\n    \n    \n      Provide accessible mechanisms for members of your organization to\n      discover and identify ownership and responsibility. These\n      mechanisms teach them who to contact for specific needs.\n    \n    \n      Customer example\n    \n    \n      AnyCompany Retail recently completed a migration of workloads from\n      an on-premises environment to their landing zone in AWS with a\n      lift and shift approach. They performed an operations review to\n      reflect on how they accomplish common operational tasks and\n      verified that their existing responsibility matrix reflects\n      operations in the new environment. When they migrated from\n      on-premises to AWS, they reduced the infrastructure teams\n      responsibilities relating to the hardware and physical\n      infrastructure. This move also revealed new opportunities to\n      evolve the operating model for their workloads.\n    \n    \n      While they identified, addressed, and documented the majority of\n      responsibilities, they also defined escalation routes for any\n      responsibilities that were missed or that may need to change as\n      operations practices evolve. To explore new opportunities to\n      standardize and improve efficiency across your workloads, provide\n      access to operations tools like AWS Systems Manager and security\n      tools like AWS Security Hub and Amazon GuardDuty. AnyCompany\n      Retail puts together a review of responsibilities and strategy\n      based on improvements they wants to address first. As the company\n      adopts new ways of working and technology patterns, they update\n      their responsibility matrix to match.\n    \n     \n\n  Implementation steps\n\n      \n      \n         \n         \n         \n         \n         \n         \n         \n      \n          \n            Start with existing documentation. Some typical source\n            documents might include:\n          \n          \n             \n             \n             \n             \n          \n              \n                Responsibility or responsible, accountable, consulted,\n                and informed (RACI) matrices\n              \n            \n              \n                Team definitions or wiki pages\n              \n            \n              \n                Service definitions and offerings\n              \n            \n              \n                Role or job descriptions\n              \n            \n        \n          \n            Review and host discussions on the documented\n            responsibilities:\n          \n          \n             \n             \n          \n              \n                Review with teams to identify misalignments between\n                documented responsibilities and responsibilities the\n                team typically performs.\n              \n            \n              \n                Discuss potential services offered by internal customers\n                to identify gaps in expectations between teams.\n              \n            \n        \n          \n            Analysis and address the discrepancies.\n          \n        \n          \n            Identify opportunities for improvement.\n          \n          \n             \n             \n             \n          \n              \n                Identify frequently-requested, resource-intensive\n                requests, which are typically strong candidates for\n                improvement.\n              \n            \n              \n                Look for best practices, understand patterns, follow prescriptive\n                guidance, and simplify and standardize improvements.              \n            \n              \n                Record improvement opportunities, and track them to\n                completion.\n              \n            \n        \n          \n            If a team doesn't already hold responsibility for managing\n            and tracking the assignment of responsibilities, identify\n            someone on the team to hold this responsibility.\n          \n        \n          \n            Define a process for teams to request clarification of\n            responsibility.\n          \n          \n             \n             \n             \n             \n             \n          \n              \n                Review the process, and verify that it is clear and\n                simple to use.\n              \n            \n              \n                Make sure that someone owns and tracks escalations to\n                their conclusion.\n              \n            \n              \n                Establish operational metrics to measure effectiveness.\n              \n            \n              \n                Create a feedback mechanisms to verify that teams can\n                highlight improvement opportunities.\n              \n            \n              \n                Implement a mechanism for periodic review.\n              \n            \n        \n          \n            Document in a discoverable and accessible location.\n          \n          \n             \n          \n              \n                Wikis or documentation portal are common choices.\n              \n            \n        \n      \n        Level of effort for the implementation\n        plan: Medium\n      \n     \n   \n\n  Resources\n\n      \n    \n      Related best practices:\n    \n    \n       \n       \n       \n       \n       \n       \n       \n    \n        \n          OPS01-BP06\n          Evaluate tradeoffs\n        \n      \n        \n          OPS03-BP02\n          Team members are empowered to take action when outcomes are at\n          risk\n        \n      \n        \n          OPS03-BP03\n          Escalation is encouraged\n        \n      \n        \n          OPS03-BP07\n          Resource teams appropriately\n        \n      \n        \n          OPS09-BP01\n          Measure operations goals and KPIs with metrics\n        \n      \n        \n          OPS09-BP03\n          Review operations metrics and prioritize improvement\n        \n      \n        \n          OPS11-BP01\n          Have a process for continuous improvement\n        \n      \n    \n      Related documents:\n    \n    \n       \n       \n       \n       \n       \n       \n       \n       \n    \n        \n          AWS           Whitepaper - Introduction to DevOps on AWS\n        \n      \n        \n          AWS           Whitepaper - AWS Cloud Adoption Framework: Operations\n          Perspective\n        \n      \n        \n          AWS           Well-Architected Framework Operational Excellence - Workload\n          level Operating model topologies\n        \n      \n        \n          AWS           Prescriptive Guidance - Building your Cloud Operating\n          Model\n        \n      \n        \n          AWS           Prescriptive Guidance - Create a RACI or RASCI matrix for a\n          cloud operating model\n        \n      \n        \n          AWS Cloud Operations \u0026 Migrations Blog - Delivering Business\n          Value with Cloud Platform Teams\n        \n      \n        \n          AWS Cloud Operations \u0026 Migrations Blog - Why a Cloud Operating\n          Model?\n        \n      \n        \n          AWS           DevOps Blog - How organizations are modernizing for cloud\n          operations\n        \n      \n    \n      Related videos:\n    \n    \n       \n       \n    \n        \n          AWS           Summit Online - Cloud Operating Models for Accelerated\n          Transformation\n        \n      \n        \n          AWS           re:Invent 2023 - Future-proofing cloud security: A new\n          operating model\n        \n      \n  \u003cdiv\u003e\u003cdiv\u003e\u003cdiv\u003e\u003cdiv id=\"js_error_message\"\u003e\u003cp\u003e\u003cimg src=\"https://d1ge0kk1l5kms0.cloudfront.net/images/G/01/webservices/console/warning.png\" alt=\"Warning\" /\u003e \u003cstrong\u003eJavascript is disabled or is unavailable in your browser.\u003c/strong\u003e\u003c/p\u003e\u003cp\u003eTo use the Amazon Web Services Documentation, Javascript must be enabled. Please refer to your browser's Help pages for instructions.\u003c/p\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003eDocument ConventionsOPS02-BP03 Operations activities have identified owners\n  responsible for their performanceOPS02-BP05 Mechanisms exist to request additions, changes, and\n  exceptions",
  "OPS02-BP05 Mechanisms exist to request additions, changes, and\n  exceptionsYou can make requests to owners of processes, procedures, and resources. Requests \n    include additions, changes, and exceptions. These requests go through a change management \n    process. Make informed decisions to approve requests where viable and determined to be \n    appropriate after an evaluation of benefits and risks. \n    Desired outcome:\n  \n     \n     \n  \n      \n        You can make requests to change processes, procedures, and resources based on assigned ownership.\n      \n    \n      \n        Changes are made in a deliberate manner, weighing benefits and risks.\n      \n    \n      Common anti-patterns:\n    \n       \n       \n    \n        \n          You must update the way you deploy your application, but there is no way to request a change to the deployment process from the operations team.\n        \n      \n        \n          The disaster recovery plan must be updated, but there is no identified owner to request changes to.\n        \n      \n      Benefits of establishing this best practice:\n    \n     \n     \n     \n  \n      \n        Processes, procedures, and resources can evolve as requirements change.\n      \n    \n      \n        Owners can make informed decisions when to make changes.\n      \n    \n      \n        Changes are made in a deliberate manner.\n      \n    \n    Level of risk exposed if this best practice\n    is not established: Medium\n  \n\n  Implementation guidance\n    \n      To implement this best practice, you need to be able to request changes to processes, \n      procedures, and resources. The change management process can be lightweight. Document \n      the change management process.\n    \n    \n      Customer example\n    \n    \n      AnyCompany Retail uses a responsibility assignment (RACI) matrix to identify who owns changes for processes, procedures, \n      and resources. They have a documented change management process that’s lightweight and easy \n      to follow. Using the RACI matrix and the process, anyone can submit change requests.\n    \n    \n      Implementation steps\n    \n    \n       \n       \n       \n    \n        \n          Identify the processes, procedures, and resources for your workload and the owners for each. Document them in your knowledge management system.\n        \n        \n           \n        \n            \n              If you have not implemented OPS02-BP01 Resources have identified owners, \n              OPS02-BP02 Processes and procedures have identified\n  owners, or \n              OPS02-BP03 Operations activities have identified owners\n  responsible for their performance, start with those first.\n            \n          \n      \n        \n          Work with stakeholders in your organization to develop a change management process. \n          The process should cover additions, changes, and exceptions for resources, processes, and procedures. \n        \n        \n           \n        \n            \n              You can use AWS Systems Manager Change Manager as a change management platform for workload resources.\n            \n          \n      \n        \n          Document the change management process in your knowledge management system.\n        \n      \n    \n      Level of effort for the implementation plan: Medium. Developing a change \n      management process requires alignment with multiple stakeholders across your organization.\n    \n\n      \n    \n   \n    \n    Resources\n    \n      Related best practices:\n    \n    \n       \n       \n       \n    \n        \n          OPS02-BP01 Resources have identified owners - Resources need identified owners before you build a change management process.\n        \n      \n        \n          OPS02-BP02 Processes and procedures have identified\n  owners - Processes need identified owners before you build a change management process.\n        \n      \n        \n          OPS02-BP03 Operations activities have identified owners\n  responsible for their performance - Operations activities need identified owners before you build a change management process.\n        \n      \n    \n    \n      Related documents:\n    \n    \n       \n       \n    \n        AWS Prescriptive Guidance - Foundation palybook for AWS large migrations: Creating RACI matrices\n        \n      \n        \n          Change Management in the Cloud Whitepaper\n        \n      \n    \n      Related services:\n    \n    \n       \n    \n        AWS Systems Manager Change Manager\n      \n  \u003cdiv\u003e\u003cdiv\u003e\u003cdiv\u003e\u003cdiv id=\"js_error_message\"\u003e\u003cp\u003e\u003cimg src=\"https://d1ge0kk1l5kms0.cloudfront.net/images/G/01/webservices/console/warning.png\" alt=\"Warning\" /\u003e \u003cstrong\u003eJavascript is disabled or is unavailable in your browser.\u003c/strong\u003e\u003c/p\u003e\u003cp\u003eTo use the Amazon Web Services Documentation, Javascript must be enabled. Please refer to your browser's Help pages for instructions.\u003c/p\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003eDocument ConventionsOPS02-BP04 Mechanisms exist to manage responsibilities and\n  ownershipOPS02-BP06 Responsibilities between teams are predefined or\n  negotiated",
  "OPS02-BP06 Responsibilities between teams are predefined or\n  negotiatedHave defined or negotiated agreements between teams describing how they work with \n    and support each other (for example, response times, service level objectives, or \n    service-level agreements). Inter-team communications channels are documented. \n    Understanding the impact of the teams’ work on business outcomes and the outcomes \n    of other teams and organizations informs the prioritization of their tasks and \n    helps them respond appropriately. \n    When responsibility and ownership are undefined or unknown, you are\n    at risk of both not addressing necessary activities in a timely\n    fashion and of redundant and potentially conflicting efforts\n    emerging to address those needs.\n  \n    Desired outcome:\n  \n     \n     \n  \n      \n        Inter-team working or support agreements are agreed to and documented.\n      \n    \n      \n        Teams that support or work with each other have defined communication channels and response expectations.\n      \n    \n    Common anti-patterns:\n  \n     \n     \n  \n      \n        An issue occurs in production and two separate teams start troubleshooting independent of each other. Their siloed efforts extend the outage.\n      \n    \n      \n        The operations team needs assistance from the development team but there is no agreed to response time. The request is stuck in the backlog.\n      \n    \n    Benefits of establishing this best practice:\n  \n     \n     \n     \n  \n      \n        Teams know how to interact and support each other.\n      \n    \n      \n        Expectations for responsiveness are known.\n      \n    \n      \n        Communications channels are clearly defined.\n      \n    \n    Level of risk exposed if this best practice\n    is not established: Low\n  \n\n  Implementation guidance\n    \n      Implementing this best practice means that there is no ambiguity about how teams work with each other. \n      Formal agreements codify how teams work together or support each other. Inter-team communication channels \n      are documented.\n    \n    \n      Customer example\n    \n    \n      AnyCompany Retail’s SRE team has a service level agreement with their development team. Whenever \n      the development team makes a request in their ticketing system, they can expect a response within \n      fifteen minutes. If there is a site outage, the SRE team takes lead in the investigation with support \n      from the development team.\n    \n    \n      Implementation steps\n    \n    \n       \n       \n    \n        \n          Working with stakeholders across your organization, develop agreements between teams based on processes and procedures. \n        \n        \n           \n           \n        \n            \n              If a process or procedure is shared between two teams, develop a runbook on how the teams will work together.\n            \n          \n            \n              If there are dependencies between teams, agree to a response SLA for requests. \n            \n          \n      \n        \n          Document responsibilities in your knowledge management system. \n        \n      \n    \n      Level of effort for the implementation plan: Medium. If there are no existing agreements \n      between teams, it can take effort to come to agreement with stakeholders across your organization.\n    \n   \n    \n    Resources\n    \n      Related best practices:\n    \n    \n       \n       \n    \n        \n          OPS02-BP02 Processes and procedures have identified\n  owners - Process ownership must be identified before setting agreements between teams.\n        \n      \n        \n          OPS02-BP03 Operations activities have identified owners\n  responsible for their performance - Operations activities ownership must be identified before setting agreements between teams.\n        \n      \n    \n      Related documents:\n    \n    \n       \n       \n    \n        AWS Executive Insights - Empowering Innovation with the Two-Pizza Team\n        \n      \n        \n          Introduction to DevOps on AWS - Two-Pizza Teams\n        \n      \n  \u003cdiv\u003e\u003cdiv\u003e\u003cdiv\u003e\u003cdiv id=\"js_error_message\"\u003e\u003cp\u003e\u003cimg src=\"https://d1ge0kk1l5kms0.cloudfront.net/images/G/01/webservices/console/warning.png\" alt=\"Warning\" /\u003e \u003cstrong\u003eJavascript is disabled or is unavailable in your browser.\u003c/strong\u003e\u003c/p\u003e\u003cp\u003eTo use the Amazon Web Services Documentation, Javascript must be enabled. Please refer to your browser's Help pages for instructions.\u003c/p\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003eDocument ConventionsOPS02-BP05 Mechanisms exist to request additions, changes, and\n  exceptions OPS 3. How does your organizational culture support your business\n                  outcomes? ",
  "OPS03-BP01 Provide executive sponsorship\n    At the highest level, senior leadership acts as the executive\n    sponsor to clearly set expectations and direction for the\n    organization's outcomes, including evaluating its success. The\n    sponsor advocates and drives adoption of best practices and\n    evolution of the organization.\n  \n    Desired outcome: Organizations\n    that endeavor to adopt, transform, and optimize their cloud\n    operations establish clear lines of leadership and accountability\n    for desired outcomes. The organization understands each capability\n    required by the organization to accomplish a new outcome and assigns\n    ownership to functional teams for development. Leadership actively\n    sets this direction, assigns ownership, takes accountability, and\n    defines the work. As a result, individuals across the organization\n    can mobilize, feel inspired, and actively work towards the desired\n    objectives.\n  \n    Common anti-patterns:\n  \n     \n     \n  \n      \n        There is a mandate for workload owners to migrate workloads to\n        AWS without a clear sponsor and plan for cloud operations. This\n        results in teams not consciously collaborating to improve and\n        mature their operational capabilities. Lack of operational best\n        practice standards overwhelm teams (such as operator-toil,\n        on-calls, and technical debt), which constrains innovation.\n      \n    \n      \n        A new organization-wide goal has been set to adopt an emerging\n        technology without providing leadership sponsor and strategy.\n        Teams interpret goals differently, which causes confusion on\n        where to focus efforts, why they matter, and how to measure\n        impact. Consequently, the organization loses momentum in\n        adopting the technology.\n      \n    \n    Benefits of establishing this best\n    practice: When executive sponsorship clearly communicates\n    and shares vision, direction, and goals, team members know what is\n    expected of them. Individuals and teams begin to intensely focus\n    effort in the same direction to accomplish defined objectives when\n    leaders are actively engaged. As a result, the organization maximizes\n    the ability to succeed. When you evaluate success, you can better\n    identify barriers to success so that they can be addressed through\n    intervention by the executive sponsor.\n  \n    Level of risk exposed if this best practice\n    is not established: High\n  \n\n  Implementation guidance\n\n      \n    \n       \n    \n        \n          At every phase of the cloud journey (migration, adoption, or\n          optimization), success requires active involvement at the\n          highest level of leadership with a designated executive\n          sponsor. The executive sponsor aligns the team's mindset,\n          skillsets, and ways of working to the defined strategy.\n        \n        \n           \n           \n           \n           \n           \n           \n           \n        \n            \n              Explain the\n              why: Bring clarity and\n              explain the reasoning behind the vision and strategy.\n            \n          \n            \n              Set expectations:\n              Define and publish goals for your organizations, including\n              how progress and success are measured.\n            \n          \n            \n              Track achievement of\n              goals: Measure the incremental achievement of\n              goals regularly (not just completion of tasks). Share the\n              results so that appropriate action can be taken if\n              outcomes are at risk.\n            \n          \n            \n              Provide the resources necessary to\n              achieve your goals: Bring people and teams\n              together to collaborate and build the right solutions that\n              bring about the defined outcomes. This reduces or\n              eliminates organizational friction.\n            \n          \n            \n              Advocate for your\n              teams: Remain engaged with your teams so that\n              you understand their perforamnce and whether there are\n              external factors affecting them. Identify obstacles that\n              are impeding your teams progress. Act on behalf of your\n              teams to help address obstacles and remove unnecessary\n              burdens. When your teams are impacted by external factors,\n              reevaluate goals and adjust targets as appropriate.\n            \n          \n            \n              Drive adoption of best\n              practices: Acknowledge best practices that\n              provide quantifiable benefits, and recognize the creators\n              and adopters. Encourage further adoption to magnify the\n              benefits achieved.\n            \n          \n            \n              Encourage evolution of your\n              teams: Create a culture of continual\n              improvement, and proactively learn from progress made as\n              well as failures. Encourage both personal and\n              organizational growth and development. Use data and\n              anecdotes to evolve the vision and strategy.\n            \n          \n      \n    \n      Customer example\n    \n    \n      AnyCompany Retail is in the process of business transformation\n      through rapid reinvention of customer experiences, enhancement\n      of productivity, and acceleration of growth through generative AI.\n    \n     \n\n  Implementation steps\n\n      \n      \n         \n         \n         \n         \n         \n         \n      \n          \n            Establish single-threaded leadership, and assign a primary\n            executive sponsor to lead and drive the transformation.\n          \n        \n          \n            Define clear business outcomes of your transformation, and\n            assign ownership and accountability. Empower the primary\n            executive with the authority to lead and make critical\n            decisions.\n          \n        \n          \n            Verify that your transformational strategy is very clear and\n            communicated widely by the executive sponsor to every level\n            of the organization.\n          \n          \n             \n             \n             \n          \n              \n                Establish clearly defined business objectives for IT and\n                cloud initiatives.\n              \n            \n              \n                Document key business metrics to drive IT and cloud\n                transformation.\n              \n            \n              \n                Communicate the vision consistently to all teams and\n                individuals responsible for parts of the strategy.\n              \n            \n        \n          \n            Develop communication planning matrices that specify what\n            message needs to be delivered to specified leaders,\n            managers, and individual contributors. Specify the person or\n            team that should deliver this message.\n          \n          \n             \n             \n             \n             \n          \n              \n                Fulfill communications plans consistently and reliably.\n              \n            \n              \n                Set and manage expectations through in-person events on\n                a regular basis.\n              \n            \n              \n                Accept feedback on the effectiveness of communications,\n                and adjust the communications and plan accordingly.\n              \n            \n              \n                Schedule communication events to proactively understand\n                challenges from teams, and establish a consistent\n                feedback loop that allows for correcting course where\n                necessary.\n              \n            \n        \n          \n            Actively engage each initiative from a leadership\n            perspective to verify that all impacted teams understand the\n            outcomes they are accountable to achieve.\n          \n        \n          \n            At every status meeting, executive sponsors should look for\n            blockers, inspect established metrics, anecdotes, or\n            feedback from the teams, and measure progress towards\n            objectives.\n          \n        \n      \n        Level of effort for the implementation\n        plan Medium\n      \n     \n   \n\n  Resources\n\n      \n    \n      Related best practices:\n    \n    \n       \n       \n       \n    \n        \n          OPS03-BP04\n          Communications are timely, clear, and actionable\n        \n      \n        \n          OP11-BP01\n          Have a process for continuous improvement\n        \n      \n        \n          OPS11-BP07\n          Perform operations metrics reviews\n        \n      \n    \n      Related documents:\n    \n    \n       \n       \n       \n       \n       \n    \n        \n          Untangling\n          Your Organisational Hairball: Highly Aligned\n        \n      \n        \n          The\n          Living Transformation: Pragmatically approaching\n          changes\n        \n      \n        \n          Becoming\n          a Future-Ready Enterprise\n        \n      \n        \n          7\n          Pitfalls to Avoid When Building a CCOE\n        \n      \n        \n          Navigating\n          the Cloud: Key Performance Indicators for Success\n        \n      \n    \n      Related videos:\n    \n    \n       \n    \n        \n          AWS re:Invent\n          2023: A leader's guide to generative AI: Using history to\n          shape the future (SEG204)\n        \n      \n    \n      Related examples:\n    \n    \n       \n    \n        \n          Prosci:\n          Primary Sponsor's Role \u0026 Importance\n        \n      \n  \u003cdiv\u003e\u003cdiv\u003e\u003cdiv\u003e\u003cdiv id=\"js_error_message\"\u003e\u003cp\u003e\u003cimg src=\"https://d1ge0kk1l5kms0.cloudfront.net/images/G/01/webservices/console/warning.png\" alt=\"Warning\" /\u003e \u003cstrong\u003eJavascript is disabled or is unavailable in your browser.\u003c/strong\u003e\u003c/p\u003e\u003cp\u003eTo use the Amazon Web Services Documentation, Javascript must be enabled. Please refer to your browser's Help pages for instructions.\u003c/p\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003eDocument Conventions OPS 3. How does your organizational culture support your business\n                  outcomes? OPS03-BP02 Team members are empowered to take action when\n  outcomes are at risk",
  "OPS03-BP02 Team members are empowered to take action when\n  outcomes are at risk\n    A cultural behavior of ownership instilled by leadership results in\n    any employee feeling empowered to act on behalf of the entire\n    company beyond their defined scope of role and accountability.\n    Employees can act to proactively identify risks as they emerge and\n    take appropriate action. Such a culture allows employees to make\n    high value decisions with situational awareness.\n  \n    For example, Amazon uses\n    Leadership\n      Principles as the guidelines to drive desired behavior for\n    employees to move forward in situations, solve problems, deal with\n    conflict, and take action.\n  \n    Desired outcome: Leadership has\n    influenced a new culture that allows individuals and teams to make\n    critical decisions, even at lower levels of the organization (as\n    long as decisions are defined with auditable permissions and safety\n    mechanisms). Failure is not discouraged, and teams iteratively learn\n    to improve their decision-making and responses to tackle similar\n    situations going forward. If someone's actions result in an\n    improvement that can benefit other teams, they proactively share\n    knowledge from such actions. Leadership measures operational\n    improvements and incentivizes the individual and organization for\n    adoption of such patterns.\n  \n    Common anti-patterns:\n  \n     \n     \n     \n  \n      \n        There isn't clear guidance or mechanisms in an organization for\n        what to do when a risk is identified. For example, when an\n        employee notices a phishing attack, they fail to report to the\n        security team, resulting in a large portion of the organization\n        falling for the attack. This causes a data breach.\n      \n    \n      \n        Your customers complain about service unavailability, which\n        primarily stems from failed deployments. Your SRE team is\n        responsible for the deployment tool, and an automated rollback\n        for deployments is in their long-term roadmap. In a recent\n        application rollout, one of the engineers devised a solution to\n        automate rolling back their application to a previous version.\n        Though their solution can become the pattern for SRE teams,\n        other teams do not adopt, as there is no process to track such\n        improvements. The organization continues to be plagued with\n        failed deployments impacting customers and causing further\n        negative sentiment.\n      \n    \n      \n        In order to stay compliant, your infosec team oversees a\n        long-established process to rotate shared SSH keys regularly on\n        behalf of operators connecting to their Amazon EC2 Linux\n        instances. It takes several days for the infosec teams to\n        complete rotating keys, and you are blocked from connecting to\n        those instances. No one inside or outside of infosec suggests\n        using other options on AWS to achieve the same result.\n      \n    \n    Benefits of establishing this best practice:\n     By decentralizing authority to make decisions and\n    empowering your teams to decide key decisions, you are able to\n    address issues more quickly with increasing success rates. In\n    addition, teams start to realize a sense of ownership, and failures\n    are acceptable. Experimentation becomes a cultural mainstay.\n    Managers and directors do not feel as though they are micro-managed\n    through every aspect of their work.\n  \n    Level of risk exposed if this best practice\n      is not established: Medium\n  \n    \n    Implementation guidance\n    \n    \n    \n       \n       \n       \n       \n       \n       \n       \n       \n    \n        \n          Develop a culture where it is expected that failures can\n          occur.\n        \n      \n        \n          Define clear ownership and accountability for various\n          functional areas within the organization.\n        \n      \n        \n          Communicate ownership and accountability to everyone so that\n          individuals know who can help them facilitate decentralized\n          decisions.\n        \n      \n        \n          Define your one-way and two-way door decisions to help\n          individuals know when they do need to escalate to higher\n          levels of leadership.\n        \n      \n        \n          Create organizational awareness that all employees are\n          empowered to take action at various levels when outcomes are\n          at risk. Provide your team members documentation of\n          governance, permission-levels, tools, and opportunities to\n          practice the skills necessary to respond effectively.\n        \n      \n        \n          Give your team members the opportunity to practice the skills\n          necessary to respond to various decisions. Once decision\n          levels are defined, perform game days to verify that all\n          individual contributors understand and can demonstrate the\n          process.\n        \n        \n           \n           \n           \n        \n            \n              Provide alternative safe environments where processes and\n              procedures can be tested and trained upon.\n            \n          \n            \n              Acknowledge and create awareness that team members have\n              authority to take action when the outcome has a predefined\n              level of risk.\n            \n          \n            \n              Define the authority of your team members to take action\n              by assigning permissions and access to the workloads and\n              components they support.\n            \n          \n      \n        \n          Provide ability for teams to share their learnings\n          (operational successes and failures).\n        \n      \n        \n          Empower teams to challenge the status quo, and provide\n          mechanisms to track and measure improvements, as well as their\n          impact to the organization.\n        \n      \n    \n      Level of effort for the implementation\n        plan: Medium\n    \n   \n    \n    Resources\n    \n    \n    \n      Related best practices:\n    \n    \n       \n       \n    \n        \n          OPS01-BP06 Evaluate tradeoffs while managing benefits and risks\n        \n      \n        \n          OPS02-BP05\n            Mechanisms exist to identify responsibility and\n            ownership\n        \n      \n    \n      Related documents:\n    \n    \n       \n       \n       \n       \n    \n        \n          AWS           Blog Post | The agile enterprise\n        \n      \n        \n          AWS           Blog Post | Measuring success : A paradox and a plan\n        \n      \n        \n          AWS           Blog Post | Letting go : Enabling autonomy in teams\n        \n      \n        \n          Centralize\n            or Decentralize?\n        \n      \n    \n      Related videos:\n    \n    \n       \n       \n       \n    \n        \n          re:Invent\n            2023 | How to not sabotage your transformation (SEG201)\n        \n      \n        \n          re:Invent\n            2021 | Amazon Builders' Library: Operational Excellence at\n            Amazon\n        \n      \n        \n          Centralization\n            vs. Decentralization\n        \n      \n    \n      Related examples:\n    \n    \n       \n    \n        \n          Using\n            architectural decision records to streamline technical\n            decision-making for a software development project\n        \n      \n  \u003cdiv\u003e\u003cdiv\u003e\u003cdiv\u003e\u003cdiv id=\"js_error_message\"\u003e\u003cp\u003e\u003cimg src=\"https://d1ge0kk1l5kms0.cloudfront.net/images/G/01/webservices/console/warning.png\" alt=\"Warning\" /\u003e \u003cstrong\u003eJavascript is disabled or is unavailable in your browser.\u003c/strong\u003e\u003c/p\u003e\u003cp\u003eTo use the Amazon Web Services Documentation, Javascript must be enabled. Please refer to your browser's Help pages for instructions.\u003c/p\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003eDocument ConventionsOPS03-BP01 Provide executive sponsorshipOPS03-BP03 Escalation is encouraged",
  "OPS03-BP03 Escalation is encouraged\n    Team members are encouraged by leadership to escalate issues and\n    concerns to higher-level decision makers and stakeholders if they\n    believe desired outcomes are at risk and expected standards are not\n    met. This is a feature of the organization's culture and is driven\n    at all levels. Escalation should be done early and often so that\n    risks can be identified and prevented from causing incidents.\n    Leadership does not reprimand individuals for escalating an issue.\n  \n    Desired outcome: Individuals\n    throughout the organization are comfortable to escalate problems to\n    their immediate and higher levels of leadership. Leadership has\n    deliberately and consciously established expectations that their\n    teams should feel safe to escalate any issue. A mechanism exists to\n    escalate issues at each level within the organization. When\n    employees escalate to their manager, they jointly decide the level\n    of impact and whether the issue should be escalated. In order to\n    initiate an escalation, employees are required to include a\n    recommended work plan to address the issue. If direct management\n    does not take timely action, employees are encouraged to take issues\n    to the highest level of leadership if they feel strongly that the\n    risks to the organization warrant the escalation.\n  \n    Common anti-patterns:\n  \n     \n     \n     \n  \n      \n        Executive leaders do not ask enough probing questions during\n        your cloud transformation program status meeting to find where\n        issues and blockers are occurring. Only good news is presented\n        as status. The CIO has made it clear that she only likes to hear\n        good news, as any challenges brought up make the CEO think that\n        the program is failing.\n      \n    \n      \n        You are a cloud operations engineer and you notice that the new\n        knowledge management system is not being widely adopted by\n        application teams. The company invested one year and several\n        million dollars to implement this new knowledge management\n        system, but people are still authoring their runbooks locally\n        and sharing them on an organizational cloud share, making it\n        difficult to find knowledge pertinent to supported workloads.\n        You try to bring this to leadership's attention, because\n        consistent use of this system can enhance operational\n        efficiency. When you bring this to the director who lead the\n        implementation of the knowledge management system, she\n        reprimands you because it calls the investment into question.\n      \n    \n      \n        The infosec team responsible for hardening compute resources has\n        decided to put a process in place that requires performing the\n        scans necessary to ensure that EC2 instances are fully secured\n        before the compute team releases the resource for use. This has\n        created a time delay of an additional week for resources to be\n        deployed, which breaks their SLA. The compute team is afraid to\n        escalate this to the VP over cloud because this makes the VP of\n        information security look bad.\n      \n    \n    Benefits of establishing this best\n    practice:\n  \n    Complex or critical issues are addressed before they impact the\n    business. Less time is wasted. Risks are minimized. Teams become\n    more proactive and results focused when solving problems.\n  \n    Level of risk exposed if this best practice\n    is not established: High\n  \n\n  Implementation guidance\n\n      \n    \n      The willingness and ability to escalate freely at every level in\n      the organization is an organizational and cultural foundation that\n      should be consciously developed through emphasized training,\n      leadership communications, expectation setting, and the deployment\n      of mechanisms throughout the organization at every level.\n    \n     \n\n  Implementation steps\n\n      \n      \n         \n         \n         \n         \n         \n         \n      \n          \n            Define policies, standards, and expectations for your\n            organization.\n          \n          \n             \n          \n              \n                Ensure wide adoption and understanding of policies,\n                expectations, and standards.\n              \n            \n        \n          \n            Encourage, train, and empower workers for early and frequent\n            escalation when standards are not met.\n          \n        \n          \n            Organizationally acknowledge that early and frequent\n            escalation is the best practice. Accept that escalations may\n            prove to be unfounded, and that it is better to have the\n            opportunity to prevent an incident then to miss that\n            opportunity by not escalating.\n          \n          \n             \n             \n             \n          \n              \n                Build a mechanism for escalation (like an\n                Andon\n                cord system).\n              \n            \n              \n                Have documented procedures defining when and how\n                escalation should occur.\n              \n            \n              \n                Define the series of people with increasing authority to\n                take or approve action, as well as each stakeholder's\n                contact information.\n              \n            \n        \n          \n            When escalation occurs, it should continue until the team\n            member is satisfied that the risk has been mitigated through\n            actions driven from leadership.\n          \n          \n             \n             \n          \n              \n                Escalations should include:\n              \n              \n                 \n                 \n                 \n                 \n                 \n                 \n              \n                  \n                    Description of the situation, and the nature of the\n                    risk\n                  \n                \n                  \n                    Criticality of the situation\n                  \n                \n                  \n                    Who or what is impacted\n                  \n                \n                  \n                    How great the impact is\n                  \n                \n                  \n                    Urgency if impact occurs\n                  \n                \n                  \n                    Suggested remedies and plans to mitigate\n                  \n                \n            \n              \n                Protect employees who escalate. Have policy that\n                protects team members from retribution if they escalate\n                around a non-responsive decision maker or stakeholder.\n                Have mechanisms in place to identify if this is\n                occurring and respond appropriately.\n              \n            \n        \n          \n            Encourage a culture of continuous improvement feedback loops\n            in everything that the organization produces. Feedback loops\n            act as minor escalations to individuals responsible, and\n            they identify improvement opportunities, even when\n            escalation is not needed. Continuous improvement cultures\n            force everyone to be more proactive.\n          \n        \n          \n            Leadership should periodically reemphasize the policies,\n            standards, mechanisms, and the desire for open escalation\n            and continuous feedback loops without retribution.\n          \n        \n      \n        Level of effort for the Implementation\n        Plan: Medium\n      \n     \n   \n\n  Resources\n\n      \n    \n      Related best practices:\n    \n    \n       \n    \n        \n          OPS02-BP05 Mechanisms exist to request additions, changes, and\n  exceptions\n        \n      \n    \n      Related documents:\n    \n    \n       \n       \n       \n    \n        \n          How\n          do you foster a culture of continuous improvement and learning\n          from Andon and escalation systems?\n        \n      \n        \n          The\n          Andon Cord (IT Revolution)\n        \n      \n        \n          AWS           DevOps Guidance | Establish clear escalation paths and\n          encourage constructive disagreement\n        \n      \n    \n      Related videos:\n    \n    \n       \n       \n       \n    \n        \n          Jeff\n          Bezos on how to make decisions (\u0026 increase\n          velocity)\n        \n      \n        \n          Toyota\n          Product System: Stopping Production, a Button, and an Andon\n          Electric Board\n        \n      \n        \n          Andon\n          Cord in LEAN Manufacturing\n        \n      \n    \n      Related examples:\n    \n    \n       \n    \n        \n          Working\n          with escalation plans in Incident Manager\n        \n      \n  \u003cdiv\u003e\u003cdiv\u003e\u003cdiv\u003e\u003cdiv id=\"js_error_message\"\u003e\u003cp\u003e\u003cimg src=\"https://d1ge0kk1l5kms0.cloudfront.net/images/G/01/webservices/console/warning.png\" alt=\"Warning\" /\u003e \u003cstrong\u003eJavascript is disabled or is unavailable in your browser.\u003c/strong\u003e\u003c/p\u003e\u003cp\u003eTo use the Amazon Web Services Documentation, Javascript must be enabled. Please refer to your browser's Help pages for instructions.\u003c/p\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003eDocument ConventionsOPS03-BP02 Team members are empowered to take action when\n  outcomes are at riskOPS03-BP04 Communications are timely, clear, and\n  actionable",
  "OPS03-BP04 Communications are timely, clear, and\n  actionable\n    Leadership is responsible for the creation of strong and effective\n    communications, especially when the organization adopts new\n    strategies, technologies, or ways of working. Leaders should set\n    expectations for all staff to work towards the company objectives.\n    Devise communication mechanisms that create and maintain awareness\n    among the teams responsible for running plans that are funded and\n    sponsored by leadership. Make use of cross-organizational diversity,\n    and listen attentively to multiple unique perspectives. Use this\n    perspective to increase innovation, challenge your assumptions, and\n    reduce the risk of confirmation bias. Foster inclusion, diversity,\n    and accessibility within your teams to gain beneficial perspectives.\n  \n    Desired outcome: Your\n    organization designs communication strategies to address the impact\n    of change to the organization. Teams remain informed and motivated\n    to continue working with one another rather than against each other.\n    Individuals understand how important their role is to achieve the\n    stated objectives. Email is only a passive mechanism for\n    communications and used accordingly. Management spends time with\n    their individual contributors to help them understand their\n    responsibility, the tasks to complete, and how their work\n    contributes to the overall mission. When necessary, leaders engage\n    people directly in smaller venues to convey messages and verify that\n    these messages are being delivered effectively. As a result of good\n    communications strategies, the organization performs at or above the\n    expectations of leadership. Leadership encourages and seeks diverse\n    opinions within and across teams.\n  \n    Common anti-patterns:\n  \n     \n     \n     \n     \n  \n      \n        Your organization has a five year plan to migrate all workloads\n        to AWS. The business case for cloud includes the modernization\n        of 25% of all workloads to take advantage of serverless\n        technology. The CIO communicates this strategy to direct reports\n        and expects each leader to cascade this presentation to\n        managers, directors, and individual contributors without any\n        in-person communication. The CIO steps back and expects his\n        organization to perform the new strategy.\n      \n    \n      \n        Leadership does not provide or use a mechanism for feedback, and\n        an expectation gap grows, which leads to stalled projects.\n      \n    \n      \n        You are asked to make a change to your security groups, but you\n        are not given any details of what change needs to be made, what\n        the impact of the change could be on all the workloads, and when\n        it should happen. The manager forwards an email from the VP of\n        InfoSec and adds the message \"Make this happen.\"\n      \n    \n      \n        Changes were made to your migration strategy that reduce the\n        planned modernization number from 25% to 10%. This has\n        downstream effects on the operations organization. They were not\n        informed of this strategic change and thus, they are not ready\n        with enough skilled capacity to support a greater number of\n        workloads lifted and shifted into AWS.\n      \n    \n    Benefits of establishing this best\n    practice:\n  \n     \n     \n     \n     \n  \n      \n        Your organization is well-informed on new or changed strategies,\n        and they act accordingly with strong motivation to help each\n        other achieve the overall objectives and metrics set by\n        leadership.\n      \n    \n      \n        Mechanisms exist and are used to provide timely notice to team\n        members of known risks and planned events.\n      \n    \n      \n        New ways of working (including changes to people or the\n        organization, processes, or technology), along with required\n        skills, are more effectively adopted by the organization, and\n        your organization realizes business benefits more quickly.\n      \n    \n      \n        Team members have the necessary context of the communications\n        being received, and they can be more effective in their jobs.\n      \n    \n    Level of risk exposed if this best practice\n    is not established: High\n  \n\n  Implementation guidance\n\n      \n    \n      To implement this best practice, you must work with stakeholders\n      across your organization to agree to communication standards.\n      Publicize those standards to your organization. For any\n      significant IT transitions, an established planning team can more\n      successfully manage the impact of change to its people than an\n      organization that ignores this practice. Larger organizations can\n      be more challenging when managing change because it's critical to\n      establish strong buy-in on a new strategy with all individual\n      contributors. In the absence of such a transition planning team,\n      leadership holds 100% of the responsibility for effective\n      communications. When establishing a transition planning team,\n      assign team members to work with all organizational leadership to\n      define and manage effective communications at every level.\n    \n    \n      Customer example\n    \n    \n      AnyCompany Retail signed up for AWS Enterprise Support and depends\n      on other third-party providers for its cloud operations. The\n      company uses chat and chatops as their main communication medium\n      for operational activities. Alerts and other information populate\n      specific channels. When someone must act, they clearly state the\n      desired outcome, and in many cases, they receive a runbook or\n      playbook to use. They schedule major changes to production systems\n      with a change calendar.\n    \n     \n\n  Implementation steps\n\n      \n      \n         \n         \n         \n         \n         \n         \n         \n         \n         \n         \n         \n         \n         \n         \n         \n         \n         \n         \n         \n      \n          \n            Establish a core team within the organization that has\n            accountability to build and initiate communication plans for\n            changes that happen at multiple levels within the\n            organization.\n          \n        \n          \n            Institute single-threaded ownership to achieve oversight.\n            Give individual teams the ability to innovate independently,\n            and balance the use of consistent mechanisms, which allows\n            for the right level of inspection and directional vision.\n          \n        \n          \n            Work with stakeholders across your organization to agree to\n            communication standards, practices, and plans.\n          \n        \n          \n            Verify that the core communications team collaborates with\n            organizational and program leadership to craft messages to\n            appropriate staff on behalf of leaders.\n          \n        \n          \n            Build strategic communication mechanisms to manage change\n            through announcements, shared calendars, all-hands meetings,\n            and in-person or one-on-one methods so that team members\n            have proper expectations on the actions they should take.\n          \n        \n          \n            Provide necessary context, details, and time (when possible)\n            to determine if action is necessary. When action is needed,\n            provide the required action and its impact.\n          \n        \n          \n            Implement tools that facilitate tactical communications,\n            like internal chat, email, and knowledge management.\n          \n        \n          \n            Implement mechanisms to measure and verify that all\n            communications lead to desired outcomes.\n          \n        \n          \n            Establish a feedback loop that measures the effectiveness of\n            all communications, especially when communications are related\n            to resistance to changes throughout the organization.\n          \n        \n          \n            For all AWS accounts, establish\n            alternate\n            contacts for billing, security, and operations.\n            Ideally, each contact should be an email distribution as\n            opposed to a specific individual contact.\n          \n        \n          \n            Establish an escalation and reverse escalation communication\n            plan to engage with your internal and external teams,\n            including AWS support and other third-party providers.\n          \n        \n          \n            Initiate and perform communication strategies consistently\n            throughout the life of each transformation program.\n          \n        \n          \n            Prioritize actions that are repeatable where possible to\n            safely automate at scale.\n          \n        \n          \n            When communications are required in scenarios with automated\n            actions, the communication's purpose should be to inform\n            teams, for auditing, or a part of the change management\n            process.\n          \n        \n          \n            Analyze communications from your alert systems for false\n            positives or alerts that are constantly created. Remove or\n            change these alerts so that they start when human\n            intervention is required. If an alert is initiated, provide\n            a runbook or playbook.\n          \n          \n             \n          \n              \n                You can use\n                AWS Systems Manager Documents to build playbooks and\n                runbooks for alerts.\n              \n            \n        \n          \n            Mechanisms are in place to provide notification of risks or\n            planned events in a clear and actionable way with enough\n            notice to allow appropriate responses. Use email lists or\n            chat channels to send notifications ahead of planned events.\n          \n          \n             \n          \n              \n                AWS                 Chatbot can be used to send alerts and respond to\n                events within your organizations messaging platform.\n              \n            \n        \n          \n            Provide an accessible source of information where planned\n            events can be discovered. Provide notifications of planned\n            events from the same system.\n          \n          \n             \n          \n              \n                AWS Systems Manager Change Calendar can be used to\n                create change windows when changes can occur. This\n                provides team members notice when they can make changes\n                safely.\n              \n            \n        \n          \n            Monitor vulnerability notifications and patch information to\n            understand vulnerabilities in the wild and potential risks\n            associated to your workload components. Provide notification\n            to team members so that they can act.\n          \n          \n             \n          \n              \n                You can subscribe to\n                AWS                 Security Bulletins to receive notifications of\n                vulnerabilities on AWS.\n              \n            \n        \n          \n            Seek diverse opinions and\n            perspectives: Encourage contributions from\n            everyone. Give communication opportunities to\n            under-represented groups. Rotate roles and responsibilities\n            in meetings.\n          \n          \n             \n             \n             \n          \n              \n                Expand roles and\n                responsibilities: Provide opportunities for\n                team members to take on roles that they might not\n                otherwise. They can gain experience and perspective from\n                the role and from interactions with new team members\n                with whom they might not otherwise interact. They can\n                also bring their experience and perspective to the new\n                role and team members they interact with. As perspective\n                increases, identify emergent business opportunities or\n                new opportunities for improvement. Rotate common tasks\n                between members within a team that others typically\n                perform to understand the demands and impact of\n                performing them.\n              \n            \n              \n                Provide a safe and welcoming\n                environment: Establish policy and controls\n                that protect the mental and physical safety of team\n                members within your organization. Team members should be\n                able to interact without fear of reprisal. When team\n                members feel safe and welcome, they are more likely to\n                be engaged and productive. The more diverse your\n                organization, the better your understanding can be of\n                the people you support, including your customers. When\n                your team members are comfortable, feel free to speak,\n                and are confident they are heard, they are more likely\n                to share valuable insights (for example, marketing\n                opportunities, accessibility needs, unserved market\n                segments, and unacknowledged risks in your environment).\n              \n            \n              \n                Encourage team members to\n                participate fully: Provide the resources\n                necessary for your employees to participate fully in all\n                work related activities. Team members that face daily\n                challenges develop skills for working around them. These\n                uniquely-developed skills can provide significant\n                benefit to your organization. Support team members with\n                necessary accommodations to increase the benefits you\n                can receive from their contributions.\n              \n            \n        \n     \n   \n\n  Resources\n\n      \n    \n      Related best practices:\n    \n    \n       \n       \n       \n    \n        \n          OPS03-BP01\n          Provide executive sponsorship\n        \n      \n        \n          OPS07-BP03\n          Use runbooks to perform procedures\n        \n      \n        \n          OPS07-BP04\n          Use playbooks to investigate issues\n        \n      \n    \n      Related documents:\n    \n    \n       \n       \n       \n       \n       \n       \n    \n        \n          AWS           Blog post | Accountability and empowerment are key to\n          high-performing agile organizations\n        \n      \n        \n          AWS           Executive Insights | Learn to scale innovation, not complexity\n          | Single-threaded Leaders\n        \n      \n        \n          AWS           Security Bulletins\n        \n      \n        \n          Open\n          CVE\n        \n      \n        \n          Support App in Slack to Manage Support Cases\n        \n      \n        \n          Manage\n          AWS resources in your Slack channels with Amazon Q Developer in chat applications\n        \n      \n    \n      Related services:\n    \n    \n       \n       \n       \n    \n        \n          Amazon Q Developer in chat applications\n        \n      \n        \n          AWS Systems Manager Change Calendar\n        \n      \n        \n          AWS Systems Manager Documents\n        \n      \n  \u003cdiv\u003e\u003cdiv\u003e\u003cdiv\u003e\u003cdiv id=\"js_error_message\"\u003e\u003cp\u003e\u003cimg src=\"https://d1ge0kk1l5kms0.cloudfront.net/images/G/01/webservices/console/warning.png\" alt=\"Warning\" /\u003e \u003cstrong\u003eJavascript is disabled or is unavailable in your browser.\u003c/strong\u003e\u003c/p\u003e\u003cp\u003eTo use the Amazon Web Services Documentation, Javascript must be enabled. Please refer to your browser's Help pages for instructions.\u003c/p\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003eDocument ConventionsOPS03-BP03 Escalation is encouragedOPS03-BP05 Experimentation is encouraged",
  "OPS03-BP05 Experimentation is encouragedExperimentation is a catalyst for turning new ideas into products and features. It accelerates learning and keeps team members interested and engaged. Team members are encouraged to experiment often to drive innovation. Even when an undesired result occurs, there is value in knowing what not to do. Team members are not punished for successful experiments with undesired results. \n    Desired outcome:\n  \n     \n     \n  \n      \n        Your organization encourages experimentation to foster innovation.\n      \n    \n      \n        Experiments are used as an opportunity to learn.\n      \n    \n    Common anti-patterns:\n  \n     \n     \n  \n      \n        You want to run an A/B test but there is no mechanism to run the experiment. You deploy a UI change without the ability to test it. It results in a negative customer experience.\n      \n    \n      \n        Your company only has a stage and production environment. There is no sandbox environment to experiment with new features or products so you must experiment within the production environment.\n      \n    \n    Benefits of establishing this best practice:\n  \n     \n     \n     \n  \n      \n        Experimentation drives innovation.\n      \n    \n      \n        You can react faster to feedback from users through experimentation.\n      \n    \n      \n        Your organization develops a culture of learning.\n      \n    \n    Level of risk exposed if this best practice\n    is not established: Medium\n  \n\n  Implementation guidance\n    \n      Experiments should be run in a safe manner. Leverage multiple environments to experiment without jeopardizing production resources. Use A/B testing and feature flags to test experiments. Provide team members the ability to conduct experiments in a sandbox environment.\n    \n    \n      Customer example\n    \n    \n      AnyCompany Retail encourages experimentation. Team members can use 20% of their work week to experiment or learn new technologies. They have a sandbox environment where they can innovate. A/B testing is used for new features to validate them with real user feedback.\n    \n    \n      Implementation steps\n    \n    \n       \n       \n       \n    \n        \n          Work with leadership across your organization to support experimentation. Team members should be encouraged to conduct experiments in a safe manner. \n        \n      \n        \n          Provide your team members with an environment where they can safely experiment. They must have access to an environment that is like production.\n        \n        \n           \n        \n            \n              You can use a separate AWS account to create a sandbox environment for experimentation. AWS Control Tower can be used to provision these accounts.\n            \n          \n      \n        \n          Use feature flags and A/B testing to experiment safely and gather user feedback.\n        \n        \n           \n           \n        \n            \n              AWS AppConfig Feature Flags provides the ability to create feature flags. \n            \n          \n            \n              You can use AWS Lambda versions to deploy a new version of a function for beta testing.\n            \n          \n      \n    \n      Level of effort for the implementation plan: High. Providing team members with an environment to experiment in and a safe way to conduct experiments can require significant investment. You may also need to modify application code to use feature flags or support A/B testing. \n    \n\n      \n    \n   \n    \n    Resources\n    \n      Related best practices:\n    \n    \n       \n       \n    \n        \n          OPS11-BP02 Perform post-incident analysis - Learning from incidents is an important driver for innovation along with experimentation.\n        \n      \n        \n          OPS11-BP03 Implement feedback loops - Feedback loops are an important part of experimentation.\n        \n      \n    \n      Related documents:\n    \n    \n       \n       \n       \n       \n       \n       \n       \n    \n        \n          An Inside Look at the Amazon Culture: Experimentation, Failure, and Customer Obsession\n        \n      \n        \n          Best practices for creating and managing sandbox accounts in AWS\n      \n         \n          Create a Culture of Experimentation Enabled by the Cloud\n        \n      \n        \n          Enabling experimentation and innovation in the cloud at SulAmérica Seguros\n        \n      \n         \n          Experiment More, Fail Less\n        \n      \n        \n          Organizing Your AWS Environment Using Multiple Accounts - Sandbox OU\n        \n      \n        \n          Using AWS AppConfig Feature Flags\n        \n      \n    \n      Related videos:\n    \n    \n       \n       \n       \n       \n       \n    \n        AWS On Air ft. Amazon CloudWatch Evidently | AWS Events \n        \n      \n        AWS On Air San Fran Summit 2022 ft. AWS AppConfig Feature Flags integration with Jira \n        \n      \n        AWS re:Invent 2022 - A deployment is not a release: Control your launches w/feature flags (BOA305-R) \n        \n      \n        \n          Programmatically Create an AWS account with AWS Control Tower\n      \n        \n          Set Up a Multi-Account AWS Environment that Uses Best Practices for AWS Organizations\n      \n    \n      Related examples:\n    \n    \n       \n       \n    \n        AWS Innovation Sandbox\n        \n      \n        \n          End-to-end Personalization 101 for E-Commerce\n        \n      \n    \n      Related services:\n    \n    \n       \n       \n       \n    \n        \n          Amazon CloudWatch Evidently\n        \n      \n        \n          AWS AppConfig\n        \n      \n        \n          AWS Control Tower\n        \n      \n  \u003cdiv\u003e\u003cdiv\u003e\u003cdiv\u003e\u003cdiv id=\"js_error_message\"\u003e\u003cp\u003e\u003cimg src=\"https://d1ge0kk1l5kms0.cloudfront.net/images/G/01/webservices/console/warning.png\" alt=\"Warning\" /\u003e \u003cstrong\u003eJavascript is disabled or is unavailable in your browser.\u003c/strong\u003e\u003c/p\u003e\u003cp\u003eTo use the Amazon Web Services Documentation, Javascript must be enabled. Please refer to your browser's Help pages for instructions.\u003c/p\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003eDocument ConventionsOPS03-BP04 Communications are timely, clear, and\n  actionableOPS03-BP06 Team members are encouraged to maintain\n  and grow their skill sets",
  "OPS03-BP06 Team members are encouraged to maintain\n  and grow their skill sets\n    Teams must grow their skill sets to adopt new technologies, and to\n    support changes in demand and responsibilities in support of your\n    workloads. Growth of skills in new technologies is frequently a\n    source of team member satisfaction and supports innovation. Support\n    your team members' pursuit and maintenance of industry\n    certifications that validate and acknowledge their growing skills.\n    Cross train to promote knowledge transfer and reduce the risk of\n    significant impact when you lose skilled and experienced team\n    members with institutional knowledge. Provide dedicated structured\n    time for learning.\n  \n    AWS provides resources, including the\n    AWS     Getting Started Resource Center,\n    AWS Blogs,\n    AWS Online\n    Tech Talks,\n    AWS Events and\n    Webinars, and the\n    AWS     Well-Architected Labs, that provide guidance, examples, and\n    detailed walkthroughs to educate your teams.\n  \n    Resources such as\n    Support, (AWS     re:Post,\n    Support Center), and\n    AWS     Documentation help remove technical roadblocks and improve\n    operations. Reach out to Support through Support Center for\n    help with your questions.\n  \n    AWS also shares best practices and patterns that we have learned\n    through the operation of AWS in\n    The\n    Amazon Builders' Library and a wide variety of other useful\n    educational material through the\n    AWS Blog and\n    The\n    Official AWS Podcast.\n  \n    AWS Training and\n    Certification includes free training through self-paced\n    digital courses, along with learning plans by role or domain. You\n    can also register for instructor-led training to further support the\n    development of your teams' AWS skills.\n  \n    Desired outcome: Your\n    organization constantly evaluates skill gaps and closes them with\n    structured budget and investment. Teams encourage and incentivize\n    their members with upskilling activities such as acquiring leading\n    industry certifications. Teams take advantage of dedicated\n    cross-sharing knowledge programs such as lunch-and-learns, immersion\n    days, hackathons, and gamedays. Your organization's keeps its\n    knowledge systems up-to-date and relevant to cross-train team\n    members, including new-hire onboarding trainings.\n  \n    Common anti-patterns:\n  \n     \n     \n  \n      \n        In the absence of a structured training program and budget,\n        teams experience uncertainty as they try to keep pace with\n        technology evolution, which results in increased attrition.\n      \n    \n      \n        As part of migrating to AWS, your organization demonstrates\n        skill gaps and varying cloud fluency amongst teams. Without an\n        effort to upskill, teams find themselves overtasked with legacy\n        and inefficient management of the cloud environment, which\n        causes increased operator toil. This burn out increases employee\n        dissatisfaction.\n      \n    \n    Benefits of establishing this best\n    practice: When your organization consciously invests in\n    improving the skills of its teams, it also helps accelerate and\n    scale cloud adoption and optimization. Targeted learning programs\n    drive innovation and build operational ability for teams to be\n    prepared to handle events. Teams consciously invest in the\n    implementation and evolution of best practices. Team morale is high,\n    and team members value their contribution to the business.\n  \n    Level of risk exposed if this best practice\n    is not established: Medium\n  \n\n  Implementation guidance\n\n      \n    \n      To adopt new technologies, fuel innovation, and keep pace with\n      changes in demand and responsibilities to support your workloads,\n      continually invest in the professional growth of your teams.\n    \n     \n      \n      Implementation steps\n    \n    \n       \n       \n       \n       \n       \n       \n    \n        \n          Use structured cloud advocacy\n          programs:\n          AWS           Skills Guild provides consultative training to increase\n          cloud skill confidence and ignite a culture of continuous\n          learning.\n        \n      \n        \n          Provide resources for\n          education: Provide dedicated, structured time and\n          access to training materials and lab resources, and support\n          participation in conferences and access to professional organizations\n          that provide opportunities for learning from both educators\n          and peers. Provide your junior team members with access to\n          senior team members as mentors, or allow the junior team\n          members to shadow their seniors' work and be exposed to their\n          methods and skills. Encourage learning about content not\n          directly related to work in order to have a broader\n          perspective.\n        \n      \n        \n          Encourage use of expert technical\n          resources: Leverage resources such as\n          AWS re:Post to\n          get access to curated knowledge and vibrant community.\n        \n      \n        \n          Build and maintain an up-to-date\n          knowledge repository: Use knowledge sharing\n          platforms such as wikis and runbooks. Create your own reusable\n          expert knowledge source with\n          AWS           re:Post Private to streamline collaboration, improve\n          productivity, and accelerate employee onboarding.\n        \n      \n        \n          Team education and cross-team\n          engagement: Plan for the continuing education needs\n          of your team members. Provide opportunities for team members\n          to join other teams (temporarily or permanently) to share\n          skills and best practices benefiting your entire organization.\n        \n      \n        \n          Support pursuit and maintenance of\n          industry certifications: Support your team members\n          in the acquisition and maintenance of industry certifications\n          that validate what they have learned and acknowledge their\n          accomplishments.\n        \n      \n    \n      Level of effort for the implementation\n      plan: High\n    \n     \n   \n\n  Resources\n\n      \n    \n      Related best practices:\n    \n    \n       \n       \n    \n        \n          OPS03-BP01\n          Provide executive sponsorship\n        \n      \n        \n          OPS11-BP04\n          Perform knowledge management\n        \n      \n    \n      Related documents:\n    \n    \n       \n       \n       \n       \n       \n       \n       \n       \n       \n       \n       \n       \n       \n       \n       \n    \n        \n          AWS           Whitepaper | Cloud Adoption Framework: People\n          Perspective\n        \n      \n        \n          Investing\n          in continuous learning to grow your organization's\n          future\n        \n      \n        \n          AWS           Skills Guild\n        \n      \n        \n          AWS Training and Certification\n        \n      \n        \n          Support\n        \n      \n        \n          AWS re:Post\n        \n      \n        \n          AWS           Getting Started Resource Center\n        \n      \n        \n          AWS           Blogs\n        \n      \n        \n          AWS Cloud\n          Compliance\n        \n      \n        \n          AWS           Documentation\n        \n      \n        \n          The\n          Official AWS Podcast.\n        \n      \n        \n          AWS           Online Tech Talks\n        \n      \n        \n          AWS Events\n          and Webinars\n        \n      \n        \n          AWS           Well-Architected Labs\n        \n      \n        \n          The\n          Amazon Builders' Library\n        \n      \n    \n      Related videos:\n    \n    \n       \n       \n    \n        \n          AWS           re:Invent 2023 | Reskilling at the speed of cloud: Turning\n          employees into entrepreneurs\n        \n      \n        \n          WS\n          re:Invent 2023 | Building a culture of curiosity through\n          gamification\n        \n      \n  \u003cdiv\u003e\u003cdiv\u003e\u003cdiv\u003e\u003cdiv id=\"js_error_message\"\u003e\u003cp\u003e\u003cimg src=\"https://d1ge0kk1l5kms0.cloudfront.net/images/G/01/webservices/console/warning.png\" alt=\"Warning\" /\u003e \u003cstrong\u003eJavascript is disabled or is unavailable in your browser.\u003c/strong\u003e\u003c/p\u003e\u003cp\u003eTo use the Amazon Web Services Documentation, Javascript must be enabled. Please refer to your browser's Help pages for instructions.\u003c/p\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003eDocument ConventionsOPS03-BP05 Experimentation is encouragedOPS03-BP07 Resource teams appropriately",
  "OPS03-BP07 Resource teams appropriately\n    Provision the right amount of proficient team members, and provide\n    tools and resources to support your workload needs. Overburdening\n    team members increases the risk of human error. Investments in tools\n    and resources, such as automation, can scale the effectiveness of\n    your team and help them support a greater number of workloads\n    without requiring additional capacity.\n  \n    Desired outcome:\n  \n     \n     \n     \n     \n  \n      \n        You have appropriately staffed your team to gain the skillsets\n        needed for them to operate workloads in AWS in accordance with\n        your migration plan. As your team has scaled itself up during\n        the course of your migration project, they have gained\n        proficiency in the core AWS technologies that the business plans\n        to use when migrating or modernizing their applications.\n      \n    \n      \n        You have carefully aligned your staffing plan to make efficient\n        use of resources by leveraging automation and workflow. A\n        smaller team can now manage more infrastructure on behalf of the\n        application development teams.\n      \n    \n      \n        With shifting operational priorities, any resource staffing\n        constraints are proactively identified to protect the success of\n        business initiatives.\n      \n    \n      \n        Operational metrics that report operational toil (such as\n        on-call fatigue or excessive paging) are reviewed to verify that\n        staff are not overwhelmed.\n      \n    \n    Common anti-patterns:\n  \n     \n     \n     \n  \n      \n        Your staff have not ramped up on AWS skills as you close in on\n        your multi-year cloud migration plan, which risks support of the\n        workloads and lowers employee morale.\n      \n    \n      \n        Your entire IT organization is shifting into agile ways of\n        working. The business is prioritizing the product portfolio and\n        setting metrics for what features need to be developed first.\n        Your agile process does not require teams to assign story points\n        to their work plans. As a result, it is impossible to know the\n        level of capacity required for the next amount of work, or if\n        you have the right skills assigned to the work.\n      \n    \n      \n        You are having an AWS partner migrate your workloads, and you\n        don't have a support transition plan for your teams once the\n        partner completes the migration project. Your teams struggle to\n        efficiently and effectively support the workloads.\n      \n    \n    Benefits of establishing this best\n      practice: You have appropriately-skilled team members\n    available in your organization to support the workloads. Resource\n    allocation can adapt to shifting priorities without impacting\n    performance. The result is teams being proficient at supporting\n    workloads while maximizing time to focus on innovating for\n    customers, which in turn raises employee satisfaction.\n  \n    Level of risk exposed if this best practice\n      is not established: Medium\n  \n    \n    Implementation guidance\n    \n    \n    \n      Resource planning for your cloud migration should occur at an\n      organizational level that aligns to your migration plan, as well\n      as the desired operating model being implemented to support your\n      new cloud environment. This should include understanding which\n      cloud technologies are deployed for the business and application\n      development teams. Infrastructure and operations leadership should\n      plan for skills gap analysis, training, and role definition for\n      engineers who are leading cloud adoption.\n    \n     \n      \n      Implementation steps\n      \n      \n      \n         \n         \n         \n         \n         \n      \n          \n            Define success criteria for team's success with relevant\n            operational metrics such as staff productivity (for example,\n            cost to support a workload or operator hours spent during\n            incidents).\n          \n        \n          \n            Define resource capacity planning and inspection mechanisms\n            to verify that the right balance of qualified capacity is\n            available when needed and can be adjusted over time.\n          \n        \n          \n            Create mechanisms (for example, sending a monthly survey to\n            teams) to understand work-related challenges that impact\n            teams (like increasing responsibilities, changes in\n            technology, loss of personnel, or increase in customers\n            supported).\n          \n        \n          \n            Use these mechanisms to engage with teams and spot trends\n            that may contribute to employee productivity challenges.\n            When your teams are impacted by external factors, reevaluate\n            goals and adjust targets as appropriate. Identify obstacles\n            that are impeding your team's progress.\n          \n        \n          \n            Regularly review if your currently-provisioned resources are\n            still sufficient, of if additional resources are needed, and\n            make appropriate adjustments to support teams.\n          \n        \n      \n        Level of effort for the implementation\n          plan: Medium\n      \n     \n   \n    \n    Resources\n    \n    \n    \n      Related best practices:\n    \n    \n       \n       \n       \n       \n    \n        \n          OPS03-BP06 Team members are encouraged to maintain and grow their skill sets\n        \n      \n        \n          OPS09-BP03 Review operations metrics and prioritize improvement\n        \n      \n        \n          OPS10-BP01 Use a process for event, incident, and problem management\n        \n      \n        \n          OPS10-BP07 Automate responses to events\n        \n      \n    \n      Related documents:\n    \n    \n       \n       \n       \n       \n       \n    \n        \n          AWS Cloud Adoption Framework: People Perspective\n        \n      \n        \n          Becoming\n            a Future-Ready Enterprise\n        \n      \n        \n          Prioritize\n            your Employees' Skills to Drive Business Growth\n        \n      \n        \n          High\n            performing organization - the Amazon Two-Pizza team\n        \n      \n        \n          How\n            Cloud-Mature Enterprises Succeed\n        \n      \n  \u003cdiv\u003e\u003cdiv\u003e\u003cdiv\u003e\u003cdiv id=\"js_error_message\"\u003e\u003cp\u003e\u003cimg src=\"https://d1ge0kk1l5kms0.cloudfront.net/images/G/01/webservices/console/warning.png\" alt=\"Warning\" /\u003e \u003cstrong\u003eJavascript is disabled or is unavailable in your browser.\u003c/strong\u003e\u003c/p\u003e\u003cp\u003eTo use the Amazon Web Services Documentation, Javascript must be enabled. Please refer to your browser's Help pages for instructions.\u003c/p\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003eDocument ConventionsOPS03-BP06 Team members are encouraged to maintain\n  and grow their skill sets                Prepare             ",
  "OPS04-BP01 Identify key performance indicators\n    Implementing observability in your workload starts with understanding its state and making data-driven decisions based on business requirements. One of the most effective ways to ensure alignment between monitoring activities and business objectives is by defining and monitoring key performance indicators (KPIs).\n  \n    Desired outcome: Efficient observability practices that are tightly aligned with business objectives, ensuring that monitoring efforts are always in service of tangible business outcomes.\n  \n    Common anti-patterns:\n  \n     \n     \n     \n  \n      \n        Undefined KPIs: Working without clear KPIs can lead to monitoring too much or too little, missing vital signals.\n        \n      \n    \n      \n        Static KPIs: Not revisiting or refining KPIs as the workload or business objectives evolve.\n        \n      \n    \n      \n        Misalignment: Focusing on technical metrics that don’t correlate directly with business outcomes or are harder to correlate with real-world issues.\n        \n      \n    \n    Benefits of establishing this best\n      practice:\n  \n     \n     \n     \n     \n  \n      \n        Ease of issue identification: Business KPIs often surface issues more clearly than technical metrics. A dip in a business KPI can pinpoint a problem more effectively than sifting through numerous technical metrics.\n      \n    \n      \n        Business alignment: Ensures that monitoring activities directly support business objectives.\n      \n    \n      \n        Efficiency: Prioritize monitoring resources and attention on metrics that matter.\n      \n    \n      \n        Proactivity: Recognize and address issues before they have broader business implications.\n      \n    \n    Level of risk exposed if this best practice\n      is not established: High\n  \n    \n    Implementation guidance\n    \n      To effectively define workload KPIs:\n    \n    \n       \n       \n       \n       \n       \n    \n        \n          Start with business outcomes: Before diving into metrics, understand the desired business outcomes. Is it increased sales, higher user engagement, or faster response times? \n        \n      \n        \n          Correlate technical metrics with business objectives: Not all technical metrics have a direct impact on business outcomes. Identify those that do, but it's often more straightforward to identify an issue using a business KPI.\n        \n      \n        \n          Use Amazon CloudWatch: Employ CloudWatch to define and monitor metrics that represent your KPIs.\n        \n      \n        \n          Regularly review and update KPIs: As your workload and business evolve, keep your KPIs relevant.\n        \n      \n        \n          Involve stakeholders: Involve both technical and business teams in defining and reviewing KPIs.\n        \n      \n    \n      Level of effort for the implementation\n        plan: Medium\n    \n   \n    \n    Resources\n    \n    \n    \n      Related best practices:\n    \n    \n       \n       \n       \n       \n    \n        OPS04-BP02 Implement application telemetry\n      \n        OPS04-BP03 Implement user experience telemetry\n      \n        OPS04-BP04 Implement dependency telemetry\n      \n        OPS04-BP05 Implement distributed tracing\n      \n    \n      Related documents:\n    \n    \n       \n       \n       \n    \n        AWS Observability Best Practices\n        \n      \n        \n          CloudWatch User Guide\n        \n      \n        AWS Observability Skill Builder Course\n        \n      \n    \n      Related videos:\n    \n    \n       \n    \n        \n          Developing an observability strategy\n        \n      \n    \n      Related examples:\n    \n    \n       \n    \n        \n          One\n            Observability Workshop\n        \n      \n  \u003cdiv\u003e\u003cdiv\u003e\u003cdiv\u003e\u003cdiv id=\"js_error_message\"\u003e\u003cp\u003e\u003cimg src=\"https://d1ge0kk1l5kms0.cloudfront.net/images/G/01/webservices/console/warning.png\" alt=\"Warning\" /\u003e \u003cstrong\u003eJavascript is disabled or is unavailable in your browser.\u003c/strong\u003e\u003c/p\u003e\u003cp\u003eTo use the Amazon Web Services Documentation, Javascript must be enabled. Please refer to your browser's Help pages for instructions.\u003c/p\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003eDocument Conventions OPS 4. How do you implement observability in your workload? OPS04-BP02 Implement application telemetry",
  "OPS04-BP02 Implement application telemetry\n    Application telemetry serves as the foundation for observability of\n    your workload. It's crucial to emit telemetry that offers actionable\n    insights into the state of your application and the achievement of\n    both technical and business outcomes. From troubleshooting to\n    measuring the impact of a new feature or ensuring alignment with\n    business key performance indicators (KPIs), application telemetry\n    informs the way you build, operate, and evolve your workload.\n  \n    Metrics, logs, and traces form the three primary pillars of\n    observability. These serve as diagnostic tools that describe the\n    state of your application. Over time, they assist in creating\n    baselines and identifying anomalies. However, to ensure alignment\n    between monitoring activities and business objectives, it's pivotal\n    to define and monitor KPIs. Business KPIs often make it easier to\n    identify issues compared to technical metrics alone.\n  \n    Other telemetry types, like real user monitoring (RUM) and synthetic\n    transactions, complement these primary data sources. RUM offers\n    insights into real-time user interactions, whereas synthetic\n    transactions simulate potential user behaviors, helping detect\n    bottlenecks before real users encounter them.\n  \n    Desired outcome: Derive\n    actionable insights into the performance of your workload. These\n    insights allow you to make proactive decisions about performance\n    optimization, achieve increased workload stability, streamline CI/CD\n    processes, and utilize resources effectively.\n  \n    Common anti-patterns:\n  \n     \n     \n     \n  \n      \n        Incomplete observability: Neglecting to incorporate\n        observability at every layer of the workload, resulting in blind\n        spots that can obscure vital system performance and behavior\n        insights.\n      \n    \n      \n        Fragmented data view: When data is scattered across multiple\n        tools and systems, it becomes challenging to maintain a holistic\n        view of your workload's health and performance.\n      \n    \n      \n        User-reported issues: A sign that proactive issue detection\n        through telemetry and business KPI monitoring is lacking.\n      \n    \n    Benefits of establishing this best\n    practice:\n  \n     \n     \n     \n     \n  \n      \n        Informed decision-making: With insights from telemetry and\n        business KPIs, you can make data-driven decisions.\n      \n    \n      \n        Improved operational efficiency: Data-driven resource\n        utilization leads to cost-effectiveness.\n      \n    \n      \n        Enhanced workload stability: Faster detection and resolution of\n        issues leading to improved uptime.\n      \n    \n      \n        Streamlined CI/CD processes: Insights from telemetry data\n        facilitate refinement of processes and reliable code delivery.\n      \n    \n    Level of risk exposed if this best practice\n    is not established: High\n  \n\n  Implementation guidance\n\n      \n    \n      To implement application telemetry for your workload, use AWS\n      services like\n      Amazon CloudWatch and\n      AWS X-Ray.\n      Amazon CloudWatch provides a comprehensive suite of monitoring\n      tools, allowing you to observe your resources and applications in\n      AWS and on-premises environments. It collects, tracks, and\n      analyzes metrics, consolidates and monitors log data, and responds\n      to changes in your resources, enhancing your understanding of how\n      your workload operates. In tandem, AWS X-Ray lets you trace,\n      analyze, and debug your applications, giving you a deep\n      understanding of your workload's behavior. With features like\n      service maps, latency distributions, and trace timelines, AWS X-Ray provides insights into your workload's performance and the\n      bottlenecks affecting it.\n    \n     \n\n  Implementation steps\n\n      \n      \n         \n         \n         \n         \n         \n         \n         \n         \n         \n      \n          \n            Identify what data to\n            collect: Ascertain the essential metrics, logs,\n            and traces that would offer substantial insights into your\n            workload's health, performance, and behavior.\n          \n        \n          \n            Deploy the\n            CloudWatch\n            agent: The CloudWatch agent is\n            instrumental in procuring system and application metrics and\n            logs from your workload and its underlying infrastructure.\n            The CloudWatch agent can also be used to collect\n            OpenTelemetry or X-Ray traces and send them to X-Ray.\n          \n        \n          \n            Implement anomaly detection for logs\n            and metrics: Use\n            CloudWatch Logs anomaly detection and\n            CloudWatch\n            Metrics anomaly detection to automatically identify\n            unusual activities in your application's operations. These\n            tools use machine learning algorithms to detect and alert on\n            anomalies, which enanhces your monitoring capabilities and\n            speeds up response time to potential disruptions or security\n            threats. Set up these features to proactively manage\n            application health and security.\n          \n        \n          \n            Secure sensitive log\n            data: Use\n            Amazon CloudWatch Logs data protection to mask sensitive\n            information within your logs. This feature helps maintain\n            privacy and compliance through automatic detection and\n            masking of sensitive data before it is accessed. Implement\n            data masking to securely handle and protect sensitive\n            details such as personally identifiable information (PII).\n          \n        \n          \n            Define and monitor business\n            KPIs: Establish\n            custom\n            metrics that align with your\n            business\n            outcomes.\n          \n        \n          \n            Instrument your application with AWS X-Ray: In addition to deploying the CloudWatch\n            agent, it's crucial to\n            instrument\n            your application to emit trace data. This process can\n            provide further insights into your workload's behavior and\n            performance.\n          \n        \n          \n            Standardize data collection across\n            your application: Standardize data collection\n            practices across your entire application. Uniformity aids in\n            correlating and analyzing data, providing a comprehensive\n            view of your application's behavior.\n          \n        \n          \n            Implement cross-account\n            observability: Enhance monitoring efficiency\n            across multiple AWS accounts with\n            Amazon CloudWatch cross-account observability. With this\n            feature, you can consolidate metrics, logs, and alarms from\n            different accounts into a single view, which simplifies\n            management and improves response times for identified issues\n            across your organization's AWS environment.\n          \n        \n          \n            Analyze and act on the\n            data: Once data collection and normalization are\n            in place, use\n            Amazon CloudWatch for metrics and logs analysis, and\n            AWS X-Ray for trace analysis. Such analysis can yield\n            crucial insights into your workload's health, performance,\n            and behavior, guiding your decision-making process.\n          \n        \n      \n        Level of effort for the implementation\n        plan: High\n      \n     \n   \n\n  Resources\n\n      \n    \n      Related best practices:\n    \n    \n       \n       \n       \n       \n    \n        \n          OPS04-BP01\n          Define workload KPIs\n        \n      \n        \n          OPS04-BP03\n          Implement user activity telemetry\n        \n      \n        \n          OPS04-BP04\n          Implement dependency telemetry\n        \n      \n        \n          OPS04-BP05\n          Implement transaction traceability\n        \n      \n    \n      Related documents:\n    \n    \n       \n       \n       \n       \n       \n       \n       \n    \n        \n          AWS           Observability Best Practices\n        \n      \n        \n          CloudWatch\n          User Guide\n        \n      \n        \n          AWS X-Ray Developer Guide\n        \n      \n        \n          Instrumenting\n          distributed systems for operational visibility\n        \n      \n        \n          AWS           Observability Skill Builder Course\n        \n      \n        \n          What's\n          New with Amazon CloudWatch\n        \n      \n        \n          What's\n          new with AWS X-Ray\n        \n      \n    \n      Related videos:\n    \n    \n       \n       \n    \n        \n          AWS re:Invent\n          2022 - Observability best practices at Amazon\n        \n      \n        \n          AWS re:Invent\n          2022 - Developing an observability strategy\n        \n      \n    \n      Related examples:\n    \n    \n       \n       \n    \n        \n          One\n          Observability Workshop\n        \n      \n        \n          AWS           Solutions Library: Application Monitoring with Amazon CloudWatch\n        \n      \n  \u003cdiv\u003e\u003cdiv\u003e\u003cdiv\u003e\u003cdiv id=\"js_error_message\"\u003e\u003cp\u003e\u003cimg src=\"https://d1ge0kk1l5kms0.cloudfront.net/images/G/01/webservices/console/warning.png\" alt=\"Warning\" /\u003e \u003cstrong\u003eJavascript is disabled or is unavailable in your browser.\u003c/strong\u003e\u003c/p\u003e\u003cp\u003eTo use the Amazon Web Services Documentation, Javascript must be enabled. Please refer to your browser's Help pages for instructions.\u003c/p\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003eDocument ConventionsOPS04-BP01 Identify key performance indicatorsOPS04-BP03 Implement user experience telemetry",
  "OPS04-BP03 Implement user experience telemetry\n    Gaining deep insights into customer experiences and interactions with your application is crucial. Real user monitoring (RUM) and synthetic transactions serve as powerful tools for this purpose. RUM provides data about real user interactions granting an unfiltered perspective of user satisfaction, while synthetic transactions simulate user interactions, helping in detecting potential issues even before they impact real users.\n  \n    Desired outcome: A holistic view of the customer experience, proactive detection of issues, and optimization of user interactions to deliver seamless digital experiences.\n    \n  \n    Common anti-patterns:\n  \n     \n     \n  \n      \n      Applications without real user monitoring (RUM):\n      \n      \n         \n         \n      \n          \n            Delayed issue detection: Without RUM, you might not become aware of performance bottlenecks or issues until users complain. This reactive approach can lead to customer dissatisfaction.\n          \n        \n          \n            Lack of user experience insights: Not using RUM means you lose out on crucial data that shows how real users interact with your application, limiting your ability to optimize the user experience.\n          \n        \n    \n      \n        Applications without synthetic transactions:\n      \n      \n         \n         \n      \n          \n            Missed edge cases: Synthetic transactions help you test paths and functions that might not be frequently used by typical users but are critical to certain business functions. Without them, these paths could malfunction and go unnoticed.\n          \n        \n          \n            Checking for issues when the application is not being used: Regular synthetic testing can simulate times when real users aren't actively interacting with your application, ensuring the system always functions correctly.\n          \n        \n    \n    Benefits of establishing this best\n    practice: \n  \n     \n     \n     \n     \n     \n  \n      \n        Proactive issue detection: Identify and address potential issues before they impact real users.\n      \n    \n      \n        Optimized user experience: Continuous feedback from RUM aids in refining and enhancing the overall user experience.\n      \n    \n      \n        Insights on device and browser performance: Understand how your application performs across various devices and browsers, enabling further optimization.\n      \n    \n      \n        Validated business workflows: Regular synthetic transactions ensure that core functionalities and critical paths remain operational and efficient.\n      \n    \n      \n        Enhanced application performance: Leverage insights gathered from real user data to improve application responsiveness and reliability.\n      \n    \n    Level of risk exposed if this best practice\n    is not established: High\n  \n\n  Implementation guidance\n    \n      To leverage RUM and synthetic transactions for user activity telemetry, AWS offers services like Amazon CloudWatch RUM and Amazon CloudWatch Synthetics. Metrics, logs, and traces, coupled with user activity data, provide a comprehensive view of both the application's operational state and the user experience.\n    \n     \n      \n      Implementation steps\n      \n         \n         \n         \n      \n          \n            Deploy Amazon CloudWatch RUM: Integrate your application with CloudWatch RUM to collect, analyze, and present real user data.\n          \n          \n             \n             \n          \n              \n                Use the CloudWatch RUM JavaScript library to integrate RUM with your application.\n              \n            \n              \n                Set up dashboards to visualize and monitor real user data.\n              \n            \n        \n          \n            Configure CloudWatch Synthetics: Create canaries, or scripted routines, that simulate user interactions with your application. \n          \n          \n             \n             \n             \n          \n              \n                Define critical application workflows and paths.\n              \n            \n              \n                Design canaries using CloudWatch Synthetics scripts to simulate user interactions for these paths.\n              \n            \n              \n                Schedule and monitor canaries to run at specified intervals, ensuring consistent performance checks.\n              \n            \n        \n          \n            Analyze and act on data: Utilize data from RUM and synthetic transactions to gain insights and take corrective measures when anomalies are detected. Use CloudWatch dashboards and alarms to stay informed.\n          \n        \n     \n    \n      Level of effort for the implementation plan: Medium\n    \n   \n    \n    Resources\n    \n      Related best practices:\n    \n    \n       \n       \n       \n       \n    \n        \n          OPS04-BP01 Identify key performance indicators\n        \n      \n        \n          OPS04-BP02 Implement application telemetry\n        \n      \n        \n          OPS04-BP04 Implement dependency telemetry\n        \n      \n        \n          OPS04-BP05 Implement distributed tracing\n        \n      \n    \n      Related documents:\n    \n    \n       \n       \n    \n        \n          Amazon CloudWatch RUM Guide\n        \n      \n        \n          Amazon CloudWatch Synthetics Guide\n        \n      \n    \n      Related videos:\n    \n    \n        \n       \n    \n        \n          Optimize applications through end user insights with Amazon CloudWatch RUM\n        \n      \n        AWS on Air ft. Real-User Monitoring for Amazon CloudWatch\n        \n      \n    \n      Related examples:\n    \n    \n       \n       \n       \n    \n        \n          One Observability Workshop\n        \n      \n        \n          Git Repository for Amazon CloudWatch RUM Web Client\n        \n      \n        \n          Using Amazon CloudWatch Synthetics to measure page load time\n        \n      \n  \u003cdiv\u003e\u003cdiv\u003e\u003cdiv\u003e\u003cdiv id=\"js_error_message\"\u003e\u003cp\u003e\u003cimg src=\"https://d1ge0kk1l5kms0.cloudfront.net/images/G/01/webservices/console/warning.png\" alt=\"Warning\" /\u003e \u003cstrong\u003eJavascript is disabled or is unavailable in your browser.\u003c/strong\u003e\u003c/p\u003e\u003cp\u003eTo use the Amazon Web Services Documentation, Javascript must be enabled. Please refer to your browser's Help pages for instructions.\u003c/p\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003eDocument ConventionsOPS04-BP02 Implement application telemetryOPS04-BP04 Implement dependency telemetry",
  "OPS04-BP04 Implement dependency telemetry\n    Dependency telemetry is essential for monitoring the health and\n    performance of the external services and components your workload\n    relies on. It provides valuable insights into reachability,\n    timeouts, and other critical events related to dependencies such as\n    DNS, databases, or third-party APIs. When you instrument your\n    application to emit metrics, logs, and traces about these\n    dependencies, you gain a clearer understanding of potential\n    bottlenecks, performance issues, or failures that might impact your\n    workload.\n  \n    Desired outcome: Ensure that the\n    dependencies your workload relies on are performing as expected,\n    allowing you to proactively address issues and ensure optimal\n    workload performance.\n  \n    Common anti-patterns:\n  \n     \n     \n     \n  \n      \n        Overlooking external dependencies: Focusing only on internal\n        application metrics while neglecting metrics related to external\n        dependencies.\n      \n    \n      \n        Lack of proactive monitoring: Waiting for issues to arise\n        instead of continuously monitoring dependency health and\n        performance.\n      \n    \n      \n        Siloed monitoring: Using multiple, disparate monitoring tools\n        which can result in fragmented and inconsistent views of\n        dependency health.\n      \n    \n    Benefits of establishing this best\n    practice:\n  \n     \n     \n     \n     \n  \n      \n        Improved workload reliability: By ensuring that external\n        dependencies are consistently available and performing\n        optimally.\n      \n    \n      \n        Faster issue detection and resolution: Proactively identifying\n        and addressing issues with dependencies before they impact the\n        workload.\n      \n    \n      \n        Comprehensive view: Gaining a holistic view of both internal and\n        external components that influence workload health.\n      \n    \n      \n        Enhanced workload scalability: By understanding the scalability\n        limits and performance characteristics of external dependencies.\n      \n    \n    Level of risk exposed if this best practice\n    is not established: High\n  \n\n  Implementation guidance\n\n      \n    \n      Implement dependency telemetry by starting with identifying the\n      services, infrastructure, and processes that your workload depends\n      on. Quantify what good conditions look like when those\n      dependencies are functioning as expected, and then determine what\n      data will be needed to measure those. With that information you\n      can craft dashboards and alerts that provide insights to your\n      operations teams on the state of those dependencies. Use AWS tools\n      to discover and quantify the impacts when dependencies cannot\n      deliver as needed. Continually revisit your strategy to account\n      for changes in priorities, goals, and gained insights.\n    \n     \n\n  Implementation steps\n\n      \n      \n        To implement dependency telemetry effectively:\n      \n      \n         \n         \n         \n         \n         \n         \n         \n         \n      \n          \n            Identify external\n            dependencies: Collaborate with stakeholders to\n            pinpoint the external dependencies your workload relies on.\n            External dependencies can encompass services like external\n            databases, third-party APIs, network connectivity routes to\n            other environments, and DNS services. The first step towards\n            effective dependency telemetry is being comprehensive in\n            understanding what those dependencies are.\n          \n        \n          \n            Develop a monitoring\n            strategy: Once you have a clear picture of your\n            external dependencies, architect a monitoring strategy\n            tailored to them. This involves understanding the\n            criticality of each dependency, its expected behavior, and\n            any associated service-level agreements or targets (SLA or\n            SLTs). Set up proactive alerts to notify you of status\n            changes or performance deviations.\n          \n        \n          \n            Use\n            network\n            monitoring: Use\n            Internet\n            Monitor and\n            Network\n            Monitor, which provide comprehensive insights into\n            global internet and network conditions. These tools help you\n            understand and respond to outages, disruptions, or\n            performance degradations that affect your external\n            dependencies.\n          \n        \n          \n            Stay informed with\n              AWS Health: AWS Health is the authoritative source of information about the health of your AWS Cloud resources. Use AWS Health to visualize and receive notifications about any current service events and upcoming changes, such as planned lifecycle events, so you can take steps to mitigate impacts.\n          \n          \n             \n             \n             \n          \n              \n                Create purpose-fit AWS Health event notifications to e-mail and chat channels through AWS User Notifications, and integrate programatically with your monitoring and alerting tools through Amazon EventBridge or the AWS Health API.\n              \n            \n              \n                Plan and track progress on health events that require action by integrating with change management or ITSM tools (like Jira or ServiceNow) that you may already use through Amazon EventBridge or the AWS Health API.\n              \n            \n              \n                If you use AWS Organizations, enable\n                organization view for \n                AWS Health to aggregate AWS Health events across accounts.\n              \n            \n        \n          \n            Instrument your application with\n            AWS X-Ray: AWS X-Ray provides insights into\n            how applications and their underlying dependencies are\n            performing. By tracing requests from start to end, you can\n            identify bottlenecks or failures in the external services or\n            components your application relies on.\n          \n        \n          \n            Use\n            Amazon DevOps Guru: This machine learning-driven\n            service identifies operational issues, predicts when\n            critical issues might occur, and recommends specific actions\n            to take. It's invaluable for gaining insights into\n            dependencies and ensuring they're not the source of\n            operational problems.\n          \n        \n          \n            Monitor regularly:\n            Continually monitor metrics and logs related to external\n            dependencies. Set up alerts for unexpected behavior or\n            degraded performance.\n          \n        \n          \n            Validate after changes:\n            Whenever there's an update or change in any of the external\n            dependencies, validate their performance and check their\n            alignment with your application's requirements.\n          \n        \n      \n        Level of effort for the implementation\n        plan: Medium\n      \n     \n   \n\n  Resources\n\n      \n    \n      Related best practices:\n    \n    \n       \n       \n       \n       \n       \n    \n        \n          OPS04-BP01\n          Define workload KPIs\n        \n      \n        \n          OPS04-BP02\n          Implement application telemetry\n        \n      \n        \n          OPS04-BP03\n          Implement user activity telemetry\n        \n      \n        \n          OPS04-BP05\n          Implement transaction traceability\n        \n      \n        \n          OP08-BP04\n          Create actionable alerts\n        \n      \n    \n      Related documents:\n    \n    \n       \n       \n       \n       \n    \n        \n          Amazon\n          Personal AWS Health Dashboard User Guide\n        \n      \n        \n          AWS           Internet Monitor User Guide\n        \n      \n        \n          AWS X-Ray Developer Guide\n        \n      \n        \n          AWS           DevOps Guru User Guide\n        \n      \n    \n      Related videos:\n    \n    \n       \n       \n       \n    \n        \n          Visibility\n          into how internet issues impact app performance\n        \n      \n        \n          Introduction\n          to Amazon DevOps Guru\n        \n      \n        \n          Manage\n          resource lifecycle events at scale with AWS Health\n        \n      \n    \n      Related examples:\n    \n    \n       \n       \n    \n        \n          AWS Health Aware\n        \n      \n        \n          Using\n          Tag-Based Filtering to Manage AWS Health Monitoring and\n          Alerting at Scale\n        \n      \n  \u003cdiv\u003e\u003cdiv\u003e\u003cdiv\u003e\u003cdiv id=\"js_error_message\"\u003e\u003cp\u003e\u003cimg src=\"https://d1ge0kk1l5kms0.cloudfront.net/images/G/01/webservices/console/warning.png\" alt=\"Warning\" /\u003e \u003cstrong\u003eJavascript is disabled or is unavailable in your browser.\u003c/strong\u003e\u003c/p\u003e\u003cp\u003eTo use the Amazon Web Services Documentation, Javascript must be enabled. Please refer to your browser's Help pages for instructions.\u003c/p\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003eDocument ConventionsOPS04-BP03 Implement user experience telemetryOPS04-BP05 Implement distributed tracing",
  "OPS04-BP05 Implement distributed tracing\n    Distributed tracing offers a way to monitor and visualize requests as they traverse through various components of a distributed system. By capturing trace data from multiple sources and analyzing it in a unified view, teams can better understand how requests flow, where bottlenecks exist, and where optimization efforts should focus.\n  \n    Desired outcome: Achieve a holistic view of requests flowing through your distributed system, allowing for precise debugging, optimized performance, and improved user experiences.\n  \n    Common anti-patterns:\n  \n     \n     \n  \n      \n        Inconsistent instrumentation: Not all services in a distributed system are instrumented for tracing.\n      \n    \n      \n        Ignoring latency: Only focusing on errors and not considering the latency or gradual performance degradations.\n      \n    \n    Benefits of establishing this best\n    practice: \n  \n     \n     \n     \n  \n      Comprehensive system overview: Visualizing the entire path of requests, from entry to exit.\n    \n      \n        Enhanced debugging: Quickly identifying where failures or performance issues occur.\n      \n    \n      \n        Improved user experience: Monitoring and optimizing based on actual user data, ensuring the system meets real-world demands.\n      \n    \n    Level of risk exposed if this best practice\n    is not established: High\n  \n\n  Implementation guidance\n    \n      Begin by identifying all of the elements of your workload that require instrumentation. Once all components are accounted for, leverage tools such as AWS X-Ray and OpenTelemetry to gather trace data for analysis with tools like X-Ray and Amazon CloudWatch ServiceLens Map. Engage in regular reviews with developers, and supplement these discussions with tools like Amazon DevOps Guru, X-Ray Analytics and X-Ray Insights to help uncover deeper findings. Establish alerts from trace data to notify when outcomes, as defined in the workload monitoring plan, are at risk.\n    \n     \n      \n      Implementation steps\n      \n        To implement distributed tracing effectively:\n      \n      \n         \n         \n         \n         \n         \n         \n         \n         \n      \n          \n            Adopt AWS X-Ray: Integrate X-Ray into your application to gain insights into its behavior, understand its performance, and pinpoint bottlenecks. Utilize X-Ray Insights for automatic trace analysis.\n          \n        \n          \n            Instrument your services: Verify that every service, from an AWS Lambda function to an EC2 instance, sends trace data. The more services you instrument, the clearer the end-to-end view.\n          \n        \n          \n            Incorporate CloudWatch Real User Monitoring and synthetic monitoring: Integrate Real User Monitoring (RUM) and synthetic monitoring with X-Ray. This allows for capturing real-world user experiences and simulating user interactions to identify potential issues.\n          \n        \n          \n            Use the CloudWatch agent: The agent can send traces from either X-Ray or OpenTelemetry, enhancing the depth of insights obtained.\n           \n        \n          \n            Use Amazon DevOps Guru: DevOps Guru uses data from X-Ray, CloudWatch, AWS Config, and AWS CloudTrail to provide actionable recommendations.\n          \n        \n          \n            Analyze traces: Regularly review the trace data to discern patterns, anomalies, or bottlenecks that might impact your application's performance.\n          \n        \n          \n            Set up alerts: Configure alarms in CloudWatch for unusual patterns or extended latencies, allowing proactive issue addressing.\n          \n        \n          \n            Continuous improvement: Revisit your tracing strategy as services are added or modified to capture all relevant data points.\n          \n        \n     \n    \n      Level of effort for the implementation plan: Medium\n    \n   \n\n  Resources\n    \n      Related best practices:\n    \n    \n       \n       \n       \n       \n    \n        \n          OPS04-BP01 Identify key performance indicators\n        \n      \n        \n          OPS04-BP02 Implement application telemetry\n        \n      \n        \n          OPS04-BP03 Implement user experience telemetry\n        \n      \n        \n          OPS04-BP04 Implement dependency telemetry\n        \n      \n\n      \n    \n      Related documents:\n    \n    \n       \n       \n       \n    \n        AWS X-Ray Developer Guide\n        \n      \n        \n          Amazon CloudWatch agent User Guide\n        \n      \n        \n          Amazon DevOps Guru User Guide\n        \n      \n    \n      Related videos:\n    \n    \n       \n       \n    \n        \n          Use AWS X-Ray Insights\n        \n      \n        AWS on Air ft. Observability: Amazon CloudWatch and AWS X-Ray\n      \n    \n      Related examples:\n    \n    \n       \n    \n        \n          Instrumenting your application for AWS X-Ray\n      \n  \u003cdiv\u003e\u003cdiv\u003e\u003cdiv\u003e\u003cdiv id=\"js_error_message\"\u003e\u003cp\u003e\u003cimg src=\"https://d1ge0kk1l5kms0.cloudfront.net/images/G/01/webservices/console/warning.png\" alt=\"Warning\" /\u003e \u003cstrong\u003eJavascript is disabled or is unavailable in your browser.\u003c/strong\u003e\u003c/p\u003e\u003cp\u003eTo use the Amazon Web Services Documentation, Javascript must be enabled. Please refer to your browser's Help pages for instructions.\u003c/p\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003eDocument ConventionsOPS04-BP04 Implement dependency telemetry OPS 5. How do you reduce defects, ease remediation, and improve flow into\n                  production? ",
  "OPS05-BP01 Use version control\n    Use version control to activate tracking of changes and releases.\n  \n    Many AWS services offer version control capabilities. Use a revision\n    or source control system such as\n    Git to manage code and other artifacts such as\n    version-controlled\n    AWS CloudFormation templates of your infrastructure.\n  \n    Desired outcome: Your teams collaborate on code.  When merged, the code is consistent and no changes are lost. Errors are easily reverted through correct versioning.\n  \n    Common anti-patterns:\n  \n     \n     \n     \n     \n  \n      \n        You have been developing and storing your code on your\n        workstation. You have had an unrecoverable storage failure on\n        the workstation and your code is lost.\n      \n    \n      \n        After overwriting the existing code with your changes, you\n        restart your application and it is no longer operable. You are\n        unable to revert the change.\n      \n    \n      \n        You have a write lock on a report file that someone else needs\n        to edit. They contact you asking that you stop work on it so\n        that they can complete their tasks.\n      \n    \n      \n        Your research team has been working on a detailed analysis that\n        shapes your future work. Someone has accidentally saved\n        their shopping list over the final report. You are unable to\n        revert the change and have to recreate the report.\n      \n    \n    Benefits of establishing this best\n    practice: By using version control capabilities you can\n    easily revert to known good states and previous versions, and limit the\n    risk of assets being lost.\n  \n    Level of risk exposed if this best practice\n    is not established: High\n  \n\n  Implementation guidance\n    \n      Maintain assets in version controlled repositories. Doing so supports tracking changes, deploying new versions, detecting changes to existing versions, and reverting to prior versions (for example, rolling back to a known good state in the event of a failure). Integrate the version control capabilities of your configuration management systems into your procedures.\n    \n   \n\n  Resources\n\n    \n      Related best practices:\n    \n    \n       \n    \n        \n          OPS05-BP04 Use build and deployment management systems\n        \n      \n    \n      Related videos:\n    \n    \n       \n       \n    \n        AWS re:Invent 2023 - How Lockheed Martin builds software faster, powered by DevSecOps\n        \n      \n        AWS re:Invent 2023 - How GitHub operationalizes AI for team collaboration and productivity\n        \n      \n  \u003cdiv\u003e\u003cdiv\u003e\u003cdiv\u003e\u003cdiv id=\"js_error_message\"\u003e\u003cp\u003e\u003cimg src=\"https://d1ge0kk1l5kms0.cloudfront.net/images/G/01/webservices/console/warning.png\" alt=\"Warning\" /\u003e \u003cstrong\u003eJavascript is disabled or is unavailable in your browser.\u003c/strong\u003e\u003c/p\u003e\u003cp\u003eTo use the Amazon Web Services Documentation, Javascript must be enabled. Please refer to your browser's Help pages for instructions.\u003c/p\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003eDocument Conventions OPS 5. How do you reduce defects, ease remediation, and improve flow into\n                  production? OPS05-BP02 Test and validate changes",
  "OPS05-BP02 Test and validate changes\n    Every change deployed must be tested to avoid errors in production.\n    This best practice is focused on testing changes from version\n    control to artifact build. Besides application code changes, testing\n    should include infrastructure, configuration, security controls, and\n    operations procedures. Testing takes many forms, from unit tests to\n    software component analysis (SCA). Move tests further to the left in\n    the software integration and delivery process results in higher\n    certainty of artifact quality.\n  \n    Your organization must develop testing standards for all software\n    artifacts. Automated tests reduce toil and avoid manual test errors.\n    Manual tests may be necessary in some cases. Developers must have\n    access to automated test results to create feedback loops that\n    improve software quality.\n  \n    Desired outcome: Your software\n    changes are tested before they are delivered. Developers have access\n    to test results and validations. Your organization has a testing\n    standard that applies to all software changes.\n  \n    Common anti-patterns:\n  \n     \n     \n     \n  \n      \n        You deploy a new software change without any tests. It fails to\n        run in production, which leads to an outage.\n      \n    \n      \n        New security groups are deployed with AWS CloudFormation without\n        being tested in a pre-production environment. The security\n        groups make your app unreachable for your customers.\n      \n    \n      \n        A method is modified but there are no unit tests. The software\n        fails when it is deployed to production.\n      \n    \n    Benefits of establishing this best\n    practice: Change fail rate of software deployments are\n    reduced. Software quality is improved. Developers have increased\n    awareness on the viability of their code. Security policies can be\n    rolled out with confidence to support organization's compliance.\n    Infrastructure changes such as automatic scaling policy updates are\n    tested in advance to meet traffic needs.\n  \n    Level of risk exposed if this best practice\n    is not established: High\n  \n\n  Implementation guidance\n\n      \n    \n      Testing is done on all changes, from application code to\n      infrastructure, as part of your continuous integration practice.\n      Test results are published so that developers have fast feedback.\n      Your organization has a testing standard that all changes must\n      pass.\n    \n    \n      Use the power of generative AI with Amazon Q Developer to improve\n      developer productivity and code quality. Amazon Q Developer includes\n      generation of code suggestions (based on large language models),\n      production of unit tests (including boundary conditions), and code\n      security enhancements through detection and remediation of security\n      vulnerabilities.\n    \n    \n      Customer example\n    \n    \n      As part of their continuous integration pipeline, AnyCompany\n      Retail conducts several types of tests on all software artifacts.\n      They practice test driven development so all software has unit\n      tests. Once the artifact is built, they run end-to-end tests.\n      After this first round of tests is complete, they run a static\n      application security scan, which looks for known vulnerabilities.\n      Developers receive messages as each testing gate is passed. Once\n      all tests are complete, the software artifact is stored in an\n      artifact repository.\n    \n     \n\n  Implementation steps\n\n      \n      \n         \n         \n      \n          \n            Work with stakeholders in your organization to develop a\n            testing standard for software artifacts. What standard tests\n            should all artifacts pass? Are there compliance or\n            governance requirements that must be included in the test\n            coverage? Do you need to conduct code quality tests? When\n            tests complete, who needs to know?\n          \n          \n             \n          \n              \n                The\n                AWS                 Deployment Pipeline Reference Architecture\n                contains an authoritative list of types of tests that\n                can be conducted on software artifacts as part of an\n                integration pipeline.\n              \n            \n        \n          \n            Instrument your application with the necessary tests based\n            on your software testing standard. Each set of tests should\n            complete in under ten minutes. Tests should run as part of\n            an integration pipeline.\n          \n          \n             \n             \n             \n             \n          \n              \n                Use\n                Amazon Q Developer, a generative AI tool that can help\n                create unit test cases (including boundary conditions),\n                generate functions using code and comments, and\n                implement well-known algorithms.\n              \n            \n              \n                Use\n                Amazon CodeGuru Reviewer to test your application code\n                for defects.\n              \n            \n              \n                You can use\n                AWS CodeBuild to conduct tests on software artifacts.\n              \n            \n              \n                AWS CodePipeline can orchestrate your software tests\n                into a pipeline.\n              \n            \n        \n     \n   \n\n  Resources\n\n      \n    \n      Related best practices:\n    \n    \n       \n       \n       \n       \n    \n        \n          OPS05-BP01\n          Use version control\n        \n      \n        \n          OPS05-BP06\n          Share design standards\n        \n      \n        \n          OPS05-BP07\n          Implement practices to improve code quality\n        \n      \n        \n          OPS05-BP10\n          Fully automate integration and deployment\n        \n      \n    \n      Related documents:\n    \n    \n       \n       \n       \n       \n       \n       \n       \n       \n       \n       \n       \n       \n       \n       \n    \n        \n          Adopt\n          a test-driven development approach\n        \n      \n        \n          Accelerate\n          your Software Development Lifecycle with Amazon Q\n        \n      \n        \n          Amazon Q Developer, now generally available, includes previews of new\n          capabilities to reimagine developer experience\n        \n      \n        \n          The\n          Ultimate Cheat Sheet for Using Amazon Q Developer in Your\n          IDE\n        \n      \n        \n          Shift-Left\n          Workload, leveraging AI for Test Creation\n        \n      \n        \n          Amazon Q Developer Center\n        \n      \n        \n          10\n          ways to build applications faster with Amazon CodeWhisperer\n        \n      \n        \n          Looking\n          beyond code coverage with Amazon CodeWhisperer\n        \n      \n        \n          Best\n          Practices for Prompt Engineering with Amazon CodeWhisperer\n        \n      \n        \n          Automated\n          AWS CloudFormation Testing Pipeline with TaskCat and\n          CodePipeline\n        \n      \n        \n          Building\n          end-to-end AWS DevSecOps CI/CD pipeline with open source SCA,\n          SAST, and DAST tools\n        \n      \n        \n          Getting\n          started with testing serverless applications\n        \n      \n        \n          My\n          CI/CD pipeline is my release captain\n        \n      \n        \n          Practicing\n          Continuous Integration and Continuous Delivery on AWS\n          Whitepaper\n        \n      \n    \n      Related videos:\n    \n    \n       \n       \n       \n       \n       \n       \n    \n        \n          Implement\n          an API with Amazon Q Developer Agent for Software\n          Development\n        \n      \n        \n          Installing,\n          Configuring, \u0026 Using Amazon Q Developer with JetBrains\n          IDEs (How-to)\n        \n      \n        \n          Mastering\n          the art of Amazon CodeWhisperer - YouTube playlist\n        \n      \n        \n          AWS           re:Invent 2020: Testable infrastructure: Integration testing\n          on AWS\n        \n      \n        \n          AWS           Summit ANZ 2021 - Driving a test-first strategy with CDK and\n          test driven development\n        \n      \n        \n          Testing\n          Your Infrastructure as Code with AWS CDK\n        \n      \n    \n      Related resources:\n    \n    \n       \n       \n       \n       \n    \n        \n          AWS           Deployment Pipeline Reference Architecture -\n          Application\n        \n      \n        \n          AWS           Kubernetes DevSecOps Pipeline\n        \n      \n        \n          Run\n          unit tests for a Node.js application from GitHub by using AWS CodeBuild\n        \n      \n        \n          Use\n          Serverspec for test-driven development of infrastructure\n          code\n        \n      \n    \n      Related services:\n    \n    \n       \n       \n       \n       \n    \n        \n          Amazon Q Developer\n        \n      \n        \n          Amazon CodeGuru Reviewer\n        \n      \n        \n          AWS CodeBuild\n        \n      \n        \n          AWS CodePipeline\n        \n      \n  \u003cdiv\u003e\u003cdiv\u003e\u003cdiv\u003e\u003cdiv id=\"js_error_message\"\u003e\u003cp\u003e\u003cimg src=\"https://d1ge0kk1l5kms0.cloudfront.net/images/G/01/webservices/console/warning.png\" alt=\"Warning\" /\u003e \u003cstrong\u003eJavascript is disabled or is unavailable in your browser.\u003c/strong\u003e\u003c/p\u003e\u003cp\u003eTo use the Amazon Web Services Documentation, Javascript must be enabled. Please refer to your browser's Help pages for instructions.\u003c/p\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003eDocument ConventionsOPS05-BP01 Use version controlOPS05-BP03 Use configuration management systems",
  "OPS05-BP03 Use configuration management systems\n    Use configuration management systems to make and track configuration changes. These systems reduce errors caused by manual processes and reduce the level of effort to deploy changes.\n    \n  Static configuration management sets values when initializing a resource that are expected to remain consistent throughout the resource’s lifetime. Dynamic configuration management sets values at initialization that can or are expected to change during the lifetime of a resource. For example, you could set a feature toggle to activate functionality in your code through a configuration change, or change the level of log detail during an incident.Configurations should be deployed in a known and consistent state. You should use automated inspection to continually monitor resource configurations across environments and regions. These controls should be defined as code and management automated to ensure rules are consistently appplied across environments. Changes to configurations should be updated through agreed change control procedures and applied consistently, honoring version control. Application configuration should be managed independently of application and infrastructure code. This allows for consistent deployment across multiple environments. Configuration changes do not result in rebuilding or redeploying the application.\n  \n    Desired outcome: You configure, validate, and deploy as part of your continuous integration, continuous delivery (CI/CD) pipeline. You monitor to validate configurations are correct. This minimizes any impact to end users and customers.\n  \n    Common anti-patterns:\n  \n     \n     \n     \n     \n  \n      \n        You manually update the web server configuration across your\n        fleet and a number of servers become unresponsive due to update\n        errors.\n      \n    \n      \n        You manually update your application server fleet over the\n        course of many hours. The inconsistency in configuration during\n        the change causes unexpected behaviors.\n      \n    \n      \n        Someone has updated your security groups and your web servers\n        are no longer accessible. Without knowledge of what was changed\n        you spend significant time investigating the issue extending\n        your time to recovery.\n      \n    \n      \n        You push a pre-production configuration into production through CI/CD without validation. You expose users and customers to incorrect data and services.\n      \n    \n    Benefits of establishing this best\n      practice: Adopting configuration management systems reduces the level of effort to make and track changes, and the frequency of errors caused by manual procedures. Configuration management systems provide assurances with regards to governance, compliance, and regulatory requirements.\n    \n  \n    Level of risk exposed if this best practice\n    is not established: Medium\n  \n\n  Implementation guidance\n    \n      Configuration management systems are used to track and implement changes to application and environment configurations. Configuration management systems are also used to reduce errors caused by manual processes, make configuration changes repeatable and auditable, and reduce the level of effort.\n    \n    \n      On AWS, you can use\n      AWS Config to continually monitor your AWS resource\n      configurations\n      across\n        accounts and Regions. It helps you to track their\n      configuration history, understand how a configuration change would\n      affect other resources, and audit them against expected or desired\n      configurations using\n      AWS Config Rules and\n      AWS Config Conformance Packs.\n    \n    \n      For dynamic configurations in your applications running on\n      Amazon EC2 instances, AWS Lambda, containers, mobile applications, or IoT devices, you can use\n      AWS AppConfig to configure, validate, deploy, and monitor them across your environments.\n    \n     \n      \n      Implementation steps\n      \n         \n         \n         \n         \n         \n         \n      \n          \n            Identify configuration owners.\n          \n          \n             \n          \n              \n                Make configurations owners aware of any compliance, governance, or regulatory needs.\n              \n            \n        \n          \n            Identify configuration items and deliverables.\n          \n          \n             \n             \n          \n              \n                Configuration items are all application and environmental configurations affected by a deployment within your CI/CD pipeline.\n              \n            \n              \n                Deliverables include success criteria, validation, and what to monitor.\n              \n            \n        \n          \n            Select tools for configuration management based on your business requirements and delivery pipeline.\n            \n          \n        \n          \n            Consider weighted deployments such as canary deployments for significant configuration changes to minimize the impact of incorrect configurations.\n          \n        \n          \n            Integrate your configuration management into your CI/CD pipeline.\n          \n        \n          \n            Validate all changes pushed.\n          \n        \n     \n   \n\n  Resources\n\n    \n      Related best practices:\n    \n    \n       \n       \n       \n       \n    \n        \n          OPS06-BP01 Plan for unsuccessful changes\n        \n      \n        \n          OPS06-BP02 Test deployments\n        \n      \n        \n          OPS06-BP03 Employ safe deployment strategies\n        \n      \n        \n          OPS06-BP04 Automate testing and rollback\n        \n      \n    \n      Related documents:\n    \n    \n       \n       \n       \n       \n       \n       \n       \n       \n       \n       \n    \n        AWS Control Tower\n      \n        AWS Landing Zone Accelerator\n        \n      \n        AWS Config\n      \n        \n          What is AWS Config?\n        \n      \n        \n          AWS AppConfig\n        \n      \n        \n          What is AWS CloudFormation?\n        \n      \n        \n          AWS           Developer Tools\n        \n      \n        AWS CodeBuild\n      \n        AWS CodePipeline\n      \n        AWS CodeDeploy\n      \n    \n      Related videos:\n    \n    \n       \n       \n       \n    \n        AWS re:Invent 2022 - Proactive governance and compliance for AWS workloads\n        \n      \n        AWS re:Invent 2020: Achieve compliance as code using AWS Config\n      \n        \n          Manage and Deploy Application Configurations with AWS AppConfig\n      \n  \u003cdiv\u003e\u003cdiv\u003e\u003cdiv\u003e\u003cdiv id=\"js_error_message\"\u003e\u003cp\u003e\u003cimg src=\"https://d1ge0kk1l5kms0.cloudfront.net/images/G/01/webservices/console/warning.png\" alt=\"Warning\" /\u003e \u003cstrong\u003eJavascript is disabled or is unavailable in your browser.\u003c/strong\u003e\u003c/p\u003e\u003cp\u003eTo use the Amazon Web Services Documentation, Javascript must be enabled. Please refer to your browser's Help pages for instructions.\u003c/p\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003eDocument ConventionsOPS05-BP02 Test and validate changesOPS05-BP04 Use build and deployment management systems",
  "OPS05-BP04 Use build and deployment management systems\n    Use build and deployment management systems. These systems reduce errors caused by manual processes and reduce the level of effort to deploy changes.\n  \n    In AWS, you can build continuous integration/continuous deployment\n    (CI/CD) pipelines using services such as\n    AWS     Developer Tools (for example,\n    AWS CodeBuild,\n    AWS CodePipeline, and\n    AWS CodeDeploy).\n  \n    Desired outcome: Your build and deployment management systems support your organization's continuous integration continuous delivery (CI/CD) system that provide capabilities for automating safe rollouts with the correct configurations.\n  \n    Common anti-patterns:\n  \n     \n     \n     \n     \n  \n      \n        After compiling your code on your development system, you copy\n        the executable onto your production systems and it fails to\n        start. The local log files indicates that it has failed due to\n        missing dependencies.\n      \n    \n      \n        You successfully build your application with new features in\n        your development environment and provide the code to quality\n        assurance (QA). It fails QA because it is missing static assets.\n      \n    \n      \n        On Friday, after much effort, you successfully built your\n        application manually in your development environment including\n        your newly coded features. On Monday, you are unable to repeat\n        the steps that allowed you to successfully build your\n        application.\n      \n    \n      \n        You perform the tests you have created for your new release.\n        Then you spend the next week setting up a test environment and\n        performing all the existing integration tests followed by the\n        performance tests. The new code has an unacceptable performance\n        impact and must be redeveloped and then retested.\n      \n    \n    Benefits of establishing this best\n    practice: By providing mechanisms to manage build and\n    deployment activities you reduce the level of effort to perform\n    repetitive tasks, free your team members to focus on their high\n    value creative tasks, and limit the introduction of error from\n    manual procedures.\n  \n    Level of risk exposed if this best practice\n    is not established: Medium\n  \n\n  Implementation guidance\n    \n      Build and deployment management systems are used to track and implement change, reduce errors caused by manual processes, and reduce the level of effort required for safe deployments. Fully automate the integration and deployment pipeline from code check-in through build, testing, deployment, and validation. This reduces lead time, decreases cost, encourages increased frequency of change, reduces the level of effort, and increases collaboration.\n    \n     \n      \n      Implementation steps\n     \n    \n       \n        \n       \n       \n      Diagram showing a CI/CD pipeline using AWS CodePipeline and related services\n    \n     \n    \n       \n       \n       \n       \n    \n        \n          Use a version control system to store and manage assets (such as documents, source code, and binary files).\n        \n      \n        \n          Use CodeBuild to compile your source code, runs unit tests, and produces artifacts that are ready to deploy.\n        \n      \n        \n          Use CodeDeploy as a deployment service that automates application deployments to Amazon EC2 instances, on-premises instances, serverless AWS Lambda functions, or Amazon ECS.\n        \n      \n        \n          Monitor your deployments.\n        \n      \n   \n\n  Resources\n\n    \n      Related best practices:\n    \n    \n       \n    \n        \n          OPS06-BP04 Automate testing and rollback\n        \n      \n    \n      Related documents:\n    \n    \n       \n       \n       \n       \n    \n        \n          AWS           Developer Tools\n        \n      \n        \n          What\n          is AWS CodeBuild?\n        \n      \n        AWS CodeBuild\n      \n        \n          What\n          is AWS CodeDeploy?\n        \n      \n    \n      Related videos:\n    \n    \n       \n    \n        AWS re:Invent 2022 - AWS Well-Architected best practices for DevOps on AWS\n      \n  \u003cdiv\u003e\u003cdiv\u003e\u003cdiv\u003e\u003cdiv id=\"js_error_message\"\u003e\u003cp\u003e\u003cimg src=\"https://d1ge0kk1l5kms0.cloudfront.net/images/G/01/webservices/console/warning.png\" alt=\"Warning\" /\u003e \u003cstrong\u003eJavascript is disabled or is unavailable in your browser.\u003c/strong\u003e\u003c/p\u003e\u003cp\u003eTo use the Amazon Web Services Documentation, Javascript must be enabled. Please refer to your browser's Help pages for instructions.\u003c/p\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003eDocument ConventionsOPS05-BP03 Use configuration management systemsOPS05-BP05 Perform patch management",
  "OPS05-BP05 Perform patch management\n    Perform patch management to gain features, address issues, and remain compliant with governance. Automate patch management to reduce errors caused by manual processes, scale, and reduce the level of effort to patch.\n  \n    Patch and vulnerability management are part of your benefit and risk\n    management activities. It is preferable to have immutable\n    infrastructures and deploy workloads in verified known good states.\n    Where that is not viable, patching in place is the remaining option.\n  \n    AWS Health is the authoritative source of information about planned lifecycle events and other action-required events that affect the health of your AWS Cloud resources. You should be aware of upcoming changes and updates that should be performed. Major planned lifecycle events are sent at least six months in advance.\n  \n    Amazon EC2 Image Builder provides pipelines to update machine images. As a part of patch management, consider Amazon Machine Images (AMIs) using an AMI image pipeline or container images with a Docker image pipeline, while AWS Lambda provides patterns for custom runtimes and additional libraries to remove vulnerabilities. \n  \n    You should manage updates to Amazon Machine Images for Linux or Windows Server images using Amazon EC2 Image Builder. You can use Amazon Elastic Container Registry (Amazon ECR) with your existing pipeline to manage Amazon ECS images and manage Amazon EKS images. Lambda includes version management features.\n  \n    Patching should not be performed on production systems without first\n    testing in a safe environment. Patches should only be applied if\n    they support an operational or business outcome. On AWS, you can use\n    AWS Systems Manager Patch Manager to automate the process of\n    patching managed systems and schedule the activity using\n    Systems Manager Maintenance Windows.\n  \n    Desired outcome: Your AMI and container images are patched, up-to-date, and ready for launch.  You are able to track the status of all deployed images and know patch compliance.  You are able to report on current status and have a process to meet your compliance needs.\n  \n    Common anti-patterns:\n  \n     \n     \n     \n     \n     \n     \n     \n  \n      \n        You are given a mandate to apply all new security patches within\n        two hours resulting in multiple outages due to application\n        incompatibility with patches.\n      \n    \n      \n        An unpatched library results in unintended consequences as\n        unknown parties use vulnerabilities within it to access your\n        workload.\n      \n    \n      \n        You patch the developer environments automatically without\n        notifying the developers. You receive multiple complaints from\n        the developers that their environment cease to operate as\n        expected.\n      \n    \n      \n        You have not patched the commercial off-the-shelf software on a\n        persistent instance. When you have an issue with the software\n        and contact the vendor, they notify you that version is not\n        supported and you have to patch to a specific level to\n        receive any assistance.\n      \n    \n      \n        A recently released patch for the encryption software you used\n        has significant performance improvements. Your unpatched system\n        has performance issues that remain in place as a result of not\n        patching.\n      \n    \n      \n        You are notified of a zero-day vulnerability requiring an emergency fix and you have to patch all your environments manually.\n      \n    \n      \n        You are not aware of critical actions needed to maintain your resources, such as mandatory version updates because you do not review upcoming planned lifecycle events and other information. You lose critical time for planning and execution, resulting in emergency changes for your teams and potential impact or unexpected downtime.\n      \n    \n    Benefits of establishing this best\n      practice: By establishing a patch management process, including your criteria for patching and methodology for distribution across your environments, you can scale and report on patch levels.  This provides assurances around security patching and ensure clear visibility on the status of known fixes being in place. This encourages adoption of desired features and capabilities, the rapid removal of issues, and sustained compliance with governance. Implement patch management systems and automation to reduce the level of effort to deploy patches and limit errors caused by manual processes.\n  \n    Level of risk exposed if this best practice\n    is not established: Medium\n  \n\n  Implementation guidance\n    \n      Patch systems to remediate issues, to gain desired features or capabilities, and to remain compliant with governance policy and vendor support requirements. In immutable systems, deploy with the appropriate patch set to achieve the desired result. Automate the patch management mechanism to reduce the elapsed time to patch, to avoid errors caused by manual processes, and lower the level of effort to patch.\n    \n     \n      \n      Implementation steps\n      \n        For Amazon EC2 Image Builder:\n      \n      \n         \n         \n         \n         \n         \n         \n      \n          \n            Using Amazon EC2 Image Builder, specify pipeline details:\n          \n          \n             \n             \n             \n          \n              \n                Create an image pipeline and name it\n              \n            \n              \n                Define pipeline schedule and time zone\n              \n            \n              \n                Configure any dependencies\n              \n            \n        \n          \n            Choose a recipe:\n          \n          \n             \n             \n             \n             \n             \n          \n              \n                Select existing recipe or create a new one\n              \n            \n              \n                Select image type\n              \n            \n              \n                Name and version your recipe\n              \n            \n              \n                Select your base image\n              \n            \n              \n                Add build components and add to target registry\n              \n            \n        \n          \n            Optional - define your infrastructure configuration.\n          \n        \n          \n            Optional - define configuration settings.\n          \n        \n          \n            Review settings.\n          \n        \n          \n            Maintain recipe hygiene regularly.\n          \n        \n      \n        For Systems Manager Patch Manager:\n      \n      \n         \n         \n         \n      \n          \n            Create a patch baseline.\n          \n        \n          \n            Select a patching operations method.\n          \n        \n          \n            Enable compliance reporting and scanning. \n          \n        \n     \n   \n\n  Resources\n\n    \n      Related best practices:\n    \n    \n       \n    \n        \n          OPS06-BP04 Automate testing and rollback\n        \n      \n    \n      Related documents:\n    \n    \n       \n       \n       \n       \n       \n       \n       \n    \n        \n          What is Amazon EC2 Image Builder\n        \n      \n        \n          Create an image pipeline using the Amazon EC2 Image Builder\n        \n      \n        \n          Create a container image pipeline\n        \n      \n        \n          AWS Systems Manager Patch Manager\n        \n      \n        \n          Working with Patch Manager\n        \n      \n        \n          Working with patch compliance reports\n        \n      \n        AWS Developer Tools\n        \n      \n    \n      Related videos:\n    \n    \n       \n       \n       \n    \n        \n          CI/CD\n          for Serverless Applications on AWS\n        \n      \n        \n          Design with\n          Ops in Mind\n        \n        \n          Related examples:\n        \n      \n        AWS Systems Manager Patch Manager tutorials\n        \n      \n  \u003cdiv\u003e\u003cdiv\u003e\u003cdiv\u003e\u003cdiv id=\"js_error_message\"\u003e\u003cp\u003e\u003cimg src=\"https://d1ge0kk1l5kms0.cloudfront.net/images/G/01/webservices/console/warning.png\" alt=\"Warning\" /\u003e \u003cstrong\u003eJavascript is disabled or is unavailable in your browser.\u003c/strong\u003e\u003c/p\u003e\u003cp\u003eTo use the Amazon Web Services Documentation, Javascript must be enabled. Please refer to your browser's Help pages for instructions.\u003c/p\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003eDocument ConventionsOPS05-BP04 Use build and deployment management systemsOPS05-BP06 Share design standards",
  "OPS05-BP06 Share design standards\n    Share best practices across teams to increase awareness and maximize the benefits of development efforts. Document them and keep them up to date as your architecture evolves. If shared standards are enforced in your organization, it’s critical that mechanisms exist to request additions, changes, and exceptions to standards. Without this option, standards become a constraint on innovation.\n  \n    Desired outcome: Design standards are shared across teams in your organizations. They are documented and kept up-to-date as best practices evolve.\n  \n    Common anti-patterns:\n  \n     \n     \n  \n      Two development teams have each created a user authentication service. Your users must maintain a separate set of credentials for each part of the system they want to access. \n    \n      Each team manages their own infrastructure. A new compliance requirement forces a change to your infrastructure and each team implements it in a different way.\n    \n    Benefits of establishing this best\n      practice: Using shared standards supports the adoption of best practices and maximizes the benefits of development efforts. Documenting and updating design standards keeps your organization up-to-date with best practices and security and compliance requirements.\n  \n    Level of risk exposed if this best practice\n    is not established: Medium\n  \n\n  Implementation guidance\n    \n      Share existing best practices, design standards, checklists, operating procedures, guidance, and governance requirements across teams. Have procedures to request changes, additions, and exceptions to design standards to support improvement and innovation. Make teams are aware of published content. Have a mechanism to keep design standards up-to-date as new best practices emerge.\n    \n    \n      Customer example\n    \n    \n      AnyCompany Retail has a cross-functional architecture team that creates software architecture patterns. This team builds the architecture with compliance and governance built in. Teams that adopt these shared standards get the benefits of having compliance and governance built in. They can quickly build on top of the design standard. The architecture team meets quarterly to evaluate architecture patterns and update them if necessary.\n    \n     \n      \n      Implementation steps\n   \n    \n       \n       \n       \n    \n        \n          Identify a cross-functional team that owns developing and updating design standards. This team should work with stakeholders across your organization to develop design standards, operating procedures, checklists, guidance, and governance requirements. Document the design standards and share them within your organization. \n        \n        \n           \n        \n            \n              AWS Service Catalog can be used to create portfolios representing design standards using infrastructure as code. You can share portfolios across accounts.\n            \n          \n      \n        \n          Have a mechanism in place to keep design standards up-to-date as new best practices are identified.\n        \n      \n        \n          If design standards are centrally enforced, have a process to request changes, updates, and exemptions. \n        \n      \n    \n      Level of effort for the implementation plan: Medium. Developing a process to create and share design standards can take coordination and cooperation with stakeholders across your organization. \n    \n     \n   \n\n  Resources\n    \n      Related best practices:\n    \n    \n       \n       \n       \n       \n       \n    \n        \n          OPS01-BP03 Evaluate governance requirements - Governance requirements influence design standards.\n        \n      \n        \n          OPS01-BP04 Evaluate compliance requirements - Compliance is a vital input in creating design standards.\n        \n      \n        \n          OPS07-BP02 Ensure a consistent review of\n      operational readiness - Operational readiness checklists are a mechanism to implement design standards when designing your workload.\n        \n      \n        \n          OPS11-BP01 Have a process for continuous improvement - Updating design standards is a part of continuous improvement.\n        \n      \n        \n          OPS11-BP04 Perform knowledge management - As part of your knowledge management practice, document and share design standards.\n        \n      \n\n      \n    \n      Related documents:\n    \n    \n       \n       \n       \n       \n       \n    \n        \n          Automate AWS Backups with AWS Service Catalog\n      \n        AWS Service Catalog Account Factory-Enhanced\n        \n      \n        \n          How Expedia Group built Database as a Service (DBaaS) offering using AWS Service Catalog\n      \n         \n          Maintain visibility over the use of cloud architecture patterns\n        \n      \n        \n          Simplify sharing your AWS Service Catalog portfolios in an AWS Organizations setup\n        \n      \n    \n      Related videos:\n    \n    \n       \n       \n    \n        AWS Service Catalog – Getting Started\n        \n      \n        AWS re:Invent 2020: Manage your AWS Service Catalog portfolios like an expert\n        \n      \n    \n      Related examples:\n    \n    \n       \n       \n    \n        AWS Service Catalog Reference Architecture\n        \n      \n        AWS Service Catalog Workshop\n        \n      \n    \n      Related services:\n    \n    \n       \n    \n        \n          AWS Service Catalog\n        \n      \n  \u003cdiv\u003e\u003cdiv\u003e\u003cdiv\u003e\u003cdiv id=\"js_error_message\"\u003e\u003cp\u003e\u003cimg src=\"https://d1ge0kk1l5kms0.cloudfront.net/images/G/01/webservices/console/warning.png\" alt=\"Warning\" /\u003e \u003cstrong\u003eJavascript is disabled or is unavailable in your browser.\u003c/strong\u003e\u003c/p\u003e\u003cp\u003eTo use the Amazon Web Services Documentation, Javascript must be enabled. Please refer to your browser's Help pages for instructions.\u003c/p\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003eDocument ConventionsOPS05-BP05 Perform patch managementOPS05-BP07 Implement practices to improve code quality",
  "OPS05-BP07 Implement practices to improve code quality\n    Implement practices to improve code quality and minimize defects.\n    Some examples include test-driven development, code reviews,\n    standards adoption, and pair programming. Incorporate these\n    practices into your continuous integration and delivery process.\n  \n    Desired outcome: Your\n    organization uses best practices like code reviews or pair\n    programming to improve code quality. Developers and operators adopt\n    code quality best practices as part of the software development\n    lifecycle.\n  \n    Common anti-patterns:\n  \n     \n     \n     \n  \n      \n        You commit code to the main branch of your application without a\n        code review. The change automatically deploys to production and\n        causes an outage.\n      \n    \n      \n        A new application is developed without any unit, end-to-end, or\n        integration tests. There is no way to test the application\n        before deployment.\n      \n    \n      \n        Your teams make manual changes in production to address defects.\n        Changes do not go through testing or code reviews and are not\n        captured or logged through continuous integration and delivery\n        processes.\n      \n    \n    Benefits of establishing this best\n    practice: By adopting practices to improve code quality,\n    you can help minimize issues introduced to production. Code quality\n    best practices include pair programming, code\n    reviews, and implementation of AI productivity tools.\n  \n    Level of risk exposed if this best practice\n    is not established: Medium\n  \n\n  Implementation guidance\n\n      \n    \n      Implement practices to improve code quality to minimize defects\n      before they are deployed. Use practices like test-driven\n      development, code reviews, and pair programming to increase the\n      quality of your development.\n    \n    \n      Use the power of generative AI with Amazon Q Developer to improve\n      developer productivity and code quality. Amazon Q Developer includes\n      generation of code suggestions (based on large language models),\n      production of unit tests (including boundary conditions), and code\n      security enhancements through detection and remediation of security\n      vulnerabilities.\n    \n    \n      Customer example\n    \n    \n      AnyCompany Retail adopts several practices to improve code\n      quality. They have adopted test-driven development as the standard\n      for writing applications. For some new features, they will have\n      developers pair program together during a sprint. Every pull\n      request goes through a code review by a senior developer before\n      being integrated and deployed.\n    \n     \n\n  Implementation steps\n\n      \n      \n         \n      \n          \n            Adopt code quality practices like test-driven development,\n            code reviews, and pair programming into your continuous\n            integration and delivery process. Use these techniques to\n            improve software quality.\n          \n          \n             \n             \n          \n              \n                Use\n                Amazon Q Developer, a generative AI tool that can help\n                create unit test cases (including boundary conditions),\n                generate functions using code and comments, implement\n                well-known algorithms, detect security policy violations\n                and vulnerabilities in your code, detect secrets, scan\n                infrastructure as code (IaC), document code, and learn\n                third-party code libraries more quickly.\n              \n            \n              \n                Amazon CodeGuru Reviewer can provide programming\n                recommendations for Java and Python code using machine\n                learning.\n              \n            \n        \n      \n        Level of effort for the implementation\n        plan: Medium. There are many ways of implementing\n        this best practice, but getting organizational adoption may be\n        challenging.\n      \n     \n   \n\n  Resources\n\n      \n    \n      Related best practices:\n    \n    \n       \n       \n    \n        \n          OPS05-BP02\n          Test and validate changes\n        \n      \n        \n          OPS05-BP06\n          Share design standards\n        \n      \n    \n      Related documents:\n    \n    \n       \n       \n       \n       \n       \n       \n       \n       \n       \n       \n       \n       \n       \n       \n       \n       \n       \n       \n    \n        \n          Adopt\n          a test-driven development approach\n        \n      \n        \n          Accelerate\n          your Software Development Lifecycle with Amazon Q\n        \n      \n        \n          Amazon Q Developer, now generally available, includes previews of new\n          capabilities to reimagine developer experience\n        \n      \n        \n          The\n          Ultimate Cheat Sheet for Using Amazon Q Developer in Your\n          IDE\n        \n      \n        \n          Shift-Left\n          Workload, leveraging AI for Test Creation\n        \n      \n        \n          Amazon Q Developer Center\n        \n      \n        \n          10\n          ways to build applications faster with Amazon CodeWhisperer\n        \n      \n        \n          Looking\n          beyond code coverage with Amazon CodeWhisperer\n        \n      \n        \n          Best\n          Practices for Prompt Engineering with Amazon CodeWhisperer\n        \n      \n        \n          Agile\n          Software Guide\n        \n      \n        \n          My\n          CI/CD pipeline is my release captain\n        \n      \n        \n          Automate\n          code reviews with Amazon CodeGuru Reviewer\n        \n      \n        \n          Adopt\n          a test-driven development approach\n        \n      \n        \n          How\n          DevFactory builds better applications with Amazon CodeGuru\n        \n      \n        \n          On\n          Pair Programming\n        \n      \n        \n          RENGA\n          Inc. automates code reviews with Amazon CodeGuru\n        \n      \n        \n          The\n          Art of Agile Development: Test-Driven Development\n        \n      \n        \n          Why\n          code reviews matter (and actually save time!)\n        \n      \n    \n      Related videos:\n    \n    \n       \n       \n       \n       \n       \n    \n        \n          Implement\n          an API with Amazon Q Developer Agent for Software\n          Development\n        \n      \n        \n          Installing,\n          Configuring, \u0026 Using Amazon Q Developer with JetBrains\n          IDEs (How-to)\n        \n      \n        \n          Mastering\n          the art of Amazon CodeWhisperer - YouTube playlist\n        \n      \n        \n          AWS           re:Invent 2020: Continuous improvement of code quality with\n          Amazon CodeGuru\n        \n      \n        \n          AWS           Summit ANZ 2021 - Driving a test-first strategy with CDK and\n          test driven development\n        \n      \n    \n      Related services:\n    \n    \n       \n       \n       \n    \n        \n          Amazon Q Developer\n        \n      \n        \n          Amazon CodeGuru Reviewer\n        \n      \n        \n          Amazon CodeGuru Profiler\n        \n      \n  \u003cdiv\u003e\u003cdiv\u003e\u003cdiv\u003e\u003cdiv id=\"js_error_message\"\u003e\u003cp\u003e\u003cimg src=\"https://d1ge0kk1l5kms0.cloudfront.net/images/G/01/webservices/console/warning.png\" alt=\"Warning\" /\u003e \u003cstrong\u003eJavascript is disabled or is unavailable in your browser.\u003c/strong\u003e\u003c/p\u003e\u003cp\u003eTo use the Amazon Web Services Documentation, Javascript must be enabled. Please refer to your browser's Help pages for instructions.\u003c/p\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003eDocument ConventionsOPS05-BP06 Share design standardsOPS05-BP08 Use multiple environments",
  "OPS05-BP08 Use multiple environments\n    Use multiple environments to experiment, develop, and test your\n    workload. Use increasing levels of controls as environments approach\n    production to gain confidence your workload operates as intended\n    when deployed.\n  \n    Desired outcome: You have multiple environments that reflect your compliance and governance needs. You test and promote code through environments on your path to production.\n  \n     \n     \n     \n  \n      \n        Your organization does this through the establishment of a landing zone, which provides governance, controls, account automations, networking, security, and operational observability. Manage these landing zone capabilities by using multiple environments. A common example is a sandbox organization for developing and testing changes to an AWS Control Tower-based landing zone, which includes AWS IAM Identity Center and  policies such as service control policies (SCPs). All of these elements can significantly impact the access to and operation of AWS accounts within the landing zone.\n      \n    \n      \n        In addition to these services, your teams extend the landing zones capabilites with solutions published by AWS and AWS partners or as custom solutions developed within your organization. Examples of solutions published by AWS include Customizations for AWS Control Tower (CfCT) and AWS Control Tower Account Factory for Terraform (AFT). \n      \n    \n      \n        Your organization applies the same principles of testing, promoting code, and policy changes for the landing zone through environments on your path to production. This strategy provides a stable and secure landing zone environment for your application and workload teams. \n      \n    \n    Common anti-patterns:\n  \n     \n     \n     \n     \n     \n     \n     \n     \n     \n  \n      \n        You are performing development in a shared development\n        environment and another developer overwrites your code changes.\n      \n    \n      \n        The restrictive security controls on your shared development\n        environment are preventing you from experimenting with new\n        services and features.\n      \n    \n      \n        You perform load testing on your production systems and cause an\n        outage for your users.\n      \n    \n      \n        A critical error resulting in data loss has occurred in\n        production. In your production environment, you attempt to\n        recreate the conditions that lead to the data loss so that you\n        can identify how it happened and prevent it from happening\n        again. To prevent further data loss during testing, you are\n        forced to make the application unavailable to your users.\n      \n    \n      \n        You are operating a multi-tenant service and are unable to\n        support a customer request for a dedicated environment.\n      \n    \n      \n        You may not always test, but when you do, you test in your production environment.\n      \n    \n      \n        You believe that the simplicity of a single environment\n        overrides the scope of impact of changes within the environment.\n      \n    \n      \n        You upgrade a key landing zone capability, but the change impairs your team's ability to vend accounts for either new projects or your existing workloads.\n      \n    \n      \n        You apply new controls to your AWS accounts, but the change impacts your workload team's ability to deploy changes within their AWS accounts.\n      \n    \n    Benefits of establishing this best\n      practice: When you deploy multiple environments, you can support multiple simultaneous development, testing, and production environments without creating conflicts between developers or user communities. For complex capabilities such as landing zones, it significantly reduces the risk of changes, simplifies the improvement process, and reduces the risk of critical updates to the environment. Organizations that use landing zones naturally benefit from multi-accounts in their AWS environment, with account structure, governance, network, and security configurations. Over time, as your organization grows, the landing zone can evolve to secure and organize your workloads and resources.\n  \n    Level of risk exposed if this best practice\n    is not established: Medium\n  \n\n  Implementation guidance\n    \n      Use multiple environments and provide developers sandbox environments with minimized controls to aid in experimentation. Provide individual development environments to help work in parallel, increasing development agility. Implement more rigorous controls in the environments approaching production to allow developers to innovate. Use infrastructure as code and configuration management systems to deploy environments that are configured consistent with the controls present in production to ensure systems operate as expected when deployed. When environments are not in use, turn them off to avoid costs associated with idle resources (for example, development systems on evenings and weekends). Deploy production equivalent environments when load testing to improve valid results.\n    \n    \n      Teams such as platform engineering, networking, and security operations often manage capabilies at the organization level with distinct requirements. A separation of accounts alone is insufficient to provide and maintain separate environments for experimentation, development, and testing. In such cases, create separate instances of AWS Organizations.\n    \n   \n\n  Resources\n\n      \n    \n      Related documents:\n    \n    \n       \n       \n       \n       \n    \n        \n          Instance Scheduler on AWS\n      \n        \n          What\n          is AWS CloudFormation?\n        \n      \n        \n          Organizing Your AWS Environment Using Multiple Accounts - Multiple organizations - Test changes to your overall AWS environment\n        \n      \n        AWS Control Tower Guide\n        \n      \n  \u003cdiv\u003e\u003cdiv\u003e\u003cdiv\u003e\u003cdiv id=\"js_error_message\"\u003e\u003cp\u003e\u003cimg src=\"https://d1ge0kk1l5kms0.cloudfront.net/images/G/01/webservices/console/warning.png\" alt=\"Warning\" /\u003e \u003cstrong\u003eJavascript is disabled or is unavailable in your browser.\u003c/strong\u003e\u003c/p\u003e\u003cp\u003eTo use the Amazon Web Services Documentation, Javascript must be enabled. Please refer to your browser's Help pages for instructions.\u003c/p\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003eDocument ConventionsOPS05-BP07 Implement practices to improve code qualityOPS05-BP09 Make frequent, small, reversible changes",
  "OPS05-BP09 Make frequent, small, reversible changes\n    Frequent, small, and reversible changes reduce the scope and impact of a change. When used in conjunction with change management systems, configuration management systems, and build and delivery systems frequent, small, and reversible changes reduce the scope and impact of a change. This results in more effective troubleshooting and faster remediation with the option to roll back changes.\n  \n    Common anti-patterns:\n  \n     \n     \n     \n  \n      \n        You deploy a new version of your application quarterly with a change window that means a core service is turned off.\n      \n    \n      \n        You frequently make changes to your database schema without tracking changes in your management systems.\n        \n      \n    \n      \n        You perform manual in-place updates, overwriting existing installations and configurations, and have no clear roll-back plan.\n        \n      \n    \n    Benefits of establishing this best\n      practice: Development efforts are faster by deploying small changes frequently. When the changes are small, it is much easier to identify if they have unintended consequences, and they are easier to reverse. When the changes are reversible, there is less risk to implementing the change, as recovery is simplified. The change process has a reduced risk and the impact of a failed change is reduced.\n  \n    Level of risk exposed if this best practice\n    is not established: Low\n  \n\n  Implementation guidance\n    \n      Use frequent, small, and reversible changes to reduce the scope and impact of a change. This eases troubleshooting, helps with faster remediation, and provides the option to roll back a change. It also increases the rate at which you can deliver value to the business.\n    \n   \n    \n    Resources\n    \n      Related best practices:\n    \n    \n       \n       \n       \n    \n        \n          OPS05-BP03 Use configuration management systems\n        \n      \n        \n          OPS05-BP04 Use build and deployment management systems\n        \n      \n        \n          OPS06-BP04 Automate testing and rollback\n        \n      \n    \n      Related documents:\n    \n    \n       \n       \n    \n        \n          Implementing Microservices on AWS\n      \n        \n          Microservices - Observability\n        \n      \n  \u003cdiv\u003e\u003cdiv\u003e\u003cdiv\u003e\u003cdiv id=\"js_error_message\"\u003e\u003cp\u003e\u003cimg src=\"https://d1ge0kk1l5kms0.cloudfront.net/images/G/01/webservices/console/warning.png\" alt=\"Warning\" /\u003e \u003cstrong\u003eJavascript is disabled or is unavailable in your browser.\u003c/strong\u003e\u003c/p\u003e\u003cp\u003eTo use the Amazon Web Services Documentation, Javascript must be enabled. Please refer to your browser's Help pages for instructions.\u003c/p\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003eDocument ConventionsOPS05-BP08 Use multiple environmentsOPS05-BP10 Fully automate integration and deployment",
  "OPS05-BP10 Fully automate integration and deployment\n    Automate build, deployment, and testing of the workload. This\n    reduces errors caused by manual processes and reduces the effort to\n    deploy changes.\n  \n    Apply metadata using\n    Resource\n    Tags and\n    AWS Resource Groups following a consistent\n    tagging\n    strategy to aid in identification of your resources. Tag your\n    resources for organization, cost accounting, access controls, and\n    targeting the run of automated operations activities.\n  \n    Desired outcome: Developers use tools to deliver code and promote through to production.  Developers do not have to log into the AWS Management Console to deliver updates. There is a full audit trail of change and configuration, meeting the needs of governance and compliance. Processes are repeatable and are standardized across teams.  Developers are free to focus on development and code pushes, increasing productivity.\n  \n    Common anti-patterns:\n  \n     \n     \n     \n  \n      \n        On Friday, you finish authoring the new code for your feature\n        branch. On Monday, after running your code quality test scripts\n        and each of your unit tests scripts, you check in your code\n        for the next scheduled release.\n      \n    \n      \n        You are assigned to code a fix for a critical issue impacting a\n        large number of customers in production. After testing the fix,\n        you commit your code and email change management to request\n        approval to deploy it to production.\n      \n    \n      \n        As a developer, you log into the AWS Management Console to create a new development environment using non-standard methods and systems.\n      \n    \n    Benefits of establishing this best\n      practice: By implementing automated build and deployment management systems, you reduce errors caused by manual processes and reduce the effort to deploy changes helping your team members to focus on delivering business value. You increase the speed of delivery as you promote through to production.\n  \n    Level of risk exposed if this best practice\n    is not established: Low\n  \n\n  Implementation guidance\n    \n      You use build and deployment management systems to track and implement change, to reduce errors caused by manual processes, and reduce the level of effort. Fully automate the integration and deployment pipeline from code check-in through build, testing, deployment, and validation. This reduces lead time, encourages increased frequency of change, reduces the level of effort, increases the speed to market, results in increased productivity, and increases the security of your code as you promote through to production.\n    \n   \n\n  Resources\n\n    \n      Related best practices:\n    \n    \n       \n       \n    \n        \n          OPS05-BP03 Use configuration management systems\n        \n      \n        \n          OPS05-BP04 Use build and deployment management systems\n        \n      \n    \n      Related documents:\n    \n    \n       \n       \n    \n        \n          What\n          is AWS CodeBuild?\n        \n      \n        \n          What\n          is AWS CodeDeploy?\n        \n      \n    \n      Related videos:\n    \n    \n       \n    \n        AWS re:Invent 2022 - AWS Well-Architected best practices for DevOps on AWS\n      \n  \u003cdiv\u003e\u003cdiv\u003e\u003cdiv\u003e\u003cdiv id=\"js_error_message\"\u003e\u003cp\u003e\u003cimg src=\"https://d1ge0kk1l5kms0.cloudfront.net/images/G/01/webservices/console/warning.png\" alt=\"Warning\" /\u003e \u003cstrong\u003eJavascript is disabled or is unavailable in your browser.\u003c/strong\u003e\u003c/p\u003e\u003cp\u003eTo use the Amazon Web Services Documentation, Javascript must be enabled. Please refer to your browser's Help pages for instructions.\u003c/p\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003eDocument ConventionsOPS05-BP09 Make frequent, small, reversible changesOPS 6. How do you mitigate deployment risks?",
  "OPS06-BP01 Plan for unsuccessful changesPlan to revert to a known good state, or remediate in the production environment if the deployment causes an undesired outcome. Having a policy to establish such a plan helps all teams develop strategies to recover from failed changes. Some example strategies are deployment and rollback steps, change policies, feature flags, traffic isolation, and traffic shifting. A single release may include multiple related component changes. The strategy should provide the ability to withstand or recover from a failure of any component change.\n    Desired outcome: You have prepared a detailed recovery plan for your change in the event it is unsuccessful.  In addition, you have reduced the size of your release to minimize the potential impact on other workload components.  As a result, you have reduced your business impact by shortening the potential downtime caused by a failed change and increased the flexibility and efficiency of recovery times.\n  \n    Common anti-patterns:\n  \n     \n     \n     \n     \n     \n     \n  \n      \n        You performed a deployment and your application has become unstable but there appear to be active users on the system. You have to decide whether to rollback the change and impact the active users or wait to rollback the change knowing the users may be impacted regardless.\n      \n    \n      \n        After making a routine change, your new environments are accessible, but one of your subnets has become unreachable. You have to decide whether to rollback everything or try to fix the inaccessible subnet. While you are making that determination, the subnet remains unreachable.\n      \n    \n      \n        Your systems are not architected in a way that allows them to be updated with smaller releases. As a result, you have difficulty in reversing those bulk changes during a failed deployment.\n      \n    \n      \n        You do not use infrastructure as code (IaC) and you made manual updates to your infrastructure that resulted in an undesired configuration.  You are unable to effectively track and revert the manual changes.\n      \n    \n      \n        Because you have not measured increased frequency of your deployments, your team is not incentivized to reduce the size of their changes and improve their rollback plans for each change, leading to more risk and increased failure rates.\n      \n    \n      \n        You do not measure the total duration of an outage caused by unsuccessful changes. Your team is unable to prioritize and improve its deployment process and recovery plan effectiveness.\n      \n    \n    Benefits of establishing this best\n      practice: Having a plan to recover from unsuccessful changes minimizes the mean time to recover (MTTR) and reduces your business impact.\n  \n    Level of risk exposed if this best practice\n    is not established: High\n  \n\n  Implementation guidance\n    \n      A consistent, documented policy and practice adopted by release teams allows an organization to plan what should happen if unsuccessful changes occur. The policy should allow for fixing forward in specific circumstances.  In either situation, a fix forward or rollback plan should be well documented and tested before deployment to live production so that the time it takes to revert a change is minimized.\n    \n     \n      \n      Implementation steps\n      \n         \n         \n         \n         \n         \n         \n      \n          \n            Document the policies that require teams to have effective plans to reverse changes within a specified period.\n          \n          \n             \n             \n             \n          \n              \n                Policies should specify when a fix-forward situation is allowed.\n              \n            \n              \n                Require a documented rollback plan to be accessible by all involved.\n              \n            \n              \n                Specify the requirements to rollback (for example, when it is found that unauthorized changes have been deployed).\n              \n            \n        \n          \n            Analyze the level of impact of all changes related to each component of a workload.\n            \n          \n          \n             \n             \n             \n          \n              \n                Allow repeatable changes to be standardized, templated, and preauthorized if they follow a consistent workflow that enforces change policies.\n              \n            \n              \n                Reduce the potential impact of any change by making the size of the change smaller so recovery takes less time and causes less business impact.\n              \n            \n              \n                Ensure rollback procedures revert code to the known good state to avoid incidents where possible.\n              \n            \n        \n          \n            Integrate tools and workflows to enforce your policies programatically.\n          \n        \n          \n            Make data about changes visible to other workload owners to improve the speed of diagnosis of any failed change that cannot be rolled back.\n            \n          \n          \n             \n          \n              \n                Measure success of this practice using visible change data and identify iterative improvements.\n              \n            \n        \n          \n            Use monitoring tools to verify the success or failure of a deployment to speed up decision-making on rolling back.\n          \n        \n          \n            Measure your duration of outage during an unsuccessful change to continually improve your recovery plans.\n          \n        \n      \n        Level of effort for the implementation plan: Medium\n      \n     \n   \n    \n    Resources\n    \n      Related best practices:\n    \n    \n       \n    \n        \n          OPS06-BP04 Automate testing and rollback\n        \n      \n    \n      Related documents:\n    \n    \n       \n       \n    \n        AWS Builders Library | Ensuring Rollback Safety During Deployments\n        \n      \n        AWS Whitepaper | Change Management in the Cloud\n        \n      \n    \n      Related videos:\n    \n    \n       \n    \n        \n          re:Invent 2019 | Amazon’s approach to high-availability deployment\n        \n      \n  \u003cdiv\u003e\u003cdiv\u003e\u003cdiv\u003e\u003cdiv id=\"js_error_message\"\u003e\u003cp\u003e\u003cimg src=\"https://d1ge0kk1l5kms0.cloudfront.net/images/G/01/webservices/console/warning.png\" alt=\"Warning\" /\u003e \u003cstrong\u003eJavascript is disabled or is unavailable in your browser.\u003c/strong\u003e\u003c/p\u003e\u003cp\u003eTo use the Amazon Web Services Documentation, Javascript must be enabled. Please refer to your browser's Help pages for instructions.\u003c/p\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003eDocument ConventionsOPS 6. How do you mitigate deployment risks?OPS06-BP02 Test deployments",
  "OPS06-BP02 Test deployments\n    Test release procedures in pre-production by using the same deployment configuration, security controls, steps, and procedures as in production. Validate that all deployed steps are completed as expected, such as inspecting files, configurations, and services. Further test all changes with functional, integration, and load tests, along with any monitoring such as health checks. By doing these tests, you can identify deployment issues early with an opportunity to plan and mitigate them prior to production.\n  \n    You can create temporary parallel environments for testing every change. Automate the deployment of the test environments using infrastructure as code (IaC) to help reduce amount of work involved and ensure stability, consistency, and faster feature delivery.\n  \n    Desired outcome: Your organization adopts a test-driven development culture that includes testing deployments. This ensures teams are focused on delivering business value rather than managing releases. Teams are engaged early upon identification of deployment risks to determine the appropriate course of mitigation.\n  \n    Common anti-patterns:\n  \n     \n     \n     \n     \n  \n      \n        During production releases, untested deployments cause frequent issues that require troubleshooting and escalation.\n      \n    \n      \n        Your release contains infrastructure as code (IaC) that updates existing resources. You are unsure if the IaC runs successfully or causes impact to the resources.\n      \n    \n      \n        You deploy a new feature to your application. It doesn't work as intended and there is no visibility until it gets reported by impacted users.\n      \n    \n      \n        You update your certificates. You accidentally install the certificates to the wrong components, which goes undetected and impacts website visitors because a secure connection to the website can't be established.\n      \n    \n    Benefits of establishing this best\n      practice: Extensive testing in pre-production of deployment procedures, and the changes introduced by them, minimizes the potential impact to production caused by the deployments steps. This increases confidence during production release and minimizes operational support without slowing down velocity of the changes being delivered.\n  \n    Level of risk exposed if this best practice\n    is not established: High\n  \n\n  Implementation guidance\n\n      \n        Testing your deployment process is as important as testing the changes that result from your deployment. This can be achieved by testing your deployment steps in a pre-production environment that mirrors production as closely as possible. Common issues, such as incomplete or incorrect deployment steps, or misconfigurations, can be caught as a result before going to production. In addition, you can test your recovery steps.\n      \n    \n      Customer example\n    \n    \n      As part of their continuous integration and continuous delivery (CI/CD) pipeline, AnyCompany Retail performs the defined steps needed to release infrastructure and software updates for its customers in a production-like environment. The pipeline is comprised of pre-checks to detect drift (detecting changes to resources performed outside of your IaC) in resources prior to deployment, as well as validate actions that the IaC takes upon its initiation. It validates deployment steps, like verifying that certain files and configurations are in place and services are in running states and are responding correctly to health checks on local host before re-registering with the load balancer. Additionally, all changes flag a number of automated tests, such as functional, security, regression, integration, and load tests.\n    \n     \n      \n      Implementation steps\n      \n         \n         \n         \n         \n         \n         \n         \n         \n      \n          \n            Perform pre-install checks to mirror the pre-production environment to production.\n            \n          \n          \n             \n             \n          \n              \n                Use drift detection to detect when resources have been changed outside of AWS CloudFormation.\n              \n            \n              \n                Use change sets to validate that the intent of a stack update matches the actions that AWS CloudFormation takes when the change set is initiated.\n              \n            \n        \n          \n            This triggers a manual approval step in AWS CodePipeline to authorize the deployment to the pre-production environment.\n          \n        \n          \n            Use deployment configurations such as AWS CodeDeploy AppSpec files to define deployment and validation steps.\n          \n        \n          \n            Where applicable, integrate AWS CodeDeploy with other AWS services or integrate AWS CodeDeploy with partner product and services.\n          \n        \n          \n            \n            Monitor deployments using Amazon CloudWatch, AWS CloudTrail, and Amazon SNS event notifications.\n          \n        \n          \n            Perform post-deployment automated testing, including functional, security, regression, integration, and load testing.\n          \n        \n          \n            \n            Troubleshoot deployment issues.\n          \n        \n          \n            Successful validation of preceding steps should initiate a manual approval workflow to authorize deployment to production.\n          \n        \n      \n        Level of effort for the implementation plan: High\n      \n     \n   \n\n  Resources\n\n    \n      Related best practices:\n    \n    \n       \n    \n        \n          OPS05-BP02 Test and validate changes\n        \n      \n    \n      Related documents:\n    \n    \n       \n       \n       \n       \n       \n    \n        AWS Builders' Library | Automating safe, hands-off deployments | Test Deployments\n        \n      \n        AWS Whitepaper | Practicing Continuous Integration and Continuous Delivery on AWS\n      \n        \n          The Story of Apollo - Amazon's Deployment Engine\n        \n      \n        \n          How\n          to test and debug AWS CodeDeploy locally before you ship your\n          code\n        \n      \n        \n          Integrating Network Connectivity Testing with Infrastructure Deployment\n        \n      \n    \n      Related videos:\n    \n    \n       \n    \n        \n          re:Invent 2020 | Testing software and systems at Amazon\n        \n      \n    \n      Related examples:\n    \n    \n       \n    \n        \n          Tutorial | Deploy and Amazon ECS service with a validation test\n        \n      \n  \u003cdiv\u003e\u003cdiv\u003e\u003cdiv\u003e\u003cdiv id=\"js_error_message\"\u003e\u003cp\u003e\u003cimg src=\"https://d1ge0kk1l5kms0.cloudfront.net/images/G/01/webservices/console/warning.png\" alt=\"Warning\" /\u003e \u003cstrong\u003eJavascript is disabled or is unavailable in your browser.\u003c/strong\u003e\u003c/p\u003e\u003cp\u003eTo use the Amazon Web Services Documentation, Javascript must be enabled. Please refer to your browser's Help pages for instructions.\u003c/p\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003eDocument ConventionsOPS06-BP01 Plan for unsuccessful changesOPS06-BP03 Employ safe deployment strategies",
  "OPS06-BP03 Employ safe deployment strategies\n    Safe production roll-outs control the flow of beneficial changes with an aim to minimize any perceived impact for customers from those changes. The safety controls provide inspection mechanisms to validate desired outcomes and limit the scope of impact from any defects introduced by the changes or from deployment failures. Safe roll-outs may include strategies such as feature-flags, one-box, rolling (canary releases), immutable, traffic splitting, and blue/green deployments.\n  \n    Desired outcome: Your organization uses a continuous integration continuous delivery (CI/CD) system that provides capabilities for automating safe rollouts. Teams are required to use appropriate safe roll-out strategies.\n  \n    Common anti-patterns:\n  \n     \n     \n     \n     \n  \n      \n        You deploy an unsuccessful change to all of production all at once. As a result, all customers are impacted simultaneously.\n      \n    \n      \n        A defect introduced in a simultaneous deployment to all systems requires an emergency release. Correcting it for all customers takes several days.\n      \n    \n      \n        Managing production release requires planning and participation of several teams. This puts constraints on your ability to frequently update features for your customers.\n      \n    \n      \n        You perform a mutable deployment by modifying your existing systems. After discovering that the change was unsuccessful, you are forced to modify the systems again to restore the old version, extending your time to recovery.\n      \n    \n    Benefits of establishing this best\n      practice: Automated deployments balance speed of roll-outs against delivering beneficial changes consistently to customers. Limiting impact prevents costly deployment failures and maximizes teams ability to efficiently respond to failures.\n  \n    Level of risk exposed if this best practice\n    is not established: Medium\n  \n\n  Implementation guidance\n    \n      Continuous-delivery failures can lead to reduced service availability and bad customer experiences. To maximize the rate of successful deployments, implement safety controls in the end-to-end release process to minimize deployment errors, with a goal of achieving zero deployment failures.\n    \n    \n      Customer example\n    \n    \n      AnyCompany Retail is on a mission to achieve minimal to zero downtime deployments, meaning that there's no perceivable impact to its users during deployments. To accomplish this, the company has established deployment patterns (see the following workflow diagram), such as rolling and blue/green deployments. All teams adopt one or more of these patterns in their CI/CD pipeline.\n    \n    \n          \n            CodeDeploy workflow for Amazon EC2\n            CodeDeploy workflow for Amazon ECS\n            CodeDeploy workflow for Lambda\n          \n        \n          \n            \n              \n                 \n                  \n                 \n                 \n              \n            \n            \n              \n                 \n                  \n                 \n                 \n              \n            \n            \n              \n                 \n                  \n                 \n                 \n              \n            \n          \n        \n     \n      \n      Implementation steps\n      \n         \n         \n         \n         \n         \n         \n      \n          \n            Use an approval workflow to initiate the sequence of production roll-out steps upon promotion to production .\n          \n        \n          \n            Use an automated deployment system such as AWS CodeDeploy. AWS CodeDeploy deployment options include in-place deployments for EC2/On-Premises and blue/green deployments for EC2/On-Premises, AWS Lambda, and Amazon ECS (see the preceding workflow diagram).\n            \n          \n          \n             \n          \n              \n                Where applicable, integrate AWS CodeDeploy with other AWS services or integrate AWS CodeDeploy with partner product and services.\n              \n            \n        \n          \n            Use blue/green deployments for databases such as Amazon Aurora and Amazon RDS.\n          \n        \n          \n            \n            Monitor deployments using Amazon CloudWatch, AWS CloudTrail, and Amazon Simple Notification Service (Amazon SNS) event notifications.\n          \n        \n          \n            Perform post-deployment automated testing including functional, security, regression, integration, and any load tests.\n          \n        \n          \n            \n            Troubleshoot deployment issues.\n          \n        \n      \n        Level of effort for the implementation plan: Medium\n      \n     \n   \n    Resources\n    \n      Related best practices:\n    \n    \n       \n       \n       \n    \n        \n          OPS05-BP02 Test and validate changes\n        \n      \n        \n          OPS05-BP09 Make frequent, small, reversible changes\n        \n      \n        \n          OPS05-BP10 Fully automate integration and deployment\n        \n      \n    \n      Related documents:\n    \n    \n       \n       \n       \n       \n       \n       \n       \n       \n       \n    \n        AWS Builders Library | Automating safe, hands-off deployments | Production deployments\n        \n      \n        AWS Builders Library | My CI/CD pipeline is my release captain | Safe, automatic\n          production releases\n       \n        AWS Whitepaper | Practicing Continuous Integration and Continuous Delivery on AWS |\n          Deployment methods\n      \n        AWS CodeDeploy User Guide\n      \n        Working with deployment configurations in AWS CodeDeploy\n      \n        Set up an API Gateway canary release deployment \n      \n        Amazon ECS Deployment Types\n      \n        Fully Managed Blue/Green Deployments in Amazon Aurora and Amazon RDS\n      \n        Blue/Green deployments with AWS Elastic Beanstalk\n      \n    \n      Related videos:\n    \n    \n       \n       \n    \n        re:Invent 2020 | Hands-off: Automating continuous delivery pipelines at Amazon\n      \n        re:Invent 2019 | Amazon's Approach to high-availability deployment\n      \n    \n      Related examples:\n    \n    \n       \n       \n       \n       \n       \n       \n       \n    \n        Try a Sample Blue/Green Deployment in AWS CodeDeploy\n      \n        \n          Workshop | Building CI/CD pipelines for Lambda canary deployments using AWS CDK\n        \n      \n        \n          Workshop | Building your first DevOps Blue/Green pipeline with Amazon ECS\n        \n      \n        \n          Workshop | Building your first DevOps Blue/Green pipeline with Amazon EKS\n        \n      \n        \n          Workshop | EKS GitOps with ArgoCD\n        \n      \n        \n          Workshop | CI/CD on AWS Workshop\n        \n      \n        \n          Implementing cross-account CI/CD with AWS SAM for container-based Lambda functions\n        \n      \n  \u003cdiv\u003e\u003cdiv\u003e\u003cdiv\u003e\u003cdiv id=\"js_error_message\"\u003e\u003cp\u003e\u003cimg src=\"https://d1ge0kk1l5kms0.cloudfront.net/images/G/01/webservices/console/warning.png\" alt=\"Warning\" /\u003e \u003cstrong\u003eJavascript is disabled or is unavailable in your browser.\u003c/strong\u003e\u003c/p\u003e\u003cp\u003eTo use the Amazon Web Services Documentation, Javascript must be enabled. Please refer to your browser's Help pages for instructions.\u003c/p\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003eDocument ConventionsOPS06-BP02 Test deploymentsOPS06-BP04 Automate testing and rollback",
  "OPS06-BP04 Automate testing and rollback\n    To increase the speed, reliability, and confidence of your deployment process, have a strategy for automated testing and rollback capabilities in pre-production and production environments. Automate testing when deploying to production to simulate human and system interactions that verify the changes being deployed. Automate rollback to revert back to a previous known good state quickly. The rollback should be initiated automatically on pre-defined conditions such as when the desired outcome of your change is not achieved or when the automated test fails. Automating these two activities improves your success rate for your deployments, minimizes recovery time, and reduces the potential impact to the business.\n  \n    Desired outcome: Your automated tests and rollback strategies are integrated into your continuous integration, continuous delivery (CI/CD) pipeline. Your monitoring is able to validate against your success criteria and initiate automatic rollback upon failure. This minimizes any impact to end users and customers. For example, when all testing outcomes have been satisfied, you promote your code into the production environment where automated regression testing is initiated, leveraging the same test cases. If regression test results do not match expectations, then automated rollback is initiated in the pipeline workflow.\n  \n    Common anti-patterns:\n  \n     \n     \n     \n     \n     \n     \n  \n      \n        Your systems are not architected in a way that allows them to be updated with smaller releases. As a result, you have difficulty in reversing those bulk changes during a failed deployment.\n      \n    \n      \n        Your deployment process consists of a series of manual steps. After you deploy changes to your workload, you start post-deployment testing. After testing, you realize that your workload is inoperable and customers are disconnected. You then begin rolling back to the previous version. All of these manual steps delay overall system recovery and cause a prolonged impact to your customers.\n      \n    \n      \n        You spent time developing automated test cases for functionality that is not frequently used in your application, minimizing the return on investment in your automated testing capability.\n      \n    \n      \n        Your release is comprised of application, infrastructure, patches and configuration updates that are independent from one another. However, you have a single CI/CD pipeline that delivers all changes at once. A failure in one component forces you to revert all changes, making your rollback complex and inefficient.\n      \n    \n      \n        Your team completes the coding work in sprint one and begins sprint two work, but your plan did not include testing until sprint three. As a result, automated tests revealed defects from sprint one that had to be resolved before testing of sprint two deliverables could be started and the entire release is delayed, devaluing your automated testing.\n      \n    \n      \n        Your automated regression test cases for the production release are complete, but you are not monitoring workload health. Since you have no visibility into whether or not the service has restarted, you are not sure if rollback is needed or if it has already occurred.\n      \n    \n    Benefits of establishing this best\n      practice: Automated testing increases the transparency of your testing process and your ability to cover more features in a shorter time period. By testing and validating changes in production, you are able to identify issues immediately. Improvement in consistency with automated testing tools allows for better detection of defects. By automatically rolling back to the previous version, the impact on your customers is minimized. Automated rollback ultimately inspires more confidence in your deployment capabilities by reducing business impact. Overall, these capabilities reduce time-to-delivery while ensuring quality.\n  \n    Level of risk exposed if this best practice\n    is not established: Medium\n  \n\n  Implementation guidance\n\n      \n        Automate testing of deployed environments to confirm desired outcomes more quickly. Automate rollback to a previous known good state when pre-defined outcomes are not achieved to minimize recovery time and reduce errors caused by manual processes. Integrate testing tools with your pipeline workflow to consistently test and minimize manual inputs. Prioritize automating test cases, such as those that mitigate the greatest risks and need to be tested frequently with every change. Additionally, automate rollback based on specific conditions that are pre-defined in your test plan.\n      \n     \n      \n      Implementation steps\n      \n         \n         \n         \n         \n         \n         \n         \n         \n      \n          \n            Establish a testing lifecycle for your development lifecycle that defines each stage of the testing process from requirements planning to test case development, tool configuration, automated testing, and test case closure.\n            \n          \n          \n             \n             \n          \n              \n                Create a workload-specific testing approach from your overall test strategy.\n              \n            \n              \n                Consider a continuous testing strategy where appropriate throughout the development lifecycle.\n              \n            \n        \n          \n            Select automated tools for testing and rollback based on your business requirements and pipeline investments.\n          \n        \n          \n            Decide which test cases you wish to automate and which should be performed manually. These can be defined based on business value priority of the feature being tested. Align all team members to this plan and verify accountability for performing manual tests.\n            \n          \n          \n             \n             \n             \n          \n              \n                Apply automated testing capabilities to specific test cases that make sense for automation, such as repeatable or frequently run cases, those that require repetitive tasks, or those that are required across multiple configurations.\n              \n            \n              \n                Define test automation scripts as well as the success criteria in the automation tool so continued workflow automation can be initiated when specific cases fail.\n              \n            \n              \n                Define specific failure criteria for automated rollback.\n              \n            \n        \n          \n            Prioritize test automation to drive consistent results with thorough test case development where complexity and human interaction have a higher risk of failure.\n          \n        \n          \n            Integrate your automated testing and rollback tools into your CI/CD pipeline.\n            \n          \n          \n             \n             \n          \n              \n                Develop clear success criteria for your changes.\n              \n            \n              \n                Monitor and observe to detect these criteria and automatically reverse changes when specific rollback criteria are met.\n              \n            \n        \n          \n            Perform different types of automated production testing, such as:\n            \n          \n          \n             \n             \n             \n             \n          \n              \n                A/B testing to show results in comparison to the current version between two user testing groups.\n              \n            \n              \n                Canary testing that allows you to roll out your change to a subset of users before releasing it to all.\n              \n            \n              \n                Feature-flag testing which allows a single feature of the new version at a time to be flagged on and off from outside the application so that each new feature can be validated one at a time.\n              \n            \n              \n                Regression testing to verify new functionality with existing interrelated components.\n              \n            \n        \n          \n            Monitor the operational aspects of the application, transactions, and interactions with other applications and components. Develop reports to show success of changes by workload so that you can identify what parts of the automation and workflow can be further optimized.\n            \n          \n          \n             \n             \n          \n              \n                Develop test result reports that help you make quick decisions on whether or not rollback procedures should be invoked.\n              \n            \n              \n                Implement a strategy that allows for automated rollback based upon pre-defined failure conditions that result from one or more of your test methods.\n              \n            \n        \n          \n            Develop your automated test cases to allow for reusability across future repeatable changes.\n          \n        \n      \n        Level of effort for the implementation plan: Medium\n      \n     \n   \n\n  Resources\n\n    \n      Related best practices:\n    \n    \n       \n       \n    \n        \n          OPS06-BP01 Plan for unsuccessful changes\n        \n      \n        \n          OPS06-BP02 Test deployments\n        \n      \n    \n      Related documents:\n    \n    \n       \n       \n       \n    \n        AWS Builders Library | Ensuring rollback safety during deployments\n        \n      \n        \n          Redeploy\n          and rollback a deployment with AWS CodeDeploy\n        \n      \n        \n          8 best practices when automating your deployments with AWS CloudFormation\n      \n    \n      Related examples:\n    \n    \n       \n    \n        \n          Serverless UI testing using Selenium, AWS Lambda, AWS Fargate, and AWS Developer Tools\n        \n      \n    \n      Related videos:\n    \n    \n       \n       \n    \n        \n          re:Invent 2020 | Hands-off: Automating continuous delivery pipelines at Amazon\n        \n      \n        \n          re:Invent 2019 | Amazon's Approach to high-availability deployment\n        \n      \n  \u003cdiv\u003e\u003cdiv\u003e\u003cdiv\u003e\u003cdiv id=\"js_error_message\"\u003e\u003cp\u003e\u003cimg src=\"https://d1ge0kk1l5kms0.cloudfront.net/images/G/01/webservices/console/warning.png\" alt=\"Warning\" /\u003e \u003cstrong\u003eJavascript is disabled or is unavailable in your browser.\u003c/strong\u003e\u003c/p\u003e\u003cp\u003eTo use the Amazon Web Services Documentation, Javascript must be enabled. Please refer to your browser's Help pages for instructions.\u003c/p\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003eDocument ConventionsOPS06-BP03 Employ safe deployment strategies OPS 7. How do you know that you are ready to support a workload? ",
  "OPS07-BP01 Ensure personnel capabilityHave a mechanism to validate that you have the appropriate number of trained personnel to support the workload. They must be trained on the platform and services that make up your workload. Provide them with the knowledge necessary to operate the workload. You must have enough trained personnel to support the normal operation of the workload and troubleshoot any incidents that occur. Have enough personnel so that you can rotate during on-call and vacations to avoid burnout. \n    Desired outcome:\n  \n     \n     \n  \n      \n        There are enough trained personnel to support the workload at times when the workload is available.\n      \n    \n      \n        You provide training for your personnel on the software and services that make up your workload.\n      \n    \n    Common anti-patterns:\n  \n     \n     \n  \n      Deploying a workload without team members trained to operate the platform and services in use. \n    \n      \n        Not having enough personnel to support on-call rotations or personnel taking time off.\n      \n    \n    Benefits of establishing this best\n    practice: \n  \n     \n     \n  \n      \n        Having skilled team members helps effective support of your workload. \n      \n    \n      \n        With enough team members, you can support the workload and on-call rotations while decreasing the risk of burnout.\n      \n    \n    Level of risk exposed if this best practice\n    is not established: High\n  \n\n  Implementation guidance\n    \n      Validate that there are sufficient trained personnel to support the workload. Verify that you have enough team members to cover normal operational activities, including on-call rotations.\n    \n    \n      Customer example\n    \n    \n      AnyCompany Retail makes sure that teams supporting the workload are properly staffed and trained. They have enough engineers to support an on-call rotation. Personnel get training on the software and platform that the workload is built on and are encouraged to earn certifications. There are enough personnel so that people can take time off while still supporting the workload and the on-call rotation.\n    \n     \n      \n      Implementation steps\n    \n    \n       \n       \n       \n    \n        \n          Assign an adequate number of personnel to operate and support your workload, including on-call duties, security issues, and lifecycle events, such as end of support and certificate rotation tasks. \n        \n      \n        \n          Train your personnel on the software and platforms that compose your workload. \n        \n        \n           \n           \n        \n            \n              AWS Training and Certification has a library of courses about AWS. They provide free and paid courses, online and in-person.\n            \n          \n            \n              AWS hosts events and webinars where you learn from AWS experts.\n            \n          \n      \n        Perform the following on a regular basis:\n        \n        \n           \n           \n           \n        \n            \n              Evaluate team size and skills as operating conditions and the workload change. \n            \n          \n            \n              Adjust team size and skills to match operational requirements.\n            \n          \n            \n              Verify ability and capacity to address planned lifecycle events, unplanned security, and operational notifications through AWS Health.\n            \n          \n      \n    \n      Level of effort for the implementation plan: High. Hiring and training a team to support a workload can take significant effort but has substantial long-term benefits.\n    \n     \n   \n\n  Resources\n    \n      Related best practices:\n    \n    \n       \n    \n        \n          OPS11-BP04 Perform knowledge management - Team members must have the information necessary to operate and support the workload. Knowledge management is the key to providing that.\n        \n      \n    \n      Related documents:\n    \n    \n       \n       \n    \n        \n          AWS Events and Webinars\n        \n      \n        \n          AWS Training and Certification\n        \n      \n  \u003cdiv\u003e\u003cdiv\u003e\u003cdiv\u003e\u003cdiv id=\"js_error_message\"\u003e\u003cp\u003e\u003cimg src=\"https://d1ge0kk1l5kms0.cloudfront.net/images/G/01/webservices/console/warning.png\" alt=\"Warning\" /\u003e \u003cstrong\u003eJavascript is disabled or is unavailable in your browser.\u003c/strong\u003e\u003c/p\u003e\u003cp\u003eTo use the Amazon Web Services Documentation, Javascript must be enabled. Please refer to your browser's Help pages for instructions.\u003c/p\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003eDocument Conventions OPS 7. How do you know that you are ready to support a workload? OPS07-BP02: Ensure a consistent review of operational readiness",
  "OPS07-BP02 Ensure a consistent review of\n      operational readinessUse Operational Readiness Reviews (ORRs) to validate that you can\n    operate your workload. ORR is a mechanism developed at Amazon to\n    validate that teams can safely operate their workloads. An ORR is a\n    review and inspection process using a checklist of requirements. An\n    ORR is a self-service experience that teams use to certify their\n    workloads. ORRs include best practices from lessons learned from our\n    years of building software.\n   An ORR checklist is composed of architectural recommendations, operational process, event\n    management, and release quality. Our Correction of Error (CoE) process is a major driver of\n    these items. Your own post-incident analysis should drive the evolution of your own ORR. An ORR\n    is not only about following best practices but preventing the recurrence of events that you’ve\n    seen before. Lastly, security, governance, and compliance requirements can also be included in\n    an ORR.  Run ORRs before a workload launches to general availability and then throughout the\n    software development lifecycle. Running the ORR before launch increases your ability to operate\n    the workload safely. Periodically re-run your ORR on the workload to catch any drift from best\n    practices. You can have ORR checklists for new services launches and ORRs for periodic reviews.\n    This helps keep you up to date on new best practices that arise and incorporate lessons learned\n    from post-incident analysis. As your use of the cloud matures, you can build ORR requirements\n    into your architecture as defaults. \n    Desired outcome:  You have an ORR\n    checklist with best practices for your organization. ORRs are\n    conducted before workloads launch. ORRs are run periodically over\n    the course of the workload lifecycle.\n  \n    Common anti-patterns:\n  \n     \n     \n     \n     \n     \n  \n      You launch a workload without knowing if you can operate it. \n    \n      Governance and security requirements are not included in certifying a workload for\n        launch. \n    \n      Workloads are not re-evaluated periodically. \n    \n      Workloads launch without required procedures in place. \n    \n      You see repetition of the same root cause failures in multiple workloads. \n    \n    Benefits of establishing this best\n    practice:\n  \n     \n     \n     \n     \n  \n      \n        Your workloads include architecture, process, and management\n        best practices.\n      \n    \n      \n        Lessons learned are incorporated into your ORR process.\n      \n    \n      \n        Required procedures are in place when workloads launch.\n      \n    \n      \n        ORRs are run throughout the software lifecycle of your\n        workloads.\n      \n    \n    Level of risk if this best practice is not\n    established: High\n  \n\n  Implementation guidance\n\n      \n     An ORR is two things: a process and a checklist. Your ORR process should be adopted by\n      your organization and supported by an executive sponsor. At a minimum, ORRs must be conducted\n      before a workload launches to general availability. Run the ORR throughout the software\n      development lifecycle to keep it up to date with best practices or new requirements. The ORR\n      checklist should include configuration items, security and governance requirements, and best\n      practices from your organization. Over time, you can use services, such as  AWS Config,\n        AWS Security Hub, and AWS Control Tower Guardrails, to\n      build best practices from the ORR into guardrails for automatic detection of best practices. \n    \n      Customer example\n    \n    \n      After several production incidents, AnyCompany Retail decided to\n      implement an ORR process. They built a checklist composed of best\n      practices, governance and compliance requirements, and lessons\n      learned from outages. New workloads conduct ORRs before they\n      launch. Every workload conducts a yearly ORR with a subset of best\n      practices to incorporate new best practices and requirements that\n      are added to the ORR checklist. Over time, AnyCompany Retail used\n      AWS Config to detect some best practices, speeding up the ORR\n      process.\n    \n    \n      Implementation steps\n    \n    \n      To learn more about ORRs, read the\n      Operational\n      Readiness Reviews (ORR) whitepaper. It provides detailed\n      information on the history of the ORR process, how to build your\n      own ORR practice, and how to develop your ORR checklist. The\n      following steps are an abbreviated version of that document. For\n      an in-depth understanding of what ORRs are and how to build your\n      own, we recommend reading that whitepaper.\n    \n    \n       \n       \n       \n       \n       \n       \n    \n        Gather the key stakeholders together, including representatives from security,\n          operations, and development. \n      \n        Have each stakeholder provide at least one requirement. For the first iteration, try\n          to limit the number of items to thirty or less. \n        \n           \n        \n            \n              Appendix\n              B: Example ORR questions from the Operational\n              Readiness Reviews (ORR) whitepaper contains sample\n              questions that you can use to get started.\n            \n          \n      \n        Collect your requirements into a spreadsheet. \n        \n           \n        \n            You can use custom lenses in\n              the AWS Well-Architected Tool to develop your ORR\n              and share them across your accounts and AWS Organization. \n          \n      \n        Identify one workload to conduct the ORR on. A pre-launch workload or an internal\n          workload is ideal. \n      \n        Run through the ORR checklist and take note of any discoveries made. Discoveries might\n          be acceptable if a mitigation is in place. For any discovery that lacks a mitigation, add\n          those to your backlog of items and implement them before launch. \n      \n        Continue to add best practices and requirements to your ORR checklist over time. \n      \n     Support customers with Enterprise Support can request the Operational Readiness\n        Review Workshop from their Technical Account Manager. The workshop is an interactive\n        working backwards session to develop your own ORR\n      checklist. \n    \n      Level of effort for the implementation\n      plan: High. Adopting an ORR practice in your\n      organization requires executive sponsorship and stakeholder\n      buy-in. Build and update the checklist with inputs from across\n      your organization.\n    \n   \n\n  Resources\n\n      \n    \n      Related best practices:\n    \n    \n       \n       \n       \n       \n       \n       \n       \n       \n    \n        OPS01-BP03 Evaluate governance requirements – Governance requirements are a natural fit\n          for an ORR checklist. \n      \n        OPS01-BP04 Evaluate compliance requirements  – Compliance requirements are sometimes\n          included in an ORR checklist. Other times they are a separate process. \n      \n        OPS03-BP07 Resource teams appropriately – Team capability is a good candidate for an\n          ORR requirement. \n      \n        OPS06-BP01 Plan for unsuccessful changes  – A rollback or\n          rollforward plan must be established before you launch your workload. \n      \n        OPS07-BP01 Ensure personnel capability – To support a workload you must\n          have the required personnel. \n      \n        SEC01-BP03 Identify and validate control objectives – Security control\n          objectives make excellent ORR requirements. \n      \n        REL13-BP01 Define recovery objectives for downtime and data loss – Disaster\n          recovery plans are a good ORR requirement. \n      \n        COST02-BP01 Develop\n            policies based on your organization requirements – Cost management policies are\n          good to include in your ORR checklist. \n      \n    \n      Related documents:\n    \n    \n       \n       \n       \n       \n    \n        \n          AWS Control Tower - Guardrails in AWS Control Tower\n        \n      \n        \n          AWS Well-Architected Tool - Custom Lenses\n        \n      \n        \n          Operational\n          Readiness Review Template by Adrian Hornsby\n        \n      \n        \n          Operational\n          Readiness Reviews (ORR) Whitepaper\n        \n      \n    \n      Related videos:\n    \n    \n       \n    \n        \n          AWS Supports You | Building an Effective Operational Readiness\n          Review (ORR)\n        \n      \n    \n      Related examples:\n    \n    \n       \n    \n        \n          Sample\n          Operational Readiness Review (ORR) Lens\n        \n      \n    \n      Related services:\n    \n    \n       \n       \n       \n       \n    \n        \n          AWS Config\n        \n      \n        \n          AWS Control Tower\n        \n      \n        \n          AWS Security Hub\n        \n      \n        \n          AWS Well-Architected Tool\n        \n      \n  \u003cdiv\u003e\u003cdiv\u003e\u003cdiv\u003e\u003cdiv id=\"js_error_message\"\u003e\u003cp\u003e\u003cimg src=\"https://d1ge0kk1l5kms0.cloudfront.net/images/G/01/webservices/console/warning.png\" alt=\"Warning\" /\u003e \u003cstrong\u003eJavascript is disabled or is unavailable in your browser.\u003c/strong\u003e\u003c/p\u003e\u003cp\u003eTo use the Amazon Web Services Documentation, Javascript must be enabled. Please refer to your browser's Help pages for instructions.\u003c/p\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003eDocument ConventionsOPS07-BP01 Ensure personnel capabilityOPS07-BP03 Use runbooks to perform procedures",
  "OPS07-BP03 Use runbooks to perform procedures\n    A runbook is a documented process to achieve a\n    specific outcome. Runbooks consist of a series of steps that someone\n    follows to get something done. Runbooks have been used in operations\n    going back to the early days of aviation. In cloud operations, we\n    use runbooks to reduce risk and achieve desired outcomes. At its\n    simplest, a runbook is a checklist to complete a task.\n  \n    Runbooks are an essential part of operating your workload. From\n    onboarding a new team member to deploying a major release, runbooks\n    are the codified processes that provide consistent outcomes no\n    matter who uses them. Runbooks should be published in a central\n    location and updated as the process evolves, as updating runbooks is\n    a key component of a change management process. They should also\n    include guidance on error handling, tools, permissions, exceptions,\n    and escalations in case a problem occurs.\n  \n    As your organization matures, begin automating runbooks. Start with\n    runbooks that are short and frequently used. Use scripting languages\n    to automate steps or make steps easier to perform. As you automate\n    the first few runbooks, you'll dedicate time to automating more\n    complex runbooks. Over time, most of your runbooks should be\n    automated in some way.\n  \n    Desired outcome: Your team has a\n    collection of step-by-step guides for performing workload tasks. The\n    runbooks contain the desired outcome, necessary tools and\n    permissions, and instructions for error handling. They are stored in\n    a central location (version control system) and updated frequently.\n    For example, your runbooks provide capabilities for your teams to\n    monitor, communicate, and respond to AWS Health events for critical\n    accounts during application alarms, operational issues, and planned\n    lifecycle events.\n  \n    Common anti-patterns:\n  \n     \n     \n     \n     \n  \n      \n        Relying on memory to complete each step of a process.\n      \n    \n      \n        Manually deploying changes without a checklist.\n      \n    \n      \n        Different team members performing the same process but with\n        different steps or outcomes.\n      \n    \n      \n        Letting runbooks drift out of sync with system changes and\n        automation.\n      \n    \n    Benefits of establishing this best\n    practice:\n  \n     \n     \n     \n     \n  \n      \n        Reducing error rates for manual tasks.\n      \n    \n      \n        Operations are performed in a consistent manner.\n      \n    \n      \n        New team members can start performing tasks sooner.\n      \n    \n      \n        Runbooks can be automated to reduce toil.\n      \n    \n    Level of risk exposed if this best practice\n    is not established: Medium\n  \n\n  Implementation guidance\n\n      \n    \n      Runbooks can take several forms depending on the maturity level of\n      your organization. At a minimum, they should consist of a\n      step-by-step text document. The desired outcome should be clearly\n      indicated. Clearly document necessary special permissions or\n      tools. Provide detailed guidance on error handling and escalations\n      in case something goes wrong. List the runbook owner and publish\n      it in a central location. Once your runbook is documented,\n      validate it by having someone else on your team run it. As\n      procedures evolve, update your runbooks in accordance with your\n      change management process.\n    \n    \n      Your text runbooks should be automated as your organization\n      matures. Using services like\n      AWS Systems Manager automations, you can transform flat text\n      into automations that can be run against your workload. These\n      automations can be run in response to events, reducing the\n      operational burden to maintain your workload. AWS Systems Manager\n      Automation also provides a low-code\n      visual\n      design experience to create automation runbooks more\n      easily.\n    \n    \n      Customer example\n    \n    \n      AnyCompany Retail must perform database schema updates during\n      software deployments. The Cloud Operations Team worked with the\n      Database Administration Team to build a runbook for manually\n      deploying these changes. The runbook listed each step in the\n      process in checklist form. It included a section on error handling\n      in case something went wrong. They published the runbook on their\n      internal wiki along with their other runbooks. The Cloud\n      Operations Team plans to automate the runbook in a future sprint.\n    \n     \n      \n      Implementation steps\n    \n    \n      If you don't have an existing document\n      repository, a version control repository is a great place to start\n      building your runbook library. You can build your runbooks using\n      Markdown. We have provided an example runbook template that you\n      can use to start building runbooks.\n    \n    # Runbook Title\n## Runbook Info\n| Runbook ID | Description | Tools Used | Special Permissions | Runbook Author | Last Updated | Escalation POC | \n|-------|-------|-------|-------|-------|-------|-------|\n| RUN001 | What is this runbook for? What is the desired outcome? | Tools | Permissions | Your Name | 2022-09-21 | Escalation Name |\n## Steps\n1. Step one\n2. Step two\n    \n       \n       \n       \n       \n       \n       \n       \n    \n        \n          If you don't have an existing documentation repository or\n          wiki, create a new version control repository in your version\n          control system.\n        \n      \n        \n          Identify a process that does not have a runbook. An ideal\n          process is one that is conducted semiregularly, short in\n          number of steps, and has low impact failures.\n        \n      \n        \n          In your document repository, create a new draft Markdown\n          document using the template. Fill in Runbook Title and the\n          required fields under Runbook Info.\n        \n      \n        \n          Starting with the first step, fill in the Steps portion of the\n          runbook.\n        \n      \n        \n          Give the runbook to a team member. Have them use the runbook\n          to validate the steps. If something is missing or needs\n          clarity, update the runbook.\n        \n      \n        \n          Publish the runbook to your internal documentation store. Once\n          published, tell your team and other stakeholders.\n        \n      \n        \n          Over time, you'll build a library of runbooks. As that library\n          grows, start working to automate runbooks.\n        \n      \n    \n      Level of effort for the implementation\n      plan: Low. The minimum standard for a runbook is a\n      step-by-step text guide. Automating runbooks can increase the\n      implementation effort.\n    \n     \n   \n\n  Resources\n\n      \n    \n      Related best practices:\n    \n    \n       \n       \n       \n       \n       \n    \n        \n          OPS02-BP02\n          Processes and procedures have identified owners\n        \n      \n        \n          OPS07-BP04\n          Use playbooks to investigate issues\n        \n      \n        \n          OPS10-BP01\n          Use a process for event, incident, and problem\n          management\n        \n      \n        \n          OPS10-BP02\n          Have a process per alert\n        \n      \n        \n          OPS11-BP04\n          Perform knowledge management\n        \n      \n    \n      Related documents:\n    \n    \n       \n       \n       \n       \n       \n    \n        \n          AWS           Well-Architected Framework: Concepts: Runbook\n          development\n        \n      \n        \n          Achieving\n          Operational Excellence using automated playbook and\n          runbook\n        \n      \n        \n          AWS Systems Manager: Working with runbooks\n        \n      \n        \n          Migration\n          playbook for AWS large migrations - Task 4: Improving your\n          migration runbooks\n        \n      \n        \n          Use\n          AWS Systems Manager Automation runbooks to resolve operational\n          tasks\n        \n      \n    \n      Related videos:\n    \n    \n       \n       \n       \n    \n        \n          AWS           re:Invent 2019: DIY guide to runbooks, incident reports, and\n          incident response\n        \n      \n        \n          How\n          to automate IT Operations on AWS | Amazon Web Services\n        \n      \n        \n          Integrate\n          Scripts into AWS Systems Manager\n        \n      \n    \n      Related examples:\n    \n    \n       \n       \n       \n       \n       \n       \n       \n       \n    \n        \n          Well-Architected\n          Labs: Automating operations with Playbooks and Runbooks\n        \n      \n        \n          AWS           Blog Post: Build a Cloud Automation Practice for Operational\n          Excellence: Best Practices from AWS Managed Services\n        \n      \n        \n          AWS Systems Manager: Automation walkthroughs\n        \n      \n        \n          AWS Systems Manager: Restore a root volume from the latest\n          snapshot runbook\n        \n      \n        \n          Building\n          an AWS incident response runbook using Jupyter notebooks and\n          CloudTrail Lake\n        \n      \n        \n          Gitlab\n          - Runbooks\n        \n      \n        \n          Rubix - A\n          Python library for building runbooks in Jupyter\n          Notebooks\n        \n      \n        \n          Using\n          Document Builder to create a custom runbook\n        \n      \n    \n      Related services:\n    \n    \n       \n    \n        \n          AWS Systems Manager Automation\n        \n      \n  \u003cdiv\u003e\u003cdiv\u003e\u003cdiv\u003e\u003cdiv id=\"js_error_message\"\u003e\u003cp\u003e\u003cimg src=\"https://d1ge0kk1l5kms0.cloudfront.net/images/G/01/webservices/console/warning.png\" alt=\"Warning\" /\u003e \u003cstrong\u003eJavascript is disabled or is unavailable in your browser.\u003c/strong\u003e\u003c/p\u003e\u003cp\u003eTo use the Amazon Web Services Documentation, Javascript must be enabled. Please refer to your browser's Help pages for instructions.\u003c/p\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003eDocument ConventionsOPS07-BP02: Ensure a consistent review of operational readinessOPS07-BP04 Use playbooks to investigate issues",
  "OPS07-BP04 Use playbooks to investigate issues\n    Playbooks are step-by-step guides used to investigate an incident.\n    When incidents happen, playbooks are used to investigate, scope\n    impact, and identify a root cause. Playbooks are used for a variety\n    of scenarios, from failed deployments to security incidents. In many\n    cases, playbooks identify the root cause that a runbook is used to\n    mitigate. Playbooks are an essential component of your\n    organization's incident response plans.\n  \n    A good playbook has several key features. It guides the user, step\n    by step, through the process of discovery. Thinking outside-in, what\n    steps should someone follow to diagnose an incident? Clearly define\n    in the playbook if special tools or elevated permissions are needed\n    in the playbook. Having a communication plan to update stakeholders\n    on the status of the investigation is a key component. In situations\n    where a root cause can't be identified, the playbook should have an\n    escalation plan. If the root cause is identified, the playbook\n    should point to a runbook that describes how to resolve it.\n    Playbooks should be stored centrally and regularly maintained. If\n    playbooks are used for specific alerts, provide your team with\n    pointers to the playbook within the alert.\n  \n    As your organization matures, automate your playbooks. Start with\n    playbooks that cover low-risk incidents. Use scripting to automate\n    the discovery steps. Make sure that you have companion runbooks to\n    mitigate common root causes.\n  \n    Desired outcome: Your\n    organization has playbooks for common incidents. The playbooks are\n    stored in a central location and available to your team members.\n    Playbooks are updated frequently. For any known root causes,\n    companion runbooks are built.\n  \n    Common anti-patterns:\n  \n     \n     \n     \n     \n  \n      \n        There is no standard way to investigate an incident.\n      \n    \n      \n        Team members rely on muscle memory or institutional knowledge to\n        troubleshoot a failed deployment.\n      \n    \n      \n        New team members learn how to investigate issues through trial\n        and error.\n      \n    \n      \n        Best practices for investigating issues are not shared across\n        teams.\n      \n    \n    Benefits of establishing this best\n    practice:\n  \n     \n     \n     \n     \n     \n  \n      \n        Playbooks boost your efforts to mitigate incidents.\n      \n    \n      \n        Different team members can use the same playbook to identify a\n        root cause in a consistent manner.\n      \n    \n      \n        Known root causes can have runbooks developed for them, speeding\n        up recovery time.\n      \n    \n      \n        Playbooks help team members to start contributing sooner.\n      \n    \n      \n        Teams can scale their processes with repeatable playbooks.\n      \n    \n    Level of risk exposed if this best practice\n    is not established: Medium\n  \n\n  Implementation guidance\n\n      \n    \n      How you build and use playbooks depends on the maturity of your\n      organization. If you are new to the cloud, build playbooks in text\n      form in a central document repository. As your organization\n      matures, playbooks can become semi-automated with scripting\n      languages like Python. These scripts can be run inside a Jupyter\n      notebook to speed up discovery. Advanced organizations have fully\n      automated playbooks for common issues that are auto-remediated\n      with runbooks.\n    \n    \n      Start building your playbooks by listing common incidents that\n      happen to your workload. Choose playbooks for incidents that are\n      low risk and where the root cause has been narrowed down to a few\n      issues to start. After you have playbooks for simpler scenarios,\n      move on to the higher risk scenarios or scenarios where the root\n      cause is not well known.\n    \n    \n      Your text playbooks should be automated as your organization\n      matures. Using services like\n      AWS Systems Manager Automations, flat text can be transformed\n      into automations. These automations can be run against your\n      workload to speed up investigations. These automations can be\n      activated in response to events, reducing the mean time to\n      discover and resolve incidents.\n    \n    \n      Customers can use\n      AWS Systems Manager Incident Manager to respond to incidents.\n      This service provides a single interface to triage incidents,\n      inform stakeholders during discovery and mitigation, and\n      collaborate throughout the incident. It uses AWS Systems Manager\n      Automations to speed up detection and recovery.\n    \n    \n      Customer example\n    \n    \n      A production incident impacted AnyCompany Retail. The on-call\n      engineer used a playbook to investigate the issue. As they\n      progressed through the steps, they kept the key stakeholders,\n      identified in the playbook, up to date. The engineer identified\n      the root cause as a race condition in a backend service. Using a\n      runbook, the engineer relaunched the service, bringing AnyCompany\n      Retail back online.\n    \n     \n\n  Implementation steps\n\n      \n      \n        If you don't have an existing document repository, we suggest\n        creating a version control repository for your playbook library.\n        You can build your playbooks using Markdown, which is compatible\n        with most playbook automation systems. If you are starting from\n        scratch, use the following example playbook template.\n      \n      # Playbook Title\n## Playbook Info\n| Playbook ID | Description | Tools Used | Special Permissions | Playbook Author | Last Updated | Escalation POC | Stakeholders | Communication Plan |\n|-------|-------|-------|-------|-------|-------|-------|-------|-------|\n| RUN001 | What is this playbook for? What incident is it used for? | Tools | Permissions | Your Name | 2022-09-21 | Escalation Name | Stakeholder Name | How will updates be communicated during the investigation? |\n## Steps\n1. Step one\n2. Step two\n      \n         \n         \n         \n         \n         \n         \n         \n      \n          \n            If you don't have an existing document repository or wiki,\n            create a new version control repository for your playbooks\n            in your version control system.\n          \n        \n          \n            Identify a common issue that requires investigation. This\n            should be a scenario where the root cause is limited to a\n            few issues and resolution is low risk.\n          \n        \n          \n            Using the Markdown template, fill in the Playbook Name\n            section and the fields under Playbook Info.\n          \n        \n          \n            Fill in the troubleshooting steps. Be as clear as possible\n            on what actions to perform or what areas you should\n            investigate.\n          \n        \n          \n            Give a team member the playbook and have them go through it\n            to validate it. If there's anything missing or something\n            isn't clear, update the playbook.\n          \n        \n          \n            Publish your playbook in your document repository and inform\n            your team and any stakeholders.\n          \n        \n          \n            This playbook library will grow as you add more playbooks.\n            Once you have several playbooks, start automating them using\n            tools like AWS Systems Manager Automations to keep\n            automation and playbooks in sync.\n          \n        \n      \n        Level of effort for the implementation\n        plan: Low. Your playbooks should be text documents\n        stored in a central location. More mature organizations will\n        move towards automating playbooks.\n      \n     \n   \n\n  Resources\n\n      \n    \n      Related best practices:\n    \n    \n       \n       \n       \n       \n       \n    \n        \n          OPS02-BP02\n          Processes and procedures have identified owners\n        \n      \n        \n          OPS07-BP03\n          Use runbooks to perform procedures\n        \n      \n        \n          OPS10-BP01\n          Use a process for event, incident, and problem\n          management\n        \n      \n        \n          OPS10-BP02\n          Have a process per alert\n        \n      \n        \n          OPS11-BP04\n          Perform knowledge management\n        \n      \n    \n      Related documents:\n    \n    \n       \n       \n       \n       \n    \n        \n          AWS           Well-Architected Framework: Concepts: Playbook\n          development\n        \n      \n        \n          Achieving\n          Operational Excellence using automated playbook and\n          runbook\n        \n      \n        \n          AWS Systems Manager: Working with runbooks\n        \n      \n        \n          Use\n          AWS Systems Manager Automation runbooks to resolve operational\n          tasks\n        \n      \n    \n      Related videos:\n    \n    \n       \n       \n       \n    \n        \n          AWS           re:Invent 2019: DIY guide to runbooks, incident reports, and\n          incident response (SEC318-R1)\n        \n      \n        \n          AWS Systems Manager Incident Manager - AWS Virtual\n          Workshops\n        \n      \n        \n          Integrate\n          Scripts into AWS Systems Manager\n        \n      \n    \n      Related examples:\n    \n    \n       \n       \n       \n       \n       \n    \n        \n          AWS           Customer Playbook Framework\n        \n      \n        \n          AWS Systems Manager: Automation walkthroughs\n        \n      \n        \n          Building\n          an AWS incident response runbook using Jupyter notebooks and\n          CloudTrail Lake\n        \n      \n        \n          Rubix – A\n          Python library for building runbooks in Jupyter\n          Notebooks\n        \n      \n        \n          Using\n          Document Builder to create a custom runbook\n        \n      \n    \n      Related services:\n    \n    \n       \n       \n    \n        \n          AWS Systems Manager Automation\n        \n      \n        \n          AWS Systems Manager Incident Manager\n        \n      \n  \u003cdiv\u003e\u003cdiv\u003e\u003cdiv\u003e\u003cdiv id=\"js_error_message\"\u003e\u003cp\u003e\u003cimg src=\"https://d1ge0kk1l5kms0.cloudfront.net/images/G/01/webservices/console/warning.png\" alt=\"Warning\" /\u003e \u003cstrong\u003eJavascript is disabled or is unavailable in your browser.\u003c/strong\u003e\u003c/p\u003e\u003cp\u003eTo use the Amazon Web Services Documentation, Javascript must be enabled. Please refer to your browser's Help pages for instructions.\u003c/p\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003eDocument ConventionsOPS07-BP03 Use runbooks to perform proceduresOPS07-BP05 Make informed decisions to deploy systems and\n  changes",
  "OPS07-BP05 Make informed decisions to deploy systems and\n  changesHave processes in place for successful and unsuccessful changes to your workload. A pre-mortem is an exercise where a team simulates a failure to develop mitigation strategies. Use pre-mortems to anticipate failure and create procedures where appropriate. Evaluate the benefits and risks of deploying changes to your workload. Verify that all changes comply with governance. \n    Desired outcome:\n  \n     \n     \n  \n      \n        You make informed decisions when deploying changes to your workload.\n      \n    \n      \n        Changes comply with governance.\n      \n    \n    Common anti-patterns:\n  \n     \n     \n     \n  \n      Deploying a change to our workload without a process to handle a failed deployment.\n    \n      Making changes to your production environment that are out of compliance with governance requirements.\n    \n      Deploying a new version of your workload without establishing a baseline for resource utilization.\n    \n    Benefits of establishing this best\n    practice: \n  \n     \n     \n  \n      \n        You are prepared for unsuccessful changes to your workload.\n      \n    \n      \n        Changes to your workload are compliant with governance policies.\n      \n    \n    Level of risk exposed if this best practice\n    is not established: Low\n  \n\n  Implementation guidance\n    \n      Use pre-mortems to develop processes for unsuccessful changes. Document your processes for unsuccessful changes. Ensure that all changes comply with governance. Evaluate the benefits and risks to deploying changes to your workload.\n    \n    \n      Customer example\n    \n    \n      AnyCompany Retail regularly conducts pre-mortems to validate their processes for unsuccessful changes. They document their processes in a shared Wiki and update it frequently. All changes comply with governance requirements.\n    \n    \n      Implementation steps\n    \n    \n       \n       \n       \n    \n        \n          Make informed decisions when deploying changes to your workload. Establish and review criteria for a successful deployment. Develop scenarios or criteria that would initiate a rollback of a change. Weigh the benefits of deploying changes against the risks of an unsuccessful change. \n        \n      \n        \n          Verify that all changes comply with governance policies. \n        \n      \n        \n          Use pre-mortems to plan for unsuccessful changes and document mitigation strategies. Run a table-top exercise to model an unsuccessful change and validate roll-back procedures.\n        \n      \n    \n      Level of effort for the implementation plan: Moderate. Implementing a practice of pre-mortems requires coordination and effort from stakeholders across your organization\n    \n   \n    \n    Resources\n    \n      Related best practices:\n    \n    \n       \n       \n       \n       \n    \n        \n          OPS01-BP03 Evaluate governance requirements - Governance requirements are a key factor in determining whether to deploy a change.\n        \n      \n        \n          OPS06-BP01 Plan for unsuccessful changes - Establish plans to mitigate a failed deployment and use pre-mortems to validate them.\n        \n      \n        \n          OPS06-BP02 Test deployments - Every software change should be properly tested before deployment in order to reduce defects in production.\n        \n      \n        \n          OPS07-BP01 Ensure personnel capability - Having enough trained personnel to support the workload is essential to making an informed decision to deploy a system change.\n        \n      \n    \n      Related documents:\n    \n    \n       \n       \n       \n    \n        \n          Amazon Web Services: Risk and Compliance\n        \n      \n        AWS Shared Responsibility Model\n        \n      \n        \n          Governance in the AWS Cloud: The Right Balance Between Agility and Safety\n        \n      \n  \u003cdiv\u003e\u003cdiv\u003e\u003cdiv\u003e\u003cdiv id=\"js_error_message\"\u003e\u003cp\u003e\u003cimg src=\"https://d1ge0kk1l5kms0.cloudfront.net/images/G/01/webservices/console/warning.png\" alt=\"Warning\" /\u003e \u003cstrong\u003eJavascript is disabled or is unavailable in your browser.\u003c/strong\u003e\u003c/p\u003e\u003cp\u003eTo use the Amazon Web Services Documentation, Javascript must be enabled. Please refer to your browser's Help pages for instructions.\u003c/p\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003eDocument ConventionsOPS07-BP04 Use playbooks to investigate issuesOPS07-BP06 Create support plans for production workloads",
  "OPS07-BP06 Create support plans for production workloads\n    Enable support for any software and services that your production workload relies on.\n    Select an appropriate support level to meet your production service-level needs.  \n    Support plans for these dependencies are necessary in case there is a service \n    disruption or software issue. Document support plans and how to request support \n    for all service and software vendors. Implement mechanisms that verify that \n    support points of contacts are kept up to date.\n  \n    Desired outcome:\n  \n     \n     \n     \n  \n      \n        Implement support plans for software and services that production workloads rely on.\n      \n    \n      \n        Choose an appropriate support plan based on service-level needs.\n      \n    \n      \n        Document the support plans, support levels, and how to request support.\n      \n    \n    Common anti-patterns:\n  \n     \n     \n     \n  \n      \n        You have no support plan for a critical software vendor. Your workload \n        is impacted by them and you can do nothing to expedite a fix or get \n        timely updates from the vendor.\n      \n    \n      \n        A developer that was the primary point of contact for a software vendor left \n        the company. You are not able to reach the vendor support directly. You must \n        spend time researching and navigating generic contact systems, increasing the \n        time required to respond when needed.\n      \n    \n      \n        A production outage occurs with a software vendor. There is no documentation on \n        how to file a support case.\n      \n    \n    Benefits of establishing this best practice:\n  \n     \n     \n     \n  \n      \n        With the appropriate support level, you are able to get a response in the time frame necessary to meet service-level needs.\n      \n    \n      \n        As a supported customer you can escalate if there are production issues.\n      \n    \n      \n        Software and services vendors can assist in troubleshooting during an incident.\n      \n    \n    Level of risk exposed if this best practice\n    is not established: Low\n  \n\n  Implementation guidance\n    \n      Enable support plans for any software and services vendors that your production workload \n      relies on. Set up appropriate support plans to meet service-level needs. For AWS customers, \n      this means activating AWS Business Support or greater on any accounts where you have production \n      workloads. Meet with support vendors on a regular cadence to get updates about support \n      offerings, processes, and contacts. Document how to request support from software and \n      services vendors, including how to escalate if there is an outage. Implement mechanisms \n      to keep support contacts up to date.\n    \n    \n      Customer example\n    \n    \n      At AnyCompany Retail, all commercial software and services dependencies have support plans. \n      For example, they have AWS Enterprise Support activated on all accounts with production workloads. \n      Any developer can raise a support case when there is an issue. There is a wiki page with \n      information on how to request support, whom to notify, and best practices for expediting a case.\n    \n    \n      Implementation steps\n    \n    \n       \n       \n       \n       \n    \n        \n          Work with stakeholders in your organization to identify software and services vendors that your workload relies on. Document these dependencies.\n        \n      \n        \n          Determine service-level needs for your workload. Select a support plan that aligns with them.\n        \n      \n        \n          For commercial software and services, establish a support plan with the vendors.\n        \n        \n           \n        \n            \n              Subscribing to AWS Business Support or greater for all production accounts provides faster \n              response time from AWS Support and strongly recommended. If you don’t have premium support, \n              you must have an action plan to handle issues, which require help from AWS Support. AWS Support \n              provides a mix of tools and technology, people, and programs designed to proactively help \n              you optimize performance, lower costs, and innovate faster. In addition, AWS Business Support provides \n              additional benefits, including API access to AWS Trusted Advisor and AWS Health for programmatic integration with your systems, alongside other access methods like the AWS Management Console and Amazon EventBridge channels.\n            \n          \n      \n        \n          Document the support plan in your knowledge management tool. Include how to request support, \n          who to notify if a support case is filed, and how to escalate during an incident. A wiki is \n          a good mechanism to allow anyone to make necessary updates to documentation when they become\n          aware of changes to support processes or contacts.\n        \n      \n    \n      Level of effort for the implementation plan: Low. Most software and services vendors \n      offer opt-in support plans. Documenting and sharing support best practices on your knowledge management system \n      verifies that your team knows what to do when there is a production issue.\n    \n\n      \n    \n   \n    \n    Resources\n    \n      Related best practices:\n    \n    \n       \n    \n        \n          OPS02-BP02 Processes and procedures have identified\n  owners\n        \n      \n    \n      Related documents:\n    \n    \n       \n    \n        AWS Support Plans\n        \n      \n    \n      Related services:\n    \n    \n       \n       \n    \n        AWS Business Support\n        \n      \n        AWS Enterprise Support\n        \n      \n  \u003cdiv\u003e\u003cdiv\u003e\u003cdiv\u003e\u003cdiv id=\"js_error_message\"\u003e\u003cp\u003e\u003cimg src=\"https://d1ge0kk1l5kms0.cloudfront.net/images/G/01/webservices/console/warning.png\" alt=\"Warning\" /\u003e \u003cstrong\u003eJavascript is disabled or is unavailable in your browser.\u003c/strong\u003e\u003c/p\u003e\u003cp\u003eTo use the Amazon Web Services Documentation, Javascript must be enabled. Please refer to your browser's Help pages for instructions.\u003c/p\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003eDocument ConventionsOPS07-BP05 Make informed decisions to deploy systems and\n  changes                Operate             ",
  "OPS08-BP01 Analyze workload metrics\n    After implementing application telemetry, regularly analyze the collected metrics. While latency, requests, errors, and capacity (or quotas) provide insights into system performance, it's vital to prioritize the review of business outcome metrics. This ensures you're making data-driven decisions aligned with your business objectives.\n    \n  \n    Desired outcome: Accurate insights into workload performance that drive data-informed decisions, ensuring alignment with business objectives.\n  \n    Common anti-patterns:\n  \n     \n     \n     \n  \n      \n        Analyzing metrics in isolation without considering their impact on business outcomes.\n      \n    \n      \n        Over-reliance on technical metrics while sidelining business metrics.\n      \n    \n      \n        Infrequent review of metrics, missing out on real-time decision-making opportunities.\n      \n    \n    Benefits of establishing this best\n    practice:\n  \n     \n     \n     \n  \n      \n        Enhanced understanding of the correlation between technical performance and business outcomes.        \n      \n    \n      \n        Improved decision-making process informed by real-time data.        \n      \n    \n      \n        Proactive identification and mitigation of issues before they affect business outcomes.\n      \n    \n    Level of risk exposed if this best practice\n    is not established: Medium\n  \n\n  Implementation guidance\n    \n      Leverage tools like Amazon CloudWatch to perform metric analysis. AWS services such as CloudWatch anomaly detection and Amazon DevOps Guru can be used to detect anomalies, especially when static thresholds are unknown or when patterns of behavior are more suited for anomaly detection.\n    \n     \n      \n      Implementation steps\n      \n         \n         \n         \n         \n      \n          \n            Analyze and review: Regularly review and interpret your\n            workload metrics. \n          \n             \n             \n          \n              \n                Prioritize business outcome metrics over purely technical metrics.\n              \n            \n              \n                Understand the significance of spikes, drops, or patterns in your data.\n              \n            \n        \n          \n            Utilize Amazon CloudWatch: Use Amazon CloudWatch for a centralized view and deep-dive analysis.\n          \n          \n             \n             \n             \n             \n             \n             \n          \n              \n                Configure CloudWatch dashboards to visualize your metrics and compare them over time.\n              \n            \n              \n                Use percentiles in CloudWatch to get a clear view of metric distribution, which can help in defining SLAs and understanding outliers.\n              \n            \n              \n                Set up CloudWatch anomaly detection to identify unusual patterns without relying on static thresholds.\n              \n            \n              \n                Implement CloudWatch cross-account observability to monitor and troubleshoot applications that span multiple accounts within a Region.\n              \n            \n              \n                Use CloudWatch Metric Insights to query and analyze metric data across accounts and Regions, identifying trends and anomalies.\n              \n            \n              \n                Apply CloudWatch Metric Math to transform, aggregate, or perform calculations on your metrics for deeper insights.\n              \n            \n        \n          \n            Employ Amazon DevOps Guru: Incorporate Amazon DevOps Guru for its machine learning-enhanced anomaly detection to identify early signs of operational issues for your serverless applications and remediate them before they impact your customers.\n          \n        \n          \n            Optimize based on insights: Make informed decisions based on your metric analysis to adjust and improve your workloads.\n           \n        \n      \n        Level of effort for the Implementation Plan: Medium\n      \n     \n   \n    \n    Resources\n    \n      Related best practices:\n    \n    \n       \n       \n    \n        \n          OPS04-BP01 Identify key performance indicators\n        \n      \n        \n          OPS04-BP02 Implement application telemetry\n        \n      \n    \n      Related documents:\n    \n    \n       \n       \n       \n       \n       \n    \n        \n          The Wheel Blog - Emphasizing the importance of continually reviewing metrics\n        \n      \n        \n          Percentile are important\n        \n      \n        \n          Using AWS Cost Anomaly Detection\n      \n        \n          CloudWatch cross-account observability\n        \n      \n        \n          Query your metrics with CloudWatch Metrics Insights\n        \n      \n    \n      Related videos:\n    \n    \n       \n       \n       \n    \n        \n          Enable Cross-Account Observability in Amazon CloudWatch\n        \n      \n        \n          Introduction to Amazon DevOps Guru\n        \n      \n        \n          Continuously Analyze Metrics using AWS Cost Anomaly Detection\n      \n    \n      Related examples:\n    \n    \n       \n       \n    \n        \n          One Observability Workshop\n        \n      \n        \n          Gaining operation insights with AIOps using Amazon DevOps Guru\n        \n      \n  \u003cdiv\u003e\u003cdiv\u003e\u003cdiv\u003e\u003cdiv id=\"js_error_message\"\u003e\u003cp\u003e\u003cimg src=\"https://d1ge0kk1l5kms0.cloudfront.net/images/G/01/webservices/console/warning.png\" alt=\"Warning\" /\u003e \u003cstrong\u003eJavascript is disabled or is unavailable in your browser.\u003c/strong\u003e\u003c/p\u003e\u003cp\u003eTo use the Amazon Web Services Documentation, Javascript must be enabled. Please refer to your browser's Help pages for instructions.\u003c/p\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003eDocument Conventions OPS 8. How do you utilize workload observability in your organization? OPS08-BP02 Analyze workload logs",
  "OPS08-BP02 Analyze workload logs\n    Regularly analyzing workload logs is essential for gaining a deeper\n    understanding of the operational aspects of your application. By\n    efficiently sifting through, visualizing, and interpreting log data,\n    you can continually optimize application performance and security.\n  \n    Desired outcome: Rich insights\n    into application behavior and operations derived from thorough log\n    analysis, ensuring proactive issue detection and mitigation.\n  \n    Common anti-patterns:\n  \n     \n     \n     \n  \n      \n        Neglecting the analysis of logs until a critical issue arises.\n      \n    \n      \n        Not using the full suite of tools available for log analysis,\n        missing out on critical insights.\n      \n    \n      \n        Solely relying on manual review of logs without leveraging\n        automation and querying capabilities.\n      \n    \n    Benefits of establishing this best\n      practice:\n  \n     \n     \n     \n  \n      \n        Proactive identification of operational bottlenecks, security\n        threats, and other potential issues.\n      \n    \n      \n        Efficient utilization of log data for continuous application\n        optimization.\n      \n    \n      \n        Enhanced understanding of application behavior, aiding in\n        debugging and troubleshooting.\n      \n    \n    Level of risk exposed if this best practice\n      is not established: Medium\n  \n    \n    Implementation guidance\n    \n    \n    \n      Amazon CloudWatch Logs is a powerful tool for log analysis.\n      Integrated features like CloudWatch Logs Insights and Contributor\n      Insights make the process of deriving meaningful information from\n      logs intuitive and efficient.\n    \n     \n      \n      Implementation steps\n      \n      \n      \n         \n         \n         \n         \n         \n         \n         \n         \n      \n          \n            Set up CloudWatch Logs:\n            Configure applications and services to send logs to\n            CloudWatch Logs.\n          \n        \n          \n            Use log anomaly\n              detection: Utilize\n            Amazon CloudWatch Logs anomaly detection to automatically\n            identify and alert on unusual log patterns. This tool helps\n            you proactively manage anomalies in your logs and detect\n            potential issues early.\n          \n        \n          \n            Set up CloudWatch Logs\n              Insights: Use\n            CloudWatch Logs Insights to interactively search and analyze\n            your log data.\n          \n          \n             \n             \n             \n          \n              \n                Craft queries to extract patterns, visualize log data,\n                and derive actionable insights.\n              \n            \n              \n                Use\n                CloudWatch Logs Insights pattern analysis to analyze and\n                visualize frequent log patterns. This feature helps you\n                understand common operational trends and potential\n                outliers in your log data.\n              \n            \n              \n                Use\n                CloudWatch Logs compare (diff) to perform differential\n                analysis between different time periods or across\n                different log groups. Use this capability to pinpoint\n                changes and assess their impacts on your system's\n                performance or behavior.\n              \n            \n        \n          \n            Monitor logs in real-time with Live\n              Tail: Use\n            Amazon CloudWatch Logs Live Tail to view log data in\n            real-time. You can actively monitor your application's\n            operational activities as they occur, which provides\n            immediate visibility into system performance and potential\n            issues.\n          \n        \n          \n            Leverage Contributor\n              Insights: Use\n            CloudWatch\n              Contributor Insights to identify top talkers in high\n            cardinality dimensions like IP addresses or user-agents.\n          \n        \n          \n            Implement CloudWatch Logs metric\n              filters: Configure\n            CloudWatch Logs metric filters to convert log data into\n            actionable metrics. This allows you to set alarms or further\n            analyze patterns.\n          \n        \n          \n            Implement\n              CloudWatch\n                cross-account observability: Monitor and\n            troubleshoot applications that span multiple accounts within\n            a Region.\n          \n        \n          \n            Regular review and\n              refinement: Periodically review your log analysis\n            strategies to capture all relevant information and\n            continually optimize application performance.\n          \n        \n      \n        Level of effort for the implementation\n          plan: Medium\n      \n     \n   \n    \n    Resources\n    \n      Related best practices:\n    \n    \n       \n       \n       \n    \n        \n          OPS04-BP01 Identify key performance indicators \n        \n      \n        \n          OPS04-BP02 Implement application telemetry \n        \n      \n        \n          OPS08-BP01 Analyze workload metrics \n        \n      \n    \n      Related documents:\n    \n    \n       \n       \n       \n    \n        \n          Analyzing\n            Log Data with CloudWatch Logs Insights\n        \n      \n        \n          Using\n            CloudWatch Contributor Insights\n        \n      \n        \n          Creating\n            and Managing CloudWatch Log Metric Filters\n        \n      \n    \n      Related videos:\n    \n    \n       \n       \n    \n        \n          Analyze\n            Log Data with CloudWatch Logs Insights\n        \n      \n        \n          Use\n            CloudWatch Contributor Insights to Analyze High-Cardinality\n            Data\n        \n      \n    \n      Related examples:\n    \n    \n       \n       \n    \n        \n          CloudWatch Logs Sample Queries\n        \n      \n        \n          One\n            Observability Workshop\n        \n      \n  \u003cdiv\u003e\u003cdiv\u003e\u003cdiv\u003e\u003cdiv id=\"js_error_message\"\u003e\u003cp\u003e\u003cimg src=\"https://d1ge0kk1l5kms0.cloudfront.net/images/G/01/webservices/console/warning.png\" alt=\"Warning\" /\u003e \u003cstrong\u003eJavascript is disabled or is unavailable in your browser.\u003c/strong\u003e\u003c/p\u003e\u003cp\u003eTo use the Amazon Web Services Documentation, Javascript must be enabled. Please refer to your browser's Help pages for instructions.\u003c/p\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003eDocument ConventionsOPS08-BP01 Analyze workload metricsOPS08-BP03 Analyze workload traces",
  "OPS08-BP03 Analyze workload traces\n    Analyzing trace data is crucial for achieving a comprehensive view\n    of an application's operational journey. By visualizing and\n    understanding the interactions between various components,\n    performance can be fine-tuned, bottlenecks identified, and user\n    experiences enhanced.\n  \n    Desired outcome: Achieve clear\n    visibility into your application's distributed operations, enabling\n    quicker issue resolution and an enhanced user experience.\n  \n    Common anti-patterns:\n  \n     \n     \n     \n  \n      \n        Overlooking trace data, relying solely on logs and metrics.\n      \n    \n      \n        Not correlating trace data with associated logs.\n      \n    \n      \n        Ignoring the metrics derived from traces, such as latency and\n        fault rates.\n      \n    \n    Benefits of establishing this best\n      practice:\n  \n     \n     \n     \n     \n     \n  \n      \n        Improve troubleshooting and reduce mean time to resolution\n        (MTTR).\n      \n    \n      \n        Gain insights into dependencies and their impact.\n      \n    \n      \n        Swift identification and rectification of performance issues.\n      \n    \n      \n        Leveraging trace-derived metrics for informed decision-making.\n      \n    \n      \n        Improved user experiences through optimized component\n        interactions.\n      \n    \n    Level of risk exposed if this best practice\n      is not established: Medium\n  \n    \n    Implementation guidance\n    \n    \n    \n      AWS X-Ray offers a comprehensive suite for trace data analysis,\n      providing a holistic view of service interactions, monitoring user\n      activities, and detecting performance issues. Features like\n      ServiceLens, X-Ray Insights, X-Ray Analytics, and Amazon DevOps Guru enhance the depth of actionable insights derived from trace\n      data.\n    \n     \n      \n      Implementation steps\n      \n      \n      \n        The following steps offer a structured approach to effectively\n        implementing trace data analysis using AWS services:\n      \n      \n         \n         \n         \n         \n         \n         \n         \n         \n         \n         \n         \n      \n          \n            Integrate AWS X-Ray:\n            Ensure X-Ray is integrated with your applications to capture\n            trace data.\n          \n        \n          \n            Analyze X-Ray metrics:\n            Delve into metrics derived from X-Ray traces, such as\n            latency, request rates, fault rates, and response time\n            distributions, using the\n            service\n              map to monitor application health.\n          \n        \n          \n            Use ServiceLens: Leverage\n            the\n            ServiceLens\n              map for enhanced observability of your services and\n            applications. This allows for integrated viewing of traces,\n            metrics, logs, alarms, and other health information.\n          \n        \n          \n            Enable X-Ray Insights:\n          \n          \n             \n             \n             \n          \n              \n                Turn on\n                X-Ray\n                  Insights for automated anomaly detection in\n                traces.\n              \n            \n              \n                Examine insights to pinpoint patterns and ascertain root\n                causes, such as increased fault rates or latencies.\n              \n            \n              \n                Consult the insights timeline for a chronological\n                analysis of detected issues.\n              \n            \n        \n          \n            Use X-Ray Analytics:\n            X-Ray\n              Analytics allows you to thoroughly explore trace\n            data, pinpoint patterns, and extract insights.\n          \n        \n          \n            Use groups in X-Ray:\n            Create groups in X-Ray to filter traces based on criteria\n            such as high latency, allowing for more targeted analysis.\n          \n        \n          \n            Incorporate Amazon DevOps Guru: Engage\n            Amazon DevOps Guru to benefit from machine learning models\n            pinpointing operational anomalies in traces.\n          \n        \n          \n            Use CloudWatch\n              Synthetics: Use\n            CloudWatch\n              Synthetics to create canaries for continually\n            monitoring your endpoints and workflows. These canaries can\n            integrate with X-Ray to provide trace data for in-depth\n            analysis of the applications being tested.\n          \n        \n          \n            Use Real User Monitoring\n              (RUM): With\n            AWS X-Ray and CloudWatch RUM, you can analyze and debug\n            the request path starting from end users of your application\n            through downstream AWS managed services. This helps you\n            identify latency trends and errors that impact your end\n            users.\n          \n        \n          \n            Correlate with logs:\n            Correlate\n            trace\n              data with related logs within the X-Ray trace view\n            for a granular perspective on application behavior. This\n            allows you to view log events directly associated with\n            traced transactions.\n          \n        \n          \n            Implement\n              CloudWatch\n                cross-account observability: Monitor and\n            troubleshoot applications that span multiple accounts within\n            a Region.\n          \n        \n      \n        Level of effort for the implementation\n          plan: Medium\n      \n     \n   \n    \n    Resources\n    \n    \n      Related best practices:\n    \n    \n       \n       \n    \n        \n          OPS08-BP01 Analyze workload metrics \n        \n      \n        \n          OPS08-BP02 Analyze workload logs \n        \n      \n    \n      Related documents:\n    \n    \n       \n       \n       \n       \n    \n        \n          Using\n            ServiceLens to Monitor Application Health\n        \n      \n        \n          Exploring\n            Trace Data with X-Ray Analytics\n        \n      \n        \n          Detecting\n            Anomalies in Traces with X-Ray Insights\n        \n      \n        \n          Continuous\n            Monitoring with CloudWatch Synthetics\n        \n      \n    \n      Related videos:\n    \n    \n       \n       \n    \n        \n          Analyze\n            and Debug Applications Using Amazon CloudWatch Synthetics\n            \u0026 AWS X-Ray\n        \n      \n        \n          Use\n            AWS X-Ray Insights\n        \n      \n    \n      Related examples:\n    \n    \n       \n       \n       \n    \n        \n          One\n            Observability Workshop\n        \n      \n        \n          Implementing\n            X-Ray with AWS Lambda\n        \n      \n        \n          CloudWatch\n            Synthetics Canary Templates\n        \n      \n  \u003cdiv\u003e\u003cdiv\u003e\u003cdiv\u003e\u003cdiv id=\"js_error_message\"\u003e\u003cp\u003e\u003cimg src=\"https://d1ge0kk1l5kms0.cloudfront.net/images/G/01/webservices/console/warning.png\" alt=\"Warning\" /\u003e \u003cstrong\u003eJavascript is disabled or is unavailable in your browser.\u003c/strong\u003e\u003c/p\u003e\u003cp\u003eTo use the Amazon Web Services Documentation, Javascript must be enabled. Please refer to your browser's Help pages for instructions.\u003c/p\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003eDocument ConventionsOPS08-BP02 Analyze workload logsOPS08-BP04 Create actionable alerts",
  "OPS08-BP04 Create actionable alerts\n    Promptly detecting and responding to deviations in your\n    application's behavior is crucial. Especially vital is recognizing\n    when outcomes based on key performance indicators (KPIs) are at risk\n    or when unexpected anomalies arise. Basing alerts on KPIs ensures\n    that the signals you receive are directly tied to business or\n    operational impact. This approach to actionable alerts promotes\n    proactive responses and helps maintain system performance and\n    reliability.\n  \n    Desired outcome: Receive timely,\n    relevant, and actionable alerts for rapid identification and\n    mitigation of potential issues, especially when KPI outcomes are at\n    risk.\n  \n    Common anti-patterns:\n  \n     \n     \n     \n  \n      \n        Setting up too many non-critical alerts, leading to alert\n        fatigue.\n      \n    \n      \n        Not prioritizing alerts based on KPIs, making it hard to\n        understand the business impact of issues.\n      \n    \n      \n        Neglecting to address root causes, leading to repetitive alerts\n        for the same issue.\n      \n    \n    Benefits of establishing this best\n    practice:\n  \n     \n     \n     \n  \n      \n        Reduced alert fatigue by focusing on actionable and relevant\n        alerts.\n      \n    \n      \n        Improved system uptime and reliability through proactive issue\n        detection and mitigation.\n      \n    \n      \n        Enhanced team collaboration and quicker issue resolution by\n        integrating with popular alerting and communication tools.\n      \n    \n    Level of risk exposed if this best practice\n    is not established: High\n  \n\n  Implementation guidance\n\n      \n    \n      To create an effective alerting mechanism, it's vital to use\n      metrics, logs, and trace data that flag when outcomes based on\n      KPIs are at risk or anomalies are detected.\n    \n     \n\n  Implementation steps\n\n      \n      \n         \n         \n         \n         \n         \n         \n         \n         \n         \n      \n          \n            Determine key performance indicators\n            (KPIs): Identify your application's KPIs. Alerts\n            should be tied to these KPIs to reflect the business impact\n            accurately.\n          \n        \n          \n            Implement anomaly\n            detection:\n          \n          \n             \n             \n             \n          \n              \n                Use Amazon CloudWatch anomaly\n                detection: Set up\n                Amazon CloudWatch anomaly detection to automatically\n                detect unusual patterns, which helps you only generate\n                alerts for genuine anomalies.\n              \n            \n              \n                Use AWS X-Ray Insights:\n              \n              \n                 \n                 \n              \n                  \n                    Set up\n                    X-Ray\n                    Insights to detect anomalies in trace data.\n                  \n                \n                  \n                    Configure\n                    notifications\n                    for X-Ray Insights to be alerted on detected\n                    issues.\n                  \n                \n            \n              \n                Integrate with Amazon DevOps Guru:\n              \n              \n                 \n                 \n              \n                  \n                    Leverage\n                    Amazon DevOps Guru for its machine learning\n                    capabilities in detecting operational anomalies with\n                    existing data.\n                  \n                \n                  \n                    Navigate to the\n                    notification\n                    settings in DevOps Guru to set up anomaly\n                    alerts.\n                  \n                \n            \n        \n          \n            Implement actionable\n            alerts: Design alerts that provide adequate\n            information for immediate action.\n          \n          \n             \n          \n              \n                Monitor\n                AWS Health events with Amazon EventBridge rules, or\n                integrate programatically with the AWS Health API to\n                automate actions when you receive AWS Health events.\n                These can be general actions, such as sending all\n                planned lifecycle event messages to a chat interface, or\n                specific actions, such as the initiation of a workflow\n                in an IT service management tool.\n              \n            \n        \n          \n            Reduce alert fatigue:\n            Minimize non-critical alerts. When teams are overwhelmed\n            with numerous insignificant alerts, they can lose oversight\n            of critical issues, which diminishes the overall\n            effectiveness of the alert mechanism.\n          \n        \n          \n            Set up composite alarms:\n            Use\n            Amazon CloudWatch composite alarms to consolidate multiple\n            alarms.\n          \n        \n          \n            Integrate with alert\n            tools: Incorporate tools like\n            Ops\n            Genie and\n            PagerDuty.\n          \n        \n          \n            Engage Amazon Q Developer in chat applications:\n            Integrate\n            Amazon Q Developer in chat applications to relay alerts to Amazon Chime, Microsoft Teams,\n            and Slack.\n          \n        \n          \n            Alert based on logs: Use\n            log\n            metric filters in CloudWatch to create alarms based\n            on specific log events.\n          \n        \n          \n            Review and iterate:\n            Regularly revisit and refine alert configurations.\n          \n        \n      \n        Level of effort for the implementation\n        plan: Medium\n      \n     \n   \n\n  Resources\n\n      \n    \n      Related best practices:\n    \n    \n       \n       \n       \n       \n       \n       \n       \n       \n    \n        \n          OPS04-BP01 Identify key performance indicators\n        \n      \n        \n          OPS04-BP02 Implement application telemetry\n        \n      \n        \n          OPS04-BP03 Implement user experience telemetry\n        \n      \n        \n          OPS04-BP04 Implement dependency telemetry\n        \n      \n        \n          OPS04-BP05 Implement distributed tracing\n        \n      \n        \n          OPS08-BP01 Analyze workload metrics\n        \n      \n        \n          OPS08-BP02 Analyze workload logs\n        \n      \n        \n          OPS08-BP03 Analyze workload traces\n        \n      \n    \n      Related documents:\n    \n    \n       \n       \n       \n       \n       \n       \n       \n       \n    \n        \n          Using\n          Amazon CloudWatch alarms\n        \n      \n        \n          Create\n          a composite alarm\n        \n      \n        \n          Create\n          a CloudWatch alarm based on anomaly detection\n        \n      \n        \n          DevOps Guru Notifications\n        \n      \n        \n          X-ray\n          insights notifications\n        \n      \n        \n          Monitor,\n          operate, and troubleshoot your AWS resources with interactive\n          ChatOps\n        \n      \n        \n          Amazon CloudWatch Integration Guide | PagerDuty\n        \n      \n        \n          Integrate\n          Opsgenie with Amazon CloudWatch\n        \n      \n    \n      Related videos:\n    \n    \n       \n       \n       \n    \n        \n          Create\n          Composite Alarms in Amazon CloudWatch\n        \n      \n        \n          Amazon Q Developer in chat applications Overview\n        \n      \n        \n          AWS           On Air ft. Mutative Commands in Amazon Q Developer in chat applications\n        \n      \n    \n      Related examples:\n    \n    \n       \n       \n       \n    \n        \n          Alarms,\n          incident management, and remediation in the cloud with Amazon CloudWatch\n        \n      \n        \n          Tutorial:\n          Creating an Amazon EventBridge rule that sends notifications\n          to Amazon Q Developer in chat applications\n        \n      \n        \n          One\n          Observability Workshop\n        \n      \n  \u003cdiv\u003e\u003cdiv\u003e\u003cdiv\u003e\u003cdiv id=\"js_error_message\"\u003e\u003cp\u003e\u003cimg src=\"https://d1ge0kk1l5kms0.cloudfront.net/images/G/01/webservices/console/warning.png\" alt=\"Warning\" /\u003e \u003cstrong\u003eJavascript is disabled or is unavailable in your browser.\u003c/strong\u003e\u003c/p\u003e\u003cp\u003eTo use the Amazon Web Services Documentation, Javascript must be enabled. Please refer to your browser's Help pages for instructions.\u003c/p\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003eDocument ConventionsOPS08-BP03 Analyze workload tracesOPS08-BP05 Create dashboards",
  "OPS08-BP05 Create dashboards\n    Dashboards are the human-centric view into the telemetry data of\n    your workloads. While they provide a vital visual interface, they\n    should not replace alerting mechanisms, but complement them. When\n    crafted with care, not only can they offer rapid insights into\n    system health and performance, but they can also present\n    stakeholders with real-time information on business outcomes and the\n    impact of issues.\n  \n    Desired outcome:\n  \n    Clear, actionable insights into system and business health using\n    visual representations.\n  \n    Common anti-patterns:\n  \n     \n     \n     \n  \n      \n        Overcomplicating dashboards with too many metrics.\n      \n    \n      \n        Relying on dashboards without alerts for anomaly detection.\n      \n    \n      \n        Not updating dashboards as workloads evolve.\n      \n    \n    Benefits of this best practice:\n  \n     \n     \n     \n  \n      \n        Immediate visibility into critical system metrics and KPIs.\n      \n    \n      \n        Enhanced stakeholder communication and understanding.\n      \n    \n      \n        Rapid insight into the impact of operational issues.\n      \n    \n    Level of risk if this best practice isn't\n      established: Medium\n  \n    \n    Implementation guidance\n    \n    \n    \n      Business-centric dashboards\n    \n    \n      Dashboards tailored to business KPIs engage a wider array of\n      stakeholders. While these individuals might not be interested in\n      system metrics, they are keen on understanding the business\n      implications of these numbers. A business-centric dashboard\n      ensures that all technical and operational metrics being monitored\n      and analyzed are in sync with overarching business goals. This\n      alignment provides clarity, ensuring everyone is on the same page\n      regarding what's essential and what's not. Additionally,\n      dashboards that highlight business KPIs tend to be more\n      actionable. Stakeholders can quickly understand the health of\n      operations, areas that need attention, and the potential impact on\n      business outcomes.\n    \n    \n      With this in mind, when creating your dashboards, ensure that\n      there's a balance between technical metrics and business KPIs.\n      Both are vital, but they cater to different audiences. Ideally,\n      you should have dashboards that provide a holistic view of the\n      system's health and performance while also emphasizing key\n      business outcomes and their implications.\n    \n    \n      Amazon CloudWatch Dashboards are customizable home pages in the\n      CloudWatch console that you can use to monitor your resources in a\n      single view, even those resources that are spread across different\n      AWS Regions and accounts.\n    \n     \n      \n      Implementation steps\n      \n      \n      \n         \n         \n         \n         \n         \n         \n         \n         \n         \n         \n      \n          \n            Create a basic dashboard:\n            Create\n              a new dashboard in CloudWatch, giving it a\n            descriptive name.\n          \n        \n          \n            Use Markdown widgets:\n            Before diving into the metrics,\n            use\n              Markdown widgets to add textual context at the top of\n            your dashboard. This should explain what the dashboard\n            covers, the significance of the represented metrics, and can\n            also contain links to other dashboards and troubleshooting\n            tools.\n          \n        \n          \n            Create dashboard\n              variables:\n            Incorporate\n              dashboard variables where appropriate to allow for\n            dynamic and flexible dashboard views.\n          \n        \n          \n            Create metrics widgets:\n            Add\n              metric widgets to visualize various metrics your\n            application emits, tailoring these widgets to effectively\n            represent system health and business outcomes.\n          \n        \n          \n            Log Insights queries:\n            Utilize\n            CloudWatch\n              Log Insights to derive actionable metrics from your\n            logs and display these insights on your dashboard.\n          \n        \n          \n            Set up alarms: Integrate\n            CloudWatch\n              Alarms into your dashboard for a quick view of any\n            metrics breaching their thresholds.\n          \n        \n          \n            Use Contributor Insights:\n            Incorporate\n            CloudWatch\n              Contributor Insights to analyze high-cardinality\n            fields and get a clearer understanding of your resource's\n            top contributors.\n          \n        \n          \n            Design custom widgets:\n            For specific needs not met by standard widgets, consider\n            creating\n            custom\n              widgets. These can pull from various data sources or\n            represent data in unique ways.\n          \n        \n          \n            Use AWS Health: AWS Health is the authoritative source of information about the health of your AWS Cloud resources. Use AWS Health Dashboard out of the box, or use AWS Health data in your own dashboards and tools so you have the right information available to make informed decisions. \n          \n        \n          \n            Iterate and refine: As\n            your application evolves, regularly revisit your dashboard\n            to ensure its relevance.\n          \n        \n     \n   \n    \n    Resources\n    \n    \n    \n      Related best practices:\n    \n    \n       \n       \n       \n       \n       \n    \n        \n          OPS04-BP01 Identify key performance indicators\n        \n      \n        \n          OPS08-BP01 Analyze workload metrics\n        \n      \n        \n          OPS08-BP02 Analyze workload logs\n        \n      \n        \n          OPS08-BP03 Analyze workload traces\n        \n      \n        \n          OPS08-BP04 Create actionable alerts\n        \n      \n    \n      Related documents:\n    \n    \n       \n       \n    \n        \n          Building\n            Dashboards for Operational Visibility\n        \n      \n        \n          Using\n            Amazon CloudWatch Dashboards\n        \n      \n    \n      Related videos:\n    \n    \n       \n       \n    \n        \n          Create\n            Cross Account \u0026 Cross Region CloudWatch Dashboards\n        \n      \n        \n          AWS           re:Invent 2021 - Gain enterprise visibility with AWS Cloud\n            operation dashboards)\n        \n      \n    \n      Related examples:\n    \n    \n       \n       \n       \n       \n    \n        \n          One\n            Observability Workshop\n        \n      \n        \n          Application\n            Monitoring with Amazon CloudWatch\n        \n      \n        \n          AWS Health Events Intelligence Dashboards and Insights\n        \n      \n        \n          Visualize\n            AWS Health events using Amazon Managed Grafana\n        \n      \n  \u003cdiv\u003e\u003cdiv\u003e\u003cdiv\u003e\u003cdiv id=\"js_error_message\"\u003e\u003cp\u003e\u003cimg src=\"https://d1ge0kk1l5kms0.cloudfront.net/images/G/01/webservices/console/warning.png\" alt=\"Warning\" /\u003e \u003cstrong\u003eJavascript is disabled or is unavailable in your browser.\u003c/strong\u003e\u003c/p\u003e\u003cp\u003eTo use the Amazon Web Services Documentation, Javascript must be enabled. Please refer to your browser's Help pages for instructions.\u003c/p\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003eDocument ConventionsOPS08-BP04 Create actionable alertsOPS 9. How do you understand the health of your operations?",
  "OPS09-BP01 Measure operations goals and KPIs with metrics\n    Obtain goals and KPIs that define operations success from your organization and determine that metrics reflect these. Set baselines as a point of reference and reevaluate regularly. Develop mechanisms to collect these metrics from teams for evaluation. The DevOps Research and Assessment (DORA) metrics provide a popular method to measure progress towards DevOps practices of software delivery.\n  \n    Desired outcome:\n  \n     \n     \n  \n      The organization publishes and shares the goals and KPIs for the operations teams.\n    \n      You establish metrics that reflect these KPIs. Examples may include:\n      \n         \n         \n         \n         \n         \n      \n          \n            Ticket queue depth or average age of ticket\n          \n        \n          \n            Ticket count grouped by type of issue\n          \n        \n          \n            Time spent working issues with or without a standardized operating procedure (SOP)\n          \n        \n          \n            Amount of time spent recovering from a failed code push\n          \n        \n          \n            Call volume\n          \n        \n    \n    Common anti-patterns:\n  \n     \n     \n     \n  \n      \n        Deployment deadlines are missed because developers are pulled away to perform troubleshooting tasks. Development teams argue for more personnel, but cannot quantify how many they need because the time taken away cannot be measured.\n      \n    \n      \n        A Tier 1 desk was set up to handle user calls. Over time, more workloads were added, but no headcount was allocated to the Tier 1 desk. Customer satisfaction suffers as call times increase and issues go longer without resolution, but management sees no indicators of such, preventing any action.\n      \n    \n      \n        A problematic workload has been handed off to a separate operations team for upkeep. Unlike other workloads, this new one was not supplied with proper documentation and runbooks. As such, teams spend longer troubleshooting and addressing failures. However, there are no metrics documenting this, which makes accountability difficult.\n      \n    \n    Benefits of establishing this best\n      practice: Where workload monitoring shows the state of our applications and services, monitoring operations teams provides owners insight into changes among the consumers of those workloads, such as shifting business needs. Measure the effectiveness of these teams and evaluate them against business goals by creating metrics that can reflect the state of operations. Metrics can highlight support issues or identify when drifts occur away from a service level target. \n  \n    Level of risk exposed if this best practice\n    is not established: Medium\n  \n\n  Implementation guidance\n    Schedule time with business leaders and stakeholders to determine what the overall goals of the service will be. Determine what the tasks of various operations teams should be and what challenges they could be approached with. Using these, brainstorm key performance indicators (KPIs) that might reflect these operations goals. These might be customer satisfaction, time from feature conception to deployment, average issue resolution time, or cost efficiencies.\n    \n      Working from KPIs, identify the metrics and sources of data that might reflect these goals best. Customer satisfaction may be an combination of various metrics such as call wait or response times, satisfaction scores, and types of issues raised. Deployment times may be the sum of time needed for testing and deployment, plus any post-deployment fixes that needed to be added. Statistics showing the time spent on different types of issues (or the counts of those issues) can provide a window into where targeted effort is needed.\n    \n   \n    \n    Resources\n    \n      Related documents:\n    \n    \n       \n       \n       \n       \n       \n    \n        \n          QuickSight - Using KPIs\n        \n      \n        \n          Amazon CloudWatch - Using Metrics\n        \n      \n        \n          Building Dashboards\n        \n      \n        \n          How to track your cost optimization KPIs with KPI Dashboard\n        \n      \n        AWS DevOps Guidance\n        \n      \n    \n      Related examples:\n    \n    \n       \n       \n       \n       \n    \n        \n          Monitor the performance of your software delivery using native AWS monitoring and observability tools\n        \n      \n        \n          Balance deployment speed and stability with DORA metrics\n        \n      \n        \n          Example MLOps operational metrics in the financial services industry\n        \n      \n        \n          How to track your cost optimization KPIs with the KPI Dashboard\n        \n      \n  \u003cdiv\u003e\u003cdiv\u003e\u003cdiv\u003e\u003cdiv id=\"js_error_message\"\u003e\u003cp\u003e\u003cimg src=\"https://d1ge0kk1l5kms0.cloudfront.net/images/G/01/webservices/console/warning.png\" alt=\"Warning\" /\u003e \u003cstrong\u003eJavascript is disabled or is unavailable in your browser.\u003c/strong\u003e\u003c/p\u003e\u003cp\u003eTo use the Amazon Web Services Documentation, Javascript must be enabled. Please refer to your browser's Help pages for instructions.\u003c/p\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003eDocument ConventionsOPS 9. How do you understand the health of your operations?OPS09-BP02 Communicate status and trends to ensure visibility into operation",
  "OPS09-BP02 Communicate status and trends to ensure visibility into operation\n    Knowing the state of your operations and its trending direction is necessary to identify when outcomes may be at risk, whether or not added work can be supported, or the effects that changes have had to your teams. During operations events, having status pages that users and operations teams can refer to for information can reduce pressure on communication channels and disseminate information proactively.\n  \n    Desired outcome:\n  \n     \n     \n     \n     \n  \n      \n        Operations leaders have insight at a glance to see what sort of call volumes their teams are operating under and what efforts may be under way, such as deployments.\n      \n    \n      \n        Alerts are disseminated to stakeholders and user communities when impacts to normal operations occur. \n      \n    \n      \n        Organization leadership and stakeholders can check a status page in response to an alert or impact, and obtain information surrounding an operational event, such as points of contact, ticket information, and estimated recovery times.\n      \n    \n      \n        Reports are made available to leadership and other stakeholders to show operations statistics such as call volumes over a period of time, user satisfaction scores, numbers of outstanding tickets and their ages.\n      \n    \n    Common anti-patterns:\n  \n     \n     \n  \n      \n        A workload goes down, leaving a service unavailable. Call volumes spike as users request to know what's going on. Managers add to the volume requesting to know who's working an issue. Various operations teams duplicate efforts in trying to investigate.\n      \n    \n      \n        A desire for a new capability leads to several personnel being reassigned to an engineering effort. No backfill is provided, and issue resolution times spike. This information is not captured, and only after several weeks and dissatisfied user feedback does leadership become aware of the issue.\n      \n    \n    Benefits of establishing this best\n      practice: During operational events where the business is impacted, much time and energy can be wasted querying information from various teams attempting to understand the situation. By establishing widely-disseminated status pages and dashboards, stakeholders can quickly obtain information such as whether or not an issue was detected, who has lead on the issue, or when a return to normal operations may be expected. This frees team members from spending too much time communicating status to others and more time addressing issues. \n  \n    In addition, dashboards and reports can provide insights to decision-makers and stakeholders to see how operations teams are able to respond to business needs and how their resources are being allocated. This is crucial for determining if adequate resources are in place to support the business.\n  \n    Level of risk exposed if this best practice\n      is not established: Medium\n  \n    \n    Implementation guidance\n    \n      Build dashboards that show the current key metrics for your ops teams, and make them readily accessible both to operations leaders and management.\n    \n    \n      Build status pages that can be updated quickly to show when an incident or event is unfolding, who has ownership and who is coordinating the response. Share any steps or workarounds that users should consider on this page, and disseminate the location widely. Encourage users to check this location first when confronted with an unknown issue.\n    \n    \n      Collect and provide reports that show the health of operations over time, and distribute this to leaders and decision makers to illustrate the work of operations along with challenges and needs.\n    \n    \n      Share between teams these metrics and reports that best reflect goals and KPIs and where they have been influential in driving change. Dedicate time to these activities to elevate the importance of operations inside of and between teams.\n    \n    \n      Use AWS Health alongside your own dashboards, or integrate AWS Health events into them, so that your teams can correlate application issues to AWS service status.\n    \n   \n    \n    Resources\n    \n      Related best practices:\n    \n    \n       \n    \n        \n          OPS09-BP01 Measure operations goals and KPIs with metrics\n        \n      \n    \n      Related documents:\n    \n    \n       \n       \n    \n        \n          Measure Progress\n        \n      \n        \n          Building dashboards for operation visibility\n        \n      \n    \n      Related examples:\n    \n    \n       \n       \n       \n    \n        \n          Data Operations\n        \n      \n        \n          How to track your cost optimization KPIs with KPI Dashboard\n        \n      \n        \n          The Importance of Key Performance Indicators (KPIs) for Large-Scale Cloud Migrations\n        \n      \n  \u003cdiv\u003e\u003cdiv\u003e\u003cdiv\u003e\u003cdiv id=\"js_error_message\"\u003e\u003cp\u003e\u003cimg src=\"https://d1ge0kk1l5kms0.cloudfront.net/images/G/01/webservices/console/warning.png\" alt=\"Warning\" /\u003e \u003cstrong\u003eJavascript is disabled or is unavailable in your browser.\u003c/strong\u003e\u003c/p\u003e\u003cp\u003eTo use the Amazon Web Services Documentation, Javascript must be enabled. Please refer to your browser's Help pages for instructions.\u003c/p\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003eDocument ConventionsOPS09-BP01 Measure operations goals and KPIs with metricsOPS09-BP03 Review operations metrics and prioritize improvement",
  "OPS09-BP03 Review operations metrics and prioritize improvement\n    Setting aside dedicated time and resources for reviewing the state of operations ensures that serving the day-to-day line of business remains a priority. Pull together operations leaders and stakeholders to regularly review metrics, reaffirm or modify goals and objectives, and prioritize improvements.\n  \n    Desired outcome:\n  \n     \n     \n  \n      \n        Operations leaders and staff regularly meet to review metrics over a given reporting period. Challenges are communicated, wins are celebrated, and lessons learned are shared.\n      \n    \n      \n        Stakeholders and business leaders are regularly briefed on the state of operations and solicited for input regarding goals, KPIs, and future initiatives. Tradeoffs between service delivery, operations, and maintenance are discussed and placed into context. \n      \n    \n    Common anti-patterns:\n  \n     \n     \n  \n      \n        A new product is launched, but the Tier 1 and Tier 2 operations teams are not adequately trained to support or given additional staff. Metrics that show the decrease in ticket resolution times and increase in incident volumes are not seen by leaders. Action is taken weeks later when subscription numbers start to fall as discontent users move off the platform.\n      \n    \n      \n        A manual process for performing maintenance on a workload has been in place for a long time. While a desire to automate has been present, this was a low priority given the low importance of the system. Over time however, the system has grown in importance and now these manual processes consume a majority of operations' time. No resources are scheduled for providing increased tooling to operations, leading to staff burnout as workloads increase. Leadership becomes aware once it's reported that staff are leaving for other competitors.\n      \n    \n    Benefits of establishing this best\n      practice: In some organizations, it can become a challenge to allocate the same time and attention that is afforded to service delivery and new products or offerings. When this occurs, the line of business can suffer as the level of service expected slowly deteriorates. This is because operations does not change and evolve with the growing business, and can soon be left behind. Without regular review into the insights operations collects, the risk to the business may become visible only when it's too late. By allocating time to review metrics and procedures both among the operations staff and with leadership, the crucial role operations plays remains visible, and risks can be identified long before they reach critical levels. Operations teams get better insight into impending business changes and initiatives, allowing for proactive efforts to be undertaken. Leadership visibility into operations metrics showcases the role that these teams play in customer satisfaction, both internal and external, and let them better weigh choices for priorities, or ensure that operations has the time and resources to change and evolve with new business and workload initiatives.\n  \n    Level of risk exposed if this best practice\n    is not established: Medium\n  \n\n  Implementation guidance\n    \n      Dedicate time to review operations metrics between stakeholders and operations teams and review report data. Place these reports in the contexts of the organizations goals and objectives to determine if they're being met. Identify sources of ambiguity where goals are not clear, or where there may be conflicts between what is asked for and what is given.\n    \n    \n      Identify where time, people, and tools can aid in operations outcomes. Determine which KPIs this would impact and what targets for success should be. Revisit regularly to ensure operations is resourced sufficiently to support the line of business.\n    \n   \n    \n    Resources\n    \n      Related documents:\n    \n    \n       \n       \n       \n       \n       \n       \n       \n    \n        \n          Amazon Athena\n        \n      \n        \n          Amazon CloudWatch metrics and dimensions reference\n        \n      \n        \n          Amazon QuickSight\n        \n      \n        AWS Glue\n      \n        AWS Glue Data Catalog\n      \n        \n          Collect metrics and logs from Amazon EC2 instances and on-premises servers with the Amazon CloudWatch Agent\n        \n      \n        \n          Using Amazon CloudWatch metrics\n        \n      \n\n  \u003cdiv\u003e\u003cdiv\u003e\u003cdiv\u003e\u003cdiv id=\"js_error_message\"\u003e\u003cp\u003e\u003cimg src=\"https://d1ge0kk1l5kms0.cloudfront.net/images/G/01/webservices/console/warning.png\" alt=\"Warning\" /\u003e \u003cstrong\u003eJavascript is disabled or is unavailable in your browser.\u003c/strong\u003e\u003c/p\u003e\u003cp\u003eTo use the Amazon Web Services Documentation, Javascript must be enabled. Please refer to your browser's Help pages for instructions.\u003c/p\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003eDocument ConventionsOPS09-BP02 Communicate status and trends to ensure visibility into operation OPS 10. How do you manage workload and operations events? ",
  "OPS10-BP01 Use a process for event, incident, and problem\n  managementThe ability to efficiently manage events, incidents, and problems is key to maintaining workload health and performance. It's crucial to recognize and understand the differences between these elements to develop an effective response and resolution strategy. Establishing and following a well-defined process for each aspect helps your team swiftly and effectively handle any operational challenges that arise.\n    Desired outcome: Your organization effectively manages operational events, incidents, and problems through well-documented and centrally stored processes. These processes are consistently updated to reflect changes, streamlining handling and maintaining high service reliability and workload performance.\n  \n    Common anti-patterns:\n  \n     \n     \n     \n  \n      \n        You reactively, rather than proactively, respond to events.      \n    \n      \n        Inconsistent approaches are taken to different types of events or incidents.      \n    \n      Your organization does not analyze and learn from incidents to prevent future occurrences.\n    \n    Benefits of establishing this best\n    practice:\n  \n     \n     \n     \n     \n  \n      \n        Streamlined and standardized response processes.      \n    \n      \n        Reduced impact of incidents on services and customers.      \n    \n      \n        Expedited issue resolution.      \n    \n      \n        Continuous improvement in operational processes.      \n    \n    Level of risk exposed if this best practice\n    is not established: High\n  \n\n  Implementation guidance\n    \n      Implementing this best practice means you are tracking workload events. You have processes\n      to handle incidents and problems. The processes are documented, shared, and updated\n      frequently. Problems are identified, prioritized, and fixed.\n     \n    \n      Understanding events, incidents, and problems\n    \n    \n       \n       \n       \n    \n        \n          Events: An event is an observation of an action, occurrence, or change of state. Events can be planned or unplanned and they can originate internally or externally to the workload.\n        \n      \n        \n          Incidents: Incidents are events that require a response, like unplanned interruptions or degradations of service quality. They represent disruptions that need immediate attention to restore normal workload operation.\n        \n      \n        \n          Problems: Problems are the underlying causes of one or more incidents. Identifying and resolving problems involves digging deeper into the incidents to prevent future occurrences.\n        \n      \n     \n      \n      Implementation steps\n      \n        Events\n      \n      \n         \n         \n      \n          \n            Monitor events:\n          \n          \n             \n             \n             \n             \n          \n              \n                Implement observability and utilize workload observability.\n              \n            \n              \n                Monitor actions taken by a user, role, or an AWS service are recorded as events in AWS CloudTrail.\n              \n            \n              \n                Respond to operational changes in your applications in real time with Amazon EventBridge.\n              \n            \n              \n                Continually assess, monitor, and record resource configuration changes with AWS Config.\n              \n            \n        \n          \n            Create processes:\n          \n          \n             \n             \n             \n          \n              \n                Develop a process to assess which events are significant and require monitoring. This involves setting thresholds and parameters for normal and abnormal activities.\n              \n            \n              \n                Determine criteria escalating an event to an incident. This could be based on the severity, impact on users, or deviation from expected behavior.\n              \n            \n              \n                Regularly review the event monitoring and response processes. This includes analyzing past incidents, adjusting thresholds, and refining alerting mechanisms.\n              \n            \n        \n      \n        Incidents\n      \n      \n         \n         \n         \n      \n          \n            Respond to incidents:\n          \n          \n             \n             \n             \n             \n             \n          \n              \n                Use insights from observability tools to quickly identify and respond to incidents.\n              \n            \n              \n                Implement AWS Systems Manager Ops Center to aggregate, organize, and prioritize operational items and incidents.\n              \n            \n              \n                Use services like Amazon CloudWatch and AWS X-Ray for deeper analysis and troubleshooting.\n              \n            \n              \n                Consider AWS Managed Services (AMS) for enhanced incident management, leveraging its proactive, preventative, and detective capabilities. AMS extends operational support with services like monitoring, incident detection and response, and security management.\n              \n            \n              \n                Enterprise Support customers can use AWS Incident Detection and Response, which provides continual proactive monitoring and incident management for production workloads.\n              \n            \n        \n          \n            Create an incident management process:\n          \n          \n             \n             \n             \n          \n              \n                Establish a structured incident management process, including clear roles, communication protocols, and steps for resolution.\n              \n            \n              \n                Integrate incident management with tools like Amazon Q Developer in chat applications for efficient response and coordination.\n              \n            \n              \n                Categorize incidents by severity, with predefined incident response plans for each category.\n              \n            \n        \n          \n            Learn and improve:\n          \n          \n             \n             \n             \n             \n          \n              \n                Conduct post-incident analysis to understand root causes and resolution effectiveness.\n              \n            \n              \n                Continually update and improve response plans based on reviews and evolving practices.\n              \n            \n              \n                Document and share lessons learned across teams to enhance operational resilience.\n              \n            \n              \n                Enterprise Support customers can request the Incident Management Workshop from their Technical Account Manager. This guided workshop tests your existing incident response plan and helps you identify areas for improvement.\n              \n            \n        \n      \n        Problems\n      \n      \n         \n         \n         \n         \n      \n          \n            Identify problems:\n          \n          \n             \n             \n             \n          \n              \n                Use data from previous incidents to identify recurring patterns that may indicate deeper systemic issues.\n              \n            \n              \n                Leverage tools like AWS CloudTrail and Amazon CloudWatch to analyze trends and uncover underlying problems.\n              \n            \n              \n                Engage cross-functional teams, including operations, development, and business units, to gain diverse perspectives on the root causes.\n              \n            \n        \n          \n            Create a problem management process:\n          \n          \n             \n             \n             \n          \n              \n                Develop a structured process for problem management, focusing on long-term solutions rather than quick fixes.\n              \n            \n              \n                Incorporate root cause analysis (RCA) techniques to investigate and understand the underlying causes of incidents.\n              \n            \n              \n                Update operational policies, procedures, and infrastructure based on findings to prevent recurrence.\n              \n            \n        \n          \n            Continue to improve:\n          \n          \n             \n             \n             \n          \n              \n                Foster a culture of constant learning and improvement, encouraging teams to proactively identify and address potential problems.\n              \n            \n              \n                Regularly review and revise problem management processes and tools to align with evolving business and technology landscapes.\n              \n            \n              \n                Share insights and best practices across the organization to build a more resilient and efficient operational environment.\n              \n            \n        \n          \n            Engage AWS Support:\n          \n          \n             \n             \n          \n              \n                Use AWS support resources, such as AWS Trusted Advisor, for proactive guidance and optimization recommendations.\n              \n            \n              \n                Enterprise Support customers can access specialized programs like AWS Countdown for support during critical events.\n              \n            \n        \n      \n        Level of effort for the implementation plan: Medium\n      \n     \n   \n\n  Resources\n\n  Related best practices:\n\n    \n       \n       \n       \n       \n       \n       \n    \n        \n          OPS04-BP01 Identify key performance indicators\n        \n      \n        \n          OPS04-BP02 Implement application telemetry\n        \n      \n        \n          OPS07-BP03 Use runbooks to perform procedures\n      \n        \n          OPS07-BP04 Use playbooks to investigate issues\n        \n      \n        \n          OPS08-BP01 Analyze workload metrics\n        \n      \n        \n          OPS11-BP02 Perform post-incident analysis\n        \n      \n      \n    \n      Related documents:\n    \n    \n       \n       \n       \n       \n       \n    \n        \n          AWS\n            Security Incident Response Guide\n        \n      \n        AWS Incident Detection and Response\n        \n      \n        AWS Cloud Adoption Framework: Operations Perspective - Incident and problem management\n        \n      \n        \n          Incident\n            Management in the Age of DevOps and SRE\n        \n      \n        \n          PagerDuty - What is Incident Management?\n        \n      \n    \n      Related videos:\n    \n    \n       \n       \n       \n       \n    \n        \n          Top incident response tips from AWS\n      \n        AWS re:Invent 2022 - The Amazon Builders' Library: 25 yrs of Amazon operational excellence\n        \n      \n        AWS re:Invent 2022 - AWS Incident Detection and Response (SUP201) \n      \n        \n          Introducing Incident Manager from AWS Systems Manager\n      \n    \n      Related examples:\n    \n    \n       \n       \n       \n       \n       \n    \n        \n          AWS Proactive Services – Incident Management Workshop\n        \n      \n        \n          How to Automate Incident Response with PagerDuty and AWS Systems Manager Incident Manager\n      \n        \n          Engage Incident Responders with the On-Call Schedules in AWS Systems Manager Incident Manager\n      \n        \n          Improve the Visibility and Collaboration during Incident Handling in AWS Systems Manager Incident Manager\n      \n        \n          Incident reports and service requests in AMS\n        \n      \n    \n      Related services:\n    \n    \n       \n    \n        \n          Amazon EventBridge\n        \n      \n  \u003cdiv\u003e\u003cdiv\u003e\u003cdiv\u003e\u003cdiv id=\"js_error_message\"\u003e\u003cp\u003e\u003cimg src=\"https://d1ge0kk1l5kms0.cloudfront.net/images/G/01/webservices/console/warning.png\" alt=\"Warning\" /\u003e \u003cstrong\u003eJavascript is disabled or is unavailable in your browser.\u003c/strong\u003e\u003c/p\u003e\u003cp\u003eTo use the Amazon Web Services Documentation, Javascript must be enabled. Please refer to your browser's Help pages for instructions.\u003c/p\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003eDocument Conventions OPS 10. How do you manage workload and operations events? OPS10-BP02 Have a process per alert",
  "OPS10-BP02 Have a process per alert\n    Establishing a clear and defined process for each alert in your system is essential for effective and efficient incident management. This practice ensures that every alert leads to a specific, actionable response, improving the reliability and responsiveness of your operations.\n  \n    Desired outcome: Every alert initiates a specific, well-defined response plan. Where possible, responses are automated, with clear ownership and a defined escalation path. Alerts are linked to an up-to-date knowledge base so that any operator can respond consistently and effectively. Responses are quick and uniform across the board, enhancing operational efficiency and reliability.\n  \n    Common anti-patterns:\n  \n     \n     \n     \n  \n      \n        Alerts have no predefined response process, leading to makeshift and delayed resolutions.\n      \n    \n      \n        Alert overload causes important alerts to be overlooked.\n      \n    \n      \n        Alerts are inconsistently handled due to lack of clear ownership and responsibility.\n      \n    \n    Benefits of establishing this best\n    practice:\n  \n     \n     \n     \n     \n     \n  \n      \n        Reduced alert fatigue by only raising actionable alerts.\n      \n    \n      \n        Decreased mean time to resolution (MTTR) for operational issues.\n      \n    \n      \n        Decreased mean time to investigate (MTTI), which helps reduce MTTR.\n      \n    \n      \n        Enhanced ability to scale operational responses.\n      \n    \n      \n        Improved consistency and reliability in handling operational events.\n      \n    \n    For example, you have a defined process for AWS Health events for critical accounts, including application alarms, operational issues, and planned lifecycle events (like updating Amazon EKS versions before clusters are auto-updated), and you provide the capability for your teams to actively monitor, communicate, and respond to these events. These actions help you prevent service disruptions caused by AWS-side changes or mitigate them faster when unexpected issues occur.\n  \n    Level of risk exposed if this best practice\n    is not established: High\n  \n\n  Implementation guidance\n    \n      Having a process per alert involves establishing a clear response plan for each alert, automating responses where possible, and continually refining these processes based on operational feedback and evolving requirements.\n    \n     \n      \n      Implementation steps\n      \n        The following diagram illustrates the incident management workflow within AWS Systems Manager Incident Manager. It is designed to respond swiftly to operational issues by automatically creating incidents in response to specific events from Amazon CloudWatch or Amazon EventBridge. When an incident is created, either automatically or manually, Incident Manager centralizes the management of the incident, organizes relevant AWS resource information, and initiates predefined response plans. This includes running Systems Manager Automation runbooks for immediate action, as well as creating a parent operational work item in OpsCenter to track related tasks and analyses. This streamlined process speeds up and coordinates incident response across your AWS environment.\n      \n      \n         \n          \n         \n         \n      \n       \n      \n         \n         \n         \n         \n         \n      \n          \n            Use composite alarms: Create composite alarms in CloudWatch to group related alarms, reducing noise and allowing for more meaningful responses.\n          \n        \n          \n            Stay informed with AWS Health: AWS Health is the authoritative source of information about the health of your AWS Cloud resources. Use AWS Health to visualize and get notified of any current service events and upcoming changes, such as planned lifecycle events, so you can take steps to mitigate impacts.\n          \n          \n             \n             \n             \n          \n              \n                Create purpose-fit AWS Health event notifications to e-mail and chat channels through AWS User Notifications, and integrate programatically with your monitoring and alerting tools through Amazon EventBridge or the AWS Health API.\n              \n            \n              \n                Plan and track progress on health events that require action by integrating with change management or ITSM tools (like Jira or ServiceNow) that you may already use through Amazon EventBridge or the AWS Health API.\n              \n            \n              \n                If you use AWS Organizations, enable\n                organization view for \n                  AWS Health to aggregate AWS Health events across accounts.\n              \n            \n        \n          \n            Integrate Amazon CloudWatch alarms with Incident Manager: Configure CloudWatch alarms to automatically create incidents in AWS Systems Manager Incident Manager.\n          \n        \n          \n            Integrate Amazon EventBridge with Incident Manager: Create EventBridge rules to react to events and create incidents using defined response plans.\n          \n        \n          \n            Prepare for incidents in Incident Manager:\n          \n          \n             \n             \n             \n          \n              \n                Establish detailed response plans in Incident Manager for each type of alert.\n              \n            \n              \n                Establish chat channels through Amazon Q Developer in chat applications connected to response plans in Incident Manager, facilitating real-time communication during incidents across platforms like Slack, Microsoft Teams, and Amazon Chime.\n              \n            \n              \n                Incorporate Systems Manager Automation runbooks within Incident Manager to drive automated responses to incidents.\n              \n            \n        \n     \n   \n\n  Resources\n\n    \n      Related best practices:\n    \n    \n       \n       \n    \n        \n          OPS04-BP01 Identify key performance indicators\n        \n      \n        \n          OPS08-BP04 Create actionable alerts\n        \n      \n    \n      Related documents:\n    \n    \n       \n       \n       \n       \n    \n        AWS Cloud Adoption Framework: Operations Perspective - Incident and problem management\n        \n      \n        Using Amazon CloudWatch alarms\n      \n         \n          Setting up AWS Systems Manager Incident Manager\n      \n        \n          Preparing for incidents in Incident Manager\n        \n      \n    \n      Related videos:\n    \n    \n       \n       \n    \n        \n          Top incident response tips from AWS\n      \n        \n          re:Invent 2023 | Manage resource lifecycle events at scale with AWS Health\n      \n    \n      Related examples:\n    \n    \n       \n    \n        AWS Workshops - AWS Systems Manager Incident Manager - Automate incident response to security events\n        \n      \n  \u003cdiv\u003e\u003cdiv\u003e\u003cdiv\u003e\u003cdiv id=\"js_error_message\"\u003e\u003cp\u003e\u003cimg src=\"https://d1ge0kk1l5kms0.cloudfront.net/images/G/01/webservices/console/warning.png\" alt=\"Warning\" /\u003e \u003cstrong\u003eJavascript is disabled or is unavailable in your browser.\u003c/strong\u003e\u003c/p\u003e\u003cp\u003eTo use the Amazon Web Services Documentation, Javascript must be enabled. Please refer to your browser's Help pages for instructions.\u003c/p\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003eDocument ConventionsOPS10-BP01 Use a process for event, incident, and problem\n  managementOPS10-BP03 Prioritize operational events based on business\n  impact",
  "OPS10-BP03 Prioritize operational events based on business\n  impact\n    Responding promptly to operational events is critical, but not all events are equal. When you prioritize based on business impact, you also prioritize addressing events with the potential for significant consequences, such as safety, financial loss, regulatory violations, or damage to reputation.\n  \n    Desired outcome: Responses to operational events are prioritized based on potential impact to business operations and objectives. This makes the responses efficient and effective.\n  \n    Common anti-patterns:\n  \n     \n     \n     \n     \n  \n      \n        Every event is treated with the same level of urgency, leading to confusion and delays in addressing critical issues.\n      \n    \n      \n        You fail to distinguish between high and low impact events, leading to misallocation of resources.\n      \n    \n      \n        Your organization lacks a clear prioritization framework, resulting in inconsistent responses to operational events.\n      \n    \n      \n        Events are prioritized based on the order they are reported, rather than their impact on business outcomes.\n      \n    \n    Benefits of establishing this best\n    practice:\n  \n     \n     \n     \n  \n      \n        Ensures critical business functions receive attention first, minimizing potential damage.\n      \n    \n      \n        Improves resource allocation during multiple concurrent events.\n      \n    \n      \n        Enhances the organization's ability to maintain trust and meet regulatory requirements.\n      \n    \n    Level of risk exposed if this best practice\n    is not established: High\n  \n\n  Implementation guidance\n    \n      When faced with multiple operational events, a structured approach to prioritization based on impact and urgency is essential. This approach helps you make informed decisions, direct efforts where they're needed most, and mitigate the risk to business continuity.\n    \n     \n      \n      Implementation steps\n      \n         \n         \n         \n         \n         \n         \n      \n          \n            Assess impact: Develop a classification system to evaluate the severity of events in terms of their potential impact on business operations and objectives. The following example shows impact categories:\n          \n          \n                \n                  Impact level\n                  Description\n                \n              \n                \n                  \n                    High\n                  \n                  \n                    Affects many staff or customers, high financial impact, high reputational damage, or injury.\n                  \n                \n                \n                  \n                    Medium\n                  \n                  \n                    Affects a groups of staff or customers, moderate financial impact, or moderate reputational damage.\n                  \n                \n                \n                  \n                    Low\n                  \n                  \n                    Affects individual staff or customers, low financial impact, or low reputational damage.\n                  \n                \n              \n        \n          \n            Assess urgency: Define urgency levels for how quickly an event needs a response, considering factors such as safety, financial implications, and service-level agreements (SLAs). The following example demonstrates urgency categories:\n          \n          \n                \n                  Urgency level\n                  Description\n                \n              \n                \n                  \n                    High\n                  \n                  \n                    Exponentially increasing damage, time-sensitive work impacted, imminent escalation, or VIP users or groups affected.\n                  \n                \n                \n                  \n                    Medium\n                  \n                  \n                    Damage increases over time, or single VIP user or group affected.\n                  \n                \n                \n                  \n                    Low\n                  \n                  \n                    Marginal damage increase over time, or non-time-sensitive work impacted.\n                  \n                \n              \n        \n          \n            Create a prioritization matrix:\n          \n          \n             \n             \n             \n          \n              \n                Use a matrix to cross-reference impact and urgency, assigning priority levels to different combinations.\n              \n            \n              \n                Make the matrix accessible and understood by all team members responsible for operational event responses.\n              \n            \n              \n                The following example matrix displays incident severity according to urgency and impact:\n              \n            \n          \n                \n                  Urgency and impact\n                  High\n                  Medium\n                  Low\n                \n              \n                \n                  \n                    High\n                  \n                  \n                    Critical\n                  \n                  \n                    Urgent\n                  \n                  \n                    High\n                  \n                \n                \n                  \n                    Medium\n                  \n                  \n                    Urgent\n                  \n                  \n                    High\n                  \n                  \n                    Normal\n                  \n                \n                \n                  \n                    Low\n                  \n                  \n                    High\n                  \n                  \n                    Normal\n                  \n                  \n                    Low\n                  \n                \n              \n        \n          \n            Train and communicate: Train response teams on the prioritization matrix and the importance of following it during an event. Communicate the prioritization process to all stakeholders to set clear expectations.\n          \n        \n          \n            Integrate with incident response:\n          \n          \n             \n             \n             \n          \n              \n                Incorporate the prioritization matrix into your incident response plans and tools.\n              \n            \n              \n                Automate the classification and prioritization of events where possible to speed up response times.\n              \n            \n              \n                Enterprise Support customers can leverage AWS Incident Detection and Response, which provides 24x7 proactive monitoring and incident management for production workloads.\n              \n            \n        \n          \n            Review and adapt: Regularly review the effectiveness of the prioritization process and make adjustments based on feedback and changes in the business environment.\n          \n        \n      \n     \n   \n    \n    Resources\n    \n      Related best practices:\n    \n    \n       \n       \n       \n    \n        \n          OPS03-BP03 Escalation is encouraged\n        \n      \n        \n          OPS08-BP04 Create actionable alerts\n        \n      \n        \n          OPS09-BP01 Measure operations goals and KPIs with metrics\n        \n      \n    \n      Related documents:\n    \n    \n       \n       \n    \n        \n          Atlassian - Understanding incident severity levels\n        \n      \n        \n          IT Process Map - Checklist Incident Priority\n        \n      \n  \u003cdiv\u003e\u003cdiv\u003e\u003cdiv\u003e\u003cdiv id=\"js_error_message\"\u003e\u003cp\u003e\u003cimg src=\"https://d1ge0kk1l5kms0.cloudfront.net/images/G/01/webservices/console/warning.png\" alt=\"Warning\" /\u003e \u003cstrong\u003eJavascript is disabled or is unavailable in your browser.\u003c/strong\u003e\u003c/p\u003e\u003cp\u003eTo use the Amazon Web Services Documentation, Javascript must be enabled. Please refer to your browser's Help pages for instructions.\u003c/p\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003eDocument ConventionsOPS10-BP02 Have a process per alertOPS10-BP04 Define escalation paths",
  "OPS10-BP04 Define escalation pathsEstablish clear escalation paths within your incident response protocols to facilitate timely and effective action. This includes specifying prompts for escalation, detailing the escalation process, and pre-approving actions to expedite decision-making and reduce mean time to resolution (MTTR).\n    Desired outcome: A structured and efficient process that escalates incidents to the appropriate personnel, minimizing response times and impact.\n  \n    Common anti-patterns:\n  \n     \n     \n     \n     \n  \n      Lack of clarity on recovery procedures leads to makeshift responses during critical incidents.\n    \n      Absence of defined permissions and ownership results in delays when urgent action is needed.\n    \n      \n        Stakeholders and customers are not informed in line with expectations.\n      \n    \n      \n        Important decisions are delayed.\n      \n    \n    Benefits of establishing this best\n    practice:\n  \n     \n     \n     \n     \n  \n      \n        Streamlined incident response through predefined escalation procedures.\n      \n    \n      \n        Reduced downtime with pre-approved actions and clear ownership.\n      \n    \n      \n        Improved resource allocation and support-level adjustments according to incident severity.\n      \n    \n      \n        Improved communication to stakeholders and customers.\n      \n    \n    Level of risk exposed if this best practice\n    is not established: Medium\n  \n\n  Implementation guidance\n    \n      Properly defined escalation paths are crucial for rapid incident response. AWS Systems Manager Incident Manager supports the setup of structured escalation plans and on-call schedules, which alert the right personnel so that they are ready to act when incidents occur.\n    \n     \n      \n      Implementation steps\n      \n         \n         \n         \n         \n         \n         \n         \n         \n      \n          \n            Set up escalation prompts: Set up CloudWatch alarms to create an incident in AWS Systems Manager Incident Manager.\n          \n        \n          \n            \n              Set up on-call schedules: Create on-call schedules in Incident Manager that align with your escalation paths. Equip on-call personnel with the necessary permissions and tools to act swiftly.\n          \n        \n          \n            \n              Detail escalation procedures:\n            \n          \n          \n             \n             \n             \n             \n          \n              \n                Determine specific conditions under which an incident should be escalated.\n              \n            \n              \n                Create escalation plans in Incident Manager.\n              \n            \n              \n                Escalation channels should consist of a contact or an on-call schedule.\n              \n            \n              \n                Define the roles and responsibilities of the team at each escalation level.\n              \n            \n        \n          \n            Pre-approve mitigation actions: Collaborate with decision-makers to pre-approve actions for anticipated scenarios. Use Systems Manager Automation runbooks integrated with Incident Manager to speed up incident resolution.\n          \n        \n          \n            Specify ownership: Clearly identify internal owners for each step of the escalation path.\n          \n        \n          \n            Detail third-party escalations:\n          \n          \n             \n             \n             \n             \n             \n          \n              \n                Document third-party service-level agreements (SLAs), and align them with internal goals.\n              \n            \n              \n                Set clear protocols for vendor communication during incidents.\n              \n            \n              \n                Integrate vendor contacts into incident management tools for direct access.\n              \n            \n              \n                Conduct regular drills that include third-party response scenarios.\n              \n            \n              \n                Keep vendor escalation information well-documented and easily accessible.\n              \n            \n        \n          \n            Train and rehearse escalation plans: Train your team on the escalation process and conduct regular incident response drills or game days. Enterprise Support customers can request an Incident Management Workshop.\n          \n        \n          \n            Continue to improve: Review the effectiveness of your escalation paths regularly. Update your processes based on lessons learned from incident post-mortems and continuous feedback.\n          \n        \n      \n        Level of effort for the implementation plan: Moderate\n      \n     \n    \n   \n    \n    Resources\n    \n      Related best practices:\n    \n    \n       \n       \n       \n    \n        \n          OPS08-BP04 Create actionable alerts\n        \n      \n        \n          OPS10-BP02 Have a process per alert\n        \n      \n        \n          OPS11-BP02 Perform post-incident analysis\n        \n      \n    \n      Related documents:\n    \n    \n       \n       \n       \n       \n       \n    \n        AWS Systems Manager Incident Manager Escalation Plans\n        \n      \n        \n          Working with on-call schedules in Incident Manager\n        \n      \n        \n          Creating and Managing Runbooks\n        \n      \n        \n          Temporary elevated access management with AWS IAM Identity Center\n      \n        \n          Atlassian - Escalation policies for effective incident management\n        \n      \n  \u003cdiv\u003e\u003cdiv\u003e\u003cdiv\u003e\u003cdiv id=\"js_error_message\"\u003e\u003cp\u003e\u003cimg src=\"https://d1ge0kk1l5kms0.cloudfront.net/images/G/01/webservices/console/warning.png\" alt=\"Warning\" /\u003e \u003cstrong\u003eJavascript is disabled or is unavailable in your browser.\u003c/strong\u003e\u003c/p\u003e\u003cp\u003eTo use the Amazon Web Services Documentation, Javascript must be enabled. Please refer to your browser's Help pages for instructions.\u003c/p\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003eDocument ConventionsOPS10-BP03 Prioritize operational events based on business\n  impactOPS10-BP05 Define a customer communication plan for service-impacting events",
  "OPS10-BP05 Define a customer communication plan for service-impacting events\n    Effective communication during service impacting events is critical to maintain trust and transparency with customers. A well-defined communication plan helps your organization quickly and clearly share information, both internally and externally, during incidents.\n  \n    Desired outcome:\n  \n     \n     \n     \n  \n      \n        A robust communication plan that effectively informs customers and stakeholders during service impacting events.      \n    \n      \n        Transparency in communication to build trust and reduce customer anxiety.      \n    \n      \n        Minimizing the impact of service impacting events on customer experience and business operations.      \n    \n    Common anti-patterns:\n  \n     \n     \n     \n  \n      \n        Inadequate or delayed communication leads to customer confusion and dissatisfaction.\n      \n    \n      \n        Overly technical or vague messaging fails to convey the actual impact on users.\n      \n    \n      \n        There is no predefined communication strategy, resulting in inconsistent and reactive messaging.\n      \n    \n    Benefits of establishing this best\n    practice: \n  \n     \n     \n     \n  \n      \n        Enhanced customer trust and satisfaction through proactive and clear communication.      \n    \n      \n        Reduced burden on support teams by preemptively addressing customer concerns.      \n    \n      \n        Improved ability to manage and recover from incidents effectively.      \n    \n    Level of risk exposed if this best practice\n    is not established: Medium\n  \n\n  Implementation guidance\n    \n      Creating a comprehensive communication plan for service impacting events involves multiple facets, from choosing the right channels to crafting the message and tone. The plan should be adaptable, scalable, and cater to different outage scenarios.\n    \n    \n     \n      \n      Implementation steps\n      \n         \n         \n         \n         \n         \n         \n      \n          \n            Define roles and responsibilities:\n          \n          \n             \n             \n             \n          \n              \n                Assign a major incident manager to oversee incident response activities.\n              \n            \n              \n                Designate a communications manager responsible for coordinating all external and internal communications.\n              \n            \n              \n                Include the support manager to provide consistent communication through support tickets.\n              \n            \n        \n          \n            Identify communication channels: Select channels like workplace chat, email, SMS, social media, in-app notifications, and status pages. These channels should be resilient and able to operate independently during service impacting events.\n          \n        \n          \n            \n              Communicate quickly, clearly, and regularly to customers:\n            \n          \n          \n             \n             \n             \n             \n             \n          \n              \n                Develop templates for various service impairment scenarios, emphasizing simplicity and essential details. Include information about the service impairment, expected resolution time, and impact.\n              \n            \n              \n                Use Amazon Pinpoint to alert customers using push notifications, in-app notifications, emails, text messages, voice messages, and messages over custom channels.\n              \n            \n              \n                Use Amazon Simple Notification Service (Amazon SNS) to alert subscribers programatically or through email, mobile push notifications, and text messages.\n              \n            \n              \n                Communicate status through dashboards by sharing an Amazon CloudWatch dashboard publicly.\n              \n            \n              \n                Encourage social media engagement:\n              \n              \n                 \n                 \n                 \n              \n                  \n                    Actively monitor social media to understand customer sentiment.\n                  \n                \n                  \n                    Post on social media platforms for public updates and community engagement.\n                  \n                \n                  \n                    Prepare templates for consistent and clear social media communication.\n                  \n                \n            \n        \n          \n            Coordinate internal communication: Implement internal protocols using tools like Amazon Q Developer in chat applications for team coordination and communication. Use CloudWatch dashboards to communicate status.\n          \n        \n          \n            \n              Orchestrate communication with dedicated tools and services:\n            \n          \n          \n             \n             \n             \n          \n              \n                Use AWS Systems Manager Incident Manager with Amazon Q Developer in chat applications to set up dedicated chat channels for real-time internal communication and coordination during incidents.\n              \n            \n              \n                Use AWS Systems Manager Incident Manager runbooks to automate customer notifications through Amazon Pinpoint, Amazon SNS, or third-party tools like social media platforms during incidents.\n              \n            \n              \n                Incorporate approval workflows within runbooks to optionally review and authorize all external communications before sending.\n              \n            \n        \n          \n            \n              Practice and improve:\n            \n          \n          \n             \n             \n             \n          \n              \n                Conduct training on the use of communication tools and strategies. Empower teams to make timely decisions during incidents.\n              \n            \n              \n                Test the communication plan through regular drills or gamedays. Use these tests to refine messaging and evaluate the effectiveness of channels.\n              \n            \n              \n                Implement feedback mechanisms to assess communication effectiveness during incidents. Continually evolve the communication plan based on feedback and changing needs.\n              \n            \n        \n      \n     \n    \n      Level of effort for the implementation plan: High\n    \n   \n\n  Resources\n    \n      Related best practices:\n    \n    \n       \n       \n       \n    \n        \n          OPS07-BP03 Use runbooks to perform procedures\n        \n      \n        \n          OPS10-BP06 Communicate status through dashboards\n        \n      \n        \n          OPS11-BP02 Perform post-incident analysis\n        \n      \n    \n      Related documents:\n    \n    \n       \n       \n       \n    \n        \n          Atlassian -  Incident communication best practices \n        \n      \n        \n          Atlassian - How to write a good status update\n        \n      \n        \n          PagerDuty - A Guide to Incident Communications\n        \n      \n    \n      Related videos:\n    \n    \n       \n    \n        \n          Atlassian - Create your own incident communication plan: Incident templates\n        \n      \n    \n      Related examples:\n    \n    \n       \n    \n        \n          AWS Health Dashboard\n        \n        \n      \n  \u003cdiv\u003e\u003cdiv\u003e\u003cdiv\u003e\u003cdiv id=\"js_error_message\"\u003e\u003cp\u003e\u003cimg src=\"https://d1ge0kk1l5kms0.cloudfront.net/images/G/01/webservices/console/warning.png\" alt=\"Warning\" /\u003e \u003cstrong\u003eJavascript is disabled or is unavailable in your browser.\u003c/strong\u003e\u003c/p\u003e\u003cp\u003eTo use the Amazon Web Services Documentation, Javascript must be enabled. Please refer to your browser's Help pages for instructions.\u003c/p\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003eDocument ConventionsOPS10-BP04 Define escalation pathsOPS10-BP06 Communicate status through dashboards",
  "OPS10-BP06 Communicate status through dashboards\n    Use dashboards as a strategic tool to convey real-time operational status and key metrics to different audiences, including internal technical teams, leadership, and customers. These dashboards offer a centralized, visual representation of system health and business performance, enhancing transparency and decision-making efficiency.\n  \n    Desired outcome:\n  \n     \n     \n     \n  \n      \n        Your dashboards provide a comprehensive view of the system and business metrics relevant to different stakeholders.\n      \n    \n      \n        Stakeholders can proactively access operational information, reducing the need for frequent status requests.\n      \n    \n      \n        Real-time decision-making is enhanced during normal operations and incidents.\n      \n    \n    Common anti-patterns:\n  \n     \n     \n     \n  \n      Engineers joining an incident management call require status updates to get up to speed.\n    \n      Relying on manual reporting for management, which leads to delays and potential inaccuracies.\n    \n      \n        Operations teams are frequently interrupted for status updates during incidents.\n      \n    \n    Benefits of establishing this best\n    practice:\n  \n     \n     \n     \n  \n      \n        Empowers stakeholders with immediate access to critical information, promoting informed decision-making.      \n    \n      \n        Reduces operational inefficiencies by minimizing manual reporting and frequent status inquiries.      \n    \n      \n        Increases transparency and trust through real-time visibility into system performance and business metrics.      \n    \n    Level of risk exposed if this best practice\n    is not established: Medium\n  \n\n  Implementation guidance\n    \n      Dashboards effectively communicate the status of your system and business metrics and can be tailored to the needs of different audience groups. Tools like Amazon CloudWatch dashboards and Amazon QuickSight help you create interactive, real-time dashboards for system monitoring and business intelligence.\n    \n\n      \n     \n      \n      Implementation steps\n      \n         \n         \n         \n         \n         \n         \n      \n          \n            Identify stakeholder needs: Determine the specific information needs of different audience groups, such as technical teams, leadership, and customers.\n          \n        \n          \n            \n              Choose the right tools: Select appropriate tools like Amazon CloudWatch dashboards for system monitoring and Amazon QuickSight for interactive business intelligence. AWS Health provides a ready-to-use experience in the AWS Health Dashboard, or you can use Health events in Amazon EventBridge or through the AWS Health API to augment your own dashboards.\n          \n        \n          \n            Design effective dashboards:\n          \n          \n             \n             \n             \n             \n             \n          \n              \n                Design dashboards to clearly present relevant metrics and KPIs, ensuring they are understandable and actionable.\n              \n            \n              \n                Incorporate system-level and business-level views as needed.\n              \n            \n              \n                Include both high-level (for broad overviews) and low-level (for detailed analysis) dashboards.\n              \n            \n              \n                Integrate automated alarms within dashboards to highlight critical issues.\n              \n            \n              \n                Annotate dashboards with important metrics thresholds and goals for immediate visibility.\n              \n            \n        \n          \n            Integrate data sources:\n          \n          \n             \n             \n             \n          \n              \n                Use Amazon CloudWatch to aggregate and display metrics from various AWS services and query metrics from other data sources, creating a unified view of your system's health and business metrics.\n              \n            \n              \n                Use features like CloudWatch Logs Insights to query and visualize log data from different applications and services.\n              \n            \n              \n                Use AWS Health events to stay informed about the operational status and confirmed operational issues from AWS services through the AWS Health API or AWS Health events on Amazon EventBridge.\n              \n            \n        \n          \n            Provide self-service access:\n          \n          \n             \n             \n          \n              \n                Share CloudWatch dashboards with relevant stakeholders for self-service information access using dashboard sharing features.\n              \n            \n              \n                Ensure that dashboards are easily accessible and provide real-time, up-to-date information.\n              \n            \n        \n          \n            Regularly update and refine:\n          \n          \n             \n             \n          \n              \n                Continually update and refine dashboards to align with evolving business needs and stakeholder feedback.\n              \n            \n              \n                Regularly review the dashboards to keep them relevant and effective for conveying the necessary information.\n              \n            \n        \n      \n     \n   \n\n  Resources\n\n    \n      Related best practices:\n    \n    \n       \n    \n        \n          OPS08-BP05 Create dashboards      \n        \n      \n    \n      Related documents:\n    \n    \n       \n       \n       \n       \n       \n       \n    \n        \n          Building dashboards for operational visibility\n        \n      \n        \n          Using Amazon CloudWatch dashboards\n        \n      \n        \n          Create flexible dashboards with dashboard variables\n        \n      \n        \n          Sharing CloudWatch dashboards\n        \n      \n        \n          Query metrics from other data sources\n        \n      \n        \n          Add a custom widget to a CloudWatch dashboard\n        \n      \n    \n      Related examples:\n    \n    \n       \n    \n        \n          One Observability Workshop - Dashboards\n        \n      \n  \u003cdiv\u003e\u003cdiv\u003e\u003cdiv\u003e\u003cdiv id=\"js_error_message\"\u003e\u003cp\u003e\u003cimg src=\"https://d1ge0kk1l5kms0.cloudfront.net/images/G/01/webservices/console/warning.png\" alt=\"Warning\" /\u003e \u003cstrong\u003eJavascript is disabled or is unavailable in your browser.\u003c/strong\u003e\u003c/p\u003e\u003cp\u003eTo use the Amazon Web Services Documentation, Javascript must be enabled. Please refer to your browser's Help pages for instructions.\u003c/p\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003eDocument ConventionsOPS10-BP05 Define a customer communication plan for service-impacting eventsOPS10-BP07 Automate responses to events",
  "OPS10-BP07 Automate responses to events\n    Automating event responses is key for fast, consistent, and error-free operational handling. Create streamlined processes and use tools to automatically manage and respond to events, minimizing manual interventions and enhancing operational effectiveness.\n  \n    Desired outcome:\n  \n     \n     \n     \n  \n      \n        Reduced human errors and faster resolution times through automation.\n      \n    \n      \n        Consistent and reliable operational event handling.\n      \n    \n      \n        Enhanced operational efficiency and system reliability.\n      \n    \n    Common anti-patterns:\n  \n     \n     \n     \n  \n      Manual event handling leads to delays and errors.\n    \n      Automation is overlooked in repetitive, critical tasks.\n    \n      \n        Repetitive, manual tasks lead to alert fatigue and missing critical issues.\n      \n    \n    Benefits of establishing this best\n    practice: \n  \n     \n     \n  \n      \n        Accelerated event responses, reducing system downtime.\n      \n    \n      \n        Reliable operations with automated and consistent event handling.\n      \n    \n    Level of risk exposed if this best practice\n    is not established: Medium\n  \n\n  Implementation guidance\n    \n      Incorporate automation to create efficient operational workflows and minimize manual interventions.\n    \n     \n      \n      Implementation steps\n      \n         \n         \n         \n         \n      \n          \n            Identify automation opportunites: Determine repetitive tasks for automation, such as issue remediation, ticket enrichment, capacity management, scaling, deployments, and testing.\n          \n        \n          \n            Identify automation prompts:\n          \n          \n             \n             \n             \n          \n              \n                Assess and define specific conditions or metrics that initiate automated responses using Amazon CloudWatch alarm actions.\n              \n            \n              \n                Use Amazon EventBridge to respond to events in AWS services, custom workloads, and SaaS applications.\n              \n            \n              \n                Consider initiation events such as specific log entries, performance metrics thresholds, or state changes in AWS resources.\n              \n            \n        \n          \n            Implement event-driven automation:\n          \n          \n             \n             \n             \n             \n             \n             \n          \n              \n                Use AWS Systems Manager Automation runbooks to simplify maintenance, deployment, and remediation tasks.\n              \n            \n              \n                Creating incidents in Incident Manager automatically gathers and adds details about the involved AWS resources to the incident.\n              \n            \n              \n                Proactively monitor quotas using Quota Monitor for AWS.\n              \n            \n              \n                Automatically adjust capacity with AWS Auto Scaling to maintain availability and performance.\n              \n            \n              \n                Automate development pipelines with Amazon CodeCatalyst.\n              \n            \n              \n                Smoke test or continually monitor endpoints and APIs using synthetic monitoring.\n              \n            \n        \n          \n            Perform risk mitigation through automation:\n          \n          \n             \n             \n             \n          \n              \n                Implement automated security responses to swiftly address risks.\n              \n            \n              \n                Use AWS Systems Manager State Manager to reduce configuration drift.\n              \n            \n              \n                Remediate noncompliant resources with AWS Config Rules.\n              \n            \n        \n     \n    \n      Level of effort for the implementation plan: High\n    \n   \n\n  Resources\n\n    \n      Related best practices:\n    \n    \n       \n       \n    \n        \n          OPS08-BP04 Create actionable alerts\n        \n      \n        \n          OPS10-BP02 Have a process per alert\n        \n      \n    \n      Related documents:\n    \n    \n       \n       \n       \n       \n       \n       \n       \n       \n       \n       \n       \n    \n        \n          Using Systems Manager Automation runbooks with Incident Manager\n        \n      \n        \n          Creating incidents in Incident Manager\n        \n      \n        \n          AWS service quotas\n        \n      \n        \n          Monitor resource usage and send notifications when approaching quotas\n        \n      \n        \n          AWS Auto Scaling\n        \n      \n        \n          What is Amazon CodeCatalyst?\n        \n      \n        \n          Using Amazon CloudWatch alarms\n        \n      \n        \n          Using Amazon CloudWatch alarm actions\n        \n      \n        \n          Remediating Noncompliant Resources with AWS Config Rules\n        \n      \n        \n          Creating metrics from log events using filters\n        \n      \n        \n          AWS Systems Manager State Manager\n        \n      \n    \n      Related videos:\n    \n    \n       \n       \n       \n       \n    \n        \n          Create Automation Runbooks with AWS Systems Manager\n      \n        \n          How to automate IT Operations on AWS\n      \n        AWS Security Hub automation rules\n        \n      \n        \n          Start your software project fast with Amazon CodeCatalyst blueprints\n        \n      \n    \n      Related examples:\n    \n    \n       \n       \n       \n    \n        \n          Amazon CodeCatalyst Tutorial: Creating a project with the Modern three-tier web application blueprint\n        \n      \n        \n          One Observability Workshop\n        \n      \n        \n          Respond to incidents using Incident Manager\n        \n      \n  \u003cdiv\u003e\u003cdiv\u003e\u003cdiv\u003e\u003cdiv id=\"js_error_message\"\u003e\u003cp\u003e\u003cimg src=\"https://d1ge0kk1l5kms0.cloudfront.net/images/G/01/webservices/console/warning.png\" alt=\"Warning\" /\u003e \u003cstrong\u003eJavascript is disabled or is unavailable in your browser.\u003c/strong\u003e\u003c/p\u003e\u003cp\u003eTo use the Amazon Web Services Documentation, Javascript must be enabled. Please refer to your browser's Help pages for instructions.\u003c/p\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003eDocument ConventionsOPS10-BP06 Communicate status through dashboards                Evolve             ",
  "OPS11-BP01 Have a process for continuous improvement\n    Evaluate your workload against internal and external architecture\n    best practices. Conduct frequent, intentional workload reviews.\n    Prioritize improvement opportunities into your software development\n    cadence.\n  \n    Desired outcome:\n  \n     \n     \n  \n      \n        You analyze your workload against architecture best practices\n        frequently.\n      \n    \n      \n        You give improvement opportunities equal priority to features in\n        your software development process.\n      \n    \n    Common anti-patterns:\n  \n     \n     \n     \n  \n      \n        You have not conducted an architecture review on your workload\n        since it was deployed several years ago.\n      \n    \n      \n        You give a lower priority to improvement opportunities. Compared\n        to new features, these opportunities stay in the backlog.\n      \n    \n      \n        There is no standard for implementing modifications to best\n        practices for the organization.\n      \n    \n    Benefits of establishing this best\n      practice:\n  \n     \n     \n     \n     \n  \n      \n        Your workload is kept up-to-date on architecture best practices.\n      \n    \n      \n        You evolve your workload in an intentional manner.\n      \n    \n      \n        You can leverage organization best practices to improve all\n        workloads.\n      \n    \n      \n        You make marginal gains that have a cumulative impact, which\n        drives deeper efficiencies.\n      \n    \n    Level of risk exposed if this best practice\n      is not established: High\n  \n    \n    Implementation guidance\n    \n    \n    \n      Frequently conduct an architectural review of your workload. Use\n      internal and external best practices, evaluate your workload, and\n      identify improvement opportunities. Prioritize improvement\n      opportunities into your software development cadence.\n    \n     \n      \n      Implementation steps\n      \n      \n      \n         \n         \n      \n          \n            Conduct periodic architecture reviews of your production\n            workload with an agreed-upon frequency. Use a documented\n            architectural standard that includes AWS-specific best\n            practices.\n          \n          \n             \n             \n             \n          \n              \n                Use your internally-defined standards for these reviews.\n                If you do not have an internal standard, use the AWS\n                Well-Architected Framework.\n              \n            \n              \n                Use the AWS Well-Architected Tool to create a custom\n                lens of your internal best practices and conduct your\n                architecture review.\n              \n            \n              \n                Contact your AWS Solution Architect or Technical Account\n                Manager to conduct a guided Well-Architected Framework\n                Review of your workload.\n              \n            \n        \n          \n            Prioritize improvement opportunities identified during the\n            review into your software development process.\n          \n        \n      \n        Level of effort for the implementation\n          plan: Low. You can use the AWS Well-Architected\n        Framework to conduct your yearly architecture review.\n      \n     \n   \n    \n    Resources\n    \n    \n    \n      Related best practices:\n    \n    \n       \n       \n       \n    \n        \n          OPS11-BP02\n            Perform post-incident analysis\n        \n      \n        \n          OPS11-BP08\n            Document and share lessons learned\n        \n      \n        \n          OPS04\n            Implement Observability\n        \n      \n    \n      Related documents:\n    \n    \n       \n       \n       \n       \n    \n        \n          AWS Well-Architected Tool - Custom lenses\n        \n      \n        \n          AWS           Well-Architected Whitepaper - The review process\n        \n      \n        \n          Customize\n            Well-Architected Reviews using Custom Lenses and the AWS Well-Architected Tool\n        \n      \n        \n          Implementing\n            the AWS Well-Architected Custom Lens lifecycle in your\n            organization\n        \n      \n    \n      Related videos:\n    \n    \n       \n    \n        \n          AWS           re:Invent 2023 - Scaling AWS Well-Architected best practices\n            across your organization\n        \n      \n    \n      Related examples:\n    \n    \n       \n    \n        \n          AWS Well-Architected Tool\n        \n      \n  \u003cdiv\u003e\u003cdiv\u003e\u003cdiv\u003e\u003cdiv id=\"js_error_message\"\u003e\u003cp\u003e\u003cimg src=\"https://d1ge0kk1l5kms0.cloudfront.net/images/G/01/webservices/console/warning.png\" alt=\"Warning\" /\u003e \u003cstrong\u003eJavascript is disabled or is unavailable in your browser.\u003c/strong\u003e\u003c/p\u003e\u003cp\u003eTo use the Amazon Web Services Documentation, Javascript must be enabled. Please refer to your browser's Help pages for instructions.\u003c/p\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003eDocument ConventionsOPS 11. How do you evolve operations?OPS11-BP02 Perform post-incident analysis",
  "OPS11-BP02 Perform post-incident analysis\n    Review customer-impacting events and identify the contributing\n    factors and preventative actions. Use this information to develop\n    mitigations to limit or prevent recurrence. Develop procedures for\n    prompt and effective responses. Communicate contributing factors and\n    corrective actions as appropriate, tailored to target audiences.\n  \n    Desired outcome:\n  \n     \n     \n     \n     \n  \n      \n        You have established incident management processes that include\n        post-incident analysis.\n      \n    \n      \n        You have observability plans in place to collect data on events.\n      \n    \n      \n        With this data, you understand and collect metrics that support\n        your post-incident analysis process.\n      \n    \n      \n        You learn from incidents to improve future outcomes.\n      \n    \n    Common anti-patterns:\n  \n     \n     \n  \n      \n        You administer an application server. Approximately every 23\n        hours and 55 minutes all your active sessions are terminated.\n        You have tried to identify what is going wrong on your\n        application server. You suspect it could instead be a network\n        issue but are unable to get cooperation from the network team as\n        they are too busy to support you. You lack a predefined process\n        to follow to get support and collect the information necessary\n        to determine what is going on.\n      \n    \n      \n        You have had data loss within your workload. This is the first\n        time it has happened and the cause is not obvious. You decide it\n        is not important because you can recreate the data. Data loss\n        starts occurring with greater frequency impacting your\n        customers. This also places addition operational burden on you\n        as you restore the missing data.\n      \n    \n    Benefits of establishing this best\n      practice:\n  \n     \n     \n  \n      \n        You have a predefined process to determine the components,\n        conditions, actions, and events that contributed to an incident,\n        which helps you identify opportunities for improvement.\n      \n    \n      \n        You use data from post-incident analysis to make improvements.\n      \n    \n    Level of risk exposed if this best practice\n      is not established: High\n  \n    \n    Implementation guidance\n    \n    \n    \n      Use a process to determine contributing factors. Review all\n      customer impacting incidents. Have a process to identify and\n      document the contributing factors of an incident so that you can\n      develop mitigations to limit or prevent recurrence and you can\n      develop procedures for prompt and effective responses. Communicate\n      incident root causes as appropriate, and tailor the communication\n      to your target audience. Share learnings openly within your\n      organization.\n    \n     \n      \n      Implementation steps\n      \n      \n      \n         \n         \n         \n         \n      \n          \n            Collect metrics such as deployment change, configuration\n            change, incident start time, alarm time, time of engagement,\n            mitigation start time, and incident resolved time.\n          \n        \n          \n            Describe key time points on the timeline to understand the\n            events of the incident.\n          \n        \n          \n            Ask the following questions:\n          \n          \n             \n             \n             \n             \n             \n             \n             \n          \n              \n                Could you improve time to detection?\n              \n            \n              \n                Are there updates to metrics and alarms that would\n                detect the incident sooner?\n              \n            \n              \n                Can you improve the time to diagnosis?\n              \n            \n              \n                Are there updates to your response plans or escalation\n                plans that would engage the correct responders sooner?\n              \n            \n              \n                Can you improve the time to mitigation?\n              \n            \n              \n                Are there runbook or playbook steps that you could add\n                or improve?\n              \n            \n              \n                Can you prevent future incidents from occurring?\n              \n            \n        \n          \n            Create checklists and actions. Track and deliver all\n            actions.\n          \n        \n      \n        Level of effort for the implementation\n          plan: Medium\n      \n     \n   \n    \n    Resources\n    \n    \n    \n      Related best practices:\n    \n    \n       \n       \n    \n        \n          OPS11-BP01 Have a process for continuous improvement\n        \n      \n        \n          OPS 4 - Implement observability\n        \n      \n    \n      Related documents:\n    \n    \n       \n       \n    \n        \n          Performing\n            a post-incident analysis in Incident Manager\n        \n      \n        \n          Operational\n            Readiness Review\n        \n      \n  \u003cdiv\u003e\u003cdiv\u003e\u003cdiv\u003e\u003cdiv id=\"js_error_message\"\u003e\u003cp\u003e\u003cimg src=\"https://d1ge0kk1l5kms0.cloudfront.net/images/G/01/webservices/console/warning.png\" alt=\"Warning\" /\u003e \u003cstrong\u003eJavascript is disabled or is unavailable in your browser.\u003c/strong\u003e\u003c/p\u003e\u003cp\u003eTo use the Amazon Web Services Documentation, Javascript must be enabled. Please refer to your browser's Help pages for instructions.\u003c/p\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003eDocument ConventionsOPS11-BP01 Have a process for continuous improvementOPS11-BP03 Implement feedback loops",
  "OPS11-BP03 Implement feedback loopsFeedback loops provide actionable insights that drive decision making. \n    Build feedback loops into your procedures and workloads. This helps you \n    identify issues and areas that need improvement. They also validate investments \n    made in improvements. These feedback loops are the foundation for continuously \n    improving your workload.\n    Feedback loops fall into two categories: immediate feedback and retrospective analysis. \n    Immediate feedback is gathered through review of the performance and outcomes from operations \n    activities. This feedback comes from team members, customers, or the automated output of \n    the activity. Immediate feedback is received from things like A/B testing and shipping new \n    features, and it is essential to failing fast.\n  \n    Retrospective analysis is performed regularly to capture feedback from the review of \n    operational outcomes and metrics over time. These retrospectives happen at the end of \n    a sprint, on a cadence, or after major releases or events. This type of feedback loop \n    validates investments in operations or your workload. It helps you measure success and \n    validates your strategy.\n  \n    Desired outcome: You use immediate feedback and retrospective \n    analysis to drive improvements. There is a mechanism to capture user and team member feedback. \n    Retrospective analysis is used to identify trends that drive improvements.\n  \n    Common anti-patterns:\n  \n     \n     \n     \n     \n     \n  \n      You launch a new feature but have no way of receiving customer feedback on it.\n    \n      After investing in operations improvements, you don’t conduct a retrospective to validate them.\n    \n      You collect customer feedback but don’t regularly review it.\n    \n      Feedback loops lead to proposed action items but they aren’t included in the software development process.\n    \n      \n        Customers don’t receive feedback on improvements they’ve proposed.\n      \n    \n    Benefits of establishing this best\n    practice: \n  \n     \n     \n     \n     \n  \n      \n        You can work backwards from the customer to drive new features.\n      \n    \n      \n        Your organization culture can react to changes faster.\n      \n    \n      \n        Trends are used to identify improvement opportunities.\n      \n    \n      \n        Retrospectives validate investments made to your workload and operations.\n      \n    \n    Level of risk exposed if this best practice\n    is not established: High\n  \n\n  Implementation guidance\n\n      \n        Implementing this best practice means that you use both immediate feedback and retrospective analysis. \n        These feedback loops drive improvements. There are many mechanisms for immediate feedback, including \n        surveys, customer polls, or feedback forms. Your organization also uses retrospectives to identify \n        improvement opportunities and validate initiatives.\n      \n    \n      Customer example\n    \n    \n      AnyCompany Retail created a web form where customers can give feedback or report issues. During the weekly \n      scrum, user feedback is evaluated by the software development team. Feedback is regularly used to steer the \n      evolution of their platform. They conduct a retrospective at the end of each sprint to identify items they \n      want to improve.\n    \n   \n    \n    Implementation steps\n    \n       \n       \n    \n        Immediate feedback\n        \n           \n           \n           \n           \n        \n            \n              You need a mechanism to receive feedback from customers and team members. \n              Your operations activities can also be configured to deliver automated feedback. \n            \n          \n            \n              Your organization needs a process to review this feedback, determine what to \n              improve, and schedule the improvement.\n            \n          \n            \n              Feedback must be added into your software development process. \n            \n          \n            \n              As you make improvements, follow up with the feedback submitter.\n            \n            \n               \n            \n                 You can use AWS Systems Manager OpsCenter to create and track these improvements as OpsItems.\n              \n          \n      \n        \n          Retrospective analysis\n        \n        \n           \n           \n           \n           \n           \n           \n        \n            \n              Conduct retrospectives at the end of a development cycle, on a set cadence, or after a major release.\n            \n          \n            \n              Gather stakeholders involved in the workload for a retrospective meeting.\n            \n          \n            \n              Create three columns on a whiteboard or spreadsheet: Stop, Start, and Keep.\n            \n            \n               \n               \n               \n            \n                \n                  Stop is for anything that you want your team to\n                  stop doing. \n              \n                \n                  Start is for ideas that you want to start\n                  doing. \n              \n                \n                  Keep is for items that you want to keep doing. \n              \n          \n            \n              Go around the room and gather feedback from the stakeholders.\n            \n          \n            \n              Prioritize the feedback. Assign actions and stakeholders to any Start or Keep items.\n            \n          \n            \n              Add the actions to your software development process and communicate status updates to \n              stakeholders as you make the improvements.\n            \n          \n      \n    \n      Level of effort for the implementation plan: Medium. To implement this \n      best practice, you need a way to take in immediate feedback and analyze it. Also, you need to \n      establish a retrospective analysis process.\n    \n   \n\n  Resources\n\n    \n      Related best practices:\n    \n    \n       \n       \n       \n       \n    \n        \n          OPS01-BP01 Evaluate external customer needs: Feedback loops are a mechanism to gather external customer needs.\n        \n      \n        \n          OPS01-BP02 Evaluate internal customer needs: Internal stakeholders can use feedback loops to communicate needs and requirements.\n        \n      \n        \n          OPS11-BP02 Perform post-incident analysis: Post-incident analyses are an important form of retrospective analysis conducted after incidents.\n        \n      \n        \n          OPS11-BP07 Perform operations metrics reviews: Operations metrics reviews identify trends and areas for improvement.\n        \n      \n    \n      Related documents:\n    \n    \n       \n       \n       \n       \n       \n       \n       \n       \n       \n       \n    \n        \n          7\n            Pitfalls to Avoid When Building a CCOE\n        \n      \n        \n          Atlassian Team\n            Playbook - Retrospectives\n        \n      \n        \n          Email\n            Definitions: Feedback Loops\n        \n      \n        \n          Establishing Feedback Loops Based on the AWS Well-Architected Framework Review\n        \n      \n        \n          IBM Garage Methodology - Hold a retrospective\n        \n      \n        \n          Investopedia – The PDCS\n            Cycle\n        \n      \n        \n          Maximizing\n            Developer Effectiveness by Tim Cochran\n        \n      \n        \n          Operations Readiness Reviews (ORR) Whitepaper - Iteration\n        \n      \n        \n          ITIL CSI - Continual Service Improvement\n      \n        \n          When Toyota met e-commerce: Lean at Amazon\n        \n      \n    \n      Related videos:\n    \n    \n       \n    \n        \n          Building Effective Customer\n            Feedback Loops\n        \n      \n    \n      Related examples: \n    \n    \n       \n       \n       \n    \n        \n          Astuto - Open source customer feedback\n            tool\n        \n      \n        \n          AWS Solutions - QnABot on\n            AWS\n        \n      \n        \n          Fider - A platform to organize customer\n            feedback\n        \n      \n    \n      Related services:\n    \n    \n       \n    \n        \n          AWS Systems Manager OpsCenter\n        \n      \n  \u003cdiv\u003e\u003cdiv\u003e\u003cdiv\u003e\u003cdiv id=\"js_error_message\"\u003e\u003cp\u003e\u003cimg src=\"https://d1ge0kk1l5kms0.cloudfront.net/images/G/01/webservices/console/warning.png\" alt=\"Warning\" /\u003e \u003cstrong\u003eJavascript is disabled or is unavailable in your browser.\u003c/strong\u003e\u003c/p\u003e\u003cp\u003eTo use the Amazon Web Services Documentation, Javascript must be enabled. Please refer to your browser's Help pages for instructions.\u003c/p\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003eDocument ConventionsOPS11-BP02 Perform post-incident analysisOPS11-BP04 Perform knowledge management",
  "OPS11-BP04 Perform knowledge managementKnowledge management helps team members find the information to perform their job. In learning organizations, information is freely shared which empowers individuals. The information can be discovered or searched. Information is accurate and up to date. Mechanisms exist to create new information, update existing information, and archive outdated information. The most common example of a knowledge management platform is a content management system like a wiki. \n    Desired outcome:\n  \n     \n     \n     \n  \n      \n        Team members have access to timely, accurate information.\n      \n    \n      \n        Information is searchable.\n      \n    \n      \n        Mechanisms exist to add, update, and archive information.\n      \n    \n    Common anti-patterns:\n  \n     \n     \n     \n  \n      There is no centralized knowledge storage. Team members manage their own notes on their local machines.\n    \n      \n        You have a self-hosted wiki but no mechanisms to manage information, resulting in outdated information.\n      \n    \n      \n        Someone identifies missing information but there’s no process to request adding it the team wiki. They add it themselves but they miss a key step, leading to an outage.\n      \n    \n    Benefits of establishing this best practice:\n  \n     \n     \n     \n  \n      \n        Team members are empowered because information is shared freely.\n      \n    \n      \n        New team members are onboarded faster because documentation is up to date and searchable.\n      \n    \n      \n        Information is timely, accurate, and actionable.\n      \n    \n    Level of risk exposed if this best practice\n    is not established: High\n  \n\n  Implementation guidance\n    \n      Knowledge management is an important facet of learning organizations. To begin, you need a central repository to store your knowledge (as a common example, a self-hosted wiki). You must develop processes for adding, updating, and archiving knowledge. Develop standards for what should be documented and let everyone contribute.\n    \n    \n      Customer example\n    \n    \n      AnyCompany Retail hosts an internal Wiki where all knowledge is stored. Team members are encouraged to add to the knowledge base as they go about their daily duties. On a quarterly basis, a cross-functional team evaluates which pages are least updated and determines if they should be archived or updated.\n    \n    \n      Implementation steps\n    \n    \n       \n       \n       \n       \n    \n        \n          Start with identifying the content management system where knowledge will be stored. Get agreement from stakeholders across your organization. \n        \n        \n           \n        \n            \n              If you don’t have an existing content management system, consider running a self-hosted wiki or using a version control repository as a starting point.\n            \n          \n      \n        \n          Develop runbooks for adding, updating, and archiving information. Educate your team on these processes.\n        \n      \n        \n          Identify what knowledge should be stored in the content management system. Start with daily activities (runbooks and playbooks) that team members perform. Work with stakeholders to prioritize what knowledge is added.\n        \n      \n        \n          On a periodic basis, work with stakeholders to identify out-of-date information and archive it or bring it up to date.\n        \n      \n    \n      Level of effort for the implementation plan: Medium. If you don’t have an existing content management system, you can set up a self-hosted wiki or a version-controlled document repository.\n    \n   \n    \n    Resources\n    \n      Related best practices:\n    \n    \n       \n    \n        \n          OPS11-BP08 Document and share lessons learned - Knowledge management facilitates information sharing about lessons learned.\n        \n      \n    \n      Related documents:\n    \n    \n       \n    \n        \n          Atlassian - Knowledge Management\n        \n      \n    \n      Related examples:\n    \n    \n       \n       \n       \n       \n    \n        \n          DokuWiki\n        \n      \n        \n          Gollum\n        \n      \n         \n          MediaWiki\n        \n      \n        \n          Wiki.js\n        \n      \n  \u003cdiv\u003e\u003cdiv\u003e\u003cdiv\u003e\u003cdiv id=\"js_error_message\"\u003e\u003cp\u003e\u003cimg src=\"https://d1ge0kk1l5kms0.cloudfront.net/images/G/01/webservices/console/warning.png\" alt=\"Warning\" /\u003e \u003cstrong\u003eJavascript is disabled or is unavailable in your browser.\u003c/strong\u003e\u003c/p\u003e\u003cp\u003eTo use the Amazon Web Services Documentation, Javascript must be enabled. Please refer to your browser's Help pages for instructions.\u003c/p\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003eDocument ConventionsOPS11-BP03 Implement feedback loopsOPS11-BP05 Define drivers for improvement",
  "OPS11-BP05 Define drivers for improvement\n    Identify drivers for improvement to help you evaluate and prioritize\n    opportunities based on data and feedback loops. Explore improvement\n    opportunities in your systems and processes, and automate where\n    appropriate.\n  \n    Desired outcome:\n  \n     \n     \n     \n     \n     \n  \n      \n        You track data from across your environment.\n      \n    \n      \n        You correlate events and activities to business outcomes.\n      \n    \n      \n        You can compare and contrast between environments and systems.\n      \n    \n      \n        You maintain a detailed activity history of your deployments and\n        outcomes.\n      \n    \n      \n        You collect data to support your security posture.\n      \n    \n    Common anti-patterns:\n  \n     \n     \n     \n     \n  \n      \n        You collect data from across your environment but do not\n        correlate events and activities.\n      \n    \n      \n        You collect detailed data from across your estate, and it drives\n        high Amazon CloudWatch and AWS CloudTrail activity and cost.\n        However, you do not use this data meaningfully.\n      \n    \n      \n        You do not account for business outcomes when defining drivers\n        for improvement.\n      \n    \n      \n        You do not measure the effects of new features.\n      \n    \n    Benefits of establishing this best\n    practice:\n  \n     \n     \n     \n  \n      \n        You minimize the impact of event-based motivations or emotional\n        investment by determining criteria for improvement.\n      \n    \n      \n        You respond to business events, not just technical ones.\n      \n    \n      \n        You measure your environment to identify areas of improvement.\n      \n    \n    Level of risk exposed if this best practice\n    is not established: Medium\n  \n\n  Implementation guidance\n\n      \n    \n       \n    \n        \n          Understand drivers for improvement: You should only make\n          changes to a system when a desired outcome is supported.\n        \n        \n           \n           \n           \n        \n            \n              Desired capabilities: Evaluate desired features and\n              capabilities when evaluating opportunities for\n              improvement.\n            \n            \n               \n            \n                \n                  What's\n                  New with AWS\n                \n              \n          \n            \n              Unacceptable issues: Evaluate unacceptable issues, bugs,\n              and vulnerabilities when evaluating opportunities for\n              improvement. Track rightsizing options, and seek\n              optimization opportunities.\n            \n            \n               \n               \n               \n            \n                \n                  AWS                   Latest Security Bulletins\n                \n              \n                \n                  AWS Trusted Advisor\n                \n              \n                \n                  Cloud\n                  Intelligence Dashboards\n                \n              \n          \n            \n              Compliance requirements: Evaluate updates and changes\n              required to maintain compliance with regulation, policy,\n              or to remain under support from a third party, when\n              reviewing opportunities for improvement.\n            \n            \n               \n               \n               \n            \n                \n                  AWS                   Compliance\n                \n              \n                \n                  AWS                   Compliance Programs\n                \n              \n                \n                  AWS                   Compliance Latest News\n                \n              \n          \n      \n   \n\n  Resources\n\n      \n    \n      Related best practices:\n    \n    \n       \n       \n       \n       \n       \n       \n    \n        \n          OPS01\n          Organization priorities\n        \n      \n        \n          OPS02\n          Relationships and Ownerships\n        \n      \n        \n          OPS04-BP01\n          Identify key performance indicators\n        \n      \n        \n          OPS08\n          Utilizing Workload Observability\n        \n      \n        \n          OPS09\n          Understanding Operational Health\n        \n      \n        \n          OPS11-BP03\n          Implement feedback loops\n        \n      \n    \n      Related documents:\n    \n    \n       \n       \n       \n       \n       \n       \n       \n       \n       \n       \n       \n       \n    \n        \n          Amazon Athena\n        \n      \n        \n          QuickSight\n        \n      \n        \n          AWS           Compliance\n        \n      \n        \n          AWS           Compliance Latest News\n        \n      \n        \n          AWS           Compliance Programs\n        \n      \n        \n          AWS Glue\n        \n      \n        \n          AWS           Latest Security Bulletins\n        \n      \n        \n          AWS Trusted Advisor\n        \n      \n        \n          Export\n          your log data to Amazon S3\n        \n      \n        \n          What's New with\n          AWS\n        \n      \n        \n          The\n          Imperatives of Customer-Centric Innovation\n        \n      \n        \n          Digital\n          Transformation: Hype or a Strategic Necessity?\n        \n      \n    \n      Related Videos\n    \n    \n       \n    \n        \n          AWS           re:Invent 2023 - Improve operational efficiency and resilience\n          with Support (SUP310)\n        \n      \n  \u003cdiv\u003e\u003cdiv\u003e\u003cdiv\u003e\u003cdiv id=\"js_error_message\"\u003e\u003cp\u003e\u003cimg src=\"https://d1ge0kk1l5kms0.cloudfront.net/images/G/01/webservices/console/warning.png\" alt=\"Warning\" /\u003e \u003cstrong\u003eJavascript is disabled or is unavailable in your browser.\u003c/strong\u003e\u003c/p\u003e\u003cp\u003eTo use the Amazon Web Services Documentation, Javascript must be enabled. Please refer to your browser's Help pages for instructions.\u003c/p\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003eDocument ConventionsOPS11-BP04 Perform knowledge managementOPS11-BP06 Validate insights",
  "OPS11-BP06 Validate insights\n    Review your analysis results and responses with cross-functional\n    teams and business owners. Use these reviews to establish common\n    understanding, identify additional impacts, and determine courses of\n    action. Adjust responses as appropriate.\n  \n    Desired outcomes:\n  \n     \n     \n     \n     \n  \n      \n        You review insights with business owners on a regular basis.\n        Business owners provide additional context to newly-gained\n        insights.\n      \n    \n      \n        You review insights and request feedback from technical peers,\n        and you share your learnings across teams.\n      \n    \n      \n        You publish data and insights for other technical and business\n        teams to review. You factor in your learnings to new practices\n        by other departments.\n      \n    \n      \n        Summarize and review new insights with senior leaders. Senior\n        leaders use new insights to define strategy.\n      \n    \n    Common anti-patterns:\n  \n     \n     \n     \n  \n      \n        You release a new feature. This feature changes some of your\n        customer behaviors. Your observability does not take these\n        changes into account. You do not quantify the benefits of these\n        changes.\n      \n    \n      \n        You push a new update and neglect refreshing your CDN. The CDN\n        cache is no longer compatible with the latest release. You\n        measure the percentage of requests with errors. All of your\n        users report HTTP 400 errors when communicating with backend\n        servers. You investigate the client errors and find that because\n        you measured the wrong dimension, your time was wasted.\n      \n    \n      \n        Your service-level agreement stipulates 99.9% uptime, and your\n        recovery point objective is four hours. The service owner maintains\n        that the system is zero downtime. You implement an expensive and\n        complex replication solution, which wastes time and money.\n      \n    \n    Benefits of establishing this best practice:\n     \n  \n     \n     \n     \n  \n      \n        When you validate insights with business owners and\n        subject matter experts, you establish common understanding and more\n        effectively guide improvement.\n      \n    \n      \n        You discover hidden issues and factor them into future\n        decisions.\n      \n    \n      \n        Your focus moves from technical outcomes to business outcomes.\n      \n    \n    Level of risk exposed if this best practice\n      is not established: Medium\n  \n    \n    Implementation guidance\n    \n    \n    \n       \n    \n        \n          Validate insights: Engage with business owners and subject\n          matter experts to ensure there is common understanding and\n          agreement of the meaning of the data you have collected.\n          Identify additional concerns, potential impacts, and determine\n          a courses of action.\n        \n      \n   \n    \n    Resources\n    \n    \n    \n      Related best practices:\n    \n    \n       \n       \n       \n    \n        \n          OPS01-BP06\n            Evaluate tradeoffs while managing benefits and risks\n        \n      \n        \n          OPS02-BP06\n            Responsibilities between teams are predefined or\n            negotiated\n        \n      \n        \n          OPS11-BP03\n            Implement feedback loops\n        \n      \n    \n      Related documents:\n    \n    \n       \n    \n        \n          Designing\n            a Cloud Center of Excellence (CCOE)\n        \n      \n    \n      Related videos:\n    \n    \n       \n    \n        \n          Building\n            observability to increase resiliency\n        \n      \n  \u003cdiv\u003e\u003cdiv\u003e\u003cdiv\u003e\u003cdiv id=\"js_error_message\"\u003e\u003cp\u003e\u003cimg src=\"https://d1ge0kk1l5kms0.cloudfront.net/images/G/01/webservices/console/warning.png\" alt=\"Warning\" /\u003e \u003cstrong\u003eJavascript is disabled or is unavailable in your browser.\u003c/strong\u003e\u003c/p\u003e\u003cp\u003eTo use the Amazon Web Services Documentation, Javascript must be enabled. Please refer to your browser's Help pages for instructions.\u003c/p\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003eDocument ConventionsOPS11-BP05 Define drivers for improvementOPS11-BP07 Perform operations metrics reviews",
  "OPS11-BP07 Perform operations metrics reviews\n    Regularly perform retrospective analysis of operations metrics with\n    cross-team participants from different areas of the business. Use\n    these reviews to identify opportunities for improvement, potential\n    courses of action, and to share lessons learned. Look for\n    opportunities to improve in all of your environments (for example,\n    development, test, and production).\n  \n    Desired outcome:\n  \n     \n     \n     \n  \n      \n        You frequently review business-affecting metrics\n      \n    \n      \n        You detect and review anomalies through your observability\n        capabilities\n      \n    \n      \n        You use data to support business outcomes and goals\n      \n    \n    Common anti-patterns:\n  \n     \n     \n     \n  \n      \n        Your maintenance window interrupts a significant retail\n        promotion. The business remains unaware that there is a standard\n        maintenance window that could be delayed if there are other\n        business impacting events.\n      \n    \n      \n        You suffered an extended outage because you commonly use an\n        outdated library in your organization. You have since migrated\n        to a supported library. The other teams in your organization do\n        not know that they are at risk.\n      \n    \n      \n        You do not regularly review attainment of customer SLAs. You are\n        trending to not meet your customer SLAs. There are financial\n        penalties related to not meeting your customer SLAs.\n      \n    \n    Benefits of establishing this best\n      practice:\n  \n     \n     \n     \n  \n      \n        When you meet regularly to review operations metrics, events,\n        and incidents, you maintain common understanding across teams.\n      \n    \n      \n        Your team meets routinely to review metrics and incidents, which\n        positions you to take action on risks and recognize customer\n        SLAs.\n      \n    \n      \n        You share lessons learned, which provides data for\n        prioritization and targeted improvements for business outcomes.\n      \n    \n    Level of risk exposed if this best practice\n      is not established: Medium\n  \n    \n    Implementation guidance\n    \n    \n    \n       \n       \n       \n    \n        \n          Regularly perform retrospective analysis of operations metrics\n          with cross-team participants from different areas of the\n          business.\n        \n      \n        \n          Engage stakeholders, including the business, development, and\n          operations teams, to validate your findings from immediate\n          feedback and retrospective analysis and share lessons learned.\n        \n      \n        \n          Use their insights to identify opportunities for improvement\n          and potential courses of action.\n        \n      \n   \n    \n    Resources\n    \n    \n    \n      Related best practices:\n    \n    \n       \n       \n       \n    \n        \n          OPS08-BP05\n            Create dashboards\n        \n      \n        \n          OPS09-BP03\n            Review operations metrics and prioritize improvement\n        \n      \n        \n          OPS10-BP01\n            Use a process for event, incident, and problem\n            management\n        \n      \n    \n      Related documents:\n    \n    \n       \n       \n       \n       \n       \n    \n        \n          Amazon CloudWatch\n        \n      \n        \n          Amazon CloudWatch metrics and dimensions reference\n        \n      \n        \n          Publish\n            custom metrics\n        \n      \n        \n          Using\n            Amazon CloudWatch metrics\n        \n      \n        \n          Dashboards\n            and visualizations with CloudWatch\n        \n      \n  \u003cdiv\u003e\u003cdiv\u003e\u003cdiv\u003e\u003cdiv id=\"js_error_message\"\u003e\u003cp\u003e\u003cimg src=\"https://d1ge0kk1l5kms0.cloudfront.net/images/G/01/webservices/console/warning.png\" alt=\"Warning\" /\u003e \u003cstrong\u003eJavascript is disabled or is unavailable in your browser.\u003c/strong\u003e\u003c/p\u003e\u003cp\u003eTo use the Amazon Web Services Documentation, Javascript must be enabled. Please refer to your browser's Help pages for instructions.\u003c/p\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003eDocument ConventionsOPS11-BP06 Validate insightsOPS11-BP08 Document and share lessons learned",
  "OPS11-BP08 Document and share lessons learned\n    Document and share lessons learned from the operations activities so\n    that you can use them internally and across teams. You should share\n    what your teams learn to increase the benefit across your\n    organization. Share information and resources to prevent avoidable\n    errors and ease development efforts, and focus on delivery of\n    desired features.\n  \n    Use AWS Identity and Access Management (IAM) to define permissions\n    that permit controlled access to the resources you wish to share\n    within and across accounts.\n  \n    Desired outcome:\n  \n     \n     \n     \n  \n      \n        You use version-controlled repositories to share application\n        libraries, scripted procedures, procedure documentation, and\n        other system documentation.\n      \n    \n      \n        You share your infrastructure standards as version-controlled\n        AWS CloudFormation templates.\n      \n    \n      \n        You review lessons learned across teams.\n      \n    \n    Common anti-patterns:\n  \n     \n     \n     \n  \n      \n        You suffered an extended outage because your organization\n        commonly uses buggy library. You have since migrated to a\n        reliable library. The other teams in your organization do not\n        know they are at risk. No one documents and shares the\n        experience with this library, and they are not aware of the\n        risk.\n      \n    \n      \n        You have identified an edge case in an internally-shared\n        microservice that causes sessions to drop. You have updated your\n        calls to the service to avoid this edge case. The other teams in\n        your organization do not know that they are at risk.\n      \n    \n      \n        You have found a way to significantly reduce the CPU utilization\n        requirements for one of your microservices. You do not know if\n        any other teams could take advantage of this technique.\n      \n    \n    Benefits of establishing this best\n      practice: Share lessons learned to support improvement\n    and to maximize the benefits of experience.\n  \n    Level of risk exposed if this best practice\n      is not established: Low\n  \n    \n    Implementation guidance\n    \n    \n    \n       \n       \n    \n        \n          Document and share lessons learned: Have procedures to\n          document the lessons learned from the running of operations\n          activities and retrospective analysis so that they can be used\n          by other teams.\n        \n      \n        \n          Share learnings: Have procedures to share lessons learned and\n          associated artifacts across teams. For example, share updated\n          procedures, guidance, governance, and best practices through\n          an accessible wiki. Share scripts, code, and libraries through\n          a common repository.\n        \n        \n           \n          \n        \n            \n              Leverage AWS re:Post Private as a knowledge service to streamline collaboration and knowledge sharing within your organization.\n            \n          \n      \n   \n    \n    Resources\n    \n    \n    \n      Related best practices:\n    \n    \n       \n       \n       \n       \n       \n    \n        \n          OPS02-BP06\n            Responsibilities between teams are predefined or\n            negotiated\n        \n      \n        \n          OPS05-BP01\n            Use version control\n        \n      \n        \n          OPS05-BP06\n            Share design standards\n        \n      \n        \n          OPS11-BP03\n            Implement feedback loops\n        \n      \n        \n          OPS11-BP07\n            Perform operations metrics reviews\n        \n      \n    \n      Related documents:\n    \n    \n       \n       \n    \n        \n          Increase collaboration and securely share cloud knowledge with AWS re:Post Private\n        \n      \n        \n          Reduce project delays with a docs-as-code solution\n        \n      \n    \n      Related videos:\n    \n    \n       \n       \n    \n        AWS re:Invent 2023 - Collaborate within your company and with AWS using AWS re:Post Private\n        \n      \n        \n          Supports You | Exploring the Incident Management Tabletop\n            Exercise\n        \n      \n  \u003cdiv\u003e\u003cdiv\u003e\u003cdiv\u003e\u003cdiv id=\"js_error_message\"\u003e\u003cp\u003e\u003cimg src=\"https://d1ge0kk1l5kms0.cloudfront.net/images/G/01/webservices/console/warning.png\" alt=\"Warning\" /\u003e \u003cstrong\u003eJavascript is disabled or is unavailable in your browser.\u003c/strong\u003e\u003c/p\u003e\u003cp\u003eTo use the Amazon Web Services Documentation, Javascript must be enabled. Please refer to your browser's Help pages for instructions.\u003c/p\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003eDocument ConventionsOPS11-BP07 Perform operations metrics reviewsOPS11-BP09 Allocate time to make improvements",
  "OPS11-BP09 Allocate time to make improvements\n    Dedicate time and resources within your processes to make continuous\n    incremental improvements possible.\n  \n    Desired outcome:\n  \n     \n     \n     \n  \n      \n        You create temporary duplicates of environments, which lowers\n        the risk, effort, and cost of experimentation and testing.\n      \n    \n      \n        These duplicated environments can be used to test the\n        conclusions from your analysis, experiment, and develop and test\n        planned improvements.\n      \n    \n      \n        You run gamedays, and you use Fault Injection Service (FIS) to\n        provide the controls and guardrails that teams need to run\n        experiments in a production-like environment.\n      \n    \n    Common anti-patterns:\n  \n     \n     \n     \n  \n      \n        There is a known performance issue in your application server.\n        It is added to the backlog behind every planned feature\n        implementation. If the rate of planned features being added\n        remains constant, the performance issue would never be\n        addressed.\n      \n    \n      \n        To support continual improvement, you approve administrators and\n        developers using all their extra time to select and implement\n        improvements. No improvements are ever completed.\n      \n    \n      \n        Operational acceptance is complete, and you do not test\n        operational practices again.\n      \n    \n    Benefits of establishing this best\n      practice: By dedicating time and resources within your\n    processes, you can make continuous, incremental improvements\n    possible.\n  \n    Level of risk exposed if this best practice\n      is not established: Low\n  \n    \n    Implementation guidance\n    \n    \n    \n       \n       \n       \n       \n    \n        \n          Allocate time to make improvements: Dedicate time and\n          resources within your processes to make continuous,\n          incremental improvements.\n        \n      \n        \n          Implement changes to improve and evaluate the results to\n          determine success.\n        \n      \n        \n          If the results do not satisfy the goals and the improvement is\n          still a priority, pursue alternative courses of action.\n        \n      \n        \n          Simulate production workloads through game days, and use\n          learnings from these simulations to improve.\n        \n      \n   \n    \n    Resources\n    \n    \n    \n      Related best practices:\n    \n    \n       \n    \n        \n          OPS05-BP08\n            Use multiple environments\n        \n      \n    \n      Related videos:\n    \n    \n       \n    \n        \n          AWS           re:Invent 2023 - Improve application resilience with AWS Fault\n            Injection Service\n        \n      \n  \u003cdiv\u003e\u003cdiv\u003e\u003cdiv\u003e\u003cdiv id=\"js_error_message\"\u003e\u003cp\u003e\u003cimg src=\"https://d1ge0kk1l5kms0.cloudfront.net/images/G/01/webservices/console/warning.png\" alt=\"Warning\" /\u003e \u003cstrong\u003eJavascript is disabled or is unavailable in your browser.\u003c/strong\u003e\u003c/p\u003e\u003cp\u003eTo use the Amazon Web Services Documentation, Javascript must be enabled. Please refer to your browser's Help pages for instructions.\u003c/p\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003eDocument ConventionsOPS11-BP08 Document and share lessons learnedSecurity",
  "SEC01-BP01 Separate workloads using accounts\n      Establish common guardrails and isolation between environments\n      (such as production, development, and test) and workloads through\n      a multi-account strategy. Account-level separation is strongly\n      recommended, as it provides a strong isolation boundary for\n      security, billing, and access.\n    Desired outcome: An account structure that isolates cloud\n    operations, unrelated workloads, and environments into separate accounts, increasing security\n    across the cloud infrastructure.Common anti-patterns:\n     \n     \n  \n       Placing multiple unrelated workloads with different data sensitivity levels into the\n        same account.\n    \n       Poorly defined organizational unit (OU) structure.\n    Benefits of establishing this best practice:\n     \n     \n     \n     \n     \n  \n       Decreased scope of impact if a workload is inadvertently accessed.\n    \n       Central governance of access to AWS services, resources, and Regions.\n    \n       Maintain security of the cloud infrastructure with policies and centralized\n        administration of security services.\n    \n       Automated account creation and maintenance process.\n    \n       Centralized auditing of your infrastructure for compliance and regulatory\n        requirements.\n    \n    Level of risk exposed if this best practice is not established:\n    High \n    \n    Implementation guidance\n    \n    \n    \n    \n      AWS accounts provide a security isolation boundary between\n      workloads or resources that operate at different sensitivity\n      levels. AWS provides tools to manage your cloud workloads at\n      scale through a multi-account strategy to leverage this\n      isolation boundary. For guidance on the concepts, patterns,\n      and implementation of a multi-account strategy on AWS, see\n      Organizing\n        Your AWS Environment Using Multiple Accounts.\n    \n    \n      When you have multiple AWS accounts under central management,\n      your accounts should be organized into a hierarchy defined by\n      layers of organizational units (OUs). Security controls can\n      then be organized and applied to the OUs and member accounts,\n      establishing consistent preventative controls on member\n      accounts in the organization. The security controls are\n      inherited, allowing you to filter permissions available to\n      member accounts located at lower levels of an OU hierarchy. A\n      good design takes advantage of this inheritance to reduce the\n      number and complexity of security policies required to achieve\n      the desired security controls for each member account.\n    \n    \n      AWS Organizations and\n      AWS Control Tower are two services that you can use to\n      implement and manage this multi-account structure in your AWS\n      environment. AWS Organizations allows you to organize accounts\n      into a hierarchy defined by one or more layers of OUs, with\n      each OU containing a number of member accounts.\n      Service\n        control policies (SCPs) allow the organization\n      administrator to establish granular preventative controls on\n      member accounts, and\n      AWS Config can be used to establish proactive and detective\n      controls on member accounts. Many AWS services\n      integrate\n        with AWS Organizations to provide delegated\n      administrative controls and performing service-specific tasks\n      across all member accounts in the organization.\n    \n    \n      Layered on top of AWS Organizations,\n      AWS Control Tower provides a one-click best practices setup\n      for a multi-account AWS environment with a\n      landing\n        zone. The landing zone is the entry point to the\n      multi-account environment established by Control Tower.\n      Control Tower provides several\n      benefits\n      over AWS Organizations. Three benefits that provide improved\n      account governance are:\n    \n    \n    \n       \n       \n       \n    \n         Integrated mandatory security controls that are automatically applied to accounts\n          admitted into the organization. \n      \n         Optional controls that can be turned on or off for a given set of OUs. \n      \n        \n          AWS Control Tower Account Factory provides automated\n          deployment of accounts containing pre-approved baselines and\n          configuration options inside your organization.\n        \n      \n    \n      Implementation steps\n    \n    \n       \n       \n       \n       \n       \n    \n        \n          Design an organizational unit\n            structure: A properly designed organizational\n          unit structure reduces the management burden required to\n          create and maintain service control policies and other\n          security controls. Your organizational unit structure should\n          be\n          aligned\n            with your business needs, data sensitivity, and workload\n            structure.\n        \n      \n        \n          Create a landing zone for your\n            multi-account environment: A landing zone\n          provides a consistent security and infrastructure foundation\n          from which your organization can quickly develop, launch,\n          and deploy workloads. You can use a\n          custom-built\n            landing zone or AWS Control Tower to orchestrate your\n          environment.\n        \n      \n        \n          Establish guardrails:\n          Implement consistent security guardrails for your\n          environment through your landing zone. AWS Control Tower\n          provides a list of\n          mandatory\n          and\n          optional\n          controls that can be deployed. Mandatory controls are\n          automatically deployed when implementing Control Tower.\n          Review the list of highly recommended and optional controls,\n          and implement controls that are appropriate to your needs.\n        \n      \n        \n          Restrict access to newly added\n            Regions: For new AWS Regions, IAM resources such\n          as users and roles are only propagated to the Regions that\n          you specify. This action can be performed through the\n          console\n            when using Control Tower, or by adjusting\n          IAM\n            permission policies in AWS Organizations.\n        \n      \n        \n          Consider AWS\n            CloudFormation\n              StackSets: StackSets help you deploy\n          resources including IAM policies, roles, and groups into\n          different AWS accounts and Regions from an approved\n          template.\n        \n      \n   \n    \n    Resources\n    \n    \n    \n    Related best practices:\n    \n    \n    \n       \n    \n        SEC02-BP04 Rely on a centralized identity provider\n      \n    \n    Related documents:\n    \n    \n    \n       \n       \n       \n       \n       \n       \n       \n       \n       \n    \n        \n        \n          AWS Control Tower\n        \n        \n      \n        \n        \n          AWS               Security Audit Guidelines\n        \n        \n      \n        \n        \n          IAM\n            Best Practices\n        \n        \n      \n        \n        \n          Use\n            CloudFormation StackSets to provision resources across\n            multiple AWS accounts and regions\n        \n        \n      \n        \n        \n          Organizations\n            FAQ\n        \n        \n      \n        \n        \n          AWS Organizations terminology and concepts\n        \n        \n      \n        \n        \n          Best\n            Practices for Service Control Policies in an AWS Organizations Multi-Account Environment\n        \n        \n      \n        \n        \n          AWS               Account Management Reference Guide\n        \n        \n      \n        \n        \n          Organizing\n            Your AWS Environment Using Multiple Accounts\n        \n        \n      \n    \n    Related videos:\n    \n    \n    \n       \n       \n       \n       \n    \n        \n          Enable AWS\n            adoption at scale with automation and governance\n        \n      \n        \n          Security\n            Best Practices the Well-Architected Way\n        \n      \n        \n          Building\n            and Governing Multiple Accounts using AWS Control Tower\n        \n      \n        \n          Enable Control Tower for Existing\n            Organizations\n        \n      \n  \u003cdiv\u003e\u003cdiv\u003e\u003cdiv\u003e\u003cdiv id=\"js_error_message\"\u003e\u003cp\u003e\u003cimg src=\"https://d1ge0kk1l5kms0.cloudfront.net/images/G/01/webservices/console/warning.png\" alt=\"Warning\" /\u003e \u003cstrong\u003eJavascript is disabled or is unavailable in your browser.\u003c/strong\u003e\u003c/p\u003e\u003cp\u003eTo use the Amazon Web Services Documentation, Javascript must be enabled. Please refer to your browser's Help pages for instructions.\u003c/p\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003eDocument ConventionsSEC 1. How do you securely operate your workload? SEC01-BP02 Secure account root user and properties",
  "SEC01-BP02 Secure account root user and properties\n    The root user is the most privileged user in an AWS account, with\n    full administrative access to all resources within the account, and\n    in some cases cannot be constrained by security policies. Deactivating\n    programmatic access to the root user, establishing appropriate\n    controls for the root user, and avoiding routine use of the root\n    user helps reduce the risk of inadvertent exposure of the root\n    credentials and subsequent compromise of the cloud environment.\n  Desired outcome: Securing the root user helps reduce the\n    chance that accidental or intentional damage can occur through the misuse of root user\n    credentials. Establishing detective controls can also alert the appropriate personnel when\n    actions are taken using the root user.Common anti-patterns:\n     \n     \n     \n     \n  \n       Using the root user for tasks other than the few that require root user credentials. \n      \n    \n       Neglecting to test contingency plans on a regular basis to verify the functioning of\n        critical infrastructure, processes, and personnel during an emergency. \n    \n       Only considering the typical account login flow and neglecting to consider or test\n        alternate account recovery methods. \n    \n       Not handling DNS, email servers, and telephone providers as part of the critical\n        security perimeter, as these are used in the account recovery flow. \n    \n    Benefits of establishing this best practice: Securing access to\n    the root user builds confidence that actions in your account are controlled and audited. \n    Level of risk exposed if this best practice is not established:\n    High \n\n  Implementation guidance\n\n      \n    \n      AWS offers many tools to help secure your account. However,\n      because some of these measures are not turned on by default, you\n      must take direct action to implement them. Consider these\n      recommendations as foundational steps to securing your AWS account. As you implement these steps it’s important that you\n      build a process to continuously assess and monitor the security\n      controls.\n    \n    \n      When you first create an AWS account, you begin with one identity\n      that has complete access to all AWS services and resources in the\n      account. This identity is called the AWS account root user. You\n      can sign in as the root user using the email address and password\n      that you used to create the account. Due to the elevated access\n      granted to the AWS root user, you must limit use of the AWS root\n      user to perform tasks that\n      specifically\n      require it. The root user login credentials must be closely\n      guarded, and multi-factor authentication (MFA) should always be\n      used for the AWS account root user.\n    \n    \n      In addition to the normal authentication flow to log into your\n      root user using a username, password, and multi-factor\n      authentication (MFA) device, there are account recovery flows to\n      log into your AWS account root user given access to the email\n      address and phone number associated with your account. Therefore,\n      it is equally important to secure the root user email account\n      where the recovery email is sent and the phone number associated\n      with the account. Also consider potential circular dependencies\n      where the email address associated with the root user is hosted on\n      email servers or domain name service (DNS) resources from the same\n      AWS account.\n    \n    \n      When using AWS Organizations, there are multiple AWS accounts each\n      of which have a root user. One account is designated as the\n      management account and several layers of member accounts can then\n      be added underneath the management account. Prioritize securing\n      your management account’s root user, then address your member\n      account root users. The strategy for securing your management\n      account’s root user can differ from your member account root\n      users, and you can place preventative security controls on your\n      member account root users.\n    \n    \n      Implementation steps\n    \n       The following implementation steps are recommended to establish controls for the root\n        user. Where applicable, recommendations are cross-referenced to CIS AWS\n          Foundations benchmark version 1.4.0. In addition to these steps, consult AWS best\n          practice guidelines for securing your AWS account and resources. \n      \n        Preventative controls\n      \n      \n         \n         \n         \n         \n         \n         \n      \n           Set up accurate contact\n              information for the account. \n          \n             \n             \n             \n          \n               This information is used for the lost password recovery flow, lost MFA device\n                account recovery flow, and for critical security-related communications with your\n                team. \n            \n               Use an email address hosted by your corporate domain, preferably a distribution\n                list, as the root user’s email address. Using a distribution list rather than an\n                individual’s email account provides additional redundancy and continuity for access\n                to the root account over long periods of time. \n            \n               The phone number listed on the contact information should be a dedicated,\n                secure phone for this purpose. The phone number should not be listed or shared with\n                anyone. \n            \n        \n           Do not create access keys for the root user. If access keys exist, remove them (CIS\n            1.4). \n          \n             \n             \n          \n               Eliminate any long-lived programmatic credentials (access and secret keys) for\n                the root user. \n            \n               If root user access keys already exist, you should transition processes using\n                those keys to use temporary access keys from an AWS Identity and Access Management (IAM) role, then delete the root user access keys. \n            \n        \n           Determine if you need to store credentials for the root user. \n          \n             \n             \n          \n               If you are using AWS Organizations to create new member accounts, the initial password\n                for the root user on new member accounts is set to a random value that is not\n                exposed to you. Consider using the password reset flow from your AWS Organization\n                management account to gain access to the member account if needed. \n            \n               For standalone AWS accounts or the management AWS Organization account,\n                consider creating and securely storing credentials for the root user. Use MFA for\n                the root user. \n            \n        \n           Use preventative controls for member account root users in AWS multi-account\n            environments. \n          \n             \n             \n          \n               Consider using the Disallow Creation of Root Access Keys for the Root User preventative\n                guard rail for member accounts. \n            \n               Consider using the Disallow Actions as a Root User preventative guard rail for member\n                accounts. \n            \n        \n           If you need credentials for the root user: \n          \n             \n             \n             \n             \n             \n          \n               Use a complex password. \n            \n               Turn on multi-factor authentication (MFA) for the root user, especially for\n                AWS Organizations management (payer) accounts (CIS 1.5). \n            \n               Consider hardware MFA devices for resiliency and security, as single use\n                devices can reduce the chances that the devices containing your MFA codes might be\n                reused for other purposes. Verify that hardware MFA devices powered by a battery are\n                replaced regularly. (CIS 1.6) \n              \n                 \n              \n                   To configure MFA for the root user, follow the instructions for creating\n                    either a virtual MFA or hardware MFA device. \n                \n            \n               Consider enrolling multiple MFA devices for backup. Up to 8 MFA devices\n                  are allowed per account. \n              \n                 \n              \n                   Note that enrolling more than one MFA device for the root user\n                    automatically turns off the flow for recovering\n                      your account if the MFA device is lost. \n                \n            \n               Store the password securely, and consider circular dependencies if storing the\n                password electronically. Don’t store the password in such a way that would require\n                access to the same AWS account to obtain it. \n            \n        \n           Optional: Consider establishing a periodic password rotation schedule for the root\n            user. \n          \n             \n             \n          \n               Credential management best practices depend on your regulatory and policy\n                requirements. Root users protected by MFA are not reliant on the password as a\n                single factor of authentication. \n            \n              \n                Changing\n                  the root user password on a periodic basis reduces the risk that an\n                inadvertently exposed password can be misused. \n            \n        \n    \n      Detective controls\n    \n      \n         \n         \n      \n           Create alarms to detect use of the root credentials (CIS 1.7). Amazon GuardDuty can monitor and alert on root user API credential usage through the\n              RootCredentialUsage finding. \n        \n           Evaluate and implement the detective controls included in the AWS Well-Architected Security Pillar conformance pack for AWS Config, or if\n            using AWS Control Tower, the strongly\n              recommended controls available inside Control Tower. \n        \n    \n      Operational guidance\n    \n      \n         \n         \n         \n         \n         \n      \n           Determine who in the organization should have access to the root user credentials. \n          \n             \n             \n          \n               Use a two-person rule so that no one individual has access to all necessary\n                credentials and MFA to obtain root user access. \n            \n               Verify that the organization, and not a single individual, maintains control\n                over the phone number and email alias associated with the account (which are used\n                for password reset and MFA reset flow). \n            \n        \n           Use root user only by exception (CIS 1.7). \n          \n             \n          \n               The AWS root user must not be used for everyday tasks, even administrative\n                ones. Only log in as the root user to perform AWS tasks that require\n                  root user. All other actions should be performed by other users assuming\n                appropriate roles. \n            \n        \n           Periodically check that access to the root user is functioning so that procedures\n            are tested prior to an emergency situation requiring the use of the root user\n            credentials. \n        \n           Periodically check that the email address associated with the account and those\n            listed under Alternate\n              Contacts work. Monitor these email inboxes for security notifications you\n            might receive from \u003cabuse@amazon.com\u003e. Also ensure any phone numbers\n            associated with the account are working. \n        \n           Prepare incident response procedures to respond to root account misuse. Refer to\n            the AWS Security Incident Response Guide and the best practices in\n             the Incident Response section of the Security Pillar whitepaper\n            for more information on building an incident response strategy for your AWS account. \n        \n   \n\n  Resources\n\n      \n    Related best practices:\n    \n    \n       \n       \n       \n       \n       \n    \n        SEC01-BP01 Separate workloads using accounts\n      \n        SEC02-BP01 Use strong sign-in mechanisms\n      \n        SEC03-BP02 Grant least privilege access\n      \n        SEC03-BP03 Establish emergency access\n      process\n      \n        SEC10-BP05 Pre-provision access\n      \n    \n    \n    Related documents:\n    \n    \n       \n       \n       \n       \n       \n       \n       \n       \n       \n    \n        \n          AWS Control Tower\n        \n      \n        \n          AWS           Security Audit Guidelines\n        \n      \n        \n          IAM\n          Best Practices\n        \n      \n        \n          Amazon GuardDuty – root credential usage alert\n        \n      \n        \n          Step-by-step\n          guidance on monitoring for root credential use through\n          CloudTrail\n        \n      \n        \n          MFA\n          tokens approved for use with AWS\n        \n      \n         Implementing break\n            glass access on AWS \n      \n        \n          Top\n          10 security items to improve in your AWS account\n        \n      \n        \n          What\n          do I do if I notice unauthorized activity in my AWS account?\n        \n      \n    \n    Related videos:\n    \n    \n      \n      \n         \n         \n         \n      \n          \n            Enable AWS\n            adoption at scale with automation and governance\n          \n        \n          \n            Security\n            Best Practices the Well-Architected Way\n          \n        \n          \n          Limiting use of AWS root\n            credentials from AWS re:inforce 2022 – Security best practices with AWS\n          IAM\n        \n  \u003cdiv\u003e\u003cdiv\u003e\u003cdiv\u003e\u003cdiv id=\"js_error_message\"\u003e\u003cp\u003e\u003cimg src=\"https://d1ge0kk1l5kms0.cloudfront.net/images/G/01/webservices/console/warning.png\" alt=\"Warning\" /\u003e \u003cstrong\u003eJavascript is disabled or is unavailable in your browser.\u003c/strong\u003e\u003c/p\u003e\u003cp\u003eTo use the Amazon Web Services Documentation, Javascript must be enabled. Please refer to your browser's Help pages for instructions.\u003c/p\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003eDocument ConventionsSEC01-BP01 Separate workloads using accountsSEC01-BP03 Identify and validate control objectives",
  "SEC01-BP03 Identify and validate control objectives\n    Based on your compliance requirements and risks identified from your threat model, derive and validate the control objectives and controls that you need to apply to your workload. Ongoing validation of control objectives and controls help you measure the effectiveness of risk mitigation.\n  \n    Desired outcome: The security control objectives of your business are well-defined and aligned to your compliance requirements.  Controls are implemented and enforced through automation and policy and are continually evaluated for their effectiveness in achieving your objectives.  Evidence of effectiveness at both a point in time and over a period of time are readily reportable to auditors.\n  \n    Common anti-patterns:\n  \n     \n     \n     \n     \n  \n      \n        Regulatory requirements, market expectations, and industry standards for assurable security are not well-understood for your business\n      \n    \n      \n        Your cybersecurity frameworks and control objectives are misaligned to the requirements of your business\n      \n    \n      \n        The implementation of controls does not strongly align to your control objectives in a measurable way\n      \n    \n      \n        You do not use automation to report on the effectiveness of your controls\n      \n    \n    Level of risk exposed if this best practice\n    is not established: High\n  \n\n  Implementation guidance \n    \n      There are many common cybersecurity frameworks that can form the basis for your security control objectives. Consider the regulatory requirements, market expectations, and industry standards for your business to determine which frameworks best supports your needs. Examples include AICPA SOC 2, HITRUST, PCI-DSS, ISO 27001, and NIST SP 800-53.\n    \n    \n      For the control objectives you identify, understand how AWS services you consume help you to achieve those objectives.  Use AWS Artifact to find documentation and reports aligned to your target frameworks that describe the scope of responsibility covered by AWS and guidance for the remaining scope that is your responsibility. For further service-specific guidance as they align to various framework control statements, see AWS Customer Compliance Guides.\n    \n    \n      As you define the controls that achieve your objectives, codify enforcement using preventative controls, and automate mitigations using detective controls.  Help prevent non-compliant resource configurations and actions across your AWS Organizations using service control policies (SCP). Implement rules in AWS Config to monitor and report on non-compliant resources, then switch rules to an enforcement model once confident in their behavior.  To deploy sets of pre-defined and managed rules that align to your cybersecurity frameworks, evaluate the use of AWS Security Hub standards as your first option. The AWS Foundational Service Best Practices (FSBP) standard and the CIS AWS Foundations Benchmark are good starting points with controls that align to many objectives that are shared across multiple standard frameworks. Where Security Hub does not intrinsically have the control detections desired, it can be complemented using AWS Config conformance packs.  \n    \n    \n      Use APN Partner Bundles recommended by the AWS Global Security and Compliance Acceleration (GSCA) team to get assistance from security advisors, consulting agencies, evidence collection and reporting systems, auditors, and other complementary services when required.\n    \n      \n     \n      \n      Implementation steps\n      \n         \n         \n         \n         \n      \n          \n            Evaluate common cybersecurity frameworks, and align your control objectives to the ones chosen.\n          \n        \n          \n            Obtain relevant documentation on guidance and responsibilities for your framework using AWS Artifact.  Understand which parts of compliance fall on the AWS side of the shared responsibility model and which parts are your responsibility.\n          \n        \n          \n            Use SCPs, resource policies, role trust policies, and other guardrails to prevent non-compliant resource configurations and actions.\n          \n        \n          \n            Evaluate deploying Security Hub standards and AWS Config conformance packs that align to your control objectives.\n          \n        \n     \n   \n    \n    Resources\n    \n    \n    \n      Related best practices:\n    \n    \n       \n       \n       \n       \n       \n       \n       \n    \n        \n          SEC03-BP01\n            Define access requirements\n        \n      \n        \n          SEC04-BP01 Configure\n            service and application logging\n        \n      \n        \n          SEC07-BP01\n            Understand your data classification scheme\n        \n      \n        \n          OPS01-BP03\n            Evaluate governance requirements\n        \n      \n        \n          OPS01-BP04\n            Evaluate compliance requirements\n        \n      \n        \n          PERF01-BP05\n            Use policies and reference architectures\n        \n      \n        \n          COST02-BP01\n            Develop policies based on your organization\n            requirements\n        \n      \n    \n      Related documents:\n    \n    \n       \n    \n        \n          AWS           Customer Compliance Guides\n        \n      \n    \n      Related tools:\n    \n    \n       \n    \n        \n          AWS Artifact\n        \n      \n  \u003cdiv\u003e\u003cdiv\u003e\u003cdiv\u003e\u003cdiv id=\"js_error_message\"\u003e\u003cp\u003e\u003cimg src=\"https://d1ge0kk1l5kms0.cloudfront.net/images/G/01/webservices/console/warning.png\" alt=\"Warning\" /\u003e \u003cstrong\u003eJavascript is disabled or is unavailable in your browser.\u003c/strong\u003e\u003c/p\u003e\u003cp\u003eTo use the Amazon Web Services Documentation, Javascript must be enabled. Please refer to your browser's Help pages for instructions.\u003c/p\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003eDocument ConventionsSEC01-BP02 Secure account root user and propertiesSEC01-BP04 Stay up to date with security threats and recommendations",
  "SEC01-BP04 Stay up to date with security threats and recommendations\n    Stay up to date with the latest threats and mitigations by\n    monitoring industry threat intelligence publications and data feeds\n    for updates. Evaluate managed service offerings that automatically\n    update based on the latest threat data.\n  \n    Desired outcome: You stay\n    informed as industry publications are updated with the latest\n    threats and recommendations.  You use automation to detect potential\n    vulnerabilities and exposures as you identify new threats. You take\n    mitigating action against these threats.  You adopt AWS services\n    that automatically update with the latest threat intelligence.\n  \n    Common anti-patterns:\n  \n     \n     \n     \n  \n      \n        Not having a reliable and repeatable mechanism to stay informed\n        of the latest threat intelligence.\n      \n    \n      \n        Maintaining manual inventory of your technology portfolio,\n        workloads, and dependencies that require human review for\n        potential vulnerabilities and exposures.\n      \n    \n      \n        Not having mechanisms in place to update your workloads and\n        dependencies to the latest versions available that provide known\n        threat mitigations.\n      \n    \n    Benefits of establishing this best\n      practice: Using threat intelligence sources to stay up to\n    date reduces the risk of missing out on important changes to the\n    threat landscape that can impact your business.  Having automation\n    in place to scan, detect, and remediate where potential\n    vulnerabilities or exposures exist in your workloads and their\n    dependencies can help you mitigate risks quickly and predictably,\n    compared to manual alternatives.  This helps control time and costs\n    related to vulnerability mitigation.\n  \n    Level of risk exposed if this best practice\n      is not established: High\n  \n    \n    Implementation guidance\n    \n    \n    \n      Review trusted threat intelligence publications to stay on top of\n      the threat landscape.  Consult the\n      MITRE\n        ATT\u0026CK knowledge base for documentation on known\n      adversarial tactics, techniques, and procedures (TTPs). Review\n      MITRE's Common\n        Vulnerabilities and Exposures (CVE) list to stay informed\n      on known vulnerabilities in products you rely on. Understand\n      critical risks to web applications with the Open Worldwide\n      Application Security Project (OWASP)'s popular\n      OWASP\n        Top 10 project.\n    \n    \n      Stay up to date on AWS security events and recommended remediation\n      steps with AWS\n      Security\n        Bulletins for CVEs.\n    \n    \n      To reduce your overall effort and overhead of staying up to date,\n      consider using AWS services that automatically incorporate new\n      threat intelligence over time.  For example, Amazon GuardDuty\n      stays up to date with industry threat intelligence for detecting\n      anomalous behaviors and threat signatures within your accounts.\n       Amazon Inspector automatically keeps a database of the CVEs it\n      uses for its continuous scanning features up to date.  Both AWS WAF and AWS Shield Advanced provide managed rule groups that are\n      updated automatically as new threats emerge.\n    \n    \n      Review the\n      Well-Architected\n        operational excellence pillar for automated\n      fleet management and patching.\n    \n   \n    \n    Implementation steps\n    \n    \n    \n       \n       \n       \n    \n        \n          Subscribe to updates for threat intelligence publications that\n          are relevant to your business and industry. Subscribe to the\n          AWS Security Bulletins.\n        \n      \n        \n          Consider adopting services that incorporate new threat\n          intelligence automatically, such as Amazon GuardDuty and\n          Amazon Inspector.\n        \n      \n        \n          Deploy a fleet management and patching strategy that aligns\n          with the best practices of the Well-Architected Operational\n          Excellence Pillar.\n        \n      \n   \n    \n    Resources\n    \n    \n    \n      Related best practices:\n    \n    \n       \n       \n       \n    \n        \n          SEC01-BP07 Identify threats and prioritize mitigations using a threat model\n        \n      \n        \n          OPS01-BP05\n            Evaluate threat landscape\n        \n      \n        \n          OPS11-BP01\n            Have a process for continuous improvement\n        \n      \n  \u003cdiv\u003e\u003cdiv\u003e\u003cdiv\u003e\u003cdiv id=\"js_error_message\"\u003e\u003cp\u003e\u003cimg src=\"https://d1ge0kk1l5kms0.cloudfront.net/images/G/01/webservices/console/warning.png\" alt=\"Warning\" /\u003e \u003cstrong\u003eJavascript is disabled or is unavailable in your browser.\u003c/strong\u003e\u003c/p\u003e\u003cp\u003eTo use the Amazon Web Services Documentation, Javascript must be enabled. Please refer to your browser's Help pages for instructions.\u003c/p\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003eDocument ConventionsSEC01-BP03 Identify and validate control objectivesSEC01-BP05 Reduce security management scope",
  "SEC01-BP05 Reduce security management scope\n    Determine if you can reduce your security scope by using AWS services that shift management of certain controls to AWS (managed services).  These services can help reduce your security maintenance tasks, such as infrastructure provisioning, software setup, patching, or backups.\n  \n    Desired outcome: You consider the scope of your security management when selecting AWS services for your workload.  The cost of management overhead and maintenance tasks (the total cost of ownership, or TCO) is weighed against the cost of the services you select, in addition to other Well-Architected considerations.  You incorporate AWS control and compliance documentation into your control evaluation and verification procedures.\n  \n    Common anti-patterns:\n  \n     \n     \n     \n  \n      \n        Deploying workloads without thoroughly understanding the shared responsibility model for the services you select.\n      \n    \n      \n        Hosting databases and other technologies on virtual machines without having evaluated a managed service equivalent.\n      \n    \n      \n        Not including security management tasks into the total cost of ownership of hosting technologies on virtual machines when compared to managed service options.\n      \n    \n    Benefits of establishing this best practice: Using managed services can reduce your overall burden of managing operational security controls, which can reduce your security risks and total cost of ownership. Time that would otherwise be spent on certain security tasks can be reinvested into tasks that provide more value to your business. Managed services can also reduce the scope of your compliance requirements by shifting some control requirements to AWS.\n  \n    Level of risk exposed if this best practice\n    is not established: Medium\n  \n\n  Implementation guidance\n    \n      There are multiple ways you can integrate the components of your workload on AWS.   Installing and running technologies on Amazon EC2 instances often requires you to take on the largest share of the overall security responsibility.  To help reduce the burden of operating certain controls, identify AWS managed services that reduce the scope of your side of the shared responsibility model and understand how you can use them in your existing architecture. Examples include using the Amazon Relational Database Service (Amazon RDS) for deploying databases, Amazon Elastic Kubernetes Service (Amazon EKS) or Amazon Elastic Container Service (Amazon ECS) for orchestrating containers, or using serverless options.  When building new applications, think through which services can help reduce time and cost when it comes to implementing and managing security controls.\n    \n    \n      Compliance requirements can also be a factor when selecting services. Managed services can shift the compliance of some requirements to AWS. Discuss with your compliance team about their degree of comfort with auditing the aspects of services you operate and manage and accepting control statements in relevant AWS audit reports.  You can provide the audit artifacts found in AWS Artifact to your auditors or regulators as evidence of AWS security controls. You can also use the responsibility guidance provided by some of the AWS audit artifacts to design your architecture, along with the AWS Customer Compliance Guides. This guidance helps determine the additional security controls you should put in place in order to support the specific use cases of your system.\n    \n    \n      When using managed services, be familiar with the process of updating their resources to newer versions (for example, updating the version of a database managed by Amazon RDS, or a programming language runtime for an AWS Lambda function). While the managed service may perform this operation for you, configuring the timing of the update and understanding the impact on your operations remains your responsibility. Tools like AWS Health can help you track and manage these updates throughout your environments. \n    \n     \n      \n      Implementation steps\n      \n         \n         \n         \n         \n      \n          \n            Evaluate the components of your workload that can be replaced with a managed service.\n          \n          \n             \n          \n              \n                If you are migrating a workload to AWS, consider the reduced management (time and expense) and reduction of risk when you assess if you should rehost, refactor, replatform, rebuild, or replace your workload. Sometimes additional investment at the start of a migration can have significant savings in the long run.\n              \n            \n        \n          \n            Consider implementing managed services, like Amazon RDS, instead of installing and managing your own technology deployments.  \n          \n        \n          \n            Use the responsibility guidance in AWS Artifact to help determine the security controls you should put in place for your workload.\n          \n        \n          \n            Keep an inventory of resources in use, and stay up-to-date with new services and approaches to identify new opportunities to reduce scope.\n          \n        \n     \n   \n    \n    Resources\n    \n    \n    \n      Related best practices:\n    \n    \n       \n       \n       \n    \n        \n          PERF02-BP01\n            Select the best compute options for your workload\n        \n      \n        \n          PERF03-BP01\n            Use a purpose-built data store that best supports your data\n            access and storage requirements\n        \n      \n        \n          SUS05-BP03\n            Use managed services\n        \n      \n    \n      Related documents:\n    \n    \n       \n    \n        \n          Planned\n            lifecycle events for AWS Health\n        \n      \n    \n      Related tools:\n    \n    \n       \n       \n       \n    \n        \n          AWS Health\n        \n      \n        \n          AWS Artifact\n        \n      \n        \n          AWS           Customer Compliance Guides\n        \n      \n    \n      Related videos:\n    \n    \n       \n       \n    \n        \n          How\n            do I migrate to an Amazon RDS or Aurora MySQL DB instance\n            using AWS DMS?\n        \n      \n        \n          AWS           re:Invent 2023 - Manage resource lifecycle events at scale\n            with AWS Health\n        \n      \n  \u003cdiv\u003e\u003cdiv\u003e\u003cdiv\u003e\u003cdiv id=\"js_error_message\"\u003e\u003cp\u003e\u003cimg src=\"https://d1ge0kk1l5kms0.cloudfront.net/images/G/01/webservices/console/warning.png\" alt=\"Warning\" /\u003e \u003cstrong\u003eJavascript is disabled or is unavailable in your browser.\u003c/strong\u003e\u003c/p\u003e\u003cp\u003eTo use the Amazon Web Services Documentation, Javascript must be enabled. Please refer to your browser's Help pages for instructions.\u003c/p\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003eDocument ConventionsSEC01-BP04 Stay up to date with security threats and recommendationsSEC01-BP06 Automate deployment of standard security controls",
  "SEC01-BP06 Automate deployment of standard security controls\n    Apply modern DevOps practices as you develop and deploy security\n    controls that are standard across your AWS environments.  Define\n    standard security controls and configurations using Infrastructure\n    as Code (IaC) templates, capture changes in a version control\n    system, test changes as part of a CI/CD pipeline, and automate the\n    deployment of changes to your AWS environments.\n  \n    Desired outcome: IaC templates\n    capture standardized security controls and commit them to a version\n    control system.  CI/CD pipelines are in places that detect changes\n    and automate testing and deploying your AWS environments.\n     Guardrails are in place to detect and alert on misconfigurations in\n    templates before proceeding to deployment.  Workloads are deployed\n    into environments where standard controls are in place.  Teams have\n    access to deploy approved service configurations through a\n    self-service mechanism.  Secure backup and recovery strategies are\n    in place for control configurations, scripts, and related data.\n  \n    Common anti-patterns:\n  \n     \n     \n     \n     \n  \n      \n        Making changes to your standard security controls manually,\n        through a web console or command-line interface.\n      \n    \n      \n        Relying on individual workload teams to manually implement the\n        controls a central team defines.\n      \n    \n      \n        Relying on a central security team to deploy workload-level\n        controls at the request of a workload team.\n      \n    \n      \n        Allowing the same individuals or teams to develop, test, and\n        deploy security control automation scripts without proper\n        separation of duties or checks and balances. \n      \n    \n    Benefits of establishing this best\n      practice: Using templates to define your standard\n    security controls allows you to track and compare changes over time\n    using a version control system.  Using automation to test and deploy\n    changes creates standardization and predictability, increasing the\n    chances of a successful deployment and reducing manual repetitive\n    tasks.  Providing a self-serve mechanism for workload teams to\n    deploy approved services and configurations reduces the risk of\n    misconfiguration and misuse. This also helps them to incorporate\n    controls earlier in the development process.\n  \n    Level of risk exposed if this best practice\n      is not established: Medium\n  \n    \n    Implementation guidance\n    \n    \n    \n      When following the practices described in\n      SEC01-BP01 Separate workloads\n        using accounts, you will end up with multiple AWS accounts\n      for different environments that you manage using AWS Organizations.  While each of these environments and workloads may\n      need distinct security controls, you can standardize some security\n      controls across your organization.  Examples include integrating\n      centralized identity providers, defining networks and firewalls,\n      and configuring standard locations for storing and analyzing logs.\n       In the same way you can use infrastructure as\n        code (IaC) to apply the same rigor of application code\n      development to infrastructure provisioning, you can use IaC to\n      define and deploy your standard security controls as well.\n    \n    \n      Wherever possible, define your security controls in a declarative\n      way, such as in\n      AWS CloudFormation, and store them in a source control system.  Use DevOps practices to automate the deploying your\n      controls for more predictable releases, automated testing using\n      tools\n      like AWS CloudFormation Guard, and detecting drift between your\n      deployed controls and your desired configuration.  You can use\n      services such as\n      AWS CodePipeline,\n      AWS CodeBuild, and\n      AWS CodeDeploy to construct a CI/CD pipeline. Consider the\n      guidance in\n      Organizing\n        Your AWS Environment Using Multiple Accounts to configure\n      these services in their own accounts that are separate from other\n      deployment pipelines.\n    \n    \n      You can also define templates to standardize defining and\n      deploying AWS accounts, services, and configurations.  This\n      technique allows a central security team to manage these\n      definitions and provide them to workload teams through a\n      self-service approach.  One way to achieve this is by using\n      Service Catalog, where you can publish templates as\n      products that workload teams can incorporate\n      into their own pipeline deployments.  If you are using\n      AWS Control Tower, some templates and controls are available as\n      a starting point.  Control Tower also provides\n      the Account\n        Factory capability, allowing workload teams to create new\n      AWS accounts using the standards you define.  This capability\n      helps remove dependencies on a central team to approve and create\n      new accounts when they are identified as needed by your workload\n      teams.  You may need these accounts to isolate different workload\n      components based on reasons such as the function they serve, the\n      sensitivity of data being processed, or their behavior.\n    \n   \n    \n    Implementation steps\n    \n    \n    \n       \n       \n       \n       \n    \n        \n          Determine how you will store and maintain your templates in a\n          version control system.\n        \n      \n        \n          Create CI/CD pipelines to test and deploy your templates.\n           Define tests to check for misconfigurations and that\n          templates adhere to your company standards.\n        \n      \n        \n          Build a catalog of standardized templates for workload teams\n          to deploy AWS accounts and services according to your\n          requirements.\n        \n      \n        \n          Implement secure backup and recovery strategies for your\n          control configurations, scripts, and related data.\n        \n      \n   \n    \n    Resources\n    \n    \n    \n      Related best practices:\n    \n    \n       \n       \n       \n       \n    \n        \n          OPS05-BP01\n            Use version control\n        \n      \n        \n          OPS05-BP04\n            Use build and deployment management systems\n        \n      \n        \n          REL08-BP05\n            Deploy changes with automation\n        \n      \n        \n          SUS06-BP01\n            Adopt methods that can rapidly introduce sustainability\n            improvements\n        \n      \n    \n      Related documents:\n    \n    \n       \n    \n        \n          Organizing\n            Your AWS Environment Using Multiple Accounts\n        \n      \n    \n      Related examples:\n    \n    \n       \n       \n    \n        \n          Automate\n            account creation, and resource provisioning using Service Catalog, AWS Organizations, and AWS Lambda\n        \n      \n        \n          Strengthen\n            the DevOps pipeline and protect data with AWS Secrets Manager,\n            AWS KMS, and AWS Certificate Manager\n        \n      \n    \n      Related tools:\n    \n    \n       \n       \n    \n        \n          AWS CloudFormation Guard\n        \n      \n        \n          Landing\n            Zone Accelerator on AWS\n        \n      \n  \u003cdiv\u003e\u003cdiv\u003e\u003cdiv\u003e\u003cdiv id=\"js_error_message\"\u003e\u003cp\u003e\u003cimg src=\"https://d1ge0kk1l5kms0.cloudfront.net/images/G/01/webservices/console/warning.png\" alt=\"Warning\" /\u003e \u003cstrong\u003eJavascript is disabled or is unavailable in your browser.\u003c/strong\u003e\u003c/p\u003e\u003cp\u003eTo use the Amazon Web Services Documentation, Javascript must be enabled. Please refer to your browser's Help pages for instructions.\u003c/p\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003eDocument ConventionsSEC01-BP05 Reduce security management scopeSEC01-BP07 Identify threats and prioritize mitigations using a threat\n      model",
  "SEC01-BP07 Identify threats and prioritize mitigations using a threat\n  model\n    Perform threat modeling to identify and maintain an up-to-date register of potential threats and associated mitigations for your workload. Prioritize your threats and adapt your security control mitigations to prevent, detect, and respond. Revisit and maintain this in the context of your workload, and the evolving security landscape.\n  \n    Level of risk exposed if this best practice\n    is not established: High\n  \n\n  Implementation guidance\n    \n      What is threat modeling?\n    \n    \n      “Threat modeling works to identify, communicate, and understand threats and mitigations within the context of protecting something of value.” – The Open Web Application Security Project (OWASP) Application Threat Modeling\n    \n    \n      Why should you threat model?\n    \n    \n      Systems are complex, and are becoming increasingly more complex and capable over time, delivering more business value and increased customer satisfaction and engagement. This means that IT design decisions need to account for an ever-increasing number of use cases. This complexity and number of use-case permutations typically makes unstructured approaches ineffective for finding and mitigating threats. Instead, you need a systematic approach to enumerate the potential threats to the system, and to devise mitigations and prioritize these mitigations to make sure that the limited resources of your organization have the maximum impact in improving the overall security posture of the system.\n    \n    \n      Threat modeling is designed to provide this systematic approach, with the aim of finding and addressing issues early in the design process, when the mitigations have a low relative cost and effort compared to later in the lifecycle. This approach aligns with the industry principle of shift-left security. Ultimately, threat modeling integrates with an organization’s risk management process and helps drive decisions on which controls to implement by using a threat driven approach.\n    \n    \n      When should threat modeling be performed?\n    \n    \n      Start threat modeling as early as possible in the lifecycle of your workload, this gives you better flexibility on what to do with the threats you have identified. Much like software bugs, the earlier you identify threats, the more cost effective it is to address them. A threat model is a living document and should continue to evolve as your workloads change. Revisit your threat models over time, including when there is a major change, a change in the threat landscape, or when you adopt a new feature or service. \n    \n     \n      \n      Implementation steps\n    \n    \n      How can we perform threat modeling?\n    \n    \n      There are many different ways to perform threat modeling. Much like programming languages, there are advantages and disadvantages to each, and you should choose the way that works best for you. One approach is to start with Shostack’s 4 Question Frame for Threat Modeling, which poses open-ended questions to provide structure to your threat modeling exercise:\n    \n    \n       \n       \n       \n       \n      \n    \n        \n          What are we working on?\n        \n        \n          The purpose of this question is to help you understand and agree upon the system you are building and the details about that system that are relevant to security. Creating a model or diagram is the most popular way to answer this question, as it helps you to visualize what you are building, for example, using a data flow diagram. Writing down assumptions and important details about your system also helps you define what is in scope. This allows everyone contributing to the threat model to focus on the same thing, and avoid time-consuming detours into out-of-scope topics (including out of date versions of your system). For example, if you are building a web application, it is probably not worth your time threat modeling the operating system trusted boot sequence for browser clients, as you have no ability to affect this with your design.\n        \n      \n        \n          What can go wrong?\n        \n        \n          This is where you identify threats to your system. Threats are accidental or intentional actions or events that have unwanted impacts and could affect the security of your system. Without a clear understanding of what could go wrong, you have no way of doing anything about it.\n        \n        \n          There is no canonical list of what can go wrong. Creating this list requires brainstorming and collaboration between all of the individuals within your team and relevant personas involved in the threat modeling exercise. You can aid your brainstorming by using a model for identifying threats, such as STRIDE, which suggests different categories to evaluate: Spoofing, Tampering, Repudiation, Information Disclosure, Denial of Service, and Elevation of privilege. In addition, you might want to aid the brainstorming by reviewing existing lists and research for inspiration, including the OWASP Top 10, HiTrust Threat Catalog, and your organization’s own threat catalog.\n        \n      \n        \n          What are we going to do about it?\n        \n        \n          As was the case with the previous question, there is no canonical list of all possible mitigations. The inputs into this step are the identified threats, actors, and areas of improvement from the previous step. \n        \n        \n          Security and compliance is a shared responsibility between you and AWS. It’s important to understand that when you ask “What are we going to do about it?”, that you are also asking “Who is responsible for doing something about it?”. Understanding the balance of responsibilities between you and AWS helps you scope your threat modeling exercise to the mitigations that are under your control, which are typically a combination of AWS service configuration options and your own system-specific mitigations. \n        \n        \n          For the AWS portion of the shared responsibility, you will find that AWS services are in-scope of many compliance programs. These programs help you to understand the robust controls in place at AWS to maintain security and compliance of the cloud. The audit reports from these programs are available for download for AWS customers from AWS Artifact.\n        \n        \n          Regardless of which AWS services you are using, there’s always an element of customer responsibility, and mitigations aligned to these responsibilities should be included in your threat model. For security control mitigations for the AWS services themselves, you want to consider implementing security controls across domains, including domains such as identity and access management (authentication and authorization), data protection (at rest and in transit), infrastructure security, logging, and monitoring. The documentation for each AWS service has a dedicated security chapter that provides guidance on the security controls to consider as mitigations. Importantly, consider the code that you are writing and its code dependencies, and think about the controls that you could put in place to address those threats. These controls could be things such as input validation, session handling, and  bounds handling. Often, the majority of vulnerabilities are introduced in custom code, so focus on this area.\n        \n      \n        \n          Did we do a good job?\n        \n        \n          The aim is for your team and organization to improve both the quality of threat models and the velocity at which you are performing threat modeling over time. These improvements come from a combination of practice, learning, teaching, and reviewing. To go deeper and get hands on, it’s recommended that you and your team complete the Threat modeling the right way for builders training course or workshop. In addition, if you are looking for guidance on how to integrate threat modeling into your organization’s application development lifecycle, see How to approach threat modeling post on the AWS Security Blog.\n        \n      \n      \n        Threat Composer\n      \n      \n        To aid and guide you in performing threat modeling, consider using the Threat Composer tool, which aims to your reduce time-to-value when threat modeling. The tool helps you do the following:\n      \n      \n         \n         \n         \n         \n      \n          \n            Write useful threat statements aligned to threat grammar that work in a natural non-linear workflow\n          \n        \n          \n            Generate a human-readable threat model\n          \n        \n          \n            Generate a machine-readable threat model to allow you treat threat models as code\n          \n        \n          \n            Help you to quickly identify areas of quality and coverage improvement using the Insights Dashboard\n          \n        \n      \n        For further reference, visit Threat Composer and switch to the system-defined Example Workspace.\n      \n     \n   \n\n  Resources\n    \n      Related best practices:\n    \n    \n       \n       \n       \n       \n    \n        \n          SEC01-BP03 Identify and validate control objectives\n        \n      \n        \n          SEC01-BP04 Stay up to date with security threats and recommendations\n        \n      \n        \n          SEC01-BP05 Reduce security management scope\n        \n      \n        \n          SEC01-BP08 Evaluate and implement new security services and\n  features regularly \n        \n      \n\n      \n    \n      Related documents:\n    \n    \n       \n       \n    \n        \n          How to approach threat modeling (AWS Security Blog)\n        \n      \n        \n          NIST: Guide to Data-Centric System Threat Modelling\n        \n      \n    \n      Related videos:\n    \n    \n       \n       \n    \n        AWS Summit ANZ 2021 - How to approach threat modelling \n        \n      \n        AWS Summit ANZ 2022 - Scaling security – Optimise for fast and secure delivery\n        \n      \n    \n      Related training:\n    \n    \n       \n       \n    \n        \n          Threat modeling the right way for builders – AWS Skill Builder virtual self-paced training\n        \n      \n        \n          Threat modeling the right way for builders – AWS Workshop\n        \n      \n    \n      Related tools:\n    \n    \n       \n    \n        \n          Threat Composer\n        \n      \n  \u003cdiv\u003e\u003cdiv\u003e\u003cdiv\u003e\u003cdiv id=\"js_error_message\"\u003e\u003cp\u003e\u003cimg src=\"https://d1ge0kk1l5kms0.cloudfront.net/images/G/01/webservices/console/warning.png\" alt=\"Warning\" /\u003e \u003cstrong\u003eJavascript is disabled or is unavailable in your browser.\u003c/strong\u003e\u003c/p\u003e\u003cp\u003eTo use the Amazon Web Services Documentation, Javascript must be enabled. Please refer to your browser's Help pages for instructions.\u003c/p\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003eDocument ConventionsSEC01-BP06 Automate deployment of standard security controlsSEC01-BP08 Evaluate and implement new security services and\n  features regularly ",
  "SEC01-BP08 Evaluate and implement new security services and\n  features regularly \n    Evaluate and implement security services and features from AWS and\n    AWS Partners that help you evolve the security posture of your\n    workload. \n  \n    Desired outcome: You have a\n    standard practice in place that informs you of new features and\n    services released by AWS and AWS Partners. You evaluate how these\n    new capabilities influence the design of current and new controls\n    for your environments and workloads.\n  \n    Common anti-patterns:\n  \n     \n     \n     \n  \n      \n        You don't subscribe to AWS blogs and RSS feeds to learn of\n        relevant new features and services quickly\n      \n    \n      \n        You rely on news and updates about security services and\n        features from second-hand sources\n      \n    \n      \n        You don't encourage AWS users in your organization to stay\n        informed on the latest updates\n      \n    \n    Benefits of establishing this best\n      practice: When you stay on top of new security services\n    and features, you can make informed decisions about the\n    implementation of controls in your cloud environments and workloads.\n    These sources help raise awareness of the evolving security\n    landscape and how AWS services can be used to protect against new\n    and emerging threats.  \n  \n    Level of risk exposed if this best practice\n      is not established: Low\n  \n    \n    Implementation guidance\n    \n    \n    \n      AWS informs customers of new security services and features\n      through several channels:\n    \n    \n       \n       \n       \n       \n       \n    \n        \n          AWS What's\n            New\n        \n      \n        \n          AWS News\n            Blog\n        \n      \n        \n          AWS           Security Blog\n        \n      \n        \n          AWS           Security Bulletins\n        \n      \n        \n          AWS           documentation overview\n        \n      \n    \n      You can subscribe to an\n      AWS       Daily Feature Updates topic using Amazon Simple Notification Service (Amazon SNS) for a comprehensive daily\n      summary of updates. Some security services, such as\n      Amazon GuardDuty and\n      AWS Security Hub, provide their own SNS topics to stay informed\n      about new standards, findings, and other updates for those\n      particular services.\n    \n    \n      New services and features are also announced and described in\n      detail during\n      conferences,\n        events, and webinars conducted around the globe each year.\n      Of particular note is the annual\n      AWS       re:Inforce security conference and the more\n      general AWS       re:Invent conference. The previously-mentioned AWS news\n      channels share these conference announcements about security and\n      other services, and you can view deep dive educational breakout\n      sessions online at the\n      AWS       Events channel on YouTube.\n    \n    \n      You can also ask your\n      AWS account team about the latest security service updates and\n      recommendations. You can reach out to your team through the\n      Sales\n        Support form if you do not have their direct contact\n      information. Similarly, if you subscribed to\n      AWS       Enterprise Support, you will receive weekly updates from\n      your Technical Account Manager (TAM) and can schedule a regular\n      review meeting with them.\n    \n     \n      \n      Implementation steps\n      \n      \n      \n         \n         \n         \n         \n      \n          \n            Subscribe to the various blogs and bulletins with your\n            favorite RSS reader or to the Daily Features Updates SNS\n            topic.\n          \n        \n          \n            Evaluate which AWS events to attend to learn first-hand\n            about new features and services.\n          \n        \n          \n            Set up meetings with your AWS account team for any questions\n            about updating security services and features.\n          \n        \n          \n            Consider subscribing to Enterprise Support to have regular\n            consultations with a Technical Account Manager (TAM).\n          \n        \n     \n   \n    \n    Resources\n    \n    \n    \n      Related best practices:\n    \n    \n       \n       \n    \n        \n          PERF01-BP01\n            Learn about and understand available cloud services and\n            features\n        \n      \n        \n          COST01-BP07\n            Keep up-to-date with new service releases\n        \n      \n  \u003cdiv\u003e\u003cdiv\u003e\u003cdiv\u003e\u003cdiv id=\"js_error_message\"\u003e\u003cp\u003e\u003cimg src=\"https://d1ge0kk1l5kms0.cloudfront.net/images/G/01/webservices/console/warning.png\" alt=\"Warning\" /\u003e \u003cstrong\u003eJavascript is disabled or is unavailable in your browser.\u003c/strong\u003e\u003c/p\u003e\u003cp\u003eTo use the Amazon Web Services Documentation, Javascript must be enabled. Please refer to your browser's Help pages for instructions.\u003c/p\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003eDocument ConventionsSEC01-BP07 Identify threats and prioritize mitigations using a threat\n      modelIdentity and access management",
  "SEC02-BP01 Use strong sign-in mechanisms\n    Sign-ins (authentication using sign-in credentials) can present\n    risks when not using mechanisms like multi-factor authentication\n    (MFA), especially in situations where sign-in credentials have been\n    inadvertently disclosed or are easily guessed. Use strong sign-in\n    mechanisms to reduce these risks by requiring MFA and strong\n    password policies.\n  \n    Desired outcome: Reduce the risks\n    of unintended access to credentials in AWS by using strong sign-in\n    mechanisms for AWS Identity and Access Management (IAM) users, the\n    AWS account root user,\n    AWS IAM Identity Center, and\n    third-party identity providers. This means requiring MFA, enforcing\n    strong password policies, and detecting anomalous login behavior.\n  \n    Common anti-patterns:\n  \n     \n     \n     \n  \n      \n        Not enforcing a strong password policy for your identities\n        including complex passwords and MFA.\n      \n    \n      \n        Sharing the same credentials among different users.\n      \n    \n      \n        Not using detective controls for suspicious sign-ins.\n      \n    \n    Level of risk exposed if this best practice\n    is not established: High\n  \n\n  Implementation guidance\n\n      \n    \n      There are several ways for human identities to sign in to AWS. It\n      is an AWS best practice to rely on a centralized identity provider\n      using federation (direct SAML 2.0 federation between AWS IAM and\n      the centralized IdP or using AWS IAM Identity Center) when\n      authenticating to AWS. In this case, establish a secure sign-in\n      process with your identity provider or Microsoft Active Directory.\n    \n    \n      When you first open an AWS account, you begin with an AWS account\n      root user. You should only use the account root user to set up\n      access for your users (and for\n      tasks\n      that require the root user). It's important to turn on\n      multi-factor authentication (MFA) for the account root user\n      immediately after opening your AWS account and to secure the root\n      user using the\n      AWS       best practice guide.\n    \n    \n      AWS IAM Identity Center is designed for workforce users, and you\n      can create and manage user identities within the service and\n      secure the sign-in process with MFA. AWS Cognito, on the other\n      hand, is designed for customer identity and access management\n      (CIAM), which provides user pools and identity providers for\n      external user identities in your applications.\n    \n    \n      If you create users in AWS IAM Identity Center, secure the sign-in\n      process in that service and\n      turn\n      on MFA. For external user identities in your applications,\n      you can use\n      Amazon Cognito user pools and secure the sign-in process in that\n      service or through one of the supported identity providers in\n      Amazon Cognito user pools.\n    \n    \n      Additionally, for users in AWS IAM Identity Center, you can use\n      AWS Verified Access to provide an additional layer of security\n      by verifying the user's identity and device posture before they\n      are granted access to AWS resources.\n    \n    \n      If you are using\n      AWS Identity and Access Management (IAM) users, secure the sign-in process\n      using IAM.\n    \n    \n      You can use both AWS IAM Identity Center and direct IAM federation\n      simultaneously to manage access to AWS. You can use IAM federation\n      to manage access to the AWS Management Console and services and IAM Identity Center to manage access to business applications like QuickSight or Amazon Q Business.\n    \n    \n      Regardless of the sign-in method, it's critical to enforce a\n      strong sign-in policy.\n    \n     \n\n  Implementation steps\n\n      \n      \n        The following are general strong sign-in recommendations. The\n        actual settings you configure should be set by your company\n        policy or use a standard like\n        NIST\n        800-63.\n      \n      \n         \n         \n         \n         \n         \n      \n          \n            Require MFA. It's an\n            IAM\n            best practice to require MFA for human identities and\n            workloads. Turning on MFA provides an additional layer of\n            security requiring that users provide sign-in credentials\n            and a one-time password (OTP) or a cryptographically\n            verified and generated string from a hardware device.\n          \n        \n          \n            Enforce a minimum password length, which is a primary factor\n            in password strength.\n          \n        \n          \n            Enforce password complexity to make passwords more difficult\n            to guess.\n          \n        \n          \n            Allow users to change their own passwords.\n          \n        \n          \n            Create individual identities instead of shared credentials.\n            By creating individual identities, you can give each user a\n            unique set of security credentials. Individual users provide\n            the ability to audit each user's activity.\n          \n        \n      \n        IAM Identity Center recommendations:\n      \n      \n         \n         \n         \n      \n          \n            IAM Identity Center provides a predefined\n            password\n            policy when using the default directory that\n            establishes password length, complexity, and reuse\n            requirements.\n          \n        \n          \n            Turn\n            on MFA and configure the context-aware or always-on\n            setting for MFA when the identity source is the default\n            directory, AWS Managed Microsoft AD, or AD Connector.\n          \n        \n          \n            Allow users to\n            register\n            their own MFA devices.\n          \n        \n      \n        Amazon Cognito user pools directory recommendations:\n      \n      \n         \n         \n         \n      \n          \n            Configure the\n            Password\n            strength settings.\n          \n        \n          \n            Require\n            MFA for users.\n          \n        \n          \n            Use the Amazon Cognito user pools\n            advanced\n            security settings for features like\n            adaptive\n            authentication which can block suspicious sign-ins.\n          \n        \n      \n        IAM user recommendations:\n      \n      \n         \n         \n      \n          \n            Ideally you are using IAM Identity Center or direct\n            federation. However, you might have the need for IAM users.\n            In that case,\n            set\n            a password policy for IAM users. You can use the\n            password policy to define requirements such as minimum\n            length or whether the password requires non-alphabetic\n            characters.\n          \n        \n          \n            Create an IAM policy to\n            enforce\n            MFA sign-in so that users are allowed to manage their\n            own passwords and MFA devices.\n          \n        \n     \n   \n\n  Resources\n\n      \n    \n      Related best practices:\n    \n    \n       \n       \n       \n    \n        \n          SEC02-BP03 Store and use\n          secrets securely\n        \n      \n        \n          SEC02-BP04 Rely on a\n          centralized identity provider\n        \n      \n        \n          SEC03-BP08 Share\n          resources securely within your organization\n        \n      \n    \n      Related documents:\n    \n    \n       \n       \n       \n       \n       \n       \n    \n        \n          AWS           IAM Identity Center Password Policy\n        \n      \n        \n          IAM user password policy\n        \n      \n        \n          Setting\n          the AWS account root user password\n        \n      \n        \n          Amazon Cognito password policy\n        \n      \n        \n          AWS           credentials\n        \n      \n        \n          IAM\n          security best practices\n        \n      \n    \n      Related videos:\n    \n    \n       \n       \n    \n        \n          Managing user\n          permissions at scale with AWS IAM Identity Center\n        \n      \n        \n          Mastering\n          identity at every layer of the cake\n        \n      \n  \u003cdiv\u003e\u003cdiv\u003e\u003cdiv\u003e\u003cdiv id=\"js_error_message\"\u003e\u003cp\u003e\u003cimg src=\"https://d1ge0kk1l5kms0.cloudfront.net/images/G/01/webservices/console/warning.png\" alt=\"Warning\" /\u003e \u003cstrong\u003eJavascript is disabled or is unavailable in your browser.\u003c/strong\u003e\u003c/p\u003e\u003cp\u003eTo use the Amazon Web Services Documentation, Javascript must be enabled. Please refer to your browser's Help pages for instructions.\u003c/p\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003eDocument Conventions SEC 2. How do you manage authentication for people and machines? SEC02-BP02 Use temporary credentials",
  "SEC02-BP02 Use temporary credentials\n    When doing any type of authentication, it's best to use temporary\n    credentials instead of long-term credentials to reduce or eliminate\n    risks, such as credentials being inadvertently disclosed, shared, or\n    stolen.\n  \n    Desired outcome: To reduce the\n    risk of long-term credentials, use temporary credentials wherever\n    possible for both human and machine identities. Long-term\n    credentials create many risks, such as exposure through uploads to\n    public repositories. By using temporary credentials, you\n    significantly reduce the chances of credentials becoming\n    compromised.\n  \n    Common anti-patterns:\n  \n     \n     \n     \n     \n     \n  \n      \n        Developers using long-term access keys from IAM users rather\n        than obtaining temporary credentials from the CLI using\n        federation.\n      \n    \n      \n        Developers embedding long-term access keys in their code and\n        uploading that code to public Git repositories.\n      \n    \n      \n        Developers embedding long-term access keys in mobile apps that\n        are then made available in app stores.\n      \n    \n      \n        Users sharing long-term access keys with other users, or\n        employees leaving the company with long-term access keys still\n        in their possession.\n      \n    \n      \n        Using long-term access keys for machine identities when\n        temporary credentials could be used.\n      \n    \n    Level of risk exposed if this best practice\n    is not established: High\n  \n\n  Implementation guidance\n\n      \n    \n      Use temporary security credentials instead of long-term\n      credentials for all AWS API and CLI requests. API and CLI requests\n      to AWS services must, in nearly every case, be signed using\n      AWS       access keys. These requests can be signed with either\n      temporary or long-term credentials. The only time you should use\n      long-term credentials, also known as long-term access keys, is if\n      you are using an\n      IAM user or the\n      AWS account root user. When you federate to AWS or assume an\n      IAM\n      role through other methods, temporary credentials are\n      generated. Even when you access the AWS Management Console using\n      sign-in credentials, temporary credentials are generated for you\n      to make calls to AWS services. There are few situations where you\n      need long-term credentials and you can accomplish nearly all tasks\n      using temporary credentials.\n    \n    \n      Avoiding the use of long-term credentials in favor of temporary\n      credentials should go hand in hand with a strategy of reducing the\n      usage of IAM users in favor of federation and IAM roles. While IAM users have been used for both human and machine identities in the\n      past, we now recommend not using them to avoid the risks in using\n      long-term access keys.\n    \n     \n\n  Implementation steps\n\n      \n       \n\n  Human identities\n\n      \n        \n          For workforce identities like employees, administrators,\n          developers, and operators:\n        \n        \n           \n        \n            \n              You should rely on a\n              centralized identity provider and\n              require\n              human users to use federation with an identity provider to\n              access AWS using temporary credentials. Federation\n              for your users can be done either with\n              direct\n              federation to each AWS account or using\n              AWS               IAM Identity Center and the identity provider of\n              your choice. Federation provides a number of advantages\n              over using IAM users in addition to eliminating long-term\n              credentials. Your users can also request temporary\n              credentials from the command line for\n              direct\n              federation or by using\n              IAM Identity Center. This means that there are few uses\n              cases that require IAM users or long-term credentials for\n              your users.\n            \n          \n        \n          For third-party identities: \n        \n        \n           \n        \n            \n              When granting third parties,\n              such as software as a service (SaaS) providers, access to\n              resources in your AWS account, you can use\n              cross-account\n                roles and\n              resource-based\n                policies. Additionally, you can use the\n              Amazon Cognito OAuth 2.0 grant client credentials flow for B2B\n              SaaS customers or partners.\n            \n          \n        \n          User identities that access your AWS resources through web\n          browsers, client applications, mobile apps, or interactive\n          command-line tools: \n        \n        \n           \n        \n            \n              If you need to grant applications for\n              consumers or customers access to your AWS resources, you can\n              use\n              Amazon Cognito identity pools or\n              Amazon Cognito user pools to provide temporary credentials.\n              The permissions for the credentials are configured through IAM\n              roles. You can also define a separate IAM role with limited\n              permissions for guest users who are not authenticated.\n            \n          \n       \n       \n\n  Machine identities\n\n      \n        \n          For machine identities, you might need to use long-term\n          credentials. In these cases, you should\n          require\n          workloads to use temporary credentials with IAM roles to\n          access AWS.\n        \n        \n           \n           \n           \n           \n        \n            \n              For\n              Amazon Elastic Compute Cloud (Amazon EC2), you can use\n              roles\n              for Amazon EC2.\n            \n          \n            \n              AWS Lambda allows you to configure a\n              Lambda\n              execution role to grant the service permissions to\n              perform AWS actions using temporary credentials. There are\n              many other similar models for AWS services to grant\n              temporary credentials using IAM roles.\n            \n          \n            \n              For IoT devices, you can use the\n              AWS IoT Core credential provider to request temporary\n              credentials.\n            \n          \n            \n              For on-premises systems or systems that run outside of AWS\n              that need access to AWS resources, you can use\n              IAM\n              Roles Anywhere.\n            \n          \n        \n          There are scenarios where temporary credentials are not\n          supported, which require the use of long-term credentials. In\n          these situations, audit and\n          rotate these credentials periodically and\n          rotate\n          access keys regularly. For highly restricted IAM user\n          access keys, consider the following additional security\n          measures:\n        \n        \n           \n           \n           \n           \n           \n        \n            \n              Grant highly restricted permissions:\n            \n            \n               \n               \n            \n                \n                  Adhere to the principle of least privilege (be\n                  specific about actions, resources, and conditions).\n                \n              \n                \n                  Consider granting the IAM user only the\n                  AssumeRole operation for one\n                  specific role. Depending on the on-premise\n                  architecture, this approach helps isolate and secure\n                  the long-term IAM credentials.\n                \n              \n          \n            \n              Limit the allowed network sources and IP addresses in the\n              IAM role trust policy.\n            \n          \n            \n              Monitor usage and set up alerts for unused permissions or\n              misuse (using AWS CloudWatch Logs metric filters and\n              alarms).\n            \n          \n            \n              Enforce\n              permission\n              boundaries (service control policies (SCPs) and\n              permission boundaries complement each other - SCPs are\n              coarse-grained, while permission boundaries are\n              fine-grained).\n            \n          \n            \n              Implement a process to provision and securely store (in an\n              on-premise vault) the credentials.\n            \n          \n        \n          Some other options for scenarios requiring long-term\n          credentials include:\n        \n        \n           \n           \n           \n        \n            \n              Build your own token vending API (using Amazon API Gateway).\n            \n          \n            \n              For scenarios where you must use long-term credentials or\n              credentials other than AWS access keys (such as database\n              logins), you can use a service designed to handle the\n              management of secrets, such as\n              AWS Secrets Manager. Secrets Manager simplifies the\n              management, rotation, and secure storage of encrypted\n              secrets. Many AWS services support a\n              direct\n              integration with Secrets Manager.\n            \n          \n            \n              For multi-cloud integrations, you can use identity\n              federation based on your source credential service\n              provider (CSP) credentials (see\n              AWS STS AssumeRoleWithWebIdentity).\n            \n          \n        \n          For more information about rotating long-term credentials, see\n          rotating\n          access keys.\n        \n       \n     \n   \n\n  Resources\n\n      \n    \n      Related best practices:\n    \n    \n       \n       \n       \n    \n        \n          SEC02-BP03 Store and use\n          secrets securely\n        \n      \n        \n          SEC02-BP04 Rely on a\n          centralized identity provider\n        \n      \n        \n          SEC03-BP08 Share\n          resources securely within your organization\n        \n      \n    \n      Related documents:\n    \n    \n       \n       \n       \n       \n       \n       \n       \n       \n       \n       \n       \n    \n        \n          Temporary\n          Security Credentials\n        \n      \n        \n          AWS           Credentials\n        \n      \n        \n          IAM\n          Security Best Practices\n        \n      \n        \n          IAM\n          Roles\n        \n      \n        \n          IAM Identity Center\n        \n      \n        \n          Identity\n          Providers and Federation\n        \n      \n        \n          Rotating\n          Access Keys\n        \n      \n        \n          Security\n          Partner Solutions: Access and Access Control\n        \n      \n        \n          The\n          AWS Account Root User\n        \n      \n        \n          Access\n          AWS using a Google Cloud Platform native workload\n          identity\n        \n      \n        \n          How\n          to access AWS resources from Microsoft Entra ID tenants using\n          AWS Security Token Service\n        \n      \n    \n      Related videos:\n    \n    \n       \n       \n    \n        \n          Managing user\n          permissions at scale with AWS IAM Identity Center\n        \n      \n        \n          Mastering\n          identity at every layer of the cake\n        \n      \n  \u003cdiv\u003e\u003cdiv\u003e\u003cdiv\u003e\u003cdiv id=\"js_error_message\"\u003e\u003cp\u003e\u003cimg src=\"https://d1ge0kk1l5kms0.cloudfront.net/images/G/01/webservices/console/warning.png\" alt=\"Warning\" /\u003e \u003cstrong\u003eJavascript is disabled or is unavailable in your browser.\u003c/strong\u003e\u003c/p\u003e\u003cp\u003eTo use the Amazon Web Services Documentation, Javascript must be enabled. Please refer to your browser's Help pages for instructions.\u003c/p\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003eDocument ConventionsSEC02-BP01 Use strong sign-in mechanismsSEC02-BP03 Store and use secrets securely",
  "SEC02-BP03 Store and use secrets securely\n    A workload requires an automated capability to prove its identity to\n    databases, resources, and third-party services. This is accomplished\n    using secret access credentials, such as API access keys, passwords,\n    and OAuth tokens. Using a purpose-built service to store, manage,\n    and rotate these credentials helps reduce the likelihood that those\n    credentials become compromised.\n  \n    Desired outcome: Implementing a\n    mechanism for securely managing application credentials that\n    achieves the following goals:\n  \n     \n     \n     \n     \n     \n     \n  \n      \n        Identifying what secrets are required for the workload.\n      \n    \n      \n        Reducing the number of long-term credentials required by\n        replacing them with short-term credentials when possible.\n      \n    \n      \n        Establishing secure storage and automated rotation of remaining\n        long-term credentials.\n      \n    \n      \n        Auditing access to secrets that exist in the workload.\n      \n    \n      \n        Continual monitoring to verify that no secrets are embedded in\n        source code during the development process.\n      \n    \n      \n        Reduce the likelihood of credentials being inadvertently\n        disclosed.\n      \n    \n    Common anti-patterns:\n  \n     \n     \n     \n  \n      \n        Not rotating credentials.\n      \n    \n      \n        Storing long-term credentials in source code or configuration\n        files.\n      \n    \n      \n        Storing credentials at rest unencrypted.\n      \n    \n    Benefits of establishing this best\n    practice:\n  \n     \n     \n     \n     \n     \n     \n  \n      \n        Secrets are stored encrypted at rest and in transit.\n      \n    \n      \n        Access to credentials is gated through an API (think of it as a\n        credential vending machine).\n      \n    \n      \n        Access to a credential (both read and write) is audited and\n        logged.\n      \n    \n      \n        Separation of concerns: credential rotation is performed by a\n        separate component, which can be segregated from the rest of the\n        architecture.\n      \n    \n      \n        Secrets are automatically distributed on-demand to software\n        components and rotation occurs in a central location.\n      \n    \n      \n        Access to credentials can be controlled in a fine-grained\n        manner.\n      \n    \n    Level of risk exposed if this best practice\n    is not established: High\n  \n\n  Implementation guidance\n\n      \n    \n      In the past, credentials used to authenticate to databases,\n      third-party APIs, tokens, and other secrets might have been\n      embedded in source code or in environment files. AWS provides\n      several mechanisms to store these credentials securely,\n      automatically rotate them, and audit their usage.\n    \n    \n      The best way to approach secrets management is to follow the\n      guidance of remove, replace, and rotate. The most secure\n      credential is one that you do not have to store, manage, or\n      handle. There might be credentials that are no longer necessary to\n      the functioning of the workload that can be safely removed.\n    \n    \n      For credentials that are still required for the proper functioning\n      of the workload, there might be an opportunity to replace a\n      long-term credential with a temporary or short-term credential.\n      For example, instead of hard-coding an AWS secret access key,\n      consider replacing that long-term credential with a temporary\n      credential using IAM roles.\n    \n    \n      Some long-lived secrets might not be able to be removed or\n      replaced. These secrets can be stored in a service such as\n      AWS Secrets Manager, where they can be centrally stored,\n      managed, and rotated on a regular basis.\n    \n    \n      An audit of the workload's source code and configuration files can\n      reveal many types of credentials. The following table summarizes\n      strategies for handling common types of credentials:\n    \n    \n          \n            \n              Credential type\n            \n            \n              Description\n            \n            \n              Suggested strategy\n            \n          \n        \n          \n            \n              IAM access keys\n            \n            \n              AWS IAM access and secret keys used to assume IAM roles\n              inside of a workload\n            \n            \n              Replace: Use\n              IAM\n              roles assigned to the compute instances (such as\n              Amazon EC2 or\n              AWS Lambda) instead. For interoperability with\n              third parties that require access to resources in your AWS account, ask if they support\n              AWS               cross-account access. For mobile apps,\n              consider using temporary credentials through\n              Amazon Cognito identity pools (federated identities).\n              For workloads running outside of AWS, consider\n              IAM\n              Roles Anywhere or\n              AWS Systems Manager Hybrid Activations. For\n              containers see\n              Amazon ECS task IAM role or\n              Amazon EKS node IAM role.\n            \n          \n          \n            \n              SSH keys\n            \n            \n              Secure Shell private keys used to log into Linux EC2\n              instances, manually or as part of an automated process\n            \n            \n              Replace: Use\n              AWS Systems Manager or\n              EC2\n              Instance Connect to provide programmatic and human\n              access to EC2 instances using IAM roles.\n            \n          \n          \n            \n              Application and database credentials\n            \n            \n              Passwords – plain text string\n            \n            \n              Rotate: Store credentials in\n              AWS Secrets Manager and establish automated rotation if\n              possible.\n            \n          \n          \n            \n              Amazon RDS and Aurora Admin Database credentials\n            \n            \n              Passwords – plain text string\n            \n            \n              Replace: Use the\n              Secrets Manager integration with Amazon RDS or\n              Amazon Aurora. In addition, some RDS database types can\n              use IAM roles instead of passwords for some use cases (for\n              more detail, see\n              IAM\n              database authentication).\n            \n          \n          \n            \n              OAuth tokens\n            \n            \n              Secret tokens – plain text string\n            \n            \n              Rotate: Store tokens in\n              AWS Secrets Manager and configure automated rotation.\n            \n          \n          \n            \n              API tokens and keys\n            \n            \n              Secret tokens – plain text string\n            \n            \n              Rotate: Store in\n              AWS Secrets Manager and establish automated rotation if\n              possible.\n            \n          \n        \n    \n      A common anti-pattern is embedding IAM access keys inside source\n      code, configuration files, or mobile apps. When an IAM access key\n      is required to communicate with an AWS service, use\n      temporary\n      (short-term) security credentials. These short-term\n      credentials can be provided through\n      IAM\n      roles for EC2 instances,\n      execution\n      roles for Lambda functions,\n      Cognito\n      IAM roles for mobile user access, and\n      IoT\n      Core policies for IoT devices. When interfacing with third\n      parties, prefer\n      delegating\n      access to an IAM role with the necessary access to your\n      account's resources rather than configuring an IAM user and\n      sending the third party the secret access key for that user.\n    \n    \n      There are many cases where the workload requires the storage of\n      secrets necessary to interoperate with other services and\n      resources.\n      AWS Secrets Manager is purpose built to securely manage these\n      credentials, as well as the storage, use, and rotation of API\n      tokens, passwords, and other credentials.\n    \n    \n      AWS Secrets Manager provides five key capabilities to ensure the\n      secure storage and handling of sensitive credentials:\n      encryption\n      at rest,\n      \n      \n      encryption\n      in transit,\n      \n      \n      comprehensive\n      auditing,\n      \n      \n      fine-grained\n      access control, and\n      \n      \n      extensible\n      credential rotation. Other secret management services from\n      AWS Partners or locally developed solutions that provide similar\n      capabilities and assurances are also acceptable.\n    \n    \n      When you retrieve a secret, you can use the Secrets Manager client\n      side caching components to cache it for future use. Retrieving a\n      cached secret is faster than retrieving it from Secrets Manager.\n      Additionally, because there is a cost for calling Secrets Manager\n      APIs, using a cache can reduce your costs. For all of the ways you\n      can retrieve secrets, see\n      Get\n      secrets.\n    \n    Note\n      Some languages may\n      require you to implement your own in-memory encryption for client\n      side caching.\n    \n     \n\n  Implementation steps\n\n      \n      \n         \n         \n         \n         \n         \n         \n      \n          \n            Identify code paths containing hard-coded credentials using\n            automated tools such as\n            Amazon CodeGuru.\n          \n          \n             \n          \n              \n                Use Amazon CodeGuru to scan your code repositories. Once\n                the review is complete, filter on\n                Type=Secrets in CodeGuru to find\n                problematic lines of code.\n              \n            \n        \n          \n            Identify credentials that can be removed or replaced.\n          \n          \n             \n             \n          \n              \n                Identify credentials no longer needed and mark for\n                removal.\n              \n            \n              \n                For AWS Secret Keys that are embedded in source code,\n                replace them with IAM roles associated with the\n                necessary resources. If part of your workload is outside\n                AWS but requires IAM credentials to access AWS\n                resources, consider\n                IAM\n                Roles Anywhere or\n                AWS Systems Manager Hybrid Activations.\n              \n            \n        \n          \n            For other third-party, long-lived secrets that require the\n            use of the rotate strategy, integrate Secrets Manager into\n            your code to retrieve third-party secrets at runtime.\n          \n          \n             \n             \n          \n              \n                The CodeGuru console can automatically\n                create\n                a secret in Secrets Manager using the discovered\n                credentials.\n              \n            \n              \n                Integrate secret retrieval from Secrets Manager into\n                your application code.\n              \n              \n                 \n                 \n              \n                  \n                    Serverless Lambda functions can use a\n                    language-agnostic\n                    Lambda\n                    extension.\n                  \n                \n                  \n                    For EC2 instances or containers, AWS provides\n                    example\n                    client-side\n                    code for retrieving secrets from Secrets Manager in several popular programming\n                    languages.\n                  \n                \n            \n        \n          \n            Periodically review your code base and re-scan to verify no\n            new secrets have been added to the code.\n          \n          \n             \n          \n              \n                Consider using a tool such as\n                git-secrets\n                to prevent committing new secrets to your source code\n                repository.\n              \n            \n        \n          \n            Monitor\n            Secrets Manager activity for indications of\n            unexpected usage, inappropriate secret access, or attempts\n            to delete secrets.\n          \n        \n          \n            Reduce human exposure to credentials. Restrict access to\n            read, write, and modify credentials to an IAM role dedicated\n            for this purpose, and only provide access to assume the role\n            to a small subset of operational users.\n          \n        \n     \n   \n\n  Resources\n\n      \n    \n      Related best practices:\n    \n    \n       \n       \n    \n        \n          SEC02-BP02 Use temporary\n          credentials\n        \n      \n        \n          SEC02-BP05 Audit and rotate\n          credentials periodically\n        \n      \n    \n      Related documents:\n    \n    \n       \n       \n       \n       \n       \n       \n       \n    \n        \n          Getting\n          Started with AWS Secrets Manager\n        \n      \n        \n          Identity\n          Providers and Federation\n        \n      \n        \n          Amazon CodeGuru Introduces Secrets Detector\n        \n      \n        \n          How\n          AWS Secrets Manager uses AWS Key Management Service\n        \n      \n        \n          Secret\n          encryption and decryption in Secrets Manager\n        \n      \n        \n          Secrets Manager blog entries\n        \n      \n        \n          Amazon RDS announces integration with AWS Secrets Manager\n        \n      \n    \n      Related videos:\n    \n    \n       \n       \n       \n    \n        \n          Best Practices\n          for Managing, Retrieving, and Rotating Secrets at Scale\n        \n      \n        \n          Find\n          Hard-Coded Secrets Using Amazon CodeGuru Secrets\n          Detector\n        \n      \n        \n          Securing\n          Secrets for Hybrid Workloads Using AWS Secrets Manager\n        \n      \n    \n      Related workshops:\n    \n    \n       \n       \n    \n        \n          Store,\n          retrieve, and manage sensitive credentials in AWS Secrets Manager\n        \n      \n        \n          AWS Systems Manager Hybrid Activations\n        \n      \n  \u003cdiv\u003e\u003cdiv\u003e\u003cdiv\u003e\u003cdiv id=\"js_error_message\"\u003e\u003cp\u003e\u003cimg src=\"https://d1ge0kk1l5kms0.cloudfront.net/images/G/01/webservices/console/warning.png\" alt=\"Warning\" /\u003e \u003cstrong\u003eJavascript is disabled or is unavailable in your browser.\u003c/strong\u003e\u003c/p\u003e\u003cp\u003eTo use the Amazon Web Services Documentation, Javascript must be enabled. Please refer to your browser's Help pages for instructions.\u003c/p\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003eDocument ConventionsSEC02-BP02 Use temporary credentialsSEC02-BP04 Rely on a centralized identity provider",
  "SEC02-BP04 Rely on a centralized identity provider\n    For workforce identities (employees and contractors), rely on an\n    identity provider that allows you to manage identities in a\n    centralized place. This makes it easier to manage access across\n    multiple applications and systems, because you are creating,\n    assigning, managing, revoking, and auditing access from a single\n    location.\n  \n    Desired outcome: You have a\n    centralized identity provider where you centrally manage workforce\n    users, authentication policies (such as requiring multi-factor\n    authentication (MFA)), and authorization to systems and applications\n    (such as assigning access based on a user's group membership or\n    attributes). Your workforce users sign in to the central identity\n    provider and federate (single sign-on) to internal and external\n    applications, removing the need for users to remember multiple\n    credentials. Your identity provider is integrated with your human\n    resources (HR) systems so that personnel changes are automatically\n    synchronized to your identity provider. For example, if someone\n    leaves your organization, you can automatically revoke access to\n    federated applications and systems (including AWS). You have enabled\n    detailed audit logging in your identity provider and are monitoring\n    these logs for unusual user behavior.\n  \n    Common anti-patterns:\n  \n     \n     \n  \n      \n        You do not use federation and single-sign on. Your workforce\n        users create separate user accounts and credentials in multiple\n        applications and systems.\n      \n    \n      \n        You have not automated the lifecycle of identities for workforce\n        users, such as by integrating your identity provider with your\n        HR systems. When a user leaves your organization or changes\n        roles, you follow a manual process to delete or update their\n        records in multiple applications and systems.\n      \n    \n    Benefits of establishing this best\n    practice: By using a centralized identity provider, you\n    have a single place to manage workforce user identities and\n    policies, the ability to assign access to applications to users and\n    groups, and the ability to monitor user sign-in activity. By\n    integrating with your human resources (HR) systems, when a user\n    changes roles, these changes are synchronized to the identity\n    provider and automatically updates their assigned applications and\n    permissions. When a user leaves your organization, their identity is\n    automatically disabled in the identity provider, revoking their\n    access to federated applications and systems.\n  \n    Level of risk exposed if this best practice\n    is not established: High\n  \n\n  Implementation guidance\n\n      \n    \n      Guidance for workforce users accessing\n      AWS Workforce users like employees and contractors in\n      your organization may require access to AWS using the AWS Management Console or AWS Command Line Interface (AWS CLI) to\n      perform their job functions. You can grant AWS access to your\n      workforce users by federating from your centralized identity\n      provider to AWS at two levels: direct federation to each AWS account or federating to multiple accounts in your\n      AWS       organization.\n    \n    \n      To federate your workforce users directly with each AWS account,\n      you can use a centralized identity provider to federate to\n      AWS Identity and Access Management in that account. The flexibility of IAM\n      allows you to enable a separate\n      SAML\n      2.0 or an\n      Open\n      ID Connect (OIDC) Identity Provider for each AWS account\n      and use federated user attributes for access control. Your\n      workforce users will use their web browser to sign in to the\n      identity provider by providing their credentials (such as\n      passwords and MFA token codes). The identity provider issues a\n      SAML assertion to their browser that is submitted to the AWS Management Console sign in URL to allow the user to single sign-on\n      to the\n      AWS Management Console by assuming an IAM Role. Your users can\n      also obtain temporary AWS API credentials for use in the\n      AWS CLI or\n      AWS       SDKs from\n      AWS STS by\n      assuming\n      the IAM role using a SAML assertion from the identity\n      provider.\n    \n    \n      To federate your workforce users with multiple accounts in your\n      AWS organization, you can use\n      AWS       IAM Identity Center to centrally manage access\n      for your workforce users to AWS accounts and applications. You\n      enable Identity Center for your organization and configure your\n      identity source. IAM Identity Center provides a default identity\n      source directory which you can use to manage your users and\n      groups. Alternatively, you can choose an external identity source\n      by\n      connecting\n      to your external identity provider using SAML\n      2.0 and\n      automatically\n      provisioning users and groups using SCIM, or\n      connecting\n      to your Microsoft AD Directory using\n      AWS Directory Service. Once an identity source is configured,\n      you can assign access to users and groups to AWS accounts by\n      defining least-privilege policies in your\n      permission\n      sets. Your workforce users can authenticate through your\n      central identity provider to sign in to the\n      AWS       access portal and single-sign on to the AWS accounts and\n      cloud applications assigned to them. Your users can configure the\n      AWS CLI v2 to authenticate with Identity Center and get\n      credentials to run AWS CLI commands. Identity Center also allows\n      single-sign on access to AWS applications such as\n      Amazon SageMaker AI Studio and\n      AWS IoT Sitewise Monitor portals.\n    \n    \n      After you follow the preceding guidance, your workforce users will\n      no longer need to use IAM users and groups for normal operations\n      when managing workloads on AWS. Instead, your users and groups are\n      managed outside of AWS and users are able to access AWS resources\n      as a federated identity. Federated identities\n      use the groups defined by your centralized identity provider. You\n      should identify and remove IAM groups, IAM users, and long-lived\n      user credentials (passwords and access keys) that are no longer\n      needed in your AWS accounts. You can\n      find\n      unused credentials using\n      IAM\n      credential reports,\n      delete\n      the corresponding IAM users and\n      delete\n      IAM groups. You can apply a\n      Service\n      Control Policy (SCP) to your organization that helps\n      prevent the creation of new IAM users and groups, enforcing that\n      access to AWS is via federated identities.\n    \n    \n      Note\n        You are responsible for handling the rotation of SCIM\n        access tokens as described in the\n        Automatic\n        provisioning documentation. Additionally, you are\n        responsible for rotating the certificates supporting your\n        identity federation.\n      \n    \n    \n      Guidance for users of your\n      applications You can manage the identities of users of\n      your applications, such as a mobile app, using\n      Amazon Cognito as your centralized identity provider.\n      Amazon Cognito enables authentication, authorization, and user\n      management for your web and mobile apps. Amazon Cognito provides\n      an identity store that scales to millions of users, supports\n      social and enterprise identity federation, and offers advanced\n      security features to help protect your users and business. You can\n      integrate your custom web or mobile application with Amazon Cognito to add user authentication and access control to your\n      applications in minutes. Built on open identity standards such as\n      SAML and Open ID Connect (OIDC), Amazon Cognito supports various\n      compliance regulations and integrates with frontend and backend\n      development resources.\n    \n     \n\n  Implementation steps\n\n      \n      \n        Steps for workforce users accessing\n        AWS\n      \n      \n         \n         \n      \n          \n            Federate your workforce users to AWS using a centralized\n            identity provider using one of the following approaches:\n          \n          \n             \n             \n          \n              \n                Use IAM Identity Center to enable single sign-on to\n                multiple AWS accounts in your AWS organization by\n                federating with your identity provider.\n              \n            \n              \n                Use IAM to connect your identity provider directly to\n                each AWS account, enabling federated fine-grained\n                access.\n              \n            \n        \n          \n            Identify and remove IAM users and groups that are replaced\n            by federated identities.\n          \n        \n      \n        Steps for users of your\n        applications\n      \n      \n         \n         \n      \n          \n            Use Amazon Cognito as a centralized identity provider\n            towards your applications.\n          \n        \n          \n            Integrate your custom applications with Amazon Cognito using\n            OpenID Connect and OAuth. You can develop your custom\n            applications using the Amplify libraries that provide simple\n            interfaces to integrate with a variety of AWS services, such\n            as Amazon Cognito for authentication.\n          \n        \n     \n   \n\n  Resources\n\n      \n    \n      Related best practices:\n    \n    \n       \n       \n       \n    \n        \n          SEC02-BP06 Employ user groups\n          and attributes\n        \n      \n        \n          SEC03-BP02 Grant\n          least privilege access\n        \n      \n        \n          SEC03-BP06 Manage\n          access based on lifecycle\n        \n      \n    \n      Related documents:\n    \n    \n       \n       \n       \n       \n       \n       \n    \n        \n          Identity\n          federation in AWS\n        \n      \n        \n          Security\n          best practices in IAM\n        \n      \n        \n          AWS Identity and Access Management Best practices\n        \n      \n        \n          Getting\n          started with IAM Identity Center delegated\n          administration\n        \n      \n        \n          How\n          to use customer managed policies in IAM Identity Center for\n          advanced use cases\n        \n      \n        \n          AWS CLI v2: IAM Identity Center credential provider\n        \n      \n    \n      Related videos:\n    \n    \n       \n       \n       \n    \n        \n          AWS re:Inforce\n          2022 - AWS Identity and Access Management (IAM) deep\n          dive\n        \n      \n        \n          AWS re:Invent\n          2022 - Simplify your existing workforce access with IAM Identity Center\n        \n      \n        \n          AWS re:Invent\n          2018: Mastering Identity at Every Layer of the Cake\n        \n      \n    \n      Related examples:\n    \n    \n       \n    \n        \n          Workshop:\n          Using AWS IAM Identity Center to achieve strong identity\n          management\n        \n      \n    \n      Related tools:\n    \n    \n       \n       \n    \n        \n          AWS           Security Competency Partners: Identity and Access\n          Management\n        \n      \n        \n          saml2aws\n        \n      \n  \u003cdiv\u003e\u003cdiv\u003e\u003cdiv\u003e\u003cdiv id=\"js_error_message\"\u003e\u003cp\u003e\u003cimg src=\"https://d1ge0kk1l5kms0.cloudfront.net/images/G/01/webservices/console/warning.png\" alt=\"Warning\" /\u003e \u003cstrong\u003eJavascript is disabled or is unavailable in your browser.\u003c/strong\u003e\u003c/p\u003e\u003cp\u003eTo use the Amazon Web Services Documentation, Javascript must be enabled. Please refer to your browser's Help pages for instructions.\u003c/p\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003eDocument ConventionsSEC02-BP03 Store and use secrets securelySEC02-BP05 Audit and rotate credentials periodically",
  "SEC02-BP05 Audit and rotate credentials periodically\n    Audit and rotate credentials periodically to limit how long the\n    credentials can be used to access your resources. Long-term\n    credentials create many risks, and these risks can be reduced by\n    rotating long-term credentials regularly.\n  \n    Desired outcome: Implement\n    credential rotation to help reduce the risks associated with\n    long-term credential usage. Regularly audit and remediate\n    non-compliance with credential rotation policies.\n  \n    Common anti-patterns:\n  \n     \n     \n     \n  \n      \n        Not auditing credential use.\n      \n    \n      \n        Using long-term credentials unnecessarily.\n      \n    \n      \n        Using long-term credentials and not rotating them regularly.\n      \n    \n    Level of risk exposed if this best practice\n      is not established: High\n  \n    \n    Implementation guidance\n    \n    \n    \n      When you cannot rely on temporary credentials and require\n      long-term credentials, audit credentials to verify that defined\n      controls like\n      multi-factor\n        authentication (MFA) are enforced, rotated regularly, and\n      have the appropriate access level.\n    \n    \n      Periodic validation, preferably through an automated tool, is\n      necessary to verify that the correct controls are enforced. For\n      human identities, you should require users to change their\n      passwords periodically and retire access keys in favor of\n      temporary credentials. As you move from AWS Identity and Access Management (IAM) users to centralized identities, you can\n      generate\n        a credential report to audit your users.\n    \n    \n      We also recommend that you enforce and monitor MFA in your\n      identity provider. You can set up\n      AWS Config Rules, or use\n      AWS Security Hub Security Standards, to monitor if users have\n      configured MFA. Consider using\n      IAM\n        Roles Anywhere to provide temporary credentials for machine\n      identities. In situations when using IAM roles and temporary\n      credentials is not possible, frequent auditing and rotating access\n      keys is necessary.\n    \n     \n      \n      Implementation steps\n      \n      \n      \n         \n         \n         \n         \n         \n      \n          \n            Regularly audit\n              credentials: Auditing the identities that are\n            configured in your identity provider and IAM helps verify\n            that only authorized identities have access to your\n            workload. Such identities can include, but are not limited\n            to, IAM users, AWS IAM Identity Center users, Active\n            Directory users, or users in a different upstream identity\n            provider. For example, remove people that leave the\n            organization, and remove cross-account roles that are no\n            longer required. Have a process in place to periodically\n            audit permissions to the services accessed by an IAM entity.\n            This helps you identify the policies you need to modify to\n            remove any unused permissions. Use credential reports and\n            AWS Identity and Access Management Access Analyzer to\n            audit IAM credentials and permissions. You can use\n            Amazon CloudWatch to set up alarms for specific API calls\n            called within your AWS environment.\n            Amazon GuardDuty can also alert you to unexpected activity,\n            which might indicate overly permissive access or unintended\n            access to IAM credentials.\n          \n        \n          \n            Rotate credentials\n              regularly: When you are unable to use temporary\n            credentials, rotate long-term IAM access keys regularly\n            (maximum every 90 days). If an access key is unintentionally\n            disclosed without your knowledge, this limits how long the\n            credentials can be used to access your resources. For\n            information about rotating access keys for IAM users, see\n            Rotating\n              access keys.\n          \n        \n          \n            Review IAM permissions:\n            To improve the security of your AWS account, regularly\n            review and monitor each of your IAM policies. Verify that\n            policies adhere to the principle of least privilege.\n          \n        \n          \n            Consider automating IAM resource\n              creation and updates:\n            IAM Identity Center automates many IAM tasks, such as\n            role and policy management. Alternatively, AWS CloudFormation can be used to automate the deployment of IAM\n            resources, including roles and policies, to reduce the\n            chance of human error because the templates can be verified\n            and version controlled.\n          \n        \n          \n            Use IAM Roles Anywhere to replace\n              IAM users for machine identities:\n            IAM\n              Roles Anywhere allows you to use roles in areas that\n            you traditionally could not, such as on-premise servers. IAM\n            Roles Anywhere uses a trusted\n            X.509\n              certificate to authenticate to AWS and receive\n            temporary credentials. Using IAM Roles Anywhere avoids the\n            need to rotate these credentials, as long-term credentials\n            are no longer stored in your on-premises environment. Please\n            note that you will need to monitor and rotate the X.509\n            certificate as it approaches expiration.\n          \n        \n     \n   \n    \n    Resources\n    \n    \n    \n      Related best practices:\n    \n    \n       \n       \n    \n        \n          SEC02-BP02 Use temporary\n            credentials\n        \n      \n        \n          SEC02-BP03 Store and use\n            secrets securely\n        \n      \n    \n      Related documents:\n    \n    \n       \n       \n       \n       \n       \n       \n    \n        \n          Getting\n            Started with AWS Secrets Manager\n        \n      \n        \n          IAM\n            Best Practices\n        \n      \n        \n          Identity\n            Providers and Federation\n        \n      \n        \n          Security\n            Partner Solutions: Access and Access Control\n        \n      \n        \n          Temporary\n            Security Credentials\n        \n      \n        \n          Getting\n            credential reports for your AWS account\n        \n      \n    \n      Related videos:\n    \n    \n       \n       \n       \n    \n        \n          Best Practices\n            for Managing, Retrieving, and Rotating Secrets at Scale\n        \n      \n        \n          Managing user\n            permissions at scale with AWS IAM Identity Center\n        \n      \n        \n          Mastering\n            identity at every layer of the cake\n        \n      \n    \n    \n  \u003cdiv\u003e\u003cdiv\u003e\u003cdiv\u003e\u003cdiv id=\"js_error_message\"\u003e\u003cp\u003e\u003cimg src=\"https://d1ge0kk1l5kms0.cloudfront.net/images/G/01/webservices/console/warning.png\" alt=\"Warning\" /\u003e \u003cstrong\u003eJavascript is disabled or is unavailable in your browser.\u003c/strong\u003e\u003c/p\u003e\u003cp\u003eTo use the Amazon Web Services Documentation, Javascript must be enabled. Please refer to your browser's Help pages for instructions.\u003c/p\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003eDocument ConventionsSEC02-BP04 Rely on a centralized identity providerSEC02-BP06 Employ user groups and attributes",
  "SEC02-BP06 Employ user groups and attributes\n    Defining permissions according to user groups and attributes helps\n    reduce the number and complexity of policies, making it simpler to\n    achieve the principle of least privilege. You can use user groups to\n    manage the permissions for many people in one place based on the\n    function they perform in your organization. Attributes, such as\n    department, project, or location, can provide an additional layer of\n    permission scope when people perform a similar function but for\n    different subsets of resources.\n  \n    Desired outcome: You can apply\n    changes in permissions based on function to all users who perform\n    that function. Group membership and attributes govern user\n    permissions, reducing the need to manage permissions at the\n    individual user level. The groups and attributes you define in your\n    identity provider (IdP) are propagated automatically to your AWS\n    environments.\n  \n    Common anti-patterns:\n  \n     \n     \n     \n     \n     \n     \n  \n      \n        Managing permissions for individual users and duplicating across\n        many users.\n      \n    \n      \n        Defining groups at too high a level, granting overly-broad\n        permissions.\n      \n    \n      \n        Defining groups at too granular a level, creating duplication\n        and confusion about membership.\n      \n    \n      \n        Using groups with duplicate permissions across subsets of\n        resources when attributes can be used instead.\n      \n    \n      \n        Not managing groups, attributes, and memberships through a\n        standardized identity provider integrated with your AWS\n        environments.\n      \n    \n      \n        Using role chaining when using AWS IAM Identity Center sessions\n      \n    \n    Level of risk exposed if this best practice\n    is not established: Medium\n  \n\n  Implementation guidance\n\n      \n    \n      AWS permissions are defined in documents called\n      policies that are associated with a\n      principal, such as a user, group, role, or resource. You can scale\n      permissions management by organizing permissions assignments\n      (group, permissions, account) based on job-function, workload, and\n      SDLC environment. For your workforce, this allows you to define\n      groups based on the function your users perform for your\n      organization, rather than based on the resources being accessed.\n      For example, a WebAppDeveloper group may have a\n      policy attached for configuring services like Amazon CloudFront\n      within a development account. An\n      AutomationDeveloper group may have some\n      overlapping permissions with the\n      WebAppDeveloper group. These common permissions\n      can be captured in a separate policy and associated with both\n      groups, rather than having users from both functions belong to a\n      CloudFrontAccess group.\n    \n    \n      In addition to groups, you can use attributes\n      to further scope access. For example, you may have a\n      Project attribute for users in your\n      WebAppDeveloper group to scope access to\n      resources specific to their project. Using this technique removes\n      the need to have different groups for application developers\n      working on different projects if their permissions are otherwise\n      the same. The way you refer to attributes in permission policies\n      is based on their source, whether they are defined as part of your\n      federation protocol (such as SAML, OIDC, or SCIM), as custom SAML\n      assertions, or set within IAM Identity Center.\n    \n     \n\n  Implementation steps\n\n      \n      \n         \n         \n         \n      \n          \n            Establish where you will define groups and attributes:\n          \n          \n             \n          \n              \n                Following the guidance in\n                SEC02-BP04 Rely on a\n                centralized identity provider, you can determine\n                whether you need to define groups and attributes within\n                your identity provider, within IAM Identity Center, or\n                using IAM user groups in a specific account.\n              \n            \n        \n          \n            Define groups:\n          \n          \n             \n             \n             \n          \n              \n                Determine your groups based on function and scope of\n                access required. Consider using a hierarchical structure\n                or naming conventions to organize groups effectively.\n              \n            \n              \n                If defining within IAM Identity Center, create groups\n                and associate the desired level of access using\n                permission sets.\n              \n            \n              \n                If defining within an external identity provider,\n                determine if the provider supports the SCIM protocol and\n                consider enabling automatic provisioning within IAM Identity Center. This capability synchronizes the\n                creation, membership, and deletion of groups between\n                your provider and IAM Identity Center.\n              \n            \n        \n          \n            Define attributes:\n          \n          \n             \n             \n          \n              \n                If you use an external identity provider, both the SCIM\n                and SAML 2.0 protocols provide certain attributes by\n                default. Additional attributes can be defined and passed\n                using SAML assertions with the\n                https://aws.amazon.com/SAML/Attributes/PrincipalTag\n                attribute name. Refer to your identity provider's\n                documentation for guidance on defining and configuring\n                custom attributes.\n              \n            \n              \n                If you define roles within IAM Identity Center, enable\n                the attribute-based access control (ABAC) feature, and\n                define attributes as desired. Consider attributes that\n                align with your organization's structure or resource\n                tagging strategy.\n              \n            \n        \n      \n        If you require IAM role chaining from IAM Roles assumed through\n        IAM Identity center, values like\n        source-identity and\n        principal-tags will not propagate. For more\n        detail, see\n        Enable\n        and configure attributes for access control.\n      \n      \n         \n      \n          \n            Scope permissions based on groups and attributes:\n          \n          \n             \n             \n             \n          \n              \n                Consider including conditions in your permission\n                policies that compare the attributes of your principal\n                with the attributes of the resources being accessed. For\n                example, you can define a condition to allow access to a\n                resource only if the value of a\n                PrincipalTag condition key matches\n                the value of a ResourceTag key of the\n                same name.\n              \n            \n              \n                When defining ABAC policies, follow the guidance in the\n                ABAC\n                authorization best practices and examples.\n              \n            \n              \n                Regularly review and update your group and attribute\n                structure as your organization's needs evolve to ensure\n                optimal permissions management.\n              \n            \n        \n     \n   \n\n  Resources\n\n      \n    \n      Related best practices:\n    \n    \n       \n       \n       \n    \n        \n          SEC02-BP04 Rely on a\n          centralized identity provider\n        \n      \n        \n          SEC03-BP02 Grant\n          least privilege access\n        \n      \n        \n          COST02-BP04\n          Implement groups and roles\n        \n      \n    \n      Related documents:\n    \n    \n       \n       \n       \n       \n       \n    \n        \n          IAM\n          Best Practices\n        \n      \n        \n          Manage\n          Identities in IAM Identity Center\n        \n      \n        \n          What\n          Is ABAC for AWS?\n        \n      \n        \n          ABAC\n          In IAM Identity Center\n        \n      \n        \n          ABAC\n          Policy Examples\n        \n      \n    \n      Related videos:\n    \n    \n       \n       \n    \n        \n          Managing user\n          permissions at scale with AWS IAM Identity Center\n        \n      \n        \n          Mastering\n          identity at every layer of the cake\n        \n      \n  \u003cdiv\u003e\u003cdiv\u003e\u003cdiv\u003e\u003cdiv id=\"js_error_message\"\u003e\u003cp\u003e\u003cimg src=\"https://d1ge0kk1l5kms0.cloudfront.net/images/G/01/webservices/console/warning.png\" alt=\"Warning\" /\u003e \u003cstrong\u003eJavascript is disabled or is unavailable in your browser.\u003c/strong\u003e\u003c/p\u003e\u003cp\u003eTo use the Amazon Web Services Documentation, Javascript must be enabled. Please refer to your browser's Help pages for instructions.\u003c/p\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003eDocument ConventionsSEC02-BP05 Audit and rotate credentials periodically SEC 3. How do you manage permissions for people and machines? ",
  "SEC03-BP01 Define access requirementsEach component or resource of your workload needs to be accessed by administrators, end\n    users, or other components. Have a clear definition of who or what should have access to each\n    component, choose the appropriate identity type and method of authentication and\n    authorization.\n    Common anti-patterns:\n  \n     \n     \n     \n  \n      Hard-coding or storing secrets in your application. \n    \n      Granting custom permissions for each user. \n    \n      Using long-lived credentials. \n    \n    Level of risk exposed if this best practice\n    is not established: High\n  \n    \n    Implementation guidance\n    \n    \n     Each component or resource of your workload needs to be accessed by administrators, end\n      users, or other components. Have a clear definition of who or what should have access to each\n      component, choose the appropriate identity type and method of authentication and\n      authorization.\n    Regular access to AWS accounts within the organization should be provided using federated access or a centralized\n      identity provider. You should also centralize your identity management and ensure that there\n      is an established practice to integrate AWS access to your employee access lifecycle. For\n      example, when an employee changes to a job role with a different access level, their group\n      membership should also change to reflect their new access requirements.\n     When defining access requirements for non-human identities, determine which applications\n      and components need access and how permissions are granted. Using IAM roles built with the\n      least privilege access model is a recommended approach. AWS Managed\n        policies provide predefined IAM policies that cover most common use cases.\n    AWS services, such as AWS Secrets Manager and AWS Systems Manager\n        Parameter Store, can help decouple secrets from the application or workload securely\n      in cases where it's not feasible to use IAM roles. In Secrets Manager, you can establish\n      automatic rotation for your credentials. You can use Systems Manager to reference parameters in your\n      scripts, commands, SSM documents, configuration, and automation workflows by using the unique\n      name that you specified when you created the parameter.\n    \n      You can use\n      AWS       IAM Roles Anywhere to obtain\n      temporary\n        security credentials in IAM for workloads that run outside\n      of AWS. Your workloads can use the same\n      IAM\n        policies and\n      IAM\n        roles that you use with AWS applications to access AWS\n      resources.\n    \n    \n      Where possible, prefer short-term temporary credentials over\n      long-term static credentials. For scenarios in which you need \n      users with programmatic access and long-term credentials,\n      use access\n        key last used information to rotate and remove access keys.\n    \n    \n\n\nUsers need programmatic access if they want to interact with AWS outside of the AWS Management Console. The way to grant programmatic access depends on the type of user that's accessing AWS.\n    To grant users programmatic access, choose one of the following options.\n    \n                \n                    Which user needs programmatic access?\n                    To\n                    By\n                \n            \n                \n                    \n                        Workforce identity\n                        (Users managed in IAM Identity Center)\n                    \n                    Use temporary credentials to sign programmatic requests to the AWS CLI, AWS SDKs, or\n                        AWS APIs.\n                    \n                        Following the instructions for the interface that you want to use.\n                        \n                             \n\n\n                             \n                        \n                                For the AWS CLI, see Configuring the AWS CLI to use AWS IAM Identity Center in the\n                                        AWS Command Line Interface User Guide.\n                            \n                                For AWS SDKs, tools, and AWS APIs, see IAM Identity Center\n                                        authentication in the AWS SDKs and Tools Reference Guide.\n\n                            \n                    \n                \n                \n                    IAM\n                    Use temporary credentials to sign programmatic requests to the AWS CLI, AWS SDKs, or\n                        AWS APIs.\n                    Following the instructions in Using temporary\n                        credentials with AWS resources in the IAM User Guide.\n                \n                \n                    IAM\n                    (Not recommended)Use long-term credentials to sign programmatic requests\n                        to the AWS CLI, AWS SDKs, or AWS APIs.\n                    \n                        Following the instructions for the interface that you want to use.\n                        \n                             \n                             \n                             \n                        \n                                For the AWS CLI, see Authenticating using IAM user credentials in\n                                    the AWS Command Line Interface User Guide.\n                            \n                                For AWS SDKs and tools, see Authenticate using long-term credentials in the\n                                        AWS SDKs and Tools Reference Guide.\n                            \n                                For AWS APIs, see Managing access keys for\n                                    IAM users in the IAM User Guide.\n                            \n                    \n                \n            \n\n\n\n\n\n \n   \n    \n    Resources\n    \n    \n    \n      Related documents:\n    \n    \n       \n       \n       \n       \n       \n       \n       \n       \n       \n       \n    \n        \n          Attribute-based\n            access control (ABAC)\n        \n      \n        \n          AWS IAM Identity Center\n        \n      \n        \n          IAM\n            Roles Anywhere\n        \n      \n        \n          AWS Managed policies for IAM Identity Center\n        \n      \n        \n          AWS IAM policy conditions\n        \n      \n        \n          IAM use cases\n        \n      \n        \n          Remove\n            unnecessary credentials\n        \n      \n        \n          Working\n            with Policies\n        \n      \n        \n          How to control access to AWS resources based on AWS account, OU, or organization\n        \n      \n        \n          Identify, arrange, and manage secrets easily using enhanced search in AWS Secrets Manager\n        \n      \n    \n      Related videos:\n    \n    \n       \n       \n       \n    \n        \n          Become an IAM\n            Policy Master in 60 Minutes or Less\n        \n      \n        \n          Separation of\n            Duties, Least Privilege, Delegation, and CI/CD\n        \n      \n        \n          Streamlining\n            identity and access management for innovation\n        \n      \n  \u003cdiv\u003e\u003cdiv\u003e\u003cdiv\u003e\u003cdiv id=\"js_error_message\"\u003e\u003cp\u003e\u003cimg src=\"https://d1ge0kk1l5kms0.cloudfront.net/images/G/01/webservices/console/warning.png\" alt=\"Warning\" /\u003e \u003cstrong\u003eJavascript is disabled or is unavailable in your browser.\u003c/strong\u003e\u003c/p\u003e\u003cp\u003eTo use the Amazon Web Services Documentation, Javascript must be enabled. Please refer to your browser's Help pages for instructions.\u003c/p\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003eDocument Conventions SEC 3. How do you manage permissions for people and machines? SEC03-BP02 Grant least privilege access",
  "SEC03-BP02 Grant least privilege access\n    Grant only the access that users require to perform specific actions\n    on specific resources under specific conditions. Use group and\n    identity attributes to dynamically set permissions at scale, rather\n    than defining permissions for individual users. For example, you can\n    allow a group of developers access to manage only resources for\n    their project. This way, if a developer leaves the project, their\n    access is automatically revoked without changing the underlying\n    access policies.\n  \n    Desired outcome: Users have only\n    the minimum permissions required for their specific job functions.\n    You use separate AWS accounts to isolate developers from production\n    environments. When developers need to access production environments\n    for specific tasks, they are granted limited and controlled access\n    only for the duration of those tasks. Their production access is\n    immediately revoked after they complete the necessary work. You\n    conduct regular reviews of permissions and promptly revoke them when\n    no longer needed, such as when a user changes roles or leaves the\n    organization. You restrict administrator privileges to a small,\n    trusted group to reduce risk exposure. You give machine or system\n    accounts only the minimum permissions required to perform their\n    intended tasks.\n  \n    Common anti-patterns:\n  \n     \n     \n     \n     \n     \n  \n      \n        By default, you grant users administrator permissions.\n      \n    \n      You use the root user account for daily activities.\n\n    \n      \n        You create overly permissive policies without proper scoping.\n      \n    \n      \n        Your permissions reviews are infrequent, which leads to\n        permissions creep.\n      \n    \n      \n        You rely solely on attribute-based access control for\n        environment isolation or permissions management.\n      \n    \n    Level of risk exposed if this best practice is not established:\n    High \n\n  Implementation guidance\n\n      \n    \n      The principle of\n      least\n      privilege states that identities should only be permitted\n      to perform the smallest set of actions necessary to fulfill a\n      specific task. This balances usability, efficiency, and security.\n      Operating under this principle helps limit unintended access and\n      helps track who has access to what resources. IAM users and roles\n      have no permissions by default. The root user has full access by\n      default and should be tightly controlled, monitored, and used only\n      for\n      tasks\n      that require root access.\n    \n    \n      IAM policies are used to explicitly grant permissions to IAM roles\n      or specific resources. For example, identity-based policies can be\n      attached to IAM groups, while S3 buckets can be controlled by\n      resource-based policies.\n    \n    \n      When you create an IAM policy, you can specify the service\n      actions, resources, and conditions that must be true for AWS to\n      allow or deny access. AWS supports a variety of conditions to help\n      you scope down access. For example, by using the\n      PrincipalOrgID\n      condition\n      key, you can deny actions if the requestor isn't a part of\n      your AWS Organization.\n    \n    \n      You can also control requests that AWS services make on your\n      behalf, such as AWS CloudFormation creating an AWS Lambda\n      function, using the CalledVia condition key.\n      You can layer different policy types to establish defense-in-depth\n      and limit the overall permissions of your users. You can also\n      restrict what permissions can be granted and under what\n      conditions. For example, you can allow your workload teams to\n      create their own IAM policies for systems they build, but only if\n      they apply a\n      Permission\n      Boundary to limit the maximum permissions they can grant.\n    \n     \n\n  Implementation steps\n\n      \n      \n         \n         \n         \n         \n         \n         \n         \n         \n         \n         \n         \n         \n      \n          \n            Implement least privilege\n            policies: Assign access policies with least\n            privilege to IAM groups and roles to reflect the user's role\n            or function that you have defined.\n          \n        \n          \n            Isolate development and production\n            environments through separate AWS accounts: Use\n            separate AWS accounts for development and production\n            environments, and control access between them using\n            service\n            control policies, resource policies, and identity\n            policies.\n          \n        \n          \n            Base policies on API\n            usage: One way to determine the needed\n            permissions is to review AWS CloudTrail logs. You can use\n            this review to create permissions tailored to the actions\n            that the user actually performs within AWS.\n            IAM Access Analyzer can\n            automatically\n            generate an IAM policy based on access activity. You\n            can use IAM Access Advisor at the organization or account\n            level to\n            track\n            the last accessed information for a particular\n            policy.\n          \n        \n          \n            Consider using\n            AWS             managed policies for job functions: When\n            you begin to create fine-grained permissions policies, it\n            can be helpful to use AWS managed policies for common job\n            roles, such as billing, database administrators, and data\n            scientists. These policies can help narrow the access that\n            users have while you determine how to implement the least\n            privilege policies.\n          \n        \n          \n            Remove unnecessary\n            permissions: Detect and remove unused IAM\n            entities, credentials, and permissions to achieve the\n            principle of least privilege. You can use\n            IAM Access Analyzer to identify external and unused\n            access, and\n            IAM Access Analyzer policy generation can help fine-tune\n            permissions policies.\n          \n        \n          \n            Ensure that users have limited\n            access to production environments: Users should\n            only have access to production environments with a valid use\n            case. After the user performs the specific tasks that\n            required production access, access should be revoked.\n            Limiting access to production environments helps prevent\n            unintended production-impacting events and lowers the scope\n            of impact of unintended access.\n          \n        \n          \n            Consider permissions\n            boundaries: A\n            permissions\n            boundary is a feature for using a managed policy that\n            sets the maximum permissions that an identity-based policy\n            can grant to an IAM entity. An entity's permissions boundary\n            allows it to perform only the actions that are allowed by\n            both its identity-based policies and its permissions\n            boundaries.\n          \n        \n          \n            Refine access using attribute-based\n            access control and resource tags\n            Attribute-based\n            access control (ABAC) using resource tags can be used\n            to refine permissions when supported. You can use an ABAC\n            model that compares principal tags to resource tags to\n            refine access based on custom dimensions you define. This\n            approach can simplify and reduce the number of permission\n            policies in your organization.\n          \n          \n             \n          \n              \n                It is recommended that ABAC only be used for access\n                control when both the principals and resources are owned\n                by your AWS Organization. External parties may use the\n                same tag names and values as your organization for their\n                own principals and resources. If you rely solely on\n                these name-value pairs for granting access to external\n                party principals or resources, you may provide\n                unintended permissions.\n              \n            \n        \n          \n            Use service control policies for AWS Organizations:\n            Service\n            control policies centrally control the maximum\n            available permissions for member accounts in your\n            organization. Importantly, you can use service control\n            policies to restrict root user permissions in member\n            accounts. Also consider using AWS Control Tower, which\n            provides prescriptive managed controls that enrich AWS Organizations. You can also define your own controls within\n            Control Tower.\n          \n        \n          \n            Establish a user lifecycle policy\n            for your organization: User lifecycle policies\n            define tasks to perform when users are onboarded onto AWS,\n            change job role or scope, or no longer need access to AWS.\n            Perform permission reviews during each step of a user's\n            lifecycle to verify that permissions are properly\n            restrictive and to avoid permissions creep.\n          \n        \n          \n            Establish a regular schedule to\n            review permissions and remove any unneeded\n            permissions: You should regularly review user\n            access to verify that users do not have overly permissive\n            access.\n            AWS Config and IAM Access Analyzer can help during user\n            permissions audits.\n          \n        \n          \n            Establish a job role\n            matrix: A job role matrix visualizes the various\n            roles and access levels required within your AWS footprint.\n            With a job role matrix, you can define and separate\n            permissions based on user responsibilities within your\n            organization. Use groups instead of applying permissions\n            directly to individual users or roles.\n          \n        \n     \n   \n\n  Resources\n\n      \n    \n      Related documents:\n    \n    \n       \n       \n       \n       \n       \n       \n       \n       \n       \n       \n       \n       \n       \n       \n       \n       \n       \n    \n        \n          Grant\n          least privilege\n        \n      \n        \n          Permissions\n          boundaries for IAM entities\n        \n      \n        \n          Techniques\n          for writing least privilege IAM policies\n        \n      \n        \n          IAM Access Analyzer makes it easier to implement least privilege\n          permissions by generating IAM policies based on access\n          activity\n        \n      \n        \n          Delegate\n          permission management to developers by using IAM permissions\n          boundaries\n        \n      \n        \n          Refining\n          Permissions using last accessed information\n        \n      \n        \n          IAM\n          policy types and when to use them\n        \n      \n        \n          Testing\n          IAM policies with the IAM policy simulator\n        \n      \n        \n          Guardrails\n          in AWS Control Tower\n        \n      \n        \n          Zero\n          Trust architectures: An AWS perspective\n        \n      \n        \n          How\n          to implement the principle of least privilege with\n          CloudFormation StackSets\n        \n      \n        \n          Attribute-based\n          access control (ABAC)\n        \n      \n        \n          Reducing\n          policy scope by viewing user activity\n        \n      \n        \n          View\n          role access\n        \n      \n        \n          Use\n          Tagging to Organize Your Environment and Drive\n          Accountability\n        \n      \n        \n          AWS           Tagging Strategies\n        \n      \n        \n          Tagging\n          AWS resources\n        \n      \n    \n      Related videos:\n    \n    \n       \n       \n    \n        \n          Next-generation\n          permissions management\n        \n      \n        \n          Zero\n          Trust: An AWS perspective\n        \n      \n  \u003cdiv\u003e\u003cdiv\u003e\u003cdiv\u003e\u003cdiv id=\"js_error_message\"\u003e\u003cp\u003e\u003cimg src=\"https://d1ge0kk1l5kms0.cloudfront.net/images/G/01/webservices/console/warning.png\" alt=\"Warning\" /\u003e \u003cstrong\u003eJavascript is disabled or is unavailable in your browser.\u003c/strong\u003e\u003c/p\u003e\u003cp\u003eTo use the Amazon Web Services Documentation, Javascript must be enabled. Please refer to your browser's Help pages for instructions.\u003c/p\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003eDocument ConventionsSEC03-BP01 Define access requirementsSEC03-BP03 Establish emergency access process",
  "SEC03-BP03 Establish emergency access\n      process\n    Create a process that allows for emergency access to your workloads\n    in the unlikely event of an issue with your centralized identity\n    provider.\n  \n    You must design processes for different failure modes that may\n    result in an emergency event. For example, under normal\n    circumstances, your workforce users federate to the cloud using a\n    centralized identity provider\n    (SEC02-BP04)\n    to manage their workloads. However, if your centralized identity\n    provider fails, or the configuration for federation in the cloud is\n    modified, then your workforce users may not be able to federate into\n    the cloud. An emergency access process allows authorized\n    administrators to access your cloud resources through alternate\n    means (such as an alternate form of federation or direct user\n    access) to fix issues with your federation configuration or your\n    workloads. The emergency access process is used until the normal\n    federation mechanism is restored.\n  \n    Desired outcome:\n  \n     \n     \n     \n     \n  \n      \n        You have defined and documented the failure modes that count as\n        an emergency: consider your normal circumstances and the systems\n        your users depend on to manage their workloads. Consider how\n        each of these dependencies can fail and cause an emergency\n        situation. You may find the questions and best practices in the\n        Reliability\n          pillar useful to identify failure modes and architect\n        more resilient systems to minimize the likelihood of failures.\n      \n    \n      \n        You have documented the steps that must be followed to confirm a\n        failure as an emergency. For example, you can require your\n        identity administrators to check the status of your primary and\n        standby identity providers and, if both are unavailable, declare\n        an emergency event for identity provider failure.\n      \n    \n      \n        You have defined an emergency access process specific to each\n        type of emergency or failure mode. Being specific can reduce the\n        temptation on the part of your users to overuse a general\n        process for all types of emergencies. Your emergency access\n        processes describe the circumstances under which each process\n        should be used, and conversely situations where the process\n        should not be used and points to alternate processes that may\n        apply.\n      \n    \n      \n        Your processes are well-documented with detailed instructions\n        and playbooks that can be followed quickly and efficiently.\n        Remember that an emergency event can be a stressful time for\n        your users and they may be under extreme time pressure, so\n        design your process to be as simple as possible.\n      \n    \n    Common anti-patterns:\n  \n     \n     \n     \n     \n  \n      \n        You do not have well-documented and well-tested emergency access\n        processes. Your users are unprepared for an emergency and follow\n        improvised processes when an emergency event arises.\n      \n    \n      \n        Your emergency access processes depend on the same systems (such\n        as a centralized identity provider) as your normal access\n        mechanisms. This means that the failure of such a system may\n        impact both your normal and emergency access mechanisms and\n        impair your ability to recover from the failure.\n      \n    \n      \n        Your emergency access processes are used in non-emergency\n        situations. For example, your users frequently misuse emergency\n        access processes as they find it easier to make changes directly\n        than submit changes through a pipeline.\n      \n    \n      \n        Your emergency access processes do not generate sufficient logs\n        to audit the processes, or the logs are not monitored to alert\n        for potential misuse of the processes.\n      \n    \n    Benefits of establishing this best\n      practice:\n  \n     \n     \n  \n      \n        By having well-documented and well-tested emergency access\n        processes, you can reduce the time taken by your users to\n        respond to and resolve an emergency event. This can result in\n        less downtime and higher availability of the services you\n        provide to your customers.\n      \n    \n      \n        You can track each emergency access request and detect and alert\n        on unauthorized attempts to misuse the process for non-emergency\n        events.\n      \n    \n    Level of risk exposed if this best practice\n      is not established: Medium\n  \n\n  Implementation guidance \n    \n      This section provides guidance for creating emergency access\n      processes for several failure modes related to workloads deployed\n      on AWS, starting with common guidance that applies to all failure\n      modes and followed by specific guidance based on the type of\n      failure mode.\n    \n    \n      Common guidance for all failure\n      modes\n    \n    \n      Consider the following as you design an emergency access process\n      for a failure mode:\n    \n    \n       \n       \n       \n       \n       \n       \n       \n    \n        \n          Document the pre-conditions and assumptions of the process:\n          when the process should be used and when it should not be\n          used. It helps to detail the failure mode and document\n          assumptions, such as the state of other related systems. For\n          example, the process for the Failure Mode 2 assumes that the identity provider is\n          available, but the configuration on AWS is modified or has\n          expired.\n        \n      \n        \n          Pre-create resources needed by the emergency access process\n          (SEC10-BP05).\n          For example, pre-create the emergency access AWS account with\n          IAM users and roles, and the cross-account IAM roles in all\n          the workload accounts. This verifies that these resources are\n          ready and available when an emergency event happens. By\n          pre-creating resources, you do not have a dependency on AWS\n          control plane\n          APIs (used to create and modify AWS resources) that may be\n          unavailable in an emergency. Further, by pre-creating IAM\n          resources, you do not need to account for\n          potential\n          delays due to eventual consistency.\n        \n      \n        \n          Include emergency access processes as part of your incident\n          management plans\n          (SEC10-BP02).\n          Document how emergency events are tracked and communicated to\n          others in your organization such as peer teams, your\n          leadership, and, when applicable, externally to your customers\n          and business partners.\n        \n      \n        \n          Define the emergency access request process in your existing\n          service request workflow system if you have one. Typically,\n          such workflow systems allow you to create intake forms to\n          collect information about the request, track the request\n          through each stage of the workflow, and add both automated and\n          manual approval steps. Relate each request with a\n          corresponding emergency event tracked in your incident\n          management system. Having a uniform system for emergency\n          accesses allows you to track those requests in a single\n          system, analyze usage trends, and improve your processes.\n        \n      \n        \n          Verify that your emergency access processes can only be\n          initiated by authorized users and require approvals from the\n          user's peers or management as appropriate. The approval\n          process should operate effectively both inside and outside\n          business hours. Define how requests for approval allow\n          secondary approvers if the primary approvers are unavailable\n          and are escalated up your management chain until approved.\n        \n      \n        \n          Implement robust logging, monitoring, and alerting mechanisms\n          for the emergency access process and mechanisms. Generate\n          detailed audit logs for all successful and failed attempts to\n          gain emergency access. Correlate the activity with ongoing\n          emergency events from your incident management system, and\n          initiate alerts when actions occur outside of expected time\n          periods or when the emergency access account is used during\n          normal operations. The emergency access account should only be\n          accessed during emergencies, as break-glass procedures can be\n          considered a backdoor. Integrate with your security\n          information and event management (SIEM) tool or\n          AWS Security Hub to report and audit all activities during\n          the emergency access period. Upon returning to normal\n          operations, automatically rotate the emergency access\n          credentials, and notify the relevant teams.\n        \n      \n        \n          Test emergency access processes periodically to verify that\n          the steps are clear and grant the correct level of access\n          quickly and efficiently. Your emergency access processes\n          should be tested as part of incident response simulations\n          (SEC10-BP07)\n          and disaster recovery tests\n          (REL13-BP03).\n        \n      \n    \n      Failure Mode 1: Identity provider used to\n      federate to AWS is unavailable\n    \n    \n      As described in\n      SEC02-BP04\n      Rely on a centralized identity provider, we recommend\n      relying on a centralized identity provider to federate your\n      workforce users to grant access to AWS accounts. You can federate\n      to multiple AWS accounts in your AWS organization using IAM Identity Center, or you can federate to individual AWS accounts\n      using IAM. In both cases, workforce users authenticate with\n      your centralized identity provider before being redirected to an\n      AWS sign-in endpoint to single sign-on.\n    \n    \n      In the unlikely event that your centralized identity provider is\n      unavailable, your workforce users can't federate to AWS accounts\n      or manage their workloads. In this emergency event, you can\n      provide an emergency access process for a small set of\n      administrators to access AWS accounts to perform critical tasks\n      that cannot wait until your centralized identity providers are\n      back online. For example, your identity provider is unavailable\n      for 4 hours and during that period you need to modify the upper\n      limits of an Amazon EC2 Auto Scaling group in a Production account to\n      handle an unexpected spike in customer traffic. Your emergency\n      administrators should follow the emergency access process to gain\n      access to the specific production AWS account and make the\n      necessary changes.\n    \n    \n      The emergency access process relies on a pre-created emergency\n      access AWS account that is used solely for emergency access and\n      has AWS resources (such as IAM roles and IAM users) to support the\n      emergency access process. During normal operations, no one should\n      access the emergency access account and you must monitor and alert\n      on the misuse of this account (for more detail, see the preceding\n      Common guidance section).\n    \n    \n      The emergency access account has emergency access IAM roles with\n      permissions to assume cross-account roles in the AWS accounts that\n      require emergency access. These IAM roles are pre-created and\n      configured with trust policies that trust the emergency account's\n      IAM roles.\n    \n    \n      The emergency access process can use one of the following\n      approaches:\n    \n    \n       \n       \n    \n        \n          You can pre-create a set of\n          IAM users for your emergency administrators in the\n          emergency access account with associated strong passwords and\n          MFA tokens. These IAM users have permissions to assume the IAM\n          roles that then allow cross-account access to the AWS account\n          where emergency access is required. We recommend creating as\n          few such users as possible and assigning each user to a single\n          emergency administrator. During an emergency, an emergency\n          administrator user signs into the emergency access account\n          using their password and MFA token code, switches to the\n          emergency access IAM role in the emergency account, and\n          finally switches to the emergency access IAM role in the\n          workload account to perform the emergency change action. The\n          advantage of this approach is that each IAM user is assigned\n          to one emergency administrator and you can know which user\n          signed-in by reviewing CloudTrail events. The disadvantage is\n          that you have to maintain multiple IAM users with their\n          associated long-lived passwords and MFA tokens.\n        \n      \n        \n          You can use the emergency access\n          AWS account root user to sign into the emergency access\n          account, assume the IAM role for emergency access, and assume\n          the cross-account role in the workload account. We recommend\n          setting a strong password and multiple MFA tokens for the root\n          user. We also recommend storing the password and the MFA\n          tokens in a secure enterprise credential vault that enforces\n          strong authentication and authorization. You should secure the\n          password and MFA token reset factors: set the email address\n          for the account to an email distribution list that is\n          monitored by your cloud security administrators, and the phone\n          number of the account to a shared phone number that is also\n          monitored by security administrators. The advantage of this\n          approach is that there is one set of root user credentials to\n          manage. The disadvantage is that since this is a shared user,\n          multiple administrators have ability to sign in as the root\n          user. You must audit your enterprise vault log events to\n          identify which administrator checked out the root user\n          password.\n        \n      \n    \n      Failure Mode 2: Identity provider\n      configuration on AWS is modified or has expired\n    \n    \n      To allow your workforce users to federate to AWS accounts, you can\n      configure the IAM Identity Center with an external identity\n      provider or create an IAM Identity Provider\n      (SEC02-BP04).\n      Typically, you configure these by importing a SAML meta-data XML\n      document provided by your identity provider. The meta-data XML\n      document includes a X.509 certificate corresponding to a private\n      key that the identity provider uses to sign its SAML assertions.\n    \n    \n      These configurations on the AWS-side may be modified or deleted by\n      mistake by an administrator. In another scenario, the X.509\n      certificate imported into AWS may expire and a new meta-data XML\n      with a new certificate has not yet been imported into AWS. Both\n      scenarios can break federation to AWS for your workforce users,\n      resulting in an emergency.\n    \n    \n      In such an emergency event, you can provide your identity\n      administrators access to AWS to fix the federation issues. For\n      example, your identity administrator uses the emergency access\n      process to sign into the emergency access AWS account, switches to\n      a role in the Identity Center administrator account, and updates\n      the external identity provider configuration by importing the\n      latest SAML meta-data XML document from your identity provider to\n      re-enable federation. Once federation is fixed, your workforce\n      users continue to use the normal operating process to federate\n      into their workload accounts.\n    \n    \n      You can follow the approaches detailed in the previous Failure\n      Mode 1 to create an emergency access\n      process. You can grant least-privilege permissions to your\n      identity administrators to access only the Identity Center\n      administrator account and perform actions on Identity Center in\n      that account.\n    \n    \n      Failure Mode 3: Identity Center\n      disruption\n    \n    \n      In the unlikely event of an IAM Identity Center or AWS Region\n      disruption, we recommend that you set up a configuration that you\n      can use to provide temporary access to the AWS Management Console.\n    \n    \n      The emergency access process uses direct federation from your\n      identity provider to IAM in an emergency account. For detail\n      on the process and design considerations, see\n      Set\n      up emergency access to the AWS Management Console.\n    \n     \n\n  Implementation steps \n      \n        Common steps for all failure\n        modes\n      \n      \n         \n      \n          \n            Create an AWS account dedicated to emergency access\n            processes. Pre-create the IAM resources needed in the\n            account such as IAM roles or IAM users, and optionally IAM\n            Identity Providers. Additionally, pre-create cross-account\n            IAM roles in the workload AWS accounts with trust\n            relationships with corresponding IAM roles in the emergency\n            access account. You can use\n            AWS CloudFormation\n            StackSets with AWS Organizations to create such\n            resources in the member accounts in your organization.\n          \n        \n      \n         \n         \n         \n      \n          \n            Create AWS Organizations\n            service\n            control policies (SCPs) to deny the deletion and\n            modification of the cross-account IAM roles in the member\n            AWS accounts.\n          \n        \n          \n            Enable CloudTrail for the emergency access AWS account and\n            send the trail events to a central S3 bucket in your log\n            collection AWS account. If you are using AWS Control Tower\n            to set up and govern your AWS multi-account environment,\n            then every account you create using AWS Control Tower or enroll\n            in AWS Control Tower has CloudTrail enabled by default and sent\n            to an S3 bucket in a dedicated log archive AWS account.\n          \n        \n          \n            Monitor the emergency access account for activity by\n            creating EventBridge rules that match on console login and\n            API activity by the emergency IAM roles. Send notifications\n            to your security operations center when activity happens\n            outside of an ongoing emergency event tracked in your\n            incident management system.\n          \n        \n      \n        Additional steps for Failure Mode 1:\n        Identity provider used to federate to AWS is unavailable and\n        Failure Mode 2: Identity provider configuration on AWS is\n        modified or has expired\n      \n      \n         \n      \n          \n            Pre-create resources depending on the mechanism you choose\n            for emergency access:\n          \n          \n             \n             \n          \n              \n                Using IAM users:\n                pre-create the IAM users with strong passwords and\n                associated MFA devices.\n              \n            \n              \n                Using the emergency account root\n                user: configure the root user with a strong\n                password and store the password in your enterprise\n                credential vault. Associate multiple physical MFA\n                devices with the root user and store the devices in\n                locations that can be accessed quickly by members of\n                your emergency administrator team.\n              \n            \n        \n      \n        Additional steps for Failure Mode 3:\n        Identity center disruption\n      \n      \n         \n         \n         \n      \n          \n            As detailed in\n            Set\n            up emergency access to the AWS Management Console, in\n            the emergency access AWS account, create an IAM Identity\n            Provider to enable direct SAML federation from your identity\n            provider.\n          \n        \n          \n            Create emergency operations groups in your IdP with no\n            members.\n          \n        \n          \n            Create IAM roles corresponding to the emergency operations\n            groups in the emergency access account.\n          \n        \n     \n   \n    \n    Resources \n    \n    \n    \n    \n      Related Well-Architected best\n        practices:\n    \n    \n    \n       \n       \n       \n       \n    \n        \n          SEC02-BP04\n            Rely on a centralized identity provider\n        \n      \n        \n          SEC03-BP02\n            Grant least privilege access\n        \n      \n        \n          SEC10-BP02\n            Develop incident management plans\n        \n      \n        \n          SEC10-BP07\n            Run game days\n        \n      \n    \n    \n      Related documents:\n    \n    \n    \n       \n       \n       \n    \n        \n          Set\n            up emergency access to the AWS Management Console\n        \n      \n        \n          Enabling\n            SAML 2.0 federated users to access the AWS Management Console\n        \n      \n        \n          Break\n            glass access\n        \n      \n    \n    \n      Related videos:\n    \n    \n    \n       \n       \n    \n        \n          AWS re:Invent\n            2022 - Simplify your existing workforce access with IAM Identity Center\n        \n      \n        \n          AWS re:Inforce\n            2022 - AWS Identity and Access Management (IAM) deep dive\n        \n      \n    \n    \n      Related examples:\n    \n    \n    \n       \n       \n       \n    \n        \n          AWS           Break Glass Role\n        \n      \n        \n          AWS           customer playbook framework\n        \n      \n        \n          AWS           incident response playbook samples\n        \n      \n  \u003cdiv\u003e\u003cdiv\u003e\u003cdiv\u003e\u003cdiv id=\"js_error_message\"\u003e\u003cp\u003e\u003cimg src=\"https://d1ge0kk1l5kms0.cloudfront.net/images/G/01/webservices/console/warning.png\" alt=\"Warning\" /\u003e \u003cstrong\u003eJavascript is disabled or is unavailable in your browser.\u003c/strong\u003e\u003c/p\u003e\u003cp\u003eTo use the Amazon Web Services Documentation, Javascript must be enabled. Please refer to your browser's Help pages for instructions.\u003c/p\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003eDocument ConventionsSEC03-BP02 Grant least privilege accessSEC03-BP04 Reduce permissions continuously",
  "SEC03-BP04 Reduce permissions continuouslyAs your teams determine what access is required, remove unneeded permissions and establish review processes to achieve least privilege permissions. Continually monitor and remove unused identities and permissions for both human and machine access.\n    Desired outcome: Permission policies should adhere to the least privilege principle. As job duties and roles become better defined, your permission policies need to be reviewed to remove unnecessary permissions. This approach lessens the scope of impact should credentials be inadvertently exposed or otherwise accessed without authorization.\n  \n    Common anti-patterns:\n  \n     \n     \n     \n  \n      \n        Defaulting to granting users administrator permissions.\n      \n    \n      \n        Creating policies that are overly permissive, but without full administrator privileges.\n      \n    \n      \n        Keeping permission policies after they are no longer needed.\n      \n    \n    Level of risk exposed if this best practice\n    is not established: Medium\n  \n\n    Implementation guidance \n    \n      As teams and projects are just getting started, permissive permission policies might be used to inspire innovation and agility. For example, in a development or test environment, developers can be given access to a broad set of AWS services. We recommend that you evaluate access continuously and restrict access to only those services and service actions that are necessary to complete the current job. We recommend this evaluation for both human and machine identities. Machine identities, sometimes called system or service accounts, are identities that give AWS access to applications or servers. This access is especially important in a production environment, where overly permissive permissions can have a broad impact and potentially expose customer data.\n    \n    \n      AWS provides multiple methods to help identify unused users, roles, permissions, and credentials. AWS can also help analyze access activity of IAM users and roles, including associated access keys, and access to AWS resources such as objects in Amazon S3 buckets. AWS Identity and Access Management Access Analyzer policy generation can assist you in creating restrictive permission policies based on the actual services and actions a principal interacts with.  Attribute-based access control (ABAC) can help simplify permissions management, as you can provide permissions to users using their attributes instead of attaching permissions policies directly to each user.\n    \n     \n\n      Implementation steps\n\n      \n         \n         \n         \n         \n         \n      \n          \n            Use AWS Identity and Access Management Access Analyzer: IAM Access Analyzer helps identify resources in your organization and accounts, such as Amazon Simple Storage Service (Amazon S3) buckets or IAM roles that are shared with an external entity.\n        \n        \n          \n            Use IAM Access Analyzer policy generation: IAM Access Analyzer policy generation helps you create fine-grained permission policies based on an IAM user or role’s access activity.\n        \n        \n          \n            Test permissions across lower\n            environments before production: Start by using\n          the\n          less\n            critical sandbox and development environments to test\n          the permissions required for various job functions using IAM Access Analyzer. Then, progressively tighten and validate\n          these permissions across the testing, quality assurance, and\n          staging environments before applying them to production. The\n          lower environments can have more relaxed permissions\n          initially, as service control policies (SCPs) enforce\n          guardrails by limiting the maximum permissions granted.\n        \n        \n          \n            Determine an acceptable timeframe and usage policy for IAM users and roles: Use the last accessed timestamp to identify unused users and roles and remove them. Review service and action last accessed information to identify and scope permissions for specific users and roles. For example, you can use last accessed information to identify the specific Amazon S3 actions that your application role requires and restrict the role’s access to only those actions. Last accessed information features are available in the AWS Management Console and programmatically allow you to incorporate them into your infrastructure workflows and automated tools. \n        \n        \n          \n            Consider logging data events in AWS CloudTrail: By default, CloudTrail does not log data events such as Amazon S3 object-level activity (for example, GetObject and DeleteObject) or Amazon DynamoDB table activities (for example, PutItem and DeleteItem). Consider using logging for these events to determine what users and roles need access to specific Amazon S3 objects or DynamoDB table items. \n        \n        \n     \n   \n\n    Resources\n    \n      Related documents:\n    \n    \n       \n       \n       \n       \n       \n       \n       \n    \n        \n          Grant least privilege\n        \n      \n        \n          Remove unnecessary credentials\n        \n      \n        \n          What is AWS CloudTrail?\n        \n      \n        \n          Working with Policies\n        \n      \n        \n          Logging and monitoring DynamoDB\n        \n      \n        \n          Using CloudTrail event logging for Amazon S3 buckets and objects\n        \n      \n        \n          Getting credential reports for your AWS account\n      \n    \n      Related videos:\n    \n    \n       \n       \n       \n    \n        \n          Become an IAM Policy Master in 60 Minutes or\n            Less\n        \n      \n        \n          Separation of Duties, Least Privilege,\n            Delegation, and CI/CD\n        \n      \n        AWS re:Inforce 2022 - AWS Identity and Access Management (IAM) deep dive\n        \n      \n  \u003cdiv\u003e\u003cdiv\u003e\u003cdiv\u003e\u003cdiv id=\"js_error_message\"\u003e\u003cp\u003e\u003cimg src=\"https://d1ge0kk1l5kms0.cloudfront.net/images/G/01/webservices/console/warning.png\" alt=\"Warning\" /\u003e \u003cstrong\u003eJavascript is disabled or is unavailable in your browser.\u003c/strong\u003e\u003c/p\u003e\u003cp\u003eTo use the Amazon Web Services Documentation, Javascript must be enabled. Please refer to your browser's Help pages for instructions.\u003c/p\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003eDocument ConventionsSEC03-BP03 Establish emergency access processSEC03-BP05 Define permission guardrails for your organization",
  "SEC03-BP05 Define permission guardrails for\n      your organization\n    Use permission guardrails to reduce the scope of available\n    permissions that can be granted to principals. The permission policy\n    evaluation chain includes your guardrails to determine the\n    effective permissions of a principal when\n    making authorization decisions.  You can define guardrails using a\n    layer-based approach. Apply some guardrails broadly across your\n    entire organization and apply others granularly to temporary access\n    sessions.\n  \n    Desired outcome: You have clear\n    isolation of environments using separate AWS accounts.  Service\n    control policies (SCPs) are used to define organization-wide\n    permission guardrails. Broader guardrails are set at the hierarchy\n    levels closest to your organization root, and more strict guardrails\n    are set closer to the level of individual accounts.\n  \n    Where supported, resource policies define the conditions that a\n    principal must satisfy to gain access to a resource. Resource\n    policies also scope down the set of allowable actions, where\n    appropriate. Permission boundaries are placed on principals that\n    manage workload permissions, delegating permission management to\n    individual workload owners.\n  \n    Common anti-patterns:\n  \n     \n     \n     \n     \n  \n      \n        Creating member AWS accounts within an\n        AWS         Organization, but not using SCPs to restrict the use and\n        permissions available to their root credentials.\n      \n    \n      \n        Assigning permissions based on least privilege, but not placing\n        guardrails on the maximum set of permissions that can be\n        granted.\n      \n    \n      \n        Relying on the implicit deny foundation of\n        AWS IAM to restrict permissions, trusting that policies will not\n        grant an undesired explicit\n        allow permission.\n      \n    \n      \n        Running multiple workload environments in the same AWS account,\n        and then relying on mechanisms such as VPCs, tags, or resource\n        policies to enforce permission boundaries.\n      \n    \n    Benefits of establishing this best\n    practice: Permission guardrails help build confidence\n    that undesired permissions cannot be granted, even when a permission\n    policy attempts to do so.  This can simplify defining and managing\n    permissions by reducing the maximum scope of permissions needing\n    consideration.\n  \n    Level of risk exposed if this best practice\n    is not established: Medium\n  \n\n  Implementation guidance\n\n      \n    \n      We recommend you use a layer-based approach to define permission\n      guardrails for your organization. This approach systematically\n      reduces the maximum set of possible permissions as additional\n      layers are applied. This helps you grant access based on the\n      principle of least privilege, reducing the risk of unintended\n      access due to policy misconfiguration.\n    \n    \n      The first step to establish permission guardrails is to isolate\n      your workloads and environments into separate AWS accounts.\n      Principals from one account cannot access resources in another\n      account without explicit permission to do so, even when both\n      accounts are in the same AWS organization or under the same\n      organizational\n      unit (OU). You can use OUs to group accounts you want to\n      administer as a single unit.   \n    \n    \n      The next step is to reduce the maximum set of permissions that you\n      can grant to principals within the member accounts of your\n      organization. You can\n      use service\n      control policies (SCPs) for this purpose, which you can\n      apply to either an OU or an account. SCPs can enforce common\n      access controls, such as restricting access to specific AWS Regions, help prevent resources from being deleted, or disabling\n      potentially risky service actions. SCPs that you apply to the root\n      of your organization only affect its member accounts, not the\n      management account.  SCPs only govern the principals within your\n      organization. Your SCPs don't govern principals outside your\n      organization that are accessing your resources.\n    \n    \n      If you are using\n      AWS Control Tower, you can leverage its\n      controls\n      and\n      landing\n      zones as the foundation for your permission guardrails and\n      multi-account environment. The landing zones provide a\n      pre-configured, secure baseline environment with separate accounts\n      for different workloads and applications. The guardrails enforce\n      mandatory controls around security, operations, and compliance\n      through a combination of Service Control Policies (SCPs), AWS Config rules, and other configurations. However, when using\n      Control Tower guardrails and landing zones alongside custom\n      Organization SCPs, it's crucial to follow the best practices\n      outlined in the AWS documentation to avoid conflicts and ensure\n      proper governance. Refer to the\n      AWS Control Tower guidance for AWS Organizations for detailed\n      recommendations on managing SCPs, accounts, and organizational\n      units (OUs) within a Control Tower environment.\n    \n    \n      By adhering to these guidelines, you can effectively leverage\n      Control Tower's guardrails, landing zones, and custom SCPs while\n      mitigating potential conflicts and ensuring proper governance and\n      control over your multi-account AWS environment.\n    \n    \n      A further step is to use\n      IAM\n      resource policies to scope the available actions that you\n      can take on the resources they govern, along with any conditions\n      that the acting principal must meet. This can be as broad as\n      allowing all actions so long as the principal is part of your\n      organization (using the\n      PrincipalOrgId condition\n      key), or as granular as only allowing specific actions by a\n      specific IAM role. You can take a similar approach with conditions\n      in IAM role trust policies.  If a resource or role trust policy\n      explicitly names a principal in the same account as the role or\n      resource it governs, that principal does not need an attached IAM\n      policy that grants the same permissions.  If the principal is in a\n      different account from the resource, then the principal does need\n      an attached IAM policy that grants those permissions.\n    \n    \n      Often, a workload team will want to manage the permissions their\n      workload requires.  This may require them to create new IAM roles\n      and permission policies.  You can capture the maximum scope of\n      permissions the team is allowed to grant in an\n      IAM\n      permission boundary, and associate this document to an IAM\n      role the team can then use to manage their IAM roles and\n      permissions.  This approach can provide them the flexibility to\n      complete their work while mitigating risks of having IAM\n      administrative access.\n    \n    \n      A more granular step is to implement privileged access\n      management (PAM) and temporary elevated\n      access management (TEAM) techniques.  One example of\n      PAM is to require principals to perform multi-factor\n      authentication before taking privileged actions.  For more\n      information, see\n      Configuring\n      MFA-protected API access. TEAM requires a solution that\n      manages the approval and timeframe that a principal is allowed to\n      have elevated access.  One approach is to temporarily add the\n      principal to the role trust policy for an IAM role that has\n      elevated access.  Another approach is to, under normal operation,\n      scope down the permissions granted to a principal by an IAM role\n      using a\n      session\n      policy, and then temporarily lift this restriction during\n      the approved time window. To learn more about solutions that AWS\n      and select partners validated, see\n      Temporary\n      elevated access.\n    \n     \n\n  Implementation steps\n\n      \n      \n         \n         \n         \n         \n         \n      \n          \n            Isolate your workloads and environments into separate AWS accounts.\n          \n        \n          \n            Use SCPs to reduce the maximum set of permissions that can\n            be granted to principals within the member accounts of your\n            organization.\n          \n          \n             \n             \n          \n              \n                When defining SCPs to reduce the maximum set of\n                permissions that can be granted to principals within\n                your organization's member accounts, you can choose\n                between an allow list or\n                deny list approach. The allow list\n                strategy explicitly specifies the access that is allowed\n                and implicitly blocks all other access. The deny list\n                strategy explicitly specifies the access that isn't\n                allowed and allows all other access by default. Both\n                strategies have their advantages and trade-offs, and the\n                appropriate choice depends on your organization's\n                specific requirements and risk model. For more detail,\n                see\n                Strategy\n                for using SCPs.\n              \n            \n              \n                Additionally, review the\n                service\n                control policy examples to understand how to\n                construct SCPs effectively.\n              \n            \n        \n          \n            Use IAM resource policies to scope down and specify\n            conditions for permitted actions on resources.  Use\n            conditions in IAM role trust policies to create restrictions\n            on assuming roles.\n          \n        \n          \n            Assign IAM permission boundaries to IAM roles that workload\n            teams can then use to manage their own workload IAM roles\n            and permissions.\n          \n        \n          \n            Evaluate PAM and TEAM solutions based on your needs.\n          \n        \n     \n   \n\n  Resources\n\n      \n    \n      Related documents:\n    \n    \n       \n       \n       \n    \n        \n          Data\n          perimeters on AWS\n        \n      \n        \n          Establish\n          permissions guardrails using data perimeters\n        \n      \n        \n          Policy\n          evaluation logic\n        \n      \n    \n      Related examples:\n    \n    \n       \n    \n        \n          Service\n          control policy examples\n        \n      \n    \n      Related tools:\n    \n    \n       \n       \n    \n        \n          AWS           Solution: Temporary Elevated Access Management\n        \n      \n        \n          Validated\n          security partner solutions for TEAM\n        \n      \n  \u003cdiv\u003e\u003cdiv\u003e\u003cdiv\u003e\u003cdiv id=\"js_error_message\"\u003e\u003cp\u003e\u003cimg src=\"https://d1ge0kk1l5kms0.cloudfront.net/images/G/01/webservices/console/warning.png\" alt=\"Warning\" /\u003e \u003cstrong\u003eJavascript is disabled or is unavailable in your browser.\u003c/strong\u003e\u003c/p\u003e\u003cp\u003eTo use the Amazon Web Services Documentation, Javascript must be enabled. Please refer to your browser's Help pages for instructions.\u003c/p\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003eDocument ConventionsSEC03-BP04 Reduce permissions continuouslySEC03-BP06 Manage access based on lifecycle",
  "SEC03-BP06 Manage access based on lifecycle\n    Monitor and adjust the permissions granted to your principals\n    (users, roles, and groups) throughout their lifecycle within your\n    organization. Adjust group memberships as users change roles, and\n    remove access when a user leaves the organization.\n  \n    Desired outcome: You monitor and\n    adjust permissions throughout the lifecycle of principals within the\n    organization, reducing risk of unnecessary privileges. You grant\n    appropriate access when you create a user. You modify access as the\n    user's responsibilities change, and you remove access when the user\n    is no longer active or has left the organization. You centrally\n    manage changes to your users, roles, and groups. You use automation\n    to propagate changes to your AWS environments.\n  \n    Common anti-patterns:\n  \n     \n     \n     \n     \n  \n      \n        Granting excessive or broad access privileges to identities\n        upfront, beyond what is initially required.\n      \n    \n      \n        Not reviewing and adjusting access privileges as identities'\n        roles and responsibilities change over time.\n      \n    \n      \n        Leaving inactive or terminated identities with active access\n        privileges. This increases the risk of unauthorized access.\n      \n    \n      \n        Not leveraging automation to manage the lifecycle of identities.\n      \n    \n    Level of risk exposed if this best practice\n      is not established: Medium\n  \n    \n    Implementation guidance\n    \n    \n    \n      Carefully manage and adjust access privileges that you grant to\n      identities (such as users, roles, groups) throughout their\n      lifecycle. This lifecycle includes the initial onboarding phase,\n      ongoing changes in roles and responsibilities, and eventual\n      offboarding or termination. Proactively manage access based on the\n      stage of the lifecycle to maintain the appropriate access level.\n      Adhere to the principle of least privilege to reduce the risk of\n      excessive or unnecessary access Privileges.\n    \n    \n      You can manage the lifecycle of IAM users directly within the AWS account, or through federation from your workforce identity\n      provider to\n      AWS       IAM Identity Center. For IAM users, you can create, modify,\n      and delete users and their associated permissions within the AWS account. For federated users, you can use IAM Identity Center to\n      manage their lifecycle by synchronizing user and group information\n      from your organization's identity provider using the\n      System\n        for Cross-domain Identity Management (SCIM) protocol.\n    \n    \n      SCIM is an open standard protocol for automated provisioning and\n      deprovisioning of user identities across different systems. By\n      integrating your identity provider with IAM Identity Center using\n      SCIM, you can automatically synchronize user and group\n      information, helping to validate that access privileges are\n      granted, modified, or revoked based on changes in your\n      organization's authoritative identity source.\n    \n    \n      As the roles and responsibilities of employees change within your\n      organization, adjust their access privileges accordingly. You can\n      use IAM Identity Center's permission sets to define different job\n      roles or responsibilities and associate them with the appropriate\n      IAM policies and permissions. When an employee's role changes, you\n      can update their assigned permission set to reflect their new\n      responsibilities. Verify that they have the necessary access while\n      adhering to the principle of least privilege.\n    \n     \n      \n      Implementation steps\n      \n      \n      \n         \n         \n         \n         \n         \n         \n         \n      \n          \n            Define and document an access management lifecycle process,\n            including procedures for granting initial access, periodic\n            reviews, and offboarding.\n          \n        \n          \n            Implement\n            IAM\n              roles, groups, and permissions boundaries to manage\n            access collectively and enforce maximum permissible access\n            levels.\n          \n        \n          \n            Integrate with a\n            federated\n              identity provider (such as Microsoft Active\n            Directory, Okta, Ping Identity) as the authoritative source\n            for user and group information using IAM Identity Center.\n          \n        \n          \n            Use the\n            SCIM\n            protocol to synchronize user and group information from the\n            identity provider into IAM Identity Center's Identity Store.\n          \n        \n          \n            Create\n            permission\n              sets in IAM Identity Center that represent different\n            job roles or responsibilities within your organization.\n            Define the appropriate IAM policies and permissions for each\n            permission set.\n          \n        \n          \n            Implement regular access reviews, prompt access revocation,\n            and continuous improvement of the access management\n            lifecycle process.\n          \n        \n          \n            Provide training and awareness to employees on access\n            management best practices.\n          \n        \n     \n   \n    \n    Resources\n    \n    \n    \n      Related best practices:\n    \n    \n       \n    \n        \n          SEC02-BP04 Rely on a\n            centralized identity provider\n        \n      \n    \n      Related documents:\n    \n    \n       \n       \n       \n       \n    \n        \n          Manage\n            your identity source \n        \n      \n        \n          Manage\n            identities in IAM Identity Center\n        \n      \n        \n          Using\n            AWS Identity and Access Management Access Analyzer\n        \n      \n        \n          IAM Access Analyzer policy generation\n        \n      \n    \n      Related videos:\n    \n    \n       \n       \n       \n    \n        \n          AWS           re:Inforce 2023 - Manage temporary elevated access with AWS\n            IAM Identity Center\n        \n      \n        \n          AWS           re:Invent 2022 - Simplify your existing workforce access with\n            IAM Identity Center\n        \n      \n        \n          AWS           re:Invent 2022 - Harness power of IAM policies \u0026 rein in\n            permissions w/Access Analyzer\n        \n      \n  \u003cdiv\u003e\u003cdiv\u003e\u003cdiv\u003e\u003cdiv id=\"js_error_message\"\u003e\u003cp\u003e\u003cimg src=\"https://d1ge0kk1l5kms0.cloudfront.net/images/G/01/webservices/console/warning.png\" alt=\"Warning\" /\u003e \u003cstrong\u003eJavascript is disabled or is unavailable in your browser.\u003c/strong\u003e\u003c/p\u003e\u003cp\u003eTo use the Amazon Web Services Documentation, Javascript must be enabled. Please refer to your browser's Help pages for instructions.\u003c/p\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003eDocument ConventionsSEC03-BP05 Define permission guardrails for your organizationSEC03-BP07 Analyze public and cross-account access",
  "SEC03-BP07 Analyze public and cross-account accessContinually monitor findings that highlight public and cross-account access. Reduce public access and cross-account access to only the specific resources that require this access.\n    Desired outcome: Know which of your AWS resources are shared and with whom. Continually monitor and audit your shared resources to verify they are shared with only authorized principals.\n  \n    Common anti-patterns:\n  \n     \n     \n  \n      \n        Not keeping an inventory of shared resources.      \n      \n    \n      \n        Not following a process for approval of cross-account or public access to resources.\n      \n    \n    Level of risk exposed if this best practice\n    is not established: Low\n  \n\n  Implementation guidance \n    \n      If your account is in AWS Organizations, you can grant access to resources to the entire organization, specific organizational units, or individual accounts. If your account is not a member of an organization, you can share resources with individual accounts. You can grant direct cross-account access using resource-based policies — for example, Amazon Simple Storage Service (Amazon S3) bucket policies — or by allowing a principal in another account to assume an IAM role in your account. When using resource policies, verify that access is only granted to authorized principals. Define a process to approve all resources which are required to be publicly available.\n    \n    \n      AWS Identity and Access Management Access Analyzer uses provable security to identify all access paths to a resource from outside of its account. It reviews resource policies continuously, and reports findings of public and cross-account access to make it simple for you to analyze potentially broad access. Consider configuring IAM Access Analyzer with AWS Organizations to verify that you have visibility to all your accounts. IAM Access Analyzer also allows you to preview findings before deploying resource permissions. This allows you to validate that your policy changes grant only the intended public and cross-account access to your resources. When designing for multi-account access, you can use trust policies to control in what cases a role can be assumed. For example, you could use the PrincipalOrgId condition key to deny an attempt to assume a role from outside your AWS Organizations.\n    \n    \n      AWS Config can report resources that are misconfigured, and\n      through AWS Config policy checks, can detect resources that have\n      public access configured. Services such as\n      AWS Control Tower and\n      AWS Security Hub simplify deploying detective controls and\n      guardrails across AWS Organizations to identify and remediate\n      publicly exposed resources. For example, AWS Control Tower has a\n      managed guardrail which can detect if any\n      Amazon EBS snapshots are restorable by AWS accounts.\n    \n     \n      \n      Implementation steps\n    \n    \n       \n       \n       \n       \n    \n        \n          Consider using AWS Config for AWS Organizations: AWS Config allows you to aggregate findings from multiple accounts within an AWS Organizations to a delegated administrator account. This provides a comprehensive view, and allows you to deploy AWS Config Rules across accounts to detect publicly accessible resources. \n        \n      \n        \n          Configure AWS Identity and Access Management Access Analyzer: IAM Access Analyzer helps you identify resources in your organization and accounts, such as Amazon S3 buckets or IAM roles that are shared with an external entity.\n        \n      \n        \n          Use auto-remediation in AWS Config to respond to changes in public access configuration of Amazon S3 buckets: You can automatically turn on the block public access settings for Amazon S3 buckets.\n        \n      \n        \n          Implement monitoring and alerting to\n            identify if Amazon S3 buckets have become public:\n          You must have\n          monitoring\n            and alerting in place to identify when Amazon S3\n          Block Public Access is turned off, and if Amazon S3 buckets\n          become public. Additionally, if you are using AWS Organizations, you can create a\n          service\n            control policy that prevents changes to Amazon S3\n          public access policies.\n          AWS Trusted Advisor checks for Amazon S3 buckets that\n          have open access permissions. Bucket permissions that grant,\n          upload, or delete access to everyone create potential\n          security issues by allowing anyone to add, modify, or remove\n          items in a bucket. The Trusted Advisor check examines\n          explicit bucket permissions and associated bucket policies\n          that might override the bucket permissions. You also can use\n          AWS Config to monitor your Amazon S3 buckets for public\n          access. For more information, see\n          How\n            to Use AWS Config to Monitor for and Respond to Amazon S3\n            Buckets Allowing Public Access.\n        \n      \n    \n      When reviewing access controls for Amazon S3 buckets, it is\n      important to consider the nature of the data stored within them.\n      Amazon Macie is a service designed to help you discover and\n      protect sensitive data, such as Personally Identifiable\n      Information (PII), Protected Health Information (PHI), and\n      credentials like private keys or AWS access keys.\n    \n     \n   \n\n  Resources\n    \n      Related documents:\n    \n    \n       \n       \n       \n       \n       \n       \n       \n       \n       \n    \n        \n          Using\n            AWS Identity and Access Management Access Analyzer\n      \n        AWS Control Tower controls library\n        \n      \n        \n          AWS Foundational\n            Security Best Practices standard\n      \n        \n          AWS Config\n            Managed Rules\n        \n      \n        \n          AWS Trusted Advisor\n            check reference\n        \n      \n        \n          Monitoring AWS Trusted Advisor check results with Amazon EventBridge\n        \n      \n        \n          Managing AWS Config Rules Across All Accounts in Your Organization\n        \n      \n        AWS Config and AWS Organizations\n      \n        \n          Make your AMI publicly available for use in Amazon EC2\n        \n      \n    \n      Related videos:\n    \n    \n       \n       \n    \n        Best Practices for securing\n            your multi-account environment\n      \n        Dive Deep into\n            IAM Access Analyzer\n      \n  \u003cdiv\u003e\u003cdiv\u003e\u003cdiv\u003e\u003cdiv id=\"js_error_message\"\u003e\u003cp\u003e\u003cimg src=\"https://d1ge0kk1l5kms0.cloudfront.net/images/G/01/webservices/console/warning.png\" alt=\"Warning\" /\u003e \u003cstrong\u003eJavascript is disabled or is unavailable in your browser.\u003c/strong\u003e\u003c/p\u003e\u003cp\u003eTo use the Amazon Web Services Documentation, Javascript must be enabled. Please refer to your browser's Help pages for instructions.\u003c/p\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003eDocument ConventionsSEC03-BP06 Manage access based on lifecycleSEC03-BP08 Share resources securely within your organization",
  "SEC03-BP08 Share resources securely within your organizationAs the number of workloads grows, you might need to share access to resources in those workloads or provision \n    the resources multiple times across multiple accounts. You might have constructs to compartmentalize your \n    environment, such as having development, testing, and production environments. However, having separation \n    constructs does not limit you from being able to share securely. By sharing components that overlap, you \n    can reduce operational overhead and allow for a consistent experience without guessing what you might have \n    missed while creating the same resource multiple times. \n    Desired outcome: Minimize unintended access by using secure methods to share \n    resources within your organization, and help with your data loss prevention initiative. Reduce your \n    operational overhead compared to managing individual components, reduce errors from manually creating \n    the same component multiple times, and increase your workloads’ scalability. You can benefit from decreased \n    time to resolution in multi-point failure scenarios, and increase your confidence in determining when a \n    component is no longer needed. For prescriptive guidance on analyzing externally shared resources, \n    see SEC03-BP07 Analyze public and cross-account access. \n  \n    Common anti-patterns:\n  \n     \n     \n     \n     \n  \n      \n        Lack of process to continually monitor and automatically alert on unexpected external share.\n      \n    \n      \n        Lack of baseline on what should be shared and what should not.\n      \n    \n      \n        Defaulting to a broadly open policy rather than sharing explicitly when required.\n      \n    \n      \n        Manually creating foundational resources that overlap when required.\n      \n    \n    Level of risk exposed if this best practice\n    is not established: Medium\n  \n\n  Implementation guidance\n    \n      Architect your access controls and patterns to govern the consumption of shared resources securely and \n      only with trusted entities. Monitor shared resources and review shared resource access continuously, \n      and be alerted on inappropriate or unexpected sharing. Review \n      Analyze public and cross-account access \n      to help you establish governance to reduce the external access to only resources that require it, and to \n      establish a process to monitor continuously and alert automatically.\n    \n    \n      Cross-account sharing within AWS Organizations is supported by a number of AWS services, \n      such as AWS Security Hub, \n      Amazon GuardDuty, \n      and AWS Backup.  \n      These services allow for data to be shared to a central account, be accessible from a central account, or manage resources \n      and data from a central account. For example, AWS Security Hub can transfer findings from individual accounts to a \n      central account where you can view all the findings. AWS Backup can take a backup for a resource and share it \n      across accounts. You can use AWS Resource Access Manager (AWS RAM) to share other \n      common resources, such as VPC subnets and Transit Gateway attachments, \n      AWS Network Firewall, \n      or Amazon SageMaker AI pipelines. \n    \n    \n      To restrict your account to only share resources within your organization, use \n      service control policies (SCPs) \n      to prevent access to external principals. When sharing resources, combine identity-based controls and \n      network controls to create a data perimeter for your organization \n      to help protect against unintended access. A data perimeter is a set of preventive guardrails to help verify \n      that only your trusted identities are accessing trusted resources from expected networks. These controls place \n      appropriate limits on what resources can be shared and prevent sharing or exposing resources that should not be \n      allowed. For example, as a part of your data perimeter, you can use VPC endpoint policies and the \n      AWS:PrincipalOrgId condition to ensure the identities accessing your Amazon S3 buckets \n      belong to your organization. It is important to note that \n      SCPs do not apply to service-linked roles or AWS service principals.\n    \n    \n      When using Amazon S3, \n      turn off ACLs for your Amazon S3 bucket \n      and use IAM policies to define access control. For restricting access to an Amazon S3 origin \n      from Amazon CloudFront, migrate from origin access identity (OAI) to origin access \n      control (OAC) which supports additional features including server-side encryption with \n      AWS Key Management Service.\n    \n    \n      In some cases, you might want to allow sharing resources outside of your organization or grant a third party \n      access to your resources. For prescriptive guidance on managing permissions to share resources externally, \n      see Permissions management.\n    \n     \n      \n      Implementation steps\n    \n    \n       \n       \n       \n       \n       \n    \n        \n          Use AWS Organizations: AWS Organizations is an account management service that allows you to consolidate multiple AWS accounts \n          into an organization that you create and centrally manage. You can group your accounts into \n          organizational units (OUs) and attach different policies to each OU to help you meet your \n          budgetary, security, and compliance needs. You can also control how AWS artificial intelligence (AI) \n          and machine learning (ML) services can collect and store data, and use the multi-account management of \n          the AWS services integrated with Organizations.\n        \n      \n        \n          Integrate AWS Organizations with AWS services: When you use an AWS service to perform tasks on your behalf in the member accounts of your organization, \n          AWS Organizations creates an IAM service-linked role (SLR) for that service in each member account. You should \n          manage trusted access using the AWS Management Console, the AWS APIs, or the AWS CLI. For prescriptive guidance on \n          turning on trusted access, see Using AWS Organizations with other AWS services \n          and AWS services \n            that you can use with Organizations.\n        \n      \n        \n          Establish a data\n            perimeter: A data perimeter provides a clear\n          boundary of trust and ownership. On AWS, it is typically\n          represented as your AWS organization managed by AWS Organizations, along with any on-premises networks or\n          systems that access your AWS resources. The goal of the data\n          perimeter is to verify that access is allowed if the\n          identity is trusted, the resource is trusted, and the\n          network is expected. However, establishing a data perimeter\n          is not a one-size-fits-all approach. Evaluate and adopt the\n          control objectives outlined in the\n          Building\n            a Perimeter on AWS whitepaper based on your specific\n          security risk models and requirements. You should carefully\n          consider your unique risk posture and implement the\n          perimeter controls that align with your security needs.\n        \n      \n        \n          Use resource sharing in AWS services and restrict accordingly: Many AWS services allow you to share resources with another account, or target a resource in another \n          account, such as Amazon Machine Images (AMIs) \n          and AWS Resource Access Manager (AWS RAM). \n          Restrict the ModifyImageAttribute API to specify the trusted accounts to share the AMI with. \n          Specify the ram:RequestedAllowsExternalPrincipals condition when using AWS RAM to constrain sharing \n          to your organization only, to help prevent access from untrusted identities. For prescriptive guidance and \n          considerations, see Resource sharing and external targets.\n        \n      \n        \n          Use AWS RAM to share securely in an account or with other AWS accounts: AWS RAM helps you securely share the resources that you have \n          created with roles and users in your account and with other AWS accounts. In a multi-account \n          environment, AWS RAM allows you to create a resource once and share it with other accounts. This \n          approach helps reduce your operational overhead while providing consistency, visibility, and \n          auditability through integrations with Amazon CloudWatch and AWS CloudTrail, which you do not receive when \n          using cross-account access.\n        \n        \n        \n          If you have resources that you shared previously using a resource-based policy, you can use the \n          PromoteResourceShareCreatedFromPolicy API \n          or an equivalent to promote the resource share to a full AWS RAM resource share.\n        \n        \n          In some cases, you might need to take additional steps to share resources. For example, \n          to share an encrypted snapshot, you need to share a AWS KMS key. \n        \n      \n     \n   \n\n  Resources\n    \n      Related best practices:\n    \n    \n       \n       \n       \n    \n        \n          SEC03-BP07 Analyze public and cross-account access\n        \n      \n        \n          SEC03-BP09 Share resources securely with a third party        \n        \n      \n        \n          SEC05-BP01 Create network layers\n        \n      \n\n      \n    \n      Related documents:\n    \n    \n       \n       \n       \n       \n       \n       \n    \n        Bucket owner granting cross-account permission to objects it does not\n          own\n      \n        How to use Trust\n            Policies with IAM\n      \n        Building Data Perimeter on AWS\n      \n        How to use\n            an external ID when granting a third party access to your AWS resources\n      \n        AWS services you can use with AWS Organizations\n      \n        \n          Establishing a data perimeter on AWS: Allow only trusted identities to access company data\n        \n      \n    \n      Related videos:\n    \n    \n       \n       \n       \n    \n        Granular Access with\n            AWS Resource Access Manager\n      \n        Securing your data perimeter\n            with VPC endpoints\n      \n         Establishing a data\n            perimeter on AWS\n      \n    \n      Related tools:\n    \n    \n       \n    \n        \n          Data Perimeter Policy Examples\n        \n      \n  \u003cdiv\u003e\u003cdiv\u003e\u003cdiv\u003e\u003cdiv id=\"js_error_message\"\u003e\u003cp\u003e\u003cimg src=\"https://d1ge0kk1l5kms0.cloudfront.net/images/G/01/webservices/console/warning.png\" alt=\"Warning\" /\u003e \u003cstrong\u003eJavascript is disabled or is unavailable in your browser.\u003c/strong\u003e\u003c/p\u003e\u003cp\u003eTo use the Amazon Web Services Documentation, Javascript must be enabled. Please refer to your browser's Help pages for instructions.\u003c/p\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003eDocument ConventionsSEC03-BP07 Analyze public and cross-account accessSEC03-BP09 Share resources securely with a third party",
  "SEC03-BP09 Share resources securely with a third party\n    The security of your cloud environment doesn't stop at your\n    organization. Your organization might rely on a third party to\n    manage a portion of your data. The permission management for the\n    third-party managed system should follow the practice of\n    just-in-time access using the principle of least privilege with\n    temporary credentials. By working closely with a third party, you\n    can reduce the scope of impact and risk of unintended access\n    together.\n  \n    Desired outcome: You avoid using\n    long-term AWS Identity and Access Management (IAM) credentials like\n    access keys and secret keys, as they pose a security risk if\n    misused. Instead, you use IAM roles and temporary credentials to\n    improve your security posture and minimize the operational overhead\n    of managing long-term credentials. When granting third-party access,\n    you use a universally unique identifier (UUID) as the external ID in\n    the IAM trust policy and keep the IAM policies attached to the role\n    under your control to ensure least privilege access. For\n    prescriptive guidance on analyzing externally shared resources, see\n    SEC03-BP07 Analyze public and\n    cross-account access.\n  \n    Common anti-patterns:\n  \n     \n     \n     \n  \n      \n        Using the default IAM trust policy without any conditions.\n      \n    \n      \n        Using long-term IAM credentials and access keys.\n      \n    \n      \n        Reusing external IDs.\n      \n    \n    Level of risk exposed if this best practice\n    is not established: Medium\n  \n\n  Implementation guidance\n\n      \n    \n      You might want to allow sharing resources outside of AWS Organizations or grant a third party access to your account. For\n      example, a third party might provide a monitoring solution that\n      needs to access resources within your account. In those cases,\n      create an IAM cross-account role with only the privileges needed\n      by the third party. Additionally, define a trust policy using the\n      external\n      ID condition. When using an external ID, you or the third\n      party can generate a unique ID for each customer, third party, or\n      tenancy. The unique ID should not be controlled by anyone but you\n      after it's created. The third party must implement a process to\n      relate the external ID to the customer in a secure, auditable, and\n      reproduceable manner.\n    \n    \n      You can also use\n      IAM\n      Roles Anywhere to manage IAM roles for applications outside\n      of AWS that use AWS APIs.\n    \n    \n      If the third party no longer requires access to your environment,\n      remove the role. Avoid providing long-term credentials to a third\n      party. Maintain awareness of other AWS services that support\n      sharing, such as the AWS Well-Architected Tool allowing\n      sharing\n      a workload with other AWS accounts, and\n      AWS       Resource Access Manager helping you securely share an AWS\n      resource you own with other accounts.\n    \n     \n\n  Implementation steps\n\n      \n      \n         \n         \n         \n         \n         \n      \n          \n            Use cross-account roles to provide\n            access to external accounts.\n            Cross-account\n            roles reduce the amount of sensitive information that\n            is stored by external accounts and third parties for\n            servicing their customers. Cross-account roles allow you to\n            grant access to AWS resources in your account securely to a\n            third party, such as AWS Partners or other accounts in your\n            organization, while maintaining the ability to manage and\n            audit that access. The third party might be providing\n            service to you from a hybrid infrastructure or alternatively\n            pulling data into an offsite location.\n            IAM\n            Roles Anywhere helps you allow third-party workloads\n            to securely interact with your AWS workloads and further\n            reduce the need for long-term credentials.\n          \n          \n            You should not use long-term credentials or access keys\n            associated with users to provide external account access.\n            Instead, use cross-account roles to provide the\n            cross-account access.\n          \n        \n          \n            Perform due diligence and ensure\n            secure access for third-party SaaS providers.\n            When sharing resources with third-party SaaS providers,\n            perform thorough due diligence to ensure they have a secure\n            and responsible approach to accessing your AWS resources.\n            Evaluate their shared responsibility model to understand\n            what security measures they provide and what falls under\n            your responsibility. Ensure that the SaaS provider has a\n            secure and auditable process for accessing your resources,\n            including the use of\n            external\n            IDs and least privilege access principles. The use of\n            external IDs helps address the\n            confused\n            deputy problem.\n          \n          \n            Implement security controls to ensure secure access and\n            adherence to the principle of least privilege when granting\n            access to third-party SaaS providers. This may include the\n            use of external IDs, universally unique identifiers (UUIDs),\n            and IAM trust policies that limit access to only what is\n            strictly necessary. Work closely with the SaaS provider to\n            establish secure access mechanisms, regularly review their\n            access to your AWS resources, and conduct audits to ensure\n            compliance with your security requirements.\n          \n        \n          \n            Deprecate customer-provided\n            long-term credentials. Deprecate the use of\n            long-term credentials and use cross-account roles or IAM\n            Roles Anywhere. If you must use long-term credentials,\n            establish a plan to migrate to role-based access. For\n            details on managing keys, see\n            Identity\n            management. Also, work with your AWS account team and\n            the third party to establish a risk mitigation runbook. For\n            prescriptive guidance on responding to and mitigating the\n            potential impact of a security incident, see\n            Incident\n            response.\n          \n        \n          \n            Verify that setup has prescriptive\n            guidance or is automated. The external ID is not\n            treated as a secret, but the external ID must not be an\n            easily guessable value, such as a phone number, name, or\n            account ID. Make the external ID a read-only field so that\n            the external ID cannot be changed for the purpose of\n            impersonating the setup.\n          \n          \n            You or the third party can generate the external ID. Define\n            a process to determine who is responsible for generating the\n            ID. Regardless of the entity creating the external ID, the\n            third party enforces uniqueness and formats consistently\n            across customers.\n          \n          \n            The policy created for cross-account access in your accounts\n            must follow the\n            least-privilege\n            principle. The third party must provide a role policy\n            document or automated setup mechanism that uses an AWS CloudFormation template or an equivalent for you. This\n            reduces the chance of errors associated with manual policy\n            creation and offers an auditable trail. For more information\n            on using an AWS CloudFormation template to create\n            cross-account roles, see\n            Cross-Account\n            Roles.\n          \n          \n            The third party should provide an automated, auditable setup\n            mechanism. However, by using the role policy document\n            outlining the access needed, you should automate the setup\n            of the role. Using an AWS CloudFormation template or\n            equivalent, you should monitor for changes with drift\n            detection as part of the audit practice.\n          \n        \n          \n            Account for changes. Your\n            account structure, your need for the third party, or their\n            service offering being provided might change. You should\n            anticipate changes and failures, and plan accordingly with\n            the right people, process, and technology. Audit the level\n            of access you provide on a periodic basis, and implement\n            detection methods to alert you to unexpected changes.\n            Monitor and audit the use of the role and the datastore of\n            the external IDs. You should be prepared to revoke\n            third-party access, either temporarily or permanently, as a\n            result of unexpected changes or access patterns. Also,\n            measure the impact to your revocation operation, including\n            the time it takes to perform, the people involved, the cost,\n            and the impact to other resources.\n          \n          \n            For prescriptive guidance on detection methods, see the\n            Detection best\n            practices.\n          \n        \n     \n   \n\n  Resources\n\n      \n    \n      Related best practices:\n    \n    \n       \n       \n       \n       \n       \n    \n        \n          SEC02-BP02 Use\n          temporary credentials\n        \n      \n        \n          SEC03-BP05 Define permission\n          guardrails for your organization\n        \n      \n        \n          SEC03-BP06 Manage access\n          based on lifecycle\n        \n      \n        \n          SEC03-BP07 Analyze public and\n          cross-account access\n        \n      \n        \n          SEC04 Detection\n        \n      \n    \n      Related documents:\n    \n    \n       \n       \n       \n       \n       \n       \n       \n       \n       \n       \n    \n        \n          Bucket\n          owner granting cross-account permission to objects it does not\n          own\n        \n      \n        \n          How\n          to use trust policies with IAM roles\n        \n      \n        \n          Delegate\n          access across AWS accounts using IAM roles\n        \n      \n        \n          How\n          do I access resources in another AWS account using IAM?\n        \n      \n        \n          Security\n          best practices in IAM\n        \n      \n        \n          Cross-account\n          policy evaluation logic\n        \n      \n        \n          How\n          to use an external ID when granting access to your AWS\n          resources to a third party\n        \n      \n        \n          Collecting\n          Information from AWS CloudFormation Resources Created in\n          External Accounts with Custom Resources\n        \n      \n        \n          Securely\n          Using External ID for Accessing AWS Accounts Owned by\n          Others\n        \n      \n        \n          Extend\n          IAM roles to workloads outside of IAM with IAM Roles\n          Anywhere\n        \n      \n    \n      Related videos:\n    \n    \n       \n       \n       \n    \n        \n          How\n          do I allow users or roles in a separate AWS account access to\n          my AWS account?\n        \n      \n        \n          AWS           re:Invent 2018: Become an IAM Policy Master in 60 Minutes or\n          Less\n        \n      \n        \n          AWS           Knowledge Center Live: IAM Best Practices and Design\n          Decisions\n        \n      \n    \n      Related examples:\n    \n    \n       \n       \n    \n        \n          Configure\n          cross-account access to Amazon DynamoDB\n        \n      \n        \n          AWS STS Network Query Tool\n        \n      \n  \u003cdiv\u003e\u003cdiv\u003e\u003cdiv\u003e\u003cdiv id=\"js_error_message\"\u003e\u003cp\u003e\u003cimg src=\"https://d1ge0kk1l5kms0.cloudfront.net/images/G/01/webservices/console/warning.png\" alt=\"Warning\" /\u003e \u003cstrong\u003eJavascript is disabled or is unavailable in your browser.\u003c/strong\u003e\u003c/p\u003e\u003cp\u003eTo use the Amazon Web Services Documentation, Javascript must be enabled. Please refer to your browser's Help pages for instructions.\u003c/p\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003eDocument ConventionsSEC03-BP08 Share resources securely within your organizationDetection",
  "SEC04-BP01 Configure service and application loggingRetain security event logs from services and applications. This is a fundamental principle of security for audit, investigations, and operational use cases, and a common security requirement driven by governance, risk, and compliance (GRC) standards, policies, and procedures.\n    Desired outcome: An organization should be able to reliably and consistently retrieve security event logs from AWS services and applications in a timely manner when required to fulfill an internal process or obligation, such as a security incident response. Consider centralizing logs for better operational results.\n  \n    Common anti-patterns:\n  \n     \n     \n     \n     \n     \n  \n      \n        Logs are stored in perpetuity or deleted too soon.\n      \n    \n      \n        Everybody can access logs.\n      \n    \n      \n        Relying entirely on manual processes for log governance and use.\n      \n    \n      \n        Storing every single type of log just in case it is needed.\n      \n    \n      \n        Checking log integrity only when necessary.\n      \n    \n    Benefits of establishing this best practice: Implement a root cause analysis (RCA) mechanism for security incidents and a source of evidence for your governance, risk, and compliance obligations.\n  \n    Level of risk exposed if this best practice\n    is not established: High\n  \n\n  Implementation guidance\n    \n      During a security investigation or other use cases based on your requirements, you need to be able to review relevant logs to record and understand the full scope and timeline of the incident. Logs are also required for alert generation, indicating that certain actions of interest have happened. It is critical to select, turn on, store, and set up querying and retrieval mechanisms and alerting.  \n    \n    \n      Implementation steps\n    \n    \n       \n       \n       \n       \n       \n       \n       \n    \n        \n          Select and use log sources. Ahead of a security investigation, you need to capture relevant logs to retroactively reconstruct activity in an AWS account. Select log sources relevant to your workloads.\n        \n        \n          The log source selection criteria should be based on the use cases required by your business. Establish a trail for each AWS account using AWS CloudTrail or an AWS Organizations trail, and configure an Amazon S3 bucket for it.\n        \n        \n          AWS CloudTrail is a logging service that tracks API calls made against an AWS account capturing AWS service activity. It’s turned on by default with a 90-day retention of management events that can be retrieved through CloudTrail Event history using the AWS Management Console, the AWS CLI, or an AWS SDK. For longer retention and visibility of data events, create a CloudTrail trail and associate it with an Amazon S3 bucket, and optionally with a Amazon CloudWatch log group. Alternatively, you can create a CloudTrail Lake, which retains CloudTrail logs for up to seven years and provides a SQL-based querying facility\n        \n        \n          AWS recommends that customers using a VPC turn on network traﬃc and DNS logs using VPC Flow Logs and Amazon Route 53 resolver query logs, respectively, and streaming them to either an Amazon S3 bucket or a CloudWatch log group. You can create a VPC ﬂow log for a VPC, a subnet, or a network interface. For VPC Flow Logs, you can be selective on how and where you use Flow Logs to reduce cost.\n        \n        \n          AWS CloudTrail Logs, VPC Flow Logs, and Route 53 resolver query logs are the basic logging sources to support security investigations in AWS. You can also use Amazon Security Lake to collect, normalize, and store this log data in Apache Parquet format and Open Cybersecurity Schema Framework (OCSF), which is ready for querying. Security Lake also supports other AWS logs and logs from third-party sources. \n        \n        \n          AWS services can generate logs not captured by the basic log sources, such as Elastic Load Balancing logs, AWS WAF logs, AWS Config recorder logs, Amazon GuardDuty ﬁndings, Amazon Elastic Kubernetes Service (Amazon EKS) audit logs, and Amazon EC2 instance operating system and application logs. For a full list of logging and monitoring options, see  Appendix A: Cloud capability deﬁnitions  – Logging and Events of the AWS Security Incident Response Guide.\n        \n      \n        \n          Research logging capabilities for each AWS service and application:  Each AWS service and application provides you with options for log storage, each of which with its own retention and life-cycle capabilities. The two most common log storage services are Amazon Simple Storage Service (Amazon S3) and Amazon CloudWatch. For long retention periods, it is recommended to use Amazon S3 for its cost effectiveness and flexible lifecycle capabilities. If the primary logging option is Amazon CloudWatch Logs, as an option, you should consider archiving less frequently accessed logs to Amazon S3.\n        \n      \n        \n          Select log storage: The choice of log storage is generally related to which querying tool you use, retention capabilities, familiarity, and cost. The main options for log storage are an Amazon S3 bucket or a CloudWatch Log group.\n        \n        \n          An Amazon S3 bucket provides cost-eﬀective, durable storage with an optional lifecycle policy. Logs stored in Amazon S3 buckets can be queried using services such as Amazon Athena. \n        \n        \n          A CloudWatch log group provides durable storage and a built-in query facility through CloudWatch Logs Insights.\n        \n      \n        \n          Identify appropriate log retention: When you use an Amazon S3 bucket or CloudWatch log group to store logs, you must establish adequate lifecycles for each log source to optimize storage and retrieval costs. Customers generally have between three months to one year of logs readily available for querying, with retention of up to seven years. The choice of availability and retention should align with your security requirements and a composite of statutory, regulatory, and business mandates.\n        \n      \n        \n          Use logging for each AWS service and application with proper retention and lifecycle policies: For each AWS service or application in your organization, look for the specific logging configuration guidance:\n        \n        \n           \n           \n           \n           \n           \n           \n           \n           \n           \n           \n           \n        \n            \n              Configure AWS CloudTrail Trail\n            \n          \n            \n              Configure VPC Flow Logs\n            \n          \n            \n              Configure Amazon GuardDuty Finding Export\n            \n          \n            \n              Configure AWS Config recording\n            \n          \n            \n              Configure AWS WAF web ACL traffic\n            \n          \n            \n              Configure AWS Network Firewall network traffic logs\n            \n          \n            \n              Configure Elastic Load Balancing access logs\n            \n          \n            \n              Configure Amazon Route 53 resolver query logs\n            \n          \n            \n              Configure Amazon RDS logs\n            \n          \n            \n              Configure Amazon EKS Control Plane logs\n            \n          \n            \n              Configure Amazon CloudWatch agent for Amazon EC2 instances and on-premises servers\n            \n          \n      \n        \n          Select and implement querying mechanisms for logs: For log queries, you can use CloudWatch Logs Insights for data stored in CloudWatch log groups, and Amazon Athena and Amazon OpenSearch Service for data stored in Amazon S3. You can also use third-party querying tools such as a security information and event management (SIEM) service.\n        \n        \n          The process for selecting a log querying tool should consider the people, process, and technology aspects of your security operations. Select a tool that fulﬁlls operational, business, and security requirements, and is both accessible and maintainable in the long term. Keep in mind that log querying tools work optimally when the number of logs to be scanned is kept within the tool’s limits. It is not uncommon to have multiple querying tools because of cost or technical constraints. \n        \n        \n          For example, you might use a third-party security information and event management (SIEM) tool to perform queries for the last 90 days of data, but use Athena to perform queries beyond 90 days because of the log ingestion cost of a SIEM. Regardless of the implementation, verify that your approach minimizes the number of tools required to maximize operational eﬃciency, especially during a security event investigation.\n        \n      \n        \n          Use logs for alerting: AWS provides alerting through several security services:\n        \n        \n           \n           \n           \n        \n            \n              AWS Config monitors and records your AWS resource configurations and allows you to automate the evaluation and remediation against desired configurations.\n            \n          \n            \n              Amazon GuardDuty is a threat detection service that continually monitors for malicious activity and unauthorized behavior to protect your AWS accounts and workloads. GuardDuty ingests, aggregates, and analyzes information from sources, such as AWS CloudTrail management and data events, DNS logs, VPC Flow Logs, and Amazon EKS Audit logs. GuardDuty pulls independent data streams directly from CloudTrail, VPC Flow Logs, DNS query logs, and Amazon EKS. You don’t have to manage Amazon S3 bucket policies or modify the way you collect and store logs. It is still recommended to retain these logs for your own investigation and compliance purposes. \n            \n          \n            \n              AWS Security Hub provides a single place that aggregates, organizes, and prioritizes your security alerts or findings from multiple AWS services and optional third-party products to give you a comprehensive view of security alerts and compliance status.\n            \n          \n        \n          You can also use custom alert generation engines for security alerts not covered by these services or for speciﬁc alerts relevant to your environment. For information on building these alerts and detections, see Detection in the AWS Security Incident Response Guide.\n        \n      \n   \n\n  Resources\n    \n      Related best practices:\n    \n    \n       \n       \n       \n    \n        \n          SEC04-BP02 Capture logs, findings, and metrics\n  in standardized locations\n        \n      \n        \n          SEC07-BP04 Define scalable data lifecycle management\n        \n      \n        \n          SEC10-BP06 Pre-deploy tools\n        \n      \n    \n      Related documents:\n    \n    \n       \n       \n       \n    \n        AWS Security Incident Response Guide\n        \n      \n        \n          Getting Started with Amazon Security Lake\n        \n      \n        \n            Getting started: Amazon CloudWatch Logs\n      \n    \n      Related videos:\n    \n    \n       \n    \n        AWS re:Invent 2022 - Introducing Amazon Security Lake\n        \n      \n    \n      Related examples:\n    \n    \n       \n       \n    \n        \n          Assisted Log Enabler for AWS\n      \n        AWS Security Hub Findings Historical Export\n        \n      \n  \u003cdiv\u003e\u003cdiv\u003e\u003cdiv\u003e\u003cdiv id=\"js_error_message\"\u003e\u003cp\u003e\u003cimg src=\"https://d1ge0kk1l5kms0.cloudfront.net/images/G/01/webservices/console/warning.png\" alt=\"Warning\" /\u003e \u003cstrong\u003eJavascript is disabled or is unavailable in your browser.\u003c/strong\u003e\u003c/p\u003e\u003cp\u003eTo use the Amazon Web Services Documentation, Javascript must be enabled. Please refer to your browser's Help pages for instructions.\u003c/p\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003eDocument Conventions SEC 4. How do you detect and investigate security events? SEC04-BP02 Capture logs, findings, and metrics\n      in standardized locations",
  "SEC04-BP02 Capture logs, findings, and metrics\n  in standardized locations\n    Security teams rely on logs and findings to analyze events that may\n    indicate unauthorized activity or unintentional changes. To\n    streamline this analysis, capture security logs and findings in\n    standardized locations.  This makes data points of interest\n    available for correlation and can simplify tool integrations.\n  \n    Desired outcome: You have a\n    standardized approach to collect, analyze, and visualize log data,\n    findings, and metrics. Security teams can efficiently correlate,\n    analyze, and visualize security data across disparate systems to\n    discover potential security events and identify anomalies. Security\n    information and event management (SIEM) systems or other mechanisms\n    are integrated to query and analyze log data for timely responses,\n    tracking, and escalation of security events.\n  \n    Common anti-patterns:\n  \n     \n     \n     \n     \n  \n      \n        Teams independently own and manage logging and metrics\n        collection that is inconsistent to the organization's logging\n        strategy.\n      \n    \n      \n        Teams don't have adequate access controls to restrict visibility\n        and alteration of the data collected.\n      \n    \n      \n        Teams don't govern their security logs, findings, and metrics as\n        part of their data classification policy.\n      \n    \n      \n        Teams neglect data sovereignty and localization requirements\n        when configuring data collections.\n      \n    \n    Benefits of establishing this best\n    practice: A standardized logging solution to collect and\n    query log data and events improves insights derived from the\n    information they contain. Configuring an automated lifecycle for the\n    collected log data can reduce the costs incurred by log storage. You\n    can build fine-grained access control for the collected log\n    information according to the sensitivity of the data and access\n    patterns needed by your teams. You can integrate tooling to\n    correlate, visualize, and derive insights from the data.\n  \n    Level of risk exposed if this best practice\n    is not established: Medium\n  \n\n  Implementation guidance\n\n      \n    \n      Growth in AWS usage within an organization results in a growing\n      number of distributed workloads and environments. As each of these\n      workloads and environments generate data about the activity within\n      them, capturing and storing this data locally presents a challenge\n      for security operations. Security teams use tools such as security\n      information and event management (SIEM) systems to collect data\n      from distributed sources and undergo correlation, analysis, and\n      response workflows. This requires managing a complex set of\n      permissions for accessing the various data sources and additional\n      overhead in operating the extraction, transformation, and loading\n      (ETL) processes.\n    \n    \n      To overcome these challenges, consider aggregating all relevant\n      sources of security log data into a\n      Log Archive account as described in\n      Organizing\n      Your AWS Environment Using Multiple Accounts. This includes\n      all security-related data from your workload and logs that AWS\n      services generate, such as\n      AWS CloudTrail,\n      AWS WAF,\n      Elastic Load Balancing, and\n      Amazon Route 53. There are several benefits to capturing this data in\n      standardized locations in a separate AWS account with proper\n      cross-account permissions. This practice helps prevent log\n      tampering within compromised workloads and environments, provides\n      a single integration point for additional tools, and offers a more\n      simplified model for configuring data retention and lifecycle.\n       Evaluate the impacts of data sovereignty, compliance scopes, and\n      other regulations to determine if multiple security data storage\n      locations and retention periods are required.\n    \n    \n      To ease capturing and standardizing logs and findings, evaluate\n      Amazon\n      Security Lake in your Log Archive account. You can\n      configure Security Lake to automatically ingest data from common\n      sources such as CloudTrail, Route 53,\n      Amazon EKS,\n      and\n      VPC\n      Flow Logs. You can also configure AWS Security Hub as a\n      data source into Security Lake, allowing you to correlate findings\n      from other AWS services, such as\n      Amazon GuardDuty and\n      Amazon Inspector, with your log data.  You can also use\n      third-party data source integrations, or configure custom data\n      sources. All integrations standardize your data into the\n      Open Cybersecurity\n      Schema Framework (OCSF) format, and are stored in\n      Amazon S3\n      buckets as Parquet files, eliminating the need for ETL processing.\n    \n    \n      Storing security data in standardized locations provides advanced\n      analytics capabilities.  AWS recommends you deploy tools for\n      security analytics that operate in an AWS environment into a\n      Security\n      Tooling account that is separate from your Log Archive\n      account. This approach allows you to implement controls at depth\n      to protect the integrity and availability of the logs and log\n      management process, distinct from the tools that access them.\n       Consider using services, such as\n      Amazon Athena, to run on-demand queries that correlate multiple\n      data sources. You can also integrate visualization tools, such as\n      QuickSight. AI-powered solutions are becoming increasingly\n      available and can perform functions such as translating findings\n      into human-readable summaries and natural language interaction. These solutions are often more readily integrated by having a\n      standardized data storage location for querying.\n    \n   \n\n  Implementation steps\n\n      \n    \n       \n       \n       \n       \n    \n        \n          Create the Log Archive and Security\n          Tooling accounts\n        \n        \n           \n        \n            \n              Using AWS Organizations,\n              create\n              the Log Archive and Security Tooling accounts under\n              a security organizational unit. If you are using AWS Control Tower to manage your organization, the Log Archive\n              and Security Tooling accounts are created for you\n              automatically. Configure roles and permissions for\n              accessing and administering these accounts as required.\n            \n          \n      \n        \n          Configure your standardized security\n          data locations\n        \n        \n           \n        \n            \n              Determine your strategy for creating standardized security\n              data locations.  You can achieve this through options like\n              common data lake architecture approaches, third-party data\n              products, or\n              \n              Amazon Security Lake.  AWS recommends that you capture\n              security data from AWS Regions that are\n              opted-in\n              for your accounts, even when not actively in use.\n            \n          \n      \n        \n          Configure data source publication to\n          your standardized locations\n        \n        \n           \n        \n            \n              Identify the sources for your security data and configure\n              them to publish into your standardized locations. Evaluate\n              options to automatically export data in the desired format\n              as opposed to those where ETL processes need to be\n              developed. With Amazon Security Lake, you\n              can collect\n              data from supported AWS sources and integrated\n              third-party systems.\n            \n          \n      \n        \n          Configure tools to access your\n          standardized locations\n        \n        \n           \n        \n            \n              Configure tools such as Amazon Athena, QuickSight,\n              or third-party solutions to have the access required to\n              your standardized locations.  Configure these tools to\n              operate out of the Security Tooling account with\n              cross-account read access to the Log Archive account where\n              applicable.\n              Create\n              subscribers in Amazon Security Lake to provide\n              these tools access to your data.\n            \n          \n      \n   \n\n  Resources\n\n      \n    \n      Related best practices:\n    \n    \n       \n       \n       \n       \n    \n        \n          \n          SEC01-BP01 Separate workloads using accounts\n        \n        \n      \n        \n          SEC07-BP04 Define data lifecycle management\n        \n      \n        \n          SEC08-BP04 Enforce access control\n        \n      \n        \n          OPS08-BP02\n          Analyze workload logs\n        \n      \n    \n      Related documents:\n    \n    \n       \n       \n       \n    \n        \n          AWS           Whitepapers: Organizing Your AWS Environment Using Multiple\n          Accounts\n        \n      \n        \n          AWS           Prescriptive Guidance: AWS Security Reference Architecture\n          (AWS SRA)\n        \n      \n        \n          AWS           Prescriptive Guidance: Logging and monitoring guide for\n          application owners\n        \n      \n    \n      Related examples:\n    \n    \n       \n       \n       \n       \n       \n       \n    \n        \n          Aggregating,\n          searching, and visualizing log data from distributed sources\n          with Amazon Athena and QuickSight\n        \n      \n        \n          How\n          to visualize Amazon Security Lake findings with QuickSight\n        \n      \n        \n          Generate\n          AI powered insights for Amazon Security Lake using Amazon SageMaker AI Studio and Amazon Bedrock\n        \n      \n        \n          Identify\n          cybersecurity anomalies in your Amazon Security Lake data\n          using Amazon SageMaker AI\n        \n      \n        \n          Ingest,\n          transform, and deliver events published by Amazon Security Lake to Amazon OpenSearch Service\n        \n      \n        \n          Simplify\n            AWS CloudTrail log analysis with natural language query\n            generation in CloudTrail Lake\n        \n      \n    \n      Related tools:\n    \n    \n       \n       \n       \n       \n       \n       \n    \n        \n          Amazon Security Lake\n        \n      \n        \n          Amazon Security Lake Partner Integrations\n        \n      \n        \n          Open Cybersecurity\n          Schema Framework (OCSF)\n        \n      \n        \n          Amazon Athena\n        \n      \n        \n          QuickSight\n        \n      \n        \n          Amazon Bedrock\n        \n      \n  \u003cdiv\u003e\u003cdiv\u003e\u003cdiv\u003e\u003cdiv id=\"js_error_message\"\u003e\u003cp\u003e\u003cimg src=\"https://d1ge0kk1l5kms0.cloudfront.net/images/G/01/webservices/console/warning.png\" alt=\"Warning\" /\u003e \u003cstrong\u003eJavascript is disabled or is unavailable in your browser.\u003c/strong\u003e\u003c/p\u003e\u003cp\u003eTo use the Amazon Web Services Documentation, Javascript must be enabled. Please refer to your browser's Help pages for instructions.\u003c/p\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003eDocument ConventionsSEC04-BP01 Configure service and application loggingSEC04-BP03 Correlate and enrich security alerts",
  "SEC04-BP03 Correlate and enrich security alerts\n    Unexpected activity can generate multiple security alerts by\n    different sources, requiring further correlation and enrichment to\n    understand the full context. Implement automated correlation and\n    enrichment of security alerts to help achieve more accurate incident\n    identification and response.\n  \n    Desired outcome: As activity\n    generates different alerts within your workloads and environments,\n    automated mechanisms correlate data and enrich that data with\n    additional information. This pre-processing presents a more detailed\n    understanding of the event, which helps your investigators determine\n    the criticality of the event and if it constitutes an incident that\n    requires formal response. This process reduces the load on your\n    monitoring and investigation teams.\n  \n    Common anti-patterns:\n  \n     \n     \n     \n  \n      \n        Different groups of people investigate findings and alerts\n        generated by different systems, unless otherwise mandated by\n        separation of duty requirements.  \n      \n    \n      \n        Your organization funnels all security finding and alert data to\n        standard locations, but requires investigators to perform manual\n        correlation and enrichment.\n      \n    \n      \n        You rely solely on the intelligence of threat detection systems\n        to report on findings and establish criticality.\n      \n    \n    Benefits of establishing this best\n    practice: Automated correlation and enrichment of alerts\n    helps to reduce the overall cognitive load and manual data\n    preparation required of your investigators. This practice can reduce\n    the time it takes to determine if the event represents an incident\n    and initiate a formal response. Additional context also helps you\n    accurately assess the true severity of an event, as it may be higher\n    or lower than what any one alert suggests.\n  \n    Level of risk exposed if this best practice\n    is not established: Low \n  \n\n  Implementation guidance\n\n      \n    \n      Security alerts can come from many different sources within AWS,\n      including:\n    \n    \n       \n       \n       \n       \n       \n    \n        \n          Services such as\n          Amazon GuardDuty,\n          AWS Security Hub,\n          Amazon Macie,\n          Amazon Inspector,\n          AWS Config,\n          AWS Identity and Access Management Access Analyzer, and\n          Network Access Analyzer\n        \n      \n        \n          Alerts from automated analysis of AWS service, infrastructure,\n          and application logs, such as from\n          Security\n          Analytics for Amazon OpenSearch Service.\n        \n      \n        \n          Alarms in response to changes in your billing activity from\n          sources such as\n          Amazon CloudWatch,\n          Amazon EventBridge, or\n          AWS Budgets.\n        \n      \n        \n          Third-party sources such as threat intelligence feeds\n          and Security\n          Partner Solutions from the AWS Partner Network\n        \n      \n        \n          Contact\n          by AWS Trust \u0026 Safety or other sources, such as\n          customers or internal employees.\n        \n      \n    \n      In their most fundamental form, alerts contain information about\n      who (the principal or\n      identity) is doing what\n      (the action taken) to\n      what (the resources affected). For each of\n      these sources, identify if there are ways you can create mappings\n      across identifiers for these identities, actions, and resources as\n      the foundation for performing correlation. This can take the form\n      of integrating alert sources with a security information and event\n      management (SIEM) tool to perform automated correlation for you,\n      building your own data pipelines and processing, or a combination\n      of both.\n    \n    \n      An example of a service that can perform correlation for you is\n      Amazon Detective. Detective performs ongoing ingestion of alerts\n      from various AWS and third-party sources and uses different forms\n      of intelligence to assemble a visual graph of their relationships\n      to aid investigations.\n    \n    \n      While the initial criticality of an alert is an aid for\n      prioritization, the context in which the alert happened determines\n      its true criticality. As an example,\n      Amazon GuardDuty can alert that an Amazon EC2 instance within your\n      workload is querying an unexpected domain name. GuardDuty might\n      assign low criticality to this alert on its own. However,\n      automated correlation with other activity around the time of the\n      alert might uncover that several hundred EC2 instances were\n      deployed by the same identity, which increases overall operating\n      costs. In this event, this correlated event context would warrant\n      a new security alert and the criticality might be adjusted to\n      high, which would expedite further action.\n    \n     \n\n  Implementation steps\n\n      \n      \n         \n         \n         \n         \n      \n          \n            Identify sources for security alert information. Understand\n            how alerts from these systems represent identity, action,\n            and resources to determine where correlation is possible.\n          \n        \n          \n            Establish a mechanism for capturing alerts from different\n            sources. Consider services such as Security Hub,\n            EventBridge, and CloudWatch for this purpose.\n          \n        \n          \n            Identify sources for data correlation and enrichment.\n            Example sources include\n            AWS CloudTrail,\n            VPC\n              Flow Logs,\n            Route 53\n              Resolver logs, and infrastructure and application\n            logs. Any or all of these logs might be consumed through a\n            single integration with\n            Amazon\n              Security Lake.\n          \n        \n          \n            Integrate your alerts with your data correlation and\n            enrichment sources to create more detailed security event\n            contexts and establish criticality.\n          \n          \n             \n             \n          \n              \n                Amazon Detective, SIEM tooling, or other third-party\n                solutions can perform a certain level of ingestion,\n                correlation, and enrichment automatically.\n              \n            \n              \n                You can also use AWS services to build your own. For\n                example, you can invoke an AWS Lambda function to run an\n                Amazon Athena query against AWS CloudTrail or \n                Amazon Security Lake, and publish the results to EventBridge.\n              \n            \n        \n     \n   \n    \n    Resources\n    \n    \n    \n      Related best practices:\n    \n    \n       \n       \n       \n    \n        \n          SEC10-BP03 Prepare\n            forensic capabilities\n        \n      \n        \n          OPS08-BP04\n            Create actionable alerts\n        \n      \n        \n          REL06-BP03\n            Send notifications (Real-time processing and alarming)\n        \n      \n    \n      Related documents:\n    \n    \n       \n    \n        \n          AWS           Security Incident Response Guide\n        \n      \n    \n      Related examples:\n    \n    \n       \n    \n        \n          How\n            to enrich AWS Security Hub findings with account\n            metadata\n        \n      \n    \n      Related tools:\n    \n    \n       \n       \n       \n       \n    \n        \n          Amazon Detective\n        \n      \n        \n          Amazon EventBridge\n        \n      \n        \n          AWS Lambda\n        \n      \n        \n          Amazon Athena\n        \n      \n  \u003cdiv\u003e\u003cdiv\u003e\u003cdiv\u003e\u003cdiv id=\"js_error_message\"\u003e\u003cp\u003e\u003cimg src=\"https://d1ge0kk1l5kms0.cloudfront.net/images/G/01/webservices/console/warning.png\" alt=\"Warning\" /\u003e \u003cstrong\u003eJavascript is disabled or is unavailable in your browser.\u003c/strong\u003e\u003c/p\u003e\u003cp\u003eTo use the Amazon Web Services Documentation, Javascript must be enabled. Please refer to your browser's Help pages for instructions.\u003c/p\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003eDocument ConventionsSEC04-BP02 Capture logs, findings, and metrics\n      in standardized locationsSEC04-BP04 Initiate remediation for non-compliant\n  resources",
  "SEC04-BP04 Initiate remediation for non-compliant\n  resources\n    Your detective controls may alert on resources that are out of\n    compliance with your configuration requirements. You can initiate\n    programmatically-defined remediations, either manually or\n    automatically, to fix these resources and help minimize potential\n    impacts. When you define remediations programmatically, you can take\n    prompt and consistent action.\n  \n    While automation can enhance security operations, you should\n    implement and manage automation carefully.  Place appropriate\n    oversight and control mechanisms to verify that automated responses\n    are effective, accurate, and aligned with organizational policies\n    and risk appetite.\n  \n    Desired outcome: You define\n    resource configuration standards along with the steps to remediate\n    when resources are detected to be non-compliant. Where possible,\n    you've defined remediations programmatically so they can be\n    initiated either manually or through automation. Detection systems\n    are in place to identify non-compliant resources and publish alerts\n    into centralized tools that are monitored by your security\n    personnel. These tools support running your programmatic\n    remediations, either manually or automatically. Automatic\n    remediations have appropriate oversight and control mechanisms in\n    place to govern their use.\n  \n    Common anti-patterns:\n  \n     \n     \n     \n  \n      \n        You implement automation, but fail to thoroughly test and\n        validate remediation actions. This can result in unintended\n        consequences, such as disrupting legitimate business operations\n        or causing system instability.\n      \n    \n      \n        You improve response times and procedures through automation,\n        but without proper monitoring and mechanisms that allow human\n        intervention and judgment when needed.\n      \n    \n      \n        You rely solely on remediations, rather than having remediations\n        as one part of a broader incident response and recovery program.\n      \n    \n    Benefits of establishing this best\n    practice: Automatic remediations can respond to\n    misconfigurations faster than manual processes, which helps you\n    minimize potential business impacts and reduce the window of\n    opportunity for unintended uses. When you define remediations\n    programmatically, they are applied consistently, which reduces the\n    risk of human error. Automation also can handle a larger volume of\n    alerts simultaneously, which is particularly important in\n    environments operating at large scale.  \n  \n    Level of risk exposed if this best practice\n    is not established: Medium\n  \n\n  Implementation guidance\n\n      \n    \n      As described in SEC01-BP03 Identify and validate control objectives, services such as\n      AWS Config and\n      AWS Security Hub can help you monitor the configuration of\n      resources in your accounts for adherence to your requirements.\n      When non-compliant resources are detected, services such as AWS Security Hub, can help with routing alerts appropriately and\n      remediation. These solutions provide a central place for your\n      security investigators to monitor for issues and take corrective\n      action.\n    \n    \n      While some non-compliant resource situations are unique and\n      require human judgment to remediate, other situations have a\n      standard response that you can define programmatically. For\n      example, a standard response to a misconfigured VPC security group\n      could be to remove the disallowed rules and notify the owner.\n      Responses can be defined in\n      AWS Lambda functions,\n      AWS Systems Manager Automation documents, or through other code\n      environments you prefer. Make sure the environment is able to\n      authenticate to AWS using an IAM role with the least amount of\n      permission needed to take corrective action.\n    \n    \n      Once you define the desired remediation, you can then determine\n      your preferred means for initiating it. AWS Config can\n      initiate\n      remediations for you. If you are using Security Hub, you\n      can do this through\n      custom\n      actions, which publishes the finding information to\n      Amazon EventBridge. An EventBridge rule can then initiate your\n      remediation. You can configure remediations through Security Hub\n      to run either automatically or manually. \n    \n    \n      For programmatic remediation, we recommend that you have\n      comprehensive logs and audits for the actions taken, as well as\n      their outcomes. Review and analyze these logs to assess the\n      effectiveness of the automated processes, and identify areas of\n      improvement. Capture logs in\n      Amazon CloudWatch Logs and remediation outcomes as\n      finding\n      notes in Security Hub.\n    \n    \n      As a starting point, consider\n      Automated\n      Security Response on AWS, which has pre-built remediations\n      for resolving common security misconfigurations.\n    \n     \n\n  Implementation steps\n\n      \n      \n         \n         \n         \n         \n      \n          \n            Analyze and prioritize alerts.\n          \n          \n             \n          \n              \n                Consolidate security alerts from various AWS services\n                into Security Hub for centralized visibility,\n                prioritization, and remediation.\n              \n            \n        \n          \n            Develop remediations.\n          \n          \n             \n          \n              \n                Use services such as Systems Manager and AWS Lambda to\n                run programmatic remediations.\n              \n            \n        \n          \n            Configure how remediations are initiated.\n          \n          \n             \n             \n          \n              \n                Using Systems Manager, define custom actions that\n                publish findings to EventBridge. Configure these actions\n                to be initiated manually or automatically.\n              \n            \n              \n                You can also use\n                Amazon Simple Notification Service (SNS) to send\n                notifications and alerts to relevant stakeholders (like\n                security team or incident response teams) for manual\n                intervention or escalation, if required.\n              \n            \n        \n          \n            Review and analyze remediation logs for effectiveness and\n            improvement.\n          \n          \n             \n          \n              \n                Send log output to CloudWatch Logs. Capture outcomes as\n                finding notes in Security Hub.\n              \n            \n        \n     \n   \n    \n    Resources\n    \n    \n    \n      Related best practices:\n    \n    \n       \n    \n        \n          SEC06-BP03\n            Reduce manual management and interactive access\n        \n      \n    \n      Related documents:\n    \n    \n       \n    \n        \n          AWS           Security Incident Response Guide - Detection\n        \n      \n    \n      Related examples:\n    \n    \n       \n       \n       \n       \n    \n        \n          Automated\n            Security Response on AWS\n        \n      \n        \n          Monitor\n            EC2 instance key pairs using AWS Config\n        \n      \n        \n          Create\n            AWS Config custom rules by using AWS CloudFormation Guard\n            policies\n        \n      \n        \n          Automatically\n            remediate unencrypted Amazon RDS DB instances and\n            clusters\n        \n      \n    \n      Related tools:\n    \n    \n       \n       \n    \n        \n          AWS Systems Manager Automation\n        \n      \n        \n          Automated\n            Security Response on AWS\n        \n      \n  \u003cdiv\u003e\u003cdiv\u003e\u003cdiv\u003e\u003cdiv id=\"js_error_message\"\u003e\u003cp\u003e\u003cimg src=\"https://d1ge0kk1l5kms0.cloudfront.net/images/G/01/webservices/console/warning.png\" alt=\"Warning\" /\u003e \u003cstrong\u003eJavascript is disabled or is unavailable in your browser.\u003c/strong\u003e\u003c/p\u003e\u003cp\u003eTo use the Amazon Web Services Documentation, Javascript must be enabled. Please refer to your browser's Help pages for instructions.\u003c/p\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003eDocument ConventionsSEC04-BP03 Correlate and enrich security alertsInfrastructure protection",
  "SEC05-BP01 Create network layers\n    Segment your network topology into different layers based on logical\n    groupings of your workload components according to their data\n    sensitivity and access requirements. Distinguish between components\n    that require inbound access from the internet, such as public web\n    endpoints, and those that only need internal access, such as\n    databases. \n  \n    Desired outcome: The layers of\n    your network are part of an integral defense-in-depth approach to\n    security that complements the identity authentication and\n    authorization strategy of your workloads. Layers are in place\n    according to data sensitivity and access requirements, with\n    appropriate traffic flow and control mechanisms.\n  \n    Common anti-patterns:\n  \n     \n     \n     \n  \n      \n        You create all resources in a single VPC or subnet.\n      \n    \n      \n        You construct your network layers without consideration of data\n        sensitivity requirements, component behaviors, or functionality.\n      \n    \n      \n        You use VPCs and subnets as defaults for all network layer\n        considerations, and you don't consider how AWS managed services\n        influence your topology.\n      \n    \n    Benefits of establishing this best\n      practice: Establishing network layers is the first step\n    in restricting unnecessary pathways through the network,\n    particularly those that lead to critical systems and data. This\n    makes it harder for unauthorized actors to gain access to your\n    network and navigate to additional resources within. Discrete\n    network layers beneficially reduce the scope of analysis for\n    inspection systems, such as for intrusion detection or malware\n    prevention. This reduces the potential for false positives and\n    unnecessary processing overhead.\n  \n    Level of risk exposed if this best practice\n      is not established: High\n  \n    \n    Implementation guidance\n    \n    \n    \n      When designing a workload architecture, it is common to separate\n      components into different layers based on their responsibility.\n      For example, a web application can have a presentation layer,\n      application layer, and data layer. You can\n      take a similar approach when designing your network topology.\n      Underlying network controls can help enforce your workload's data\n      access requirements. For example, in a three-tier web application\n      architecture, you can store your static presentation layer files\n      on Amazon S3 and serve them from a content delivery network (CDN),\n      such as Amazon CloudFront. The application layer can have public\n      endpoints that an Application Load Balancer (ALB) serves in an\n      Amazon VPC public subnet (similar to a demilitarized zone, or\n      DMZ), with back-end services deployed into private subnets. The\n      data layer, that is hosting resources such as databases and shared\n      file systems, can reside in different private subnets from the\n      resources of your application layer. At each of these layer\n      boundaries (CDN, public subnet, private subnet), you can deploy\n      controls that allow only authorized traffic to traverse across\n      those boundaries.\n    \n    \n      Similar to modeling network layers based on the functional purpose\n      of your workload's components, also consider the sensitivity of\n      the data being processed. Using the web application example,\n      while all of your workload services may reside within the\n      application layer, different services may process data with\n      different sensitivity levels. In this case, dividing the\n      application layer using multiple private subnets, different VPCs\n      in the same AWS account, or even different VPCs in different AWS accounts for each level of data sensitivity may be appropriate\n      according to your control requirements.\n    \n    \n      A further consideration for network layers is the behavior\n      consistency of your workload's components. Continuing the\n      example, in the application layer you may have services that\n      accept inputs from end-users or external system integrations that\n      are inherently riskier than the inputs to other services. Examples\n      include file uploads, code scripts to run, email scanning and so\n      on. Placing these services in their own network layer helps\n      create a stronger isolation boundary around them, and can prevent\n      their unique behavior from creating false positive alerts in\n      inspection systems.\n    \n    \n      As part of your design, consider how using AWS managed services\n      influences your network topology. Explore how services such as\n      Amazon VPC Lattice can help make the interoperability of your\n      workload components across network layers easier. When using AWS Lambda, deploy in your VPC subnets unless there are specific\n      reasons not to. Determine where VPC endpoints and AWS PrivateLink\n      can simplify adhering to security policies that limit access to\n      internet gateways.\n    \n     \n      \n      Implementation steps\n      \n      \n      \n         \n         \n         \n         \n      \n          \n            Review your workload architecture. Logically group\n            components and services based on the functions they serve,\n            the sensitivity of data being processed, and their behavior.\n          \n        \n          \n            For components responding to requests from the internet,\n            consider using load balancers or other proxies to provide\n            public endpoints. Explore shifting security controls by\n            using managed services, such as CloudFront, Amazon API Gateway, Elastic Load Balancing, and AWS Amplify to host\n            public endpoints.\n          \n        \n          \n            For components running in compute environments, such as\n            Amazon EC2 instances, AWS Fargate containers, or Lambda\n            functions, deploy these into private subnets based on your\n            groups from the first step.\n          \n        \n          \n            For fully managed AWS services, such as Amazon DynamoDB,\n            Amazon Kinesis, or Amazon SQS, consider using VPC\n            endpoints as the default for access over private IP\n            addresses.\n          \n        \n     \n   \n    \n    Resources\n    \n    \n    \n      Related best practices:\n    \n    \n       \n       \n    \n        \n          REL02\n            Plan your network topology\n        \n      \n        \n          PERF04-BP01\n            Understand how networking impacts performance\n        \n      \n    \n      Related videos:\n    \n    \n       \n    \n        \n          AWS           re:Invent 2023 - AWS networking foundations\n        \n      \n    \n      Related examples:\n    \n    \n       \n       \n       \n    \n        \n          VPC\n            examples\n        \n      \n        \n          Access\n            container applications privately on Amazon ECS by using AWS Fargate, AWS PrivateLink, and a Network Load Balancer\n        \n      \n        \n          Serve\n            static content in an Amazon S3 bucket through a VPC by using\n            Amazon CloudFront\n        \n      \n  \u003cdiv\u003e\u003cdiv\u003e\u003cdiv\u003e\u003cdiv id=\"js_error_message\"\u003e\u003cp\u003e\u003cimg src=\"https://d1ge0kk1l5kms0.cloudfront.net/images/G/01/webservices/console/warning.png\" alt=\"Warning\" /\u003e \u003cstrong\u003eJavascript is disabled or is unavailable in your browser.\u003c/strong\u003e\u003c/p\u003e\u003cp\u003eTo use the Amazon Web Services Documentation, Javascript must be enabled. Please refer to your browser's Help pages for instructions.\u003c/p\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003eDocument Conventions SEC 5. How do you protect your network resources? SEC05-BP02 Control traffic flow within your network\n  layers",
  "SEC05-BP02 Control traffic flow within your network\n  layers\n    Within the layers of your network, use further segmentation to\n    restrict traffic only to the flows necessary for each workload.\n    First, focus on controlling traffic between the internet or other\n    external systems to a workload and your environment\n    (north-south traffic). Afterwards, look at\n    flows between different components and systems\n    (east-west traffic).\n  \n    Desired outcome: You permit only\n    the network flows necessary for the components of your workloads to\n    communicate with each other and their clients and any other services\n    they depend on. Your design factors in considerations such as public\n    compared to private ingress and egress, data classification,\n    regional regulations, and protocol requirements. Wherever possible,\n    you favor point-to-point flows over network peering as part of a\n    principle of least privilege design.\n  \n    Common anti-patterns:\n  \n     \n     \n     \n     \n  \n      \n        You take a perimeter-based approach to network security and only\n        control traffic flow at the boundary of your network layers.\n      \n    \n      \n        You assume all traffic within a network layer is authenticated\n        and authorized.\n      \n    \n      \n        You apply controls for either your ingress traffic or your\n        egress traffic, but not both.\n      \n    \n      \n        You rely solely on your workload components and network controls\n        to authenticate and authorize traffic.\n      \n    \n    Benefits of establishing this best\n    practice: This practice helps reduce the risk of\n    unauthorized movement within your network and adds an extra layer of\n    authorization to your workloads. By performing traffic flow control,\n    you can restrict the scope of impact of a security incident and\n    speed up detection and response.\n  \n    Level of risk exposed if this best practice\n    is not established: High\n  \n\n  Implementation guidance\n\n      \n    \n      While network layers help establish the boundaries around\n      components of your workload that serve a similar function, data\n      sensitivity level, and behavior, you can create a much\n      finer-grained level of traffic control by using techniques to\n      further segment components within these layers that follows the\n      principle of least privilege. Within AWS, network layers are\n      primarily defined using subnets according to IP address ranges\n      within an Amazon VPC. Layers can also be defined using different\n      VPCs, such as for grouping microservice environments by business\n      domain. When using multiple VPCs, mediate routing using an AWS Transit Gateway. While this provides traffic control at a Layer 4\n      level (IP address and port ranges) using security groups and route\n      tables, you can gain further control using additional services,\n      such as AWS PrivateLink, Amazon Route 53 Resolver DNS Firewall,\n      AWS Network Firewall, and AWS WAF.\n    \n    \n      Understand and inventory the data flow and communication\n      requirements of your workloads in terms of connection-initiating\n      parties, ports, protocols, and network layers. Evaluate the\n      protocols available for establishing connections and transmitting\n      data to select ones that achieve your protection requirements (for\n      example, HTTPS rather than HTTP). Capture these requirements at\n      both the boundaries of your networks and within each layer. Once\n      these requirements are identified, explore options to only allow\n      the required traffic to flow at each connection point. A good\n      starting point is to use security\n      groups within your VPC, as they can be attached to\n      resources that uses an Elastic Network Interface (ENI), such\n      Amazon EC2 instances, Amazon ECS tasks, Amazon EKS pods, or Amazon RDS databases. Unlike a Layer 4 firewall, a security group can\n      have a rule that allows traffic from another security group by its\n      identifier, minimizing updates as resources within the group\n      change over time. You can also filter traffic using both inbound\n      and outbound rules using security groups.\n    \n    \n      When traffic moves between VPCs, it's common to use VPC peering\n      for simple routing or the AWS Transit Gateway for complex routing.\n      With these approaches, you facilitate traffic flows between the\n      range of IP addresses of both the source and destination networks.\n      However, if your workload only requires traffic flows between\n      specific components in different VPCs, consider using a\n      point-to-point connection using\n      AWS PrivateLink. To do this, identify which service should act\n      as the producer and which should act as the consumer. Deploy a\n      compatible load balancer for the producer, turn on PrivateLink\n      accordingly, and then accept a connection request by the\n      consumer. The producer service is then assigned a private IP\n      address from the consumer's VPC that the consumer can use to make\n      subsequent requests. This approach reduces the need to peer the\n      networks. Include the costs for data processing and load balancing\n      as part of evaluating PrivateLink.\n    \n    \n      While security groups and PrivateLink help control the flow\n      between the components of your workloads, another major\n      consideration is how to control which DNS domains your resources\n      are allowed to access (if any). Depending on the DHCP\n      configuration of your VPCs, you can consider two different AWS\n      services for this purpose. Most customers use the default Route 53\n      Resolver DNS service (also called Amazon DNS server or\n      AmazonProvidedDNS) available to VPCs at the +2 address of its CIDR\n      range. With this approach, you can create DNS Firewall rules and\n      associate them to your VPC that determine what actions to take for\n      the domain lists you supply.\n    \n    \n      If you are not using the Route 53 Resolver, or if you want to\n      complement the Resolver with deeper inspection and flow control\n      capabilities beyond domain filtering, consider deploying an AWS Network Firewall. This service inspects individual packets using\n      either stateless or stateful rules to determine whether to deny or\n      allow the traffic. You can take a similar approach for filtering\n      inbound web traffic to your public endpoints using AWS WAF. For\n      further guidance on these services, see SEC05-BP03 Implement\n        inspection-based protection.\n    \n     \n\n  Implementation steps\n\n      \n      \n         \n         \n         \n      \n          \n            Identify the required data flows between the components of\n            your workloads.\n          \n        \n          \n            Apply multiple controls with a defense-in-depth approach for\n            both inbound and outbound traffic, including the use of\n            security groups, and route tables. \n          \n        \n          \n            Use firewalls to define fine-grained control over network\n            traffic in, out, and across your VPCs, such as the Route 53\n            Resolver DNS Firewall, AWS Network Firewall, and AWS WAF.\n            Consider using the\n            AWS Firewall Manager for centrally configuring and\n            managing your firewall rules across your organization.\n          \n        \n     \n   \n    \n    Resources\n    \n    \n    \n      Related best practices:\n    \n    \n       \n       \n    \n        \n          REL03-BP01\n            Choose how to segment your workload\n        \n      \n        \n          SEC09-BP02\n            Enforce encryption in transit\n        \n      \n    \n      Related documents:\n    \n    \n       \n       \n       \n       \n    \n        \n          Security\n            best practices for your VPC\n        \n      \n        \n          AWS           Network Optimization Tips\n        \n      \n        \n          Guidance\n            for Network Security on AWS\n        \n      \n        \n          Secure\n            your VPC's outbound network traffic in the AWS Cloud\n        \n      \n    \n      Related tools:\n    \n    \n       \n    \n        \n          AWS Firewall Manager\n        \n      \n    \n      Related videos:\n    \n    \n       \n       \n       \n    \n        \n          AWS Transit Gateway reference architectures for many VPCs\n        \n      \n        \n          Application\n            Acceleration and Protection with Amazon CloudFront, AWS WAF,\n            and AWS Shield\n        \n      \n        \n          AWS           re:Inforce 2023: Firewalls and where to put them\n        \n      \n   \n    \n  \u003cdiv\u003e\u003cdiv\u003e\u003cdiv\u003e\u003cdiv id=\"js_error_message\"\u003e\u003cp\u003e\u003cimg src=\"https://d1ge0kk1l5kms0.cloudfront.net/images/G/01/webservices/console/warning.png\" alt=\"Warning\" /\u003e \u003cstrong\u003eJavascript is disabled or is unavailable in your browser.\u003c/strong\u003e\u003c/p\u003e\u003cp\u003eTo use the Amazon Web Services Documentation, Javascript must be enabled. Please refer to your browser's Help pages for instructions.\u003c/p\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003eDocument ConventionsSEC05-BP01 Create network layersSEC05-BP03 Implement inspection-based protection",
  "SEC05-BP03 Implement inspection-based protection\n    Set up traffic inspection points between your network layers to make\n    sure data in transit matches the expected categories and patterns.\n     Analyze traffic flows, metadata, and patterns to help identify,\n    detect, and respond to events more effectively.\n  \n    Desired outcome: Traffic that\n    traverses between your network layers are inspected and authorized.\n     Allow and deny decisions are based on explicit rules, threat\n    intelligence, and deviations from baseline behaviors.  Protections\n    become stricter as traffic moves closer to sensitive data.\n  \n    Common anti-patterns:\n  \n     \n     \n     \n     \n  \n      \n        Relying solely on firewall rules based on ports and protocols.\n        Not taking advantage of intelligent systems.\n      \n    \n      \n        Authoring firewall rules based on specific current threat\n        patterns that are subject to change.\n      \n    \n      \n        Only inspecting traffic where traffic transits from private to\n        public subnets, or from public subnets to the Internet.\n      \n    \n      \n        Not having a baseline view of your network traffic to compare\n        for behavior anomalies.\n      \n    \n    Benefits of establishing this best\n      practice: Inspection systems allow you to author\n    intelligent rules, such as allowing or denying traffic only when\n    certain conditions within the traffic data exist. Benefit from\n    managed rule sets from AWS and partners, based on the latest threat\n    intelligence, as the threat landscape changes over time.  This\n    reduces the overhead of maintaining rules and researching indicators\n    of compromise, reducing the potential for false positives.\n  \n    Level of risk exposed if this best practice\n      is not established: Medium\n  \n    \n    Implementation guidance\n    \n    \n    \n      Have fine-grained control over both your stateful and stateless\n      network traffic using AWS Network Firewall, or other\n      Firewalls\n      and\n      Intrusion\n        Prevention Systems (IPS) on AWS Marketplace that you can\n      deploy behind a Gateway Load Balancer (GWLB).  AWS Network Firewall supports\n      Suricata-compatible\n      open source IPS specifications to help protect your workload.\n    \n    \n      Both the AWS Network Firewall and vendor solutions that use a GWLB\n      support different inline inspection deployment models.  For\n      example, you can perform inspection on a per-VPC basis, centralize\n      in an inspection VPC, or deploy in a hybrid model where east-west\n      traffic flows through an inspection VPC and Internet ingress is\n      inspected per-VPC.  Another consideration is whether the solution\n      supports unwrapping Transport Layer Security (TLS), enabling deep\n      packet inspection for traffic flows initiated in either direction.\n      For more information and in-depth details on these configurations,\n      see the\n      AWS Network Firewall Best Practice guide.\n    \n    \n      If you are using solutions that perform out-of-band inspections,\n      such as pcap analysis of packet data from network interfaces\n      operating in promiscuous mode, you can\n      configure VPC\n        traffic mirroring. Mirrored traffic counts towards the\n      available bandwidth of your interfaces and is subject to the same\n      data transfer charges as non-mirrored traffic. You can see if\n      virtual versions of these appliances are available on the\n      AWS Marketplace, which may support inline deployment behind a\n      GWLB.\n    \n    \n      For components that transact over HTTP-based protocols, protect\n      your application from common threats with a web application\n      firewall (WAF). AWS WAF is a web application firewall that lets you monitor and\n      block HTTP(S) requests that match your configurable rules before\n      sending to Amazon API Gateway, Amazon CloudFront, AWS AppSync or\n      an Application Load Balancer. Consider deep packet inspection when\n      you evaluate the deployment of your web application firewall, as\n      some require you to terminate TLS before traffic inspection. To\n      get started with AWS WAF, you can use\n      AWS Managed Rules in combination with your own, or use existing\n      partner\n        integrations.\n    \n    \n      You can centrally manage AWS WAF, AWS Shield Advanced, AWS Network Firewall, and Amazon VPC security groups across your AWS\n      Organization\n      with AWS Firewall Manager. \n    \n   \n    \n    Implementation steps\n    \n    \n    \n       \n       \n       \n       \n    \n        \n          Determine if you can scope inspection rules broadly, such as\n          through an inspection VPC, or if you require a more granular\n          per-VPC approach.\n        \n      \n        \n          For inline inspection solutions:\n        \n        \n           \n           \n        \n            \n              If using AWS Network Firewall, create rules, firewall\n              policies, and the firewall itself. Once these have been\n              configured, you can\n              route\n                traffic to the firewall endpoint to enable\n              inspection. \n            \n          \n            \n              If using a third-party appliance with a Gateway Load\n              Balancer (GWLB), deploy and configure your appliance in\n              one or more availability zones. Then, create your GWLB,\n              the endpoint service, endpoint, and configure routing for\n              your traffic.\n            \n          \n      \n        \n          For out-of-band inspection solutions:\n        \n        \n           \n        \n            \n              Turn on VPC Traffic Mirroring on interfaces where inbound\n              and outbound traffic should be mirrored. You can use\n              Amazon EventBridge rules to invoke an AWS Lambda function\n              to turn on traffic mirroring on interfaces when new\n              resources are created. Point the traffic mirroring\n              sessions to the Network Load Balancer in front of your\n              appliance that processes traffic.\n            \n          \n      \n        \n          For inbound web traffic solutions:\n        \n        \n           \n           \n        \n            \n              To configure AWS WAF, start by configuring a web access\n              control list (web ACL). The web ACL is a collection of\n              rules with a serially processed default action (ALLOW or\n              DENY) that defines how your WAF handles traffic. You can\n              create your own rules and groups or use AWS managed rule\n              groups in your web ACL.\n            \n          \n            \n              Once your web ACL is configured, associate the web ACL\n              with an AWS resource (like an Application Load Balancer,\n              API Gateway REST API, or CloudFront distribution) to begin\n              protecting web traffic.\n            \n          \n      \n   \n    \n    Resources\n    \n    \n    \n      Related documents:\n    \n    \n       \n       \n       \n       \n    \n        \n          What\n            is Traffic Mirroring?\n        \n      \n        \n          Implementing\n            inline traffic inspection using third-party security\n            appliances\n        \n      \n        \n          AWS Network Firewall example architectures with routing\n        \n      \n        \n          Centralized\n            inspection architecture with AWS Gateway Load Balancer and AWS Transit Gateway\n        \n      \n    \n      Related examples:\n    \n    \n       \n       \n    \n        \n          Best\n            practices for deploying Gateway Load Balancer\n        \n      \n        \n          TLS\n            inspection configuration for encrypted egress traffic and AWS Network Firewall\n        \n      \n    \n      Related tools:\n    \n    \n       \n    \n        \n          AWS Marketplace IDS/IPS\n        \n      \n  \u003cdiv\u003e\u003cdiv\u003e\u003cdiv\u003e\u003cdiv id=\"js_error_message\"\u003e\u003cp\u003e\u003cimg src=\"https://d1ge0kk1l5kms0.cloudfront.net/images/G/01/webservices/console/warning.png\" alt=\"Warning\" /\u003e \u003cstrong\u003eJavascript is disabled or is unavailable in your browser.\u003c/strong\u003e\u003c/p\u003e\u003cp\u003eTo use the Amazon Web Services Documentation, Javascript must be enabled. Please refer to your browser's Help pages for instructions.\u003c/p\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003eDocument ConventionsSEC05-BP02 Control traffic flow within your network\n  layersSEC05-BP04 Automate network protection",
  "SEC05-BP04 Automate network protection\n    Automate the deployment of your network protections using DevOps\n    practices, such as infrastructure as code (IaC)\n    and CI/CD pipelines.  These practices can help you track changes in\n    your network protections through a version control system, reduce\n    the time it takes to deploy changes, and help detect if your network\n    protections drift from your desired configuration.  \n  \n    Desired outcome: You define\n    network protections with templates and commit them into a version\n    control system.  Automated pipelines are initiated when new changes\n    are made that orchestrates their testing and deployment.  Policy\n    checks and other static tests are in place to validate changes\n    before deployment.  You deploy changes into a staging environment to\n    validate the controls are operating as expected.  Deployment into\n    your production environments is also performed automatically once\n    controls are approved.\n  \n    Common anti-patterns:\n  \n     \n     \n     \n  \n      \n        Relying on individual workload teams to each define their\n        complete network stack, protections, and automations.  Not\n        publishing standard aspects of the network stack and protections\n        centrally for workload teams to consume.\n      \n    \n      \n        Relying on a central network team to define all aspects of the\n        network, protections, and automations.  Not delegating\n        workload-specific aspects of the network stack and protections\n        to that workload's team.\n      \n    \n      \n        Striking the right balance between centralization and delegation\n        between a network team and workload teams, but not applying\n        consistent testing and deployment standards across your IaC\n        templates and CI/CD pipelines.  Not capturing required\n        configurations in tooling that checks your templates for\n        adherence.\n      \n    \n    Benefits of establishing this best\n      practice: Using templates to define your network\n    protections allows you to track and compare changes over time with a\n    version control system.  Using automation to test and deploy changes\n    creates standardization and predictability, increasing the chances\n    of a successful deployment and reducing repetitive manual\n    configurations.\n  \n    Level of risk exposed if this best practice\n      is not established: Medium \n  \n    \n    Implementation guidance\n    \n    \n    \n      A number of network protection controls described in\n      SEC05-BP02 Control traffic flows\n        within your network layers and\n      SEC05-BP03 Implement\n        inspection-based protection come with managed rules systems\n      that can update automatically based on the latest threat\n      intelligence.  Examples of protecting your web endpoints include\n      AWS WAF managed rules and\n      AWS Shield Advanced automatic application layer DDoS\n        mitigation.\n      Use AWS Network Firewall managed rule groups to stay up to date\n      with low-reputation domain lists and threat signatures as well.\n    \n    \n      Beyond managed rules, we recommend you use DevOps practices to\n      automate deploying your network resources, protections, and the\n      rules you specify.  You can capture these definitions in\n      AWS CloudFormation or another infrastructure as\n        code (IaC) tool of your choice, commit them to a\n      version control system, and deploy them using CI/CD pipelines.\n       Use this approach to gain the traditional benefits of DevOps for\n      managing your network controls, such as more predictable releases,\n      automated testing using tools like\n      AWS CloudFormation Guard, and detecting drift between your\n      deployed environment and your desired configuration.\n    \n    \n      Based on the decisions you made as part of\n      SEC05-BP01 Create network\n        layers, you may have a central management approach to\n      creating VPCs that are dedicated for ingress, egress, and\n      inspection flows.  As described in the\n      AWS       Security Reference Architecture (AWS SRA), you can define\n      these VPCs in a dedicated\n      Network\n        infrastructure account.  You can use similar techniques to\n      centrally define the VPCs used by your workloads in other\n      accounts, their security groups, AWS Network Firewall deployments,\n      Route 53 Resolver rules and DNS Firewall configurations, and other\n      network resources.  You can share these resources with your other\n      accounts with the\n      AWS Resource Access Manager.  With this approach, you can\n      simplify the automated testing and deployment of your network\n      controls to the Network account, with only one destination to\n      manage.  You can do this in a hybrid model, where you deploy and\n      share certain controls centrally and delegate other controls to\n      the individual workload teams and their respective accounts.\n    \n   \n    \n    Implementation steps\n    \n    \n    \n       \n       \n       \n       \n    \n        \n          Establish ownership over which aspects of the network and\n          protections are defined centrally, and which your workload\n          teams can maintain.\n        \n      \n        \n          Create environments to test and deploy changes to your network\n          and its protections.  For example, use a Network Testing\n          account and a Network Production account.\n        \n      \n        \n          Determine how you will store and maintain your templates in a\n          version control system.  Store central templates in a\n          repository that is distinct from workload repositories, while\n          workload templates can be stored in repositories specific to\n          that workload.\n        \n      \n        \n          Create CI/CD pipelines to test and deploy templates.  Define\n          tests to check for misconfigurations and that templates adhere\n          to your company standards.\n        \n      \n   \n    \n    Resources\n    \n    \n    \n      Related best practices:\n    \n    \n       \n    \n        \n          SEC01-BP06\n            Automate deployment of standard security controls\n        \n      \n    \n      Related documents:\n    \n    \n       \n    \n        \n          AWS           Security Reference Architecture - Network account\n        \n      \n    \n      Related examples:\n    \n    \n       \n       \n       \n    \n        \n          AWS           Deployment Pipeline Reference Architecture\n        \n      \n        \n          NetDevSecOps\n            to modernize AWS networking deployments\n        \n      \n        \n          Integrating\n            AWS CloudFormation security tests with AWS Security Hub and\n            AWS CodeBuild reports\n        \n      \n    \n      Related tools:\n    \n    \n       \n       \n       \n    \n        \n          AWS CloudFormation\n        \n      \n        \n          AWS CloudFormation Guard\n        \n      \n        \n          cfn_nag\n        \n      \n  \u003cdiv\u003e\u003cdiv\u003e\u003cdiv\u003e\u003cdiv id=\"js_error_message\"\u003e\u003cp\u003e\u003cimg src=\"https://d1ge0kk1l5kms0.cloudfront.net/images/G/01/webservices/console/warning.png\" alt=\"Warning\" /\u003e \u003cstrong\u003eJavascript is disabled or is unavailable in your browser.\u003c/strong\u003e\u003c/p\u003e\u003cp\u003eTo use the Amazon Web Services Documentation, Javascript must be enabled. Please refer to your browser's Help pages for instructions.\u003c/p\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003eDocument ConventionsSEC05-BP03 Implement inspection-based protectionSEC 6. How do you protect your compute resources?",
  "SEC06-BP01 Perform vulnerability managementFrequently scan and patch for vulnerabilities in your code, dependencies, and in your infrastructure to help protect against new threats.\n    Desired outcome: You have a\n    solution that continually scans your workload for software\n    vulnerabilities, potential defects, and unintended network exposure.\n    You have established processes and procedures to identify,\n    prioritize, and remediate these vulnerabilities based on risk\n    assessment criteria. Additionally, you have implemented automated\n    patch management for your compute instances. Your vulnerability\n    management program is integrated into your software development\n    lifecycle, with solutions to scan your source code during the CI/CD\n    pipeline.\n  \n    Common anti-patterns:\n  \n     \n     \n     \n     \n  \n      \n        Not having a vulnerability management program.\n      \n    \n      \n        Performing system patching without considering severity or risk avoidance.\n      \n    \n      \n        Using software that has passed its vendor-provided end of life (EOL) date.\n      \n    \n      \n        Deploying code into production before analyzing it for security issues.\n      \n    \n    Level of risk exposed if this best practice\n    is not established: High\n  \n    \n    Implementation guidance\n    \n    \n    \n      Vulnerability management is a key aspect of maintaining a secure\n      and robust cloud environment. It involves a comprehensive process\n      that includes security scans, identification and prioritization of\n      issues, and patch operations to resolve the identified\n      vulnerabilities. Automation plays a pivotal role in this process\n      because it facilitates continuous scanning of workloads for\n      potential issues and unintended network exposure, as well as\n      remediation efforts.\n    \n    \n      The\n      AWS       Shared Responsibility Model is a fundamental concept that\n      underpins vulnerability management. According to this model, AWS\n      is responsible for securing the underlying infrastructure,\n      including hardware, software, networking, and facilities that run\n      AWS services. Conversely, you are responsible for securing your\n      data, security configurations, and management tasks associated\n      with services like Amazon EC2 instances and Amazon S3 objects.\n    \n    \n      AWS offers a range of services to support vulnerability management\n      programs.\n      Amazon Inspector continuously scans AWS workloads for software\n      vulnerabilities and unintended network access, while\n      AWS Systems Manager Patch Manager helps manage patching across\n      Amazon EC2 instances. These services can be integrated with\n      AWS Security Hub, a cloud security posture management service\n      that automates AWS security checks, centralizes security alerts,\n      and provides a comprehensive view of an organization's security\n      posture. Furthermore,\n      Amazon CodeGuru Security uses static code analysis to identify\n      potential issues in Java and Python applications during the\n      development phase.\n    \n    \n      By incorporating vulnerability management practices into the\n      software development lifecycle, you can proactively address\n      vulnerabilities before they are introduced into production\n      environments, which reduces the risk of security events and\n      minimizes the potential impact of vulnerabilities.\n    \n     \n      \n      Implementation steps\n      \n      \n      \n         \n         \n         \n         \n         \n         \n         \n         \n         \n         \n      \n          \n            Understand the shared responsibility\n              model: Review the AWS shared responsibility model\n            to understand your responsibilities for securing your\n            workloads and data in the cloud. AWS is responsible for\n            securing the underlying cloud infrastructure, while you are\n            responsible for securing your applications, data, and the\n            services you use.\n          \n        \n          \n            Implement vulnerability\n              scanning: Configure a vulnerability scanning\n            service, such as Amazon Inspector, to automatically scan\n            your compute instances (for example, virtual machines,\n            containers, or serverless functions) for software\n            vulnerabilities, potential defects, and unintended network\n            exposure.\n          \n        \n          \n            Establish vulnerability management\n              processes: Define processes and procedures to\n            identify, prioritize, and remediate vulnerabilities. This\n            may include the setup of regular vulnerability scanning\n            schedules, establishment of risk assessment criteria, and\n            definition of remediation timelines based on vulnerability\n            severity.\n          \n        \n          \n            Set up patch management:\n            Use a patch management service to automate the process of\n            patching your compute instances, both for operating systems\n            and applications. You can configure the service to scan\n            instances for missing patches and automatically install them\n            on a schedule. Consider AWS Systems Manager Patch Manager to\n            provide this functionality.\n          \n        \n          \n            Configure malware\n              protection: Implement mechanisms to detect\n            malicious software in your environment. For example, you can\n            use tools like\n            Amazon GuardDuty to analyze, detect, and alert of malware in\n            EC2 and EBS volumes. GuardDuty can also scan newly uploaded\n            objects to Amazon S3 for potential malware or viruses and\n            take action to isolate them before they are ingested into\n            downstream processes.\n          \n        \n          \n            Integrate vulnerability scanning in\n              CI/CD pipelines: If you're using a CI/CD pipeline\n            for your application deployment, integrate vulnerability\n            scanning tools into your pipeline. Tools like Amazon CodeGuru Security and open-source options can scan your\n            source code, dependencies, and artifacts for potential\n            security issues.\n          \n        \n          \n            Configure a security monitoring\n              service: Set up a security monitoring service,\n            such as AWS Security Hub, to get a comprehensive view of\n            your security posture across multiple cloud services. The\n            service should collect security findings from various\n            sources and present them in a standardized format for easier\n            prioritization and remediation.\n          \n        \n          \n            Implement web application\n              penetration testing: If your application is a web\n            application, and your organization has the necessary skills\n            or can hire outside assistance, consider implementing web\n            application penetration testing to identify potential\n            vulnerabilities in your application.\n          \n        \n          \n            Automate with infrastructure as\n              code: Use infrastructure as code (IaC) tools,\n            such as\n            AWS CloudFormation, to automate the deployment and\n            configuration of your resources, including the security\n            services mentioned previously. This practice helps you\n            create a more consistent and standardized resource\n            architecture across multiple accounts and environments.\n          \n        \n          \n            Monitor and continually\n              improve: Continually monitor your vulnerability\n            management program's effectiveness, and make improvements as\n            needed. Review security findings, assess the effectiveness\n            of your remediation efforts, and adjust your processes and\n            tools accordingly.\n          \n        \n     \n   \n\n  Resources\n\n      \n    \n      Related documents:\n    \n    \n       \n       \n       \n       \n       \n    \n        \n          AWS Systems Manager\n        \n      \n        \n          Security\n          Overview of AWS Lambda\n        \n      \n        \n          Amazon CodeGuru\n        \n      \n        \n          Improved, Automated Vulnerability Management for Cloud Workloads with a New Amazon Inspector\n        \n      \n        \n          Automate vulnerability management and remediation in AWS using Amazon Inspector and AWS Systems Manager – Part 1\n        \n      \n    \n      Related videos:\n    \n    \n       \n       \n    \n        \n          Securing\n          Serverless and Container Services\n        \n      \n        \n          Security best\n          practices for the Amazon EC2 instance metadata service\n        \n      \n  \u003cdiv\u003e\u003cdiv\u003e\u003cdiv\u003e\u003cdiv id=\"js_error_message\"\u003e\u003cp\u003e\u003cimg src=\"https://d1ge0kk1l5kms0.cloudfront.net/images/G/01/webservices/console/warning.png\" alt=\"Warning\" /\u003e \u003cstrong\u003eJavascript is disabled or is unavailable in your browser.\u003c/strong\u003e\u003c/p\u003e\u003cp\u003eTo use the Amazon Web Services Documentation, Javascript must be enabled. Please refer to your browser's Help pages for instructions.\u003c/p\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003eDocument ConventionsSEC 6. How do you protect your compute resources?SEC06-BP02 Provision compute from hardened images",
  "SEC06-BP02 Provision compute from hardened images\n    Provide fewer opportunities for unintended access to your runtime\n    environments by deploying them from hardened images. Only acquire\n    runtime dependencies, such as container images and application\n    libraries, from trusted registries and verify their signatures.\n    Create your own private registries to store trusted images and\n    libraries for use in your build and deploy processes.\n  \n    Desired outcome: Your compute\n    resources are provisioned from hardened baseline images. You\n    retrieve external dependencies, such as container images and\n    application libraries, only from trusted registries and verify their\n    signatures. These are stored in private registries for your build\n    and deployment processes to reference. You scan and update images\n    and dependencies regularly to help protect against any newly\n    discovered vulnerabilities.\n  \n    Common anti-patterns:\n  \n     \n     \n     \n     \n  \n      \n        Acquiring images and libraries from trusted registries, but not\n        verifying their signature or performing vulnerability scans\n        before putting into use.\n      \n    \n      \n        Hardening images, but not regularly testing them for new\n        vulnerabilities or updating to the latest version.\n      \n    \n      \n        Installing or not removing software packages that are not\n        required during the expected lifecycle of the image.\n      \n    \n      \n        Relying solely on patching to keep production compute resources\n        up to date. Patching alone can still cause compute resources to\n        drift from the hardened standard over time. Patching can also\n        fail to remove malware that may have been installed by a threat\n        actor during a security event.\n      \n    \n    Benefits of establishing this best\n      practice: Hardening images helps reduce the number of\n    paths available in your runtime environment that can allow\n    unintended access to unauthorized users or services. It also can\n    reduce the scope of impact should any unintended access occur.\n  \n    Level of risk exposed if this best practice\n      is not established: High\n  \n    \n    Implementation guidance\n    \n    \n    \n      To harden your systems, start from the latest versions of\n      operating systems, container images, and application libraries.\n      Apply patches to known issues. Minimize the system by removing any\n      unneeded applications, services, device drivers, default users,\n      and other credentials. Take any other needed actions, such as\n      disabling ports to create an environment that has only the\n      resources and capabilities needed by your workloads. From this\n      baseline, you can then install software, agents, or other\n      processes you need for purposes such as workload monitoring or\n      vulnerability management.\n    \n    \n      You can reduce the burden of hardening systems by using guidance\n      that trusted sources provide, such as the\n      Center for Internet\n        Security (CIS) and the Defense Information Systems Agency\n      (DISA) Security\n        Technical Implementation Guides (STIGs). We recommend you\n      start with an\n      Amazon\n        Machine Image (AMI) published by AWS or an APN partner, and\n      use the AWS       EC2 Image Builder to automate configuration according to an\n      appropriate combination of CIS and STIG controls.\n    \n    \n      While there are available hardened images and EC2 Image Builder\n      recipes that apply the CIS or DISA STIG recommendations, you may\n      find their configuration prevents your software from running\n      successfully. In this situation, you can start from a non-hardened\n      base image, install your software, and then incrementally apply\n      CIS controls to test their impact. For any CIS control that\n      prevents your software from running, test if you can implement the\n      finer-grained hardening recommendations in a DISA instead. Keep\n      track of the different CIS controls and DISA STIG configurations\n      you are able to apply successfully. Use these to define your image\n      hardening recipes in EC2 Image Builder accordingly.\n    \n    \n      For containerized workloads, hardened images from Docker are\n      available on the\n      Amazon Elastic Container Registry (ECR)\n      public\n        repository. You can use EC2 Image Builder to harden\n      container images alongside AMIs.\n    \n    \n      Similar to operating systems and container images, you can obtain\n      code packages (or libraries) from public\n      repositories, through tooling such as pip, npm, Maven, and NuGet.\n      We recommend you manage code packages by integrating private\n      repositories, such as within\n      AWS CodeArtifact, with trusted public repositories. This\n      integration can handle retrieving, storing, and keeping packages\n      up-to-date for you. Your application build processes can then\n      obtain and test the latest version of these packages alongside\n      your application, using techniques like Software Composition\n      Analysis (SCA), Static Application Security Testing (SAST), and\n      Dynamic Application Security Testing (DAST).\n    \n    \n      For serverless workloads that use AWS Lambda, simplify managing\n      package dependencies using\n      Lambda\n        layers. Use Lambda layers to configure a set of standard\n      dependencies that are shared across different functions into a\n      standalone archive. You can create and maintain layers through\n      their own build process, providing a central way for your\n      functions to stay up-to-date.\n    \n   \n    \n    Implementation steps\n    \n    \n    \n       \n       \n       \n    \n        \n          Harden operating systems. Use base images from trusted sources\n          as a foundation for building your hardened AMIs. Use\n          EC2 Image Builder to help customize the software installed\n          on your images.\n        \n      \n        \n          Harden containerized resources. Configure containerized\n          resources to meet security best practices. When using\n          containers, implement\n          ECR\n            Image Scanning in your build pipeline and on a regular\n          basis against your image repository to look for CVEs in your\n          containers. \n        \n      \n        \n          When using serverless implementation with AWS Lambda, use\n          Lambda\n            layers to segregate application function code and\n          shared dependent libraries. Configure\n          code\n            signing for Lambda to make sure that only trusted code\n          runs in your Lambda functions.\n        \n      \n   \n    \n    Resources\n    \n    \n    \n      Related best practices:\n    \n    \n       \n    \n        \n          OPS05-BP05\n            Perform patch management\n        \n      \n    \n      Related videos:\n    \n    \n       \n    \n        \n          Deep\n            dive into AWS Lambda security\n        \n      \n    \n      Related examples:\n    \n    \n       \n       \n       \n       \n       \n    \n        \n          Quickly\n            build STIG-compliant AMI using EC2 Image Builder\n        \n      \n        \n          Building\n            better container images\n        \n      \n        \n          Using\n            Lambda layers to simplify your development process\n        \n      \n        \n          Develop\n            \u0026 Deploy AWS Lambda Layers using Serverless\n            Framework\n        \n      \n        \n          Building\n            end-to-end AWS DevSecOps CI/CD pipeline with open source SCA,\n            SAST and DAST tools\n        \n      \n  \u003cdiv\u003e\u003cdiv\u003e\u003cdiv\u003e\u003cdiv id=\"js_error_message\"\u003e\u003cp\u003e\u003cimg src=\"https://d1ge0kk1l5kms0.cloudfront.net/images/G/01/webservices/console/warning.png\" alt=\"Warning\" /\u003e \u003cstrong\u003eJavascript is disabled or is unavailable in your browser.\u003c/strong\u003e\u003c/p\u003e\u003cp\u003eTo use the Amazon Web Services Documentation, Javascript must be enabled. Please refer to your browser's Help pages for instructions.\u003c/p\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003eDocument ConventionsSEC06-BP01 Perform vulnerability managementSEC06-BP03 Reduce manual management and interactive\n  access",
  "SEC06-BP03 Reduce manual management and interactive\n  access\n    Use automation to perform deployment, configuration, maintenance,\n    and investigative tasks wherever possible. Consider manual access to\n    compute resources in cases of emergency procedures or in safe\n    (sandbox) environments, when automation is not available.\n  \n    Desired outcome: Programmatic\n    scripts and automation documents (runbooks) capture authorized\n    actions on your compute resources. These runbooks are initiated\n    either automatically, through change detection systems, or\n    manually, when human judgment is required. Direct access to compute\n    resources is only made available in emergency situations when\n    automation is not available. All manual activities are logged and\n    incorporated into a review process to continually improve your\n    automation capabilities.\n  \n    Common anti-patterns:\n  \n     \n     \n     \n     \n     \n     \n  \n      \n        Interactive access to Amazon EC2 instances with protocols such\n        as SSH or RDP.\n      \n    \n      \n        Maintaining individual user logins such as /etc/passwd or\n        Windows local users.\n      \n    \n      \n        Sharing a password or private key to access an instance among\n        multiple users.\n      \n    \n      \n        Manually installing software and creating or updating\n        configuration files.\n      \n    \n      \n        Manually updating or patching software.\n      \n    \n      \n        Logging into an instance to troubleshoot problems.\n      \n    \n    Benefits of establishing this best\n    practice: Performing actions with automation helps you to\n    reduce the operational risk of unintended changes and\n    misconfigurations. Removing the use of Secure Shell (SSH) and Remote\n    Desktop Protocol (RDP) for interactive access reduces the scope of\n    access to your compute resources. This takes away a common path for\n    unauthorized actions. Capturing your compute resource management\n    tasks in automation documents and programmatic scripts provides a\n    mechanism to define and audit the full scope of authorized\n    activities at a fine-grained level of detail.\n  \n    Level of risk exposed if this best practice\n    is not established: Medium\n  \n\n  Implementation guidance\n\n      \n    \n      Logging into an instance is a classic approach to system\n      administration. After installing the server operating system,\n      users would typically log in manually to configure the system and\n      install the desired software. During the server's lifetime, users\n      might log in to perform software updates, apply patches, change\n      configurations, and troubleshoot problems.\n    \n    \n      Manual access poses a number of risks, however. It requires a\n      server that listens for requests, such as an SSH or RDP service,\n      that can provide a potential path to unauthorized access. It also\n      increases the risk of human error associated with performing\n      manual steps. These can result in workload incidents, data\n      corruption or destruction, or other security issues. Human access\n      also requires protections against the sharing of credentials,\n      creating additional management overhead. \n    \n    \n      To mitigate these risks, you can implement an agent-based remote\n      access solution, such as\n      AWS Systems Manager. AWS Systems Manager Agent (SSM Agent) initiates an encrypted\n      channel and thus does not rely on listening for\n      externally-initiated requests. Consider configuring SSM Agent to\n      establish\n      this channel over a VPC endpoint.\n    \n    \n      Systems Manager gives you fine-grained control over how you can interact with\n      your managed instances. You define the automations to run, who can\n      run them, and when they can run. Systems Manager can apply patches, install\n      software, and make configuration changes without interactive\n      access to the instance. Systems Manager can also provide access to a remote\n      shell and log every command invoked, and its output, during the\n      session to logs and\n      Amazon S3.\n      AWS CloudTrail records invocations of Systems Manager APIs for inspection.\n    \n     \n\n  Implementation steps\n\n      \n      \n         \n         \n         \n         \n         \n         \n      \n          \n            Install\n            AWS Systems Manager Agent (SSM Agent) on your Amazon EC2 instances. Check to see if SSM Agent is included and\n            started automatically as part of your base AMI\n            configuration.\n          \n        \n          \n            Verify that the IAM Roles associated with your EC2 instance\n            profiles include the AmazonSSMManagedInstanceCore\n            managed\n            IAM policy.\n          \n        \n          \n            Disable SSH, RDP, and other remote access services running\n            on your instances. You can do this by running scripts\n            configured in the user data section of your launch\n            templates or by building customized AMIs with tools such as\n            EC2 Image Builder.\n          \n        \n          \n            Verify that the security group ingress rules applicable to\n            your EC2 instances do not permit access on port 22/tcp (SSH)\n            or port 3389/tcp (RDP). Implement detection and alerting on\n            misconfigured security groups using services such as AWS Config.\n          \n        \n          \n            Define appropriate automations, runbooks, and run commands\n            in Systems Manager. Use IAM policies to define who can perform these\n            actions and the conditions under which they are permitted.\n            Test these automations thoroughly in a non-production\n            environment. Invoke these automations when necessary,\n            instead of interactively accessing the instance.\n          \n        \n          \n            Use\n            AWS Systems Manager Session Manager to provide\n            interactive access to instances when necessary. Turn on\n            session activity logging to maintain an audit trail in\n            Amazon CloudWatch Logs or Amazon S3. \n          \n        \n     \n   \n    \n    Resources\n    \n    \n    \n      Related best practices:\n    \n    \n       \n    \n        \n          REL08-BP04\n            Deploy using immutable infrastructure\n        \n      \n    \n      Related examples:\n    \n    \n       \n    \n        \n          Replacing\n            SSH access to reduce management and security overhead with AWS Systems Manager\n        \n      \n    \n      Related tools:\n    \n    \n       \n    \n        \n          AWS Systems Manager\n        \n      \n    \n      Related videos:\n    \n    \n       \n    \n        \n          Controlling\n            User Session Access to Instances in AWS Systems Manager Session\n            Manager\n        \n      \n  \u003cdiv\u003e\u003cdiv\u003e\u003cdiv\u003e\u003cdiv id=\"js_error_message\"\u003e\u003cp\u003e\u003cimg src=\"https://d1ge0kk1l5kms0.cloudfront.net/images/G/01/webservices/console/warning.png\" alt=\"Warning\" /\u003e \u003cstrong\u003eJavascript is disabled or is unavailable in your browser.\u003c/strong\u003e\u003c/p\u003e\u003cp\u003eTo use the Amazon Web Services Documentation, Javascript must be enabled. Please refer to your browser's Help pages for instructions.\u003c/p\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003eDocument ConventionsSEC06-BP02 Provision compute from hardened imagesSEC06-BP04 Validate software integrity",
  "SEC06-BP04 Validate software integrity\n    Use cryptographic verification to validate the integrity of software\n    artifacts (including images) your workload uses.  Cryptographically\n    sign your software as a safeguard against unauthorized changes run\n    within your compute environments.\n  \n    Desired outcome: All artifacts\n    are obtained from trusted sources. Vendor website certificates are\n    validated.  Downloaded artifacts are cryptographically verified by\n    their signatures. Your own software is cryptographically signed and\n    verified by your computing environments.\n  \n    Common anti-patterns:\n  \n     \n     \n     \n     \n  \n      \n        Trusting reputable vendor websites to obtain software artifacts,\n        but ignoring certificate expiration notices.  Proceeding with\n        downloads without confirming certificates are valid.\n      \n    \n      \n        Validating vendor website certificates, but not\n        cryptographically verifying downloaded artifacts from these\n        websites.\n      \n    \n      \n        Relying solely on digests or hashes to validate software\n        integrity.  Hashes establish that artifacts have not been\n        modified from the original version, but do not validate their\n        source.\n      \n    \n      \n        Not signing your own software, code, or libraries, even when\n        only used in your own deployments. \n      \n    \n    Benefits of establishing this best\n      practice: Validating the integrity of artifacts that your\n    workload depends on helps prevent malware from entering your compute\n    environments.  Signing your software helps safeguard against\n    unauthorized running in your compute environments.   Secure your\n    software supply chain by signing and verifying code.\n  \n    Level of risk exposed if this best practice\n      is not established: Medium\n  \n    \n    Implementation guidance\n    \n    \n    \n      Operating system images, container images, and code artifacts are\n      often distributed with integrity checks available, such as through\n      a digest or hash.  These allow clients to verify integrity by\n      computing their own hash of the payload and validating it is the\n      same as the one published.  While these checks help verify that\n      the payload has not been tampered with, they do not validate the\n      payload came from the original source (its\n      provenance).  Verifying provenance requires a\n      certificate that a trusted authority issues to digitally sign the\n      artifact.\n    \n    \n      If you are using a downloaded software or artifacts in your\n      workload, check if the provider provides a public key for digital\n      signature verification.  Here are some examples of how AWS\n      provides a public key and verification instructions for software\n      we publish:\n    \n    \n       \n       \n       \n    \n        \n          EC2 Image Builder: Verify the signature of the AWSTOE installation\n            download\n        \n      \n        \n          AWS Systems Manager: Verifying the signature of SSM Agent\n        \n      \n        \n          Amazon CloudWatch: Verifying the signature of the CloudWatch agent\n            package\n        \n      \n    \n      Incorporate digital signature verification into the processes you\n      use for obtaining and hardening images, as discussed in\n      SEC06-BP02 Provision compute from\n        hardened images.\n    \n    \n      You can use\n      AWS Signer to help manage the verification of signatures, as\n      well as your own code-signing lifecycle for your own software and\n      artifacts.  Both\n      AWS Lambda and\n      Amazon Elastic Container Registry provide integrations with Signer\n      to verify the signatures of your code and images.  Using the\n      examples in the Resources section, you can incorporate Signer into\n      your continuous integration and delivery (CI/CD) pipelines to\n      automate verification of signatures and the signing of your own\n      code and images.\n    \n   \n    \n    Resources\n    \n    \n    \n      Related documents:\n    \n    \n       \n       \n       \n       \n       \n       \n    \n        \n          Cryptographic\n            Signing for Containers\n        \n      \n        \n          Best\n            Practices to help secure your container image build pipeline\n            by using AWS Signer\n        \n      \n        \n          Announcing\n            Container Image Signing with AWS Signer and Amazon EKS\n        \n      \n        \n          Configuring\n            code signing for AWS Lambda\n        \n      \n        \n          Best\n            practices and advanced patterns for Lambda code signing\n        \n      \n        \n          Code\n            signing using AWS Certificate Manager Private CA and AWS Key Management Service asymmetric keys\n        \n      \n    \n      Related examples:\n    \n    \n       \n       \n    \n        \n          Automate\n            Lambda code signing with Amazon CodeCatalyst and AWS Signer\n        \n      \n        \n          Signing\n            and Validating OCI Artifacts with AWS Signer\n        \n      \n    \n      Related tools:\n    \n    \n       \n       \n       \n       \n       \n    \n        \n          AWS Lambda\n        \n      \n        \n          AWS Signer\n        \n      \n        \n          AWS Certificate Manager\n        \n      \n        \n          AWS Key Management Service\n        \n      \n        \n          AWS CodeArtifact\n        \n      \n  \u003cdiv\u003e\u003cdiv\u003e\u003cdiv\u003e\u003cdiv id=\"js_error_message\"\u003e\u003cp\u003e\u003cimg src=\"https://d1ge0kk1l5kms0.cloudfront.net/images/G/01/webservices/console/warning.png\" alt=\"Warning\" /\u003e \u003cstrong\u003eJavascript is disabled or is unavailable in your browser.\u003c/strong\u003e\u003c/p\u003e\u003cp\u003eTo use the Amazon Web Services Documentation, Javascript must be enabled. Please refer to your browser's Help pages for instructions.\u003c/p\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003eDocument ConventionsSEC06-BP03 Reduce manual management and interactive\n  accessSEC06-BP05 Automate compute protection",
  "SEC06-BP05 Automate compute protection\n    Automate compute protection operations to reduce the need for human\n    intervention. Use automated scanning to detect potential issues\n    within your compute resources, and remediate with automated\n    programmatic responses or fleet management operations.  Incorporate\n    automation in your CI/CD processes to deploy trustworthy workloads\n    with up-to-date dependencies.\n  \n    Desired outcome: Automated\n    systems perform all scanning and patching of compute resources. You\n    use automated verification to check that software images and\n    dependencies come from trusted sources, and have not been tampered\n    with. Workloads are automatically checked for up-to-date\n    dependencies, and are signed to establish trustworthiness in AWS\n    compute environments.  Automated remediations are initiated when\n    non-compliant resources are detected. \n  \n    Common anti-patterns:\n  \n     \n     \n  \n      \n        Following the practice of immutable infrastructure, but not\n        having a solution in place for emergency patching or replacement\n        of production systems.\n      \n    \n      \n        Using automation to fix misconfigured resources, but not having\n        a manual override mechanism in place.  Situations may arise\n        where you need to adjust the requirements, and you may need to\n        suspend automations until you make these changes.\n      \n    \n    Benefits of establishing this best\n      practice: Automation can reduce the risk of unauthorized\n    access and use of your compute resources.  It helps to prevent\n    misconfigurations from making their way into production\n    environments, and detecting and fixing misconfigurations should they\n    occur.  Automation also helps to detect unauthorized access and use\n    of compute resources to reduce your time to respond.  This in turn\n    can reduce the overall scope of impact from the issue.\n  \n    Level of risk exposed if this best practice\n      is not established: Medium\n  \n    \n    Implementation guidance\n    \n    \n    \n      You can apply the automations described in the Security Pillar\n      practices for protecting your compute resources.\n      SEC06-BP01 Perform vulnerability\n        management describes how you can use\n      Amazon Inspector in both your CI/CD pipelines and for continually\n      scanning your runtime environments for known Common\n      Vulnerabilities and Exposures (CVEs).  You can use\n      AWS Systems Manager to apply patches or redeploy from fresh\n      images through automated runbooks to keep your compute fleet\n      updated with the latest software and libraries.  Use these\n      techniques to reduce the need for manual processes and interactive\n      access to your compute resources.  See\n      SEC06-BP03 Reduce manual\n        management and interactive access to learn more.\n    \n    \n      Automation also plays a role in deploying workloads that are\n      trustworthy, described in\n      SEC06-BP02 Provision compute from\n        hardened images and\n      SEC06-BP04 Validate software\n        integrity.  You can use services such as\n      EC2 Image Builder,\n      AWS Signer,\n      AWS CodeArtifact, and\n      Amazon Elastic Container Registry (ECR) to download, verify, construct,\n      and store hardened and approved images and code dependencies.  \n      Alongside Inspector, each of these can play a role in your CI/CD\n      process so your workload makes its way to production only when it\n      is confirmed that its dependencies are up-to-date and from trusted\n      sources.  Your workload is also signed so AWS compute\n      environments, such as\n      AWS Lambda and\n      Amazon Elastic Kubernetes Service (EKS) can verify it hasn't been tampered\n      with before allowing it to run.\n    \n    \n      Beyond these preventative controls, you can use automation in your\n      detective controls for your compute resources as well.  As one\n      example,\n      AWS Security Hub offers the\n      NIST\n        800-53 Rev. 5 standard that includes checks such\n      as [EC2.8]\n        EC2 instances should use Instance Metadata Service Version 2\n        (IMDSv2).  IMDSv2 uses the techniques of session\n      authentication, blocking requests that contain an\n      X-Forwarded-For HTTP header, and a network TTL\n      of 1 to stop traffic originating from external sources to retrieve\n      information about the EC2 instance. This check in Security Hub can\n      detect when EC2 instances use IMDSv1 and initiate automated\n      remediation. Learn more about automated detection and remediations\n      in SEC04-BP04\n        Initiate remediation for non-compliant resources.\n    \n  \n   \n    \n    Implementation steps\n    \n    \n    \n       \n       \n       \n       \n       \n       \n    \n        \n          Automate creating secure, compliant and hardened AMIs with\n          EC2 Image Builder.  You can produce images that incorporate\n          controls from the Center for Internet Security (CIS)\n          Benchmarks or Security Technical Implementation Guide (STIG)\n          standards from base AWS and APN partner images.\n        \n      \n        \n          Automate configuration management. Enforce and validate secure\n          configurations in your compute resources automatically by\n          using a configuration management service or tool. \n        \n        \n           \n           \n        \n            \n              Automated configuration management\n              using AWS Config\n            \n          \n            \n              Automated security and compliance posture management\n              using AWS Security Hub\n            \n          \n      \n        \n          Automate patching or replacing Amazon Elastic Compute Cloud\n          (Amazon EC2) instances. AWS Systems Manager Patch Manager\n          automates the process of patching managed instances with both\n          security-related and other types of updates. You can use Patch\n          Manager to apply patches for both operating systems and\n          applications.\n        \n        \n           \n        \n            \n              AWS Systems Manager Patch Manager\n            \n          \n      \n        \n          Automate scanning of compute resources for common\n          vulnerabilities and exposures (CVEs), and embed security\n          scanning solutions within your build pipeline.\n        \n        \n           \n           \n        \n            \n              Amazon Inspector\n            \n          \n            \n              ECR\n                Image Scanning\n            \n          \n      \n        \n          Consider Amazon GuardDuty for automatic malware and threat\n          detection to protect compute resources. GuardDuty can also\n          identify potential issues when an\n          AWS Lambda function gets invoked in your AWS environment. \n        \n        \n           \n        \n            \n              Amazon GuardDuty\n            \n          \n      \n        \n          Consider AWS Partner solutions. AWS Partners offer\n          industry-leading products that are equivalent, identical to,\n          or integrate with existing controls in your on-premises\n          environments. These products complement the existing AWS\n          services to allow you to deploy a comprehensive security\n          architecture and a more seamless experience across your cloud\n          and on-premises environments.\n        \n        \n           \n        \n            \n              Infrastructure\n                security\n            \n          \n      \n   \n   \n    \n    Resources\n    \n    \n    \n      Related best practices:\n    \n    \n       \n    \n        \n          SEC01-BP06\n            Automate deployment of standard security controls\n        \n      \n    \n      Related documents:\n    \n    \n       \n    \n        \n          Get\n            the full benefits of IMDSv2 and disable IMDSv1 across your AWS\n            infrastructure\n        \n      \n    \n      Related videos:\n    \n    \n       \n    \n        \n          Security best\n            practices for the Amazon EC2 instance metadata service\n        \n      \n  \u003cdiv\u003e\u003cdiv\u003e\u003cdiv\u003e\u003cdiv id=\"js_error_message\"\u003e\u003cp\u003e\u003cimg src=\"https://d1ge0kk1l5kms0.cloudfront.net/images/G/01/webservices/console/warning.png\" alt=\"Warning\" /\u003e \u003cstrong\u003eJavascript is disabled or is unavailable in your browser.\u003c/strong\u003e\u003c/p\u003e\u003cp\u003eTo use the Amazon Web Services Documentation, Javascript must be enabled. Please refer to your browser's Help pages for instructions.\u003c/p\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003eDocument ConventionsSEC06-BP04 Validate software integrityData protection",
  "SEC07-BP01 Understand your data classification scheme\n    Understand the classification of data your workload is processing,\n    its handling requirements, the associated business processes, where\n    the data is stored, and who the data owner is.  Your data\n    classification and handling scheme should consider the applicable\n    legal and compliance requirements of your workload and what data\n    controls are needed. Understanding the data is the first step in the\n    data classification journey. \n  \n    Desired outcome: The types of\n    data present in your workload are well-understood and documented.\n     Appropriate controls are in place to protect sensitive data based\n    on its classification.  These controls govern considerations such as\n    who is allowed to access the data and for what purpose, where the\n    data is stored, the encryption policy for that data and how\n    encryption keys are managed, the lifecycle for the data and its\n    retention requirements, appropriate destruction processes, what\n    backup and recovery processes are in place, and the auditing of\n    access.\n  \n    Common anti-patterns:\n  \n     \n     \n     \n     \n  \n      \n        Not having a formal data classification policy in place to\n        define data sensitivity levels and their handling requirements\n      \n    \n      \n        Not having a good understanding of the sensitivity levels of\n        data within your workload, and not capturing this information in\n        architecture and operations documentation\n      \n    \n      \n        Failing to apply the appropriate controls around your data based\n        on its sensitivity and requirements, as outlined in your data\n        classification and handling policy\n      \n    \n      \n        Failing to provide feedback about data classification and\n        handling requirements to owners of the policies.\n      \n    \n    Benefits of establishing this best practice: This practice removes ambiguity around the appropriate\n    handling of data within your workload.  Applying a formal policy\n    that defines the sensitivity levels of data in your organization and\n    their required protections can help you comply with legal\n    regulations and other cybersecurity attestations and certifications.\n     Workload owners can have confidence in knowing where sensitive data\n    is stored and what protection controls are in place.  Capturing\n    these in documentation helps new team members better understand them\n    and maintain controls early in their tenure. These practices can\n    also help reduce costs by right sizing the controls for each type of\n    data.\n  \n    Level of risk exposed if this best practice\n      is not established: High\n  \n    \n    Implementation guidance\n    \n    \n    \n      When designing a workload, you may be considering ways to protect\n      sensitive data intuitively.  For example, in a multi-tenant\n      application, it is intuitive to think of each tenant's data as\n      sensitive and put protections in place so that one tenant can't\n      access the data of another tenant.  Likewise, you may intuitively\n      design access controls so only administrators can modify data\n      while other users have only read-level access or no access at all.\n    \n    \n      By having these data sensitivity levels defined and captured in\n      policy, along with their data protection requirements, you can\n      formally identify what data resides in your workload. You can then\n      determine if the right controls are in place, if the controls can\n      be audited, and what responses are appropriate if data is found to\n      be mishandled.\n    \n    \n      To help identify where sensitive data resides within your\n      workload, consider using a data catalog. A data catalog is a\n      database that maps data in your organization, its location,\n      sensitivity level, and the controls in place to protect that data.\n      Additionally, consider using\n      resource\n        tags where available.  For example, you can apply a tag\n      that has a tag key of\n      Classification and a tag\n        value of PHI for protected health\n      information (PHI), and another tag that has a tag\n        key of Sensitivity and a\n      tag value of High.\n       Services such as\n      AWS Config can then be used to monitor these resources for\n      changes and alert if they are modified in a way that brings them\n      out of compliance with your protection requirements (such as\n      changing the encryption settings).  You can capture the standard\n      definition of your tag keys and acceptable values using\n      tag\n        policies, a feature of AWS Organizations. It is not\n      recommended that the tag key or value contains private or\n      sensitive data.\n    \n  \n   \n    \n    Implementation steps\n    \n    \n    \n       \n       \n       \n       \n    \n        \n          Understand your organization's data classification scheme and\n          protection requirements.\n        \n      \n        \n          Identify the types of sensitive data processed by your\n          workloads.\n        \n      \n        \n          Capture the data in a data catalog that provides a single\n          view of where data resides in the organization and the level\n          of sensitivity of that data.\n        \n      \n        \n          Consider using resource and data-level tagging, where\n          available, to tag data with its sensitivity level and other\n          operational metadata that can help with monitoring and\n          incident response.\n        \n        \n           \n        \n            \n               AWS Organizations tag policies can be used to enforce\n              tagging standards.\n            \n          \n      \n   \n   \n    \n    Resources\n    \n    \n    \n      Related best practices:\n    \n    \n       \n    \n        \n          SUS04-BP01\n            Implement a data classification policy\n        \n      \n    \n      Related documents:\n    \n    \n       \n       \n    \n        \n          Data\n            Classification whitepaper\n        \n      \n        \n          Best\n            Practices for Tagging AWS Resources\n        \n      \n    \n      Related examples:\n    \n    \n       \n    \n        \n          AWS Organizations Tag Policy Syntax and Examples\n        \n      \n    \n      Related tools\n    \n    \n       \n    \n        \n          AWS           Tag Editor\n        \n      \n  \u003cdiv\u003e\u003cdiv\u003e\u003cdiv\u003e\u003cdiv id=\"js_error_message\"\u003e\u003cp\u003e\u003cimg src=\"https://d1ge0kk1l5kms0.cloudfront.net/images/G/01/webservices/console/warning.png\" alt=\"Warning\" /\u003e \u003cstrong\u003eJavascript is disabled or is unavailable in your browser.\u003c/strong\u003e\u003c/p\u003e\u003cp\u003eTo use the Amazon Web Services Documentation, Javascript must be enabled. Please refer to your browser's Help pages for instructions.\u003c/p\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003eDocument ConventionsSEC 7. How do you classify your data?SEC07-BP02 Apply data protection controls based on data sensitivity",
  "SEC07-BP02 Apply data protection controls based on data sensitivity\n    Apply data protection controls that provide an appropriate level of\n    control for each class of data defined in your classification\n    policy.  This practice can allow you to protect sensitive data from\n    unauthorized access and use, while preserving the availability and\n    use of data.\n  \n    Desired outcome: You have a\n    classification policy that defines the different levels of\n    sensitivity for data in your organization.  For each of these\n    sensitivity levels, you have clear guidelines published for approved\n    storage and handling services and locations, and their required\n    configuration.  You implement the controls for each level according\n    to the level of protection required and their associated costs.  You\n    have monitoring and alerting in place to detect if data is present\n    in unauthorized locations, processed in unauthorized environments,\n    accessed by unauthorized actors, or the configuration of related\n    services becomes non-compliant.\n  \n    Common anti-patterns:\n  \n     \n     \n     \n     \n     \n  \n      \n        Applying the same level of protection controls across all data.\n        This may lead to over-provisioning security controls for\n        low-sensitivity data, or insufficient protection of highly\n        sensitive data.\n      \n    \n      \n        Not involving relevant stakeholders from security, compliance,\n        and business teams when defining data protection controls.\n      \n    \n      \n        Overlooking the operational overhead and costs associated with\n        implementing and maintaining data protection controls.\n      \n    \n      \n        Not conducting periodic data protection control reviews to\n        maintain alignment with classification policies.\n      \n    \n      \n        Not having a complete inventory of where data resides at rest\n        and in transit.\n      \n    \n    Benefits of establishing this best\n      practice: By aligning your controls to the classification\n    level of your data, your organization can invest in higher levels of\n    control where needed. This can include increasing resources on\n    securing, monitoring, measuring, remediating, and reporting.  Where\n    fewer controls are appropriate, you can improve the accessibility\n    and completeness of data for your workforce, customers, or\n    constituents.  This approach gives your organization the most\n    flexibility with data usage, while still adhering to data protection\n    requirements.\n  \n    Level of risk exposed if this best practice\n      is not established: High\n  \n    \n    Implementation guidance\n    \n    \n    \n      Implementing data protection controls based on data sensitivity\n      levels involves several key steps. First, identify the different\n      data sensitivity levels within your workload architecture (such as\n      public, internal, confidential, and restricted) and evaluate where\n      you store and process this data. Next, define isolation boundaries\n      around data based on its sensitivity level. We recommend you\n      separate data into different AWS accounts, using\n      service\n        control policies (SCPs) to restrict services and actions\n      allowed for each data sensitivity level. This way, you can create\n      strong isolation boundaries and enforce the principle of least\n      privilege.\n    \n    \n      After you define the isolation boundaries, implement appropriate\n      protection controls based on the data sensitivity levels. Refer to\n      best practices for Protecting\n        data at rest and\n      Protecting data in\n        transit to implement relevant controls like encryption,\n      access controls, and auditing. Consider techniques like\n      tokenization or anonymization to reduce the sensitivity level of\n      your data. Simplify applying consistent data policies across your\n      business with a centralized system for tokenization and\n      de-tokenization.\n    \n    \n      Continuously monitor and test the effectiveness of the implemented\n      controls. Regularly review and update the data classification\n      scheme, risk assessments, and protection controls as your\n      organization's data landscape and threats evolve. Align the\n      implemented data protection controls with relevant industry\n      regulations, standards, and legal requirements. Further, provide\n      security awareness and training to help employees understand the\n      data classification scheme and their responsibilities in handling\n      and protecting sensitive data.\n    \n  \n   \n    \n    Implementation steps\n    \n    \n    \n       \n       \n       \n       \n       \n    \n        \n          Identify the classification and sensitivity levels of data\n          within your workload.\n        \n      \n        \n          Define isolation boundaries for each level and determine an\n          enforcement strategy.\n        \n      \n        \n          Evaluate the controls you define that govern access,\n          encryption, auditing, retention, and others required by your\n          data classification policy.\n        \n      \n        \n          Evaluate options to reduce the sensitivity level of data where\n          appropriate, such as using tokenization or anonymization.\n        \n      \n        \n          Verify your controls using automated testing and monitoring of\n          your configured resources.\n        \n      \n   \n   \n      \n      Resources\n      \n      \n      \n        Related best practices:\n      \n      \n         \n         \n      \n          \n            PERF03-BP01\n              Use a purpose-built data store that best supports your data\n              access and storage requirements\n          \n        \n          \n            COST04-BP05\n              Enforce data retention policies\n          \n        \n      \n        Related documents:\n      \n      \n         \n         \n         \n         \n      \n          \n            Data\n              Classification whitepaper\n          \n        \n          \n            Best\n              Practices for Security, Identify, \u0026 Compliance\n          \n        \n          \n            AWS KMS Best Practices\n          \n        \n          \n            Encryption\n              best practices and features for AWS services\n          \n        \n      \n        Related examples:\n      \n      \n         \n         \n      \n          \n            Building\n              a serverless tokenization solution to mask sensitive\n              data\n          \n        \n          \n            How\n              to use tokenization to improve data security and reduce\n              audit scope\n          \n        \n      \n        Related tools:\n      \n      \n         \n         \n         \n      \n          \n            AWS Key Management Service (AWS KMS)\n          \n        \n          \n            AWS CloudHSM\n          \n        \n          \n            AWS Organizations\n          \n        \n    \u003cdiv\u003e\u003cdiv\u003e\u003cdiv\u003e\u003cdiv id=\"js_error_message\"\u003e\u003cp\u003e\u003cimg src=\"https://d1ge0kk1l5kms0.cloudfront.net/images/G/01/webservices/console/warning.png\" alt=\"Warning\" /\u003e \u003cstrong\u003eJavascript is disabled or is unavailable in your browser.\u003c/strong\u003e\u003c/p\u003e\u003cp\u003eTo use the Amazon Web Services Documentation, Javascript must be enabled. Please refer to your browser's Help pages for instructions.\u003c/p\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003eDocument ConventionsSEC07-BP01 Understand your data classification schemeSEC07-BP03 Automate identification and classification",
  "SEC07-BP03 Automate identification and classification\n    Automating the identification and classification of data can help\n    you implement the correct controls. Using automation to augment\n    manual determination reduces the risk of human error and exposure.\n  \n    Desired outcome: You are able to\n    verify whether the proper controls are in place based on your\n    classification and handling policy. Automated tools and services\n    help you to identify and classify the sensitivity level of your\n    data.  Automation also helps you continually monitor your\n    environments to detect and alert if data is being stored or handled\n    in unauthorized ways so corrective action can be taken quickly.\n  \n    Common anti-patterns:\n  \n     \n     \n     \n  \n      \n        Relying solely on manual processes for data identification and\n        classification, which can be error-prone and time-consuming.\n         This can lead to inefficient and inconsistent data\n        classification, especially as data volumes grow.\n      \n    \n      \n        Not having mechanisms to track and manage data assets across the\n        organization.\n      \n    \n      \n        Overlooking the need for continuous monitoring and\n        classification of data as it moves and evolves within the\n        organization.\n      \n    \n    Benefits of establishing this best\n      practice: Automating data identification and\n    classification can lead to more consistent and accurate application\n    of data protection controls, reducing the risk of human error.\n     Automation can also provide visibility into sensitive data access\n    and movement, helping you detect unauthorized handling and take\n    corrective action.\n  \n    Level of risk exposed if this best practice\n      is not established: Medium\n  \n    \n    Implementation guidance\n    \n    \n    \n      While human judgment is often used to classify data during the\n      initial design phases of a workload, consider having systems in\n      place that automate identification and classification on test data\n      as a preventive control. For example, developers can be provided a\n      tool or service to scan representative data to determine its\n      sensitivity.  Within AWS, you can upload data sets into\n      Amazon S3 and\n      scan them using\n      Amazon Macie,\n      Amazon Comprehend, or\n      Amazon Comprehend Medical.  Likewise, consider scanning data as\n      part of unit and integration testing to detect where sensitive\n      data is not expected. Alerting on sensitive data at this stage can\n      highlight gaps in protections before deployment to production.\n      Other features such as sensitive data detection in\n      AWS Glue,\n      Amazon SNS, and\n      Amazon CloudWatch can also be used to detect PII and take\n      mitigating action. For any automated tool or service, understand\n      how it defines sensitive data, and augment it with other human or\n      automated solutions to close any gaps as needed.\n    \n    \n      As a detective control, use ongoing monitoring of your\n      environments to detect if sensitive data is being stored in\n      non-compliant ways.  This can help detect situations such as\n      sensitive data being emitted into log files or being copied to a\n      data analytics environment without proper de-identification or\n      redaction.  Data that is stored in Amazon S3 can be continually\n      monitored for sensitive data using Amazon Macie.  \n    \n     \n      \n      Implementation steps\n      \n      \n      \n         \n         \n         \n         \n         \n      \n          \n            Review the data classification scheme within your\n            organization described in\n            SEC07-BP01.\n          \n          \n             \n          \n              \n                With an understanding of your organization's data\n                classification scheme, you can establish accurate\n                processes for automated identification and\n                classification that align with your company's policies.\n              \n            \n        \n          \n            Perform an initial scan of your environments for automated\n            identification and classification.\n          \n          \n             \n             \n          \n              \n                An initial full scan of your data can help produce a\n                comprehensive understanding of where sensitive data\n                resides in your environments. When a full scan is not\n                initially required or is unable to be completed up-front\n                due to cost, evaluate if data sampling techniques are\n                suitable to achieve your outcomes. For example, Amazon Macie can be configured to perform a broad automated\n                sensitive data discovery operation across your S3\n                buckets.  This capability uses sampling techniques to\n                cost-efficiently perform a preliminary analysis of where\n                sensitive data resides.  A deeper analysis of S3 buckets\n                can then be performed using a sensitive data discovery\n                job. Other data stores can also be exported to S3 to be\n                scanned by Macie.\n              \n            \n              \n                Establish access control defined in\n                SEC07-BP02 for\n                your data storage resources identified within your scan.\n              \n            \n        \n          \n            Configure ongoing scans of your environments.\n          \n          \n             \n          \n              \n                The automated sensitive data discovery capability of\n                Macie can be used to perform ongoing scans of your\n                environments.  Known S3 buckets that are authorized to\n                store sensitive data can be excluded using an allow list\n                in Macie.\n              \n            \n        \n          \n            Incorporate identification and classification into your\n            build and test processes.\n          \n          \n             \n          \n              \n                Identify tools that developers can use to scan data for\n                sensitivity while workloads are in development.  Use\n                these tools as part of integration testing to alert when\n                sensitive data is unexpected and prevent further\n                deployment.\n              \n            \n        \n          \n            Implement a system or runbook to take action when sensitive\n            data is found in unauthorized locations.\n          \n          \n             \n             \n          \n              \n                Restrict access to data using auto-remediation. For\n                example, you can move this data to an S3 bucket with\n                restricted access or tag the object if you use\n                attribute-based access control (ABAC). Additionally,\n                consider masking the data when it is detected.\n              \n            \n              \n                Alert your data protection and incident response teams\n                to investigate the root cause of the incident. Any\n                learnings they identify can help prevent future\n                incidents.\n              \n            \n        \n     \n   \n      \n      Resources\n     \n    \n    \n      Related documents:\n    \n    \n       \n       \n       \n    \n        \n          AWS Glue: Detect and process sensitive data\n        \n      \n        \n          Using\n            managed data identifiers in Amazon SNS\n        \n      \n        \n          Amazon CloudWatch Logs: Help protect sensitive log data with\n            masking\n        \n      \n    \n      Related examples:\n    \n    \n       \n       \n    \n        \n          Enabling\n            data classification for Amazon RDS database with Macie\n        \n      \n        \n          Detecting\n            sensitive data in DynamoDB with Macie\n        \n      \n    \n      Related tools:\n    \n    \n       \n       \n       \n       \n    \n        \n          Amazon Macie\n        \n      \n        \n          Amazon Comprehend\n        \n      \n        \n          Amazon Comprehend Medical\n        \n      \n        \n          AWS Glue\n        \n      \n    \u003cdiv\u003e\u003cdiv\u003e\u003cdiv\u003e\u003cdiv id=\"js_error_message\"\u003e\u003cp\u003e\u003cimg src=\"https://d1ge0kk1l5kms0.cloudfront.net/images/G/01/webservices/console/warning.png\" alt=\"Warning\" /\u003e \u003cstrong\u003eJavascript is disabled or is unavailable in your browser.\u003c/strong\u003e\u003c/p\u003e\u003cp\u003eTo use the Amazon Web Services Documentation, Javascript must be enabled. Please refer to your browser's Help pages for instructions.\u003c/p\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003eDocument ConventionsSEC07-BP02 Apply data protection controls based on data sensitivitySEC07-BP04 Define scalable data lifecycle management",
  "SEC07-BP04 Define scalable data lifecycle management\n    Understand your data lifecycle requirements as they relate to your\n    different levels of data classification and handling.  This can\n    include how data is handled when it first enters your environment,\n    how data is transformed, and the rules for its destruction. Consider\n    factors such as retention periods, access, auditing, and tracking\n    provenance.\n  \n    Desired outcome: You classify\n    data as close as possible to the point and time of ingestion. When\n    data classification requires masking, tokenization, or other\n    processes that reduce sensitivity level, you perform these actions\n    as close as possible to point and time of ingestion.\n  \n    You delete data in accordance with your policy when it is no longer\n    appropriate to keep, based on its classification.\n  \n    Common anti-patterns:\n  \n     \n     \n     \n     \n     \n  \n      \n        Implementing a one-size-fits-all approach to data lifecycle\n        management, without considering varying sensitivity levels and\n        access requirements.\n      \n    \n      \n        Considering lifecycle management only from the perspective of\n        either data that is usable, or data that is backed up, but not\n        both.\n      \n    \n      \n        Assuming that data that has entered your workload is valid,\n        without establishing its value or provenance.\n      \n    \n      \n        Relying on data durability as a substitute for data backups and\n        protection.\n      \n    \n      \n        Retaining data beyond its usefulness and required retention\n        period.\n      \n    \n    Benefits of establishing this best\n      practice: A well-defined and scalable data lifecycle\n    management strategy helps maintain regulatory compliance, improves\n    data security, optimizes storage costs, and enables efficient data\n    access and sharing while maintaining appropriate controls.\n  \n    Level of risk exposed if this best practice\n      is not established: High\n  \n    \n    Implementation guidance\n    \n    \n    \n      Data within a workload is often dynamic.  The form it takes when\n      entering your workload environment can be different from when it\n      is stored or used in business logic, reporting, analytics, or\n      machine learning.  In addition, the value of data can change over\n      time. Some data is temporal in nature and loses value as it gets\n      older.  Consider how these changes to your data impact evaluation\n      under your data classification scheme and associated controls.\n       Where possible, use an automated lifecycle mechanism, such as\n      Amazon S3 lifecycle policies and the\n      Amazon Data Lifecycle Manager, to configure your data retention,\n      archiving, and expiration processes. For data stored in DynamoDB,\n      you can use the\n      Time\n        To Live (TTL) feature to define a per-item expiration\n      timestamp. \n    \n    \n      Distinguish between data that is available for use, and data that\n      is stored as a backup.  Consider using\n      AWS Backup to automate the backup of data across AWS services.\n       Amazon EBS snapshots provide a way to copy an EBS volume and store\n      it using S3 features, including lifecycle, data protection, and\n      access to protection mechanisms. Two of these mechanisms are\n      S3\n        Object Lock and\n      AWS Backup Vault Lock, which can provide you with additional\n      security and control over your backups. Manage clear separation of\n      duties and access for backups. Isolate backups at the account\n      level to maintain separation from the affected environment during\n      an event.\n    \n    \n      Another aspect of lifecycle management is recording the history of\n      data as it progresses through your workload, called data\n        provenance tracking. This can give confidence that you\n      know where the data came from, any transformations performed, what\n      owner or process made those changes, and when.  Having this\n      history helps with troubleshooting issues and investigations\n      during potential security events.  For example, you can log\n      metadata about transformations in an\n      Amazon DynamoDB table.  Within a data lake, you can keep copies of\n      transformed data in different S3 buckets for each data pipeline\n      stage. Store schema and timestamp information in an\n      AWS Glue Data Catalog.  Regardless of your solution, consider\n      the requirements of your end users to determine the appropriate\n      tooling you need to report on your data provenance.  This will\n      help you determine how to best track your provenance.\n    \n\n   \n    \n    Implementation steps\n    \n    \n    \n       \n       \n       \n    \n        \n          Analyze the workload's data types, sensitivity levels, and\n          access requirements to classify the data and define\n          appropriate lifecycle management strategies.\n        \n      \n        \n          Design and implement data retention policies and automated\n          destruction processes that align with legal, regulatory, and\n          organizational requirements.\n        \n      \n        \n          Establish processes and automation for continuous monitoring,\n          auditing, and adjustment of data lifecycle management\n          strategies, controls, and policies as workload requirements\n          and regulations evolve.\n        \n        \n           \n        \n            \n              Detect resources that do not have automated lifecycle\n              management turned on with\n              AWS Config\n            \n          \n      \n   \n   \n      \n      Resources\n      \n      \n      \n        Related best practices:\n      \n      \n         \n         \n      \n          \n            COST04-BP05\n              Enforce data retention policies\n          \n        \n          \n            SUS04-BP03\n              Use policies to manage the lifecycle of your datasets\n          \n        \n      \n        Related documents:\n      \n      \n         \n         \n         \n      \n          \n            Data\n              Classification Whitepaper\n          \n        \n          \n            AWS             Blueprint for Ransomware Defense\n          \n        \n          \n            DevOps\n              Guidance: Improve traceability with data provenance\n              tracking\n          \n        \n      \n        Related examples:\n      \n      \n         \n         \n      \n          \n            How\n              to protect sensitive data for its entire lifecycle in\n              AWS\n          \n        \n          \n            Build\n              data lineage for data lakes using AWS Glue, Amazon Neptune,\n              and Spline\n          \n        \n      \n        Related tools:\n      \n      \n         \n         \n         \n      \n          \n            AWS Backup\n          \n        \n          \n            Amazon Data Lifecycle Manager\n          \n        \n          \n            AWS Identity and Access Management Access Analyzer\n          \n        \n    \u003cdiv\u003e\u003cdiv\u003e\u003cdiv\u003e\u003cdiv id=\"js_error_message\"\u003e\u003cp\u003e\u003cimg src=\"https://d1ge0kk1l5kms0.cloudfront.net/images/G/01/webservices/console/warning.png\" alt=\"Warning\" /\u003e \u003cstrong\u003eJavascript is disabled or is unavailable in your browser.\u003c/strong\u003e\u003c/p\u003e\u003cp\u003eTo use the Amazon Web Services Documentation, Javascript must be enabled. Please refer to your browser's Help pages for instructions.\u003c/p\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003eDocument ConventionsSEC07-BP03 Automate identification and classification SEC 8. How do you protect your data at rest? ",
  "SEC08-BP01 Implement secure key management\n    Secure key management includes the storage, rotation, access\n    control, and monitoring of key material required to secure data at\n    rest for your workload.\n  \n    Desired outcome: You have a\n    scalable, repeatable, and automated key management mechanism. The\n    mechanism enforces least privilege access to key material and\n    provides the correct balance between key availability,\n    confidentiality, and integrity. You monitor access to the keys, and\n    if rotation of key material is required, you rotate them using an\n    automated process. You do not allow key material to be accessed by\n    human operators.\n  Common anti-patterns:\n  \n     \n     \n     \n  \n      \n        Human access to unencrypted key material.\n      \n    \n      \n        Creating custom cryptographic algorithms.\n      \n    \n      \n        Overly broad permissions to access key material.\n      \n    \n    Benefits of establishing this best practice: By establishing a\n    secure key management mechanism for your workload, you can help provide protection for your\n    content against unauthorized access. Additionally, you may be subject to regulatory requirements\n    to encrypt your data. An effective key management solution can provide technical mechanisms\n    aligned to those regulations to protect key material. \n    Level of risk exposed if this best practice\n      is not established: High\n  \n    \n    Implementation guidance\n    \n    \n    \n      Encryption of data at rest is a fundamental security control. To\n      implement this control, your workload needs a mechanism to\n      securely store and manage the key material used to encrypt your\n      data at rest.\n    \n    \n      AWS offers the AWS Key Management Service (AWS KMS) to provide\n      durable, secure, and redundant storage for AWS KMS keys.\n      Many\n        AWS services integrate with AWS KMS to support encryption\n      of your data. AWS KMS uses FIPS 140-2 Level 3 validated hardware\n      security modules to protect your keys. There is no mechanism to\n      export AWS KMS keys in plain text.\n    \n    \n      When deploying workloads using a multi-account strategy, you\n      should keep AWS KMS keys in the same account as the workload that\n      uses them.\n      This\n        distributed model places the responsibility for managing\n      the AWS KMS keys with your team. In other use cases, your\n      organization may choose to store AWS KMS keys into a centralized\n      account. This centralized structure requires additional policies\n      to enable the cross-account access required for the workload\n      account to access keys stored in the centralized account, but may\n      be more applicable in use cases where a single key is shared\n      across multiple AWS accounts.\n    \n    \n      Regardless of where the key material is stored, you should tightly\n      control access to the key through the use of\n      key\n        policies and IAM policies. Key policies are the primary way\n      to control access to an AWS KMS key. Additionally, AWS KMS key\n      grants can provide access to AWS services to encrypt and decrypt\n      data on your behalf. Review the\n      guidance\n        for access control to your AWS KMS keys.\n    \n    \n      You should monitor the use of encryption keys to detect unusual\n      access patterns. Operations performed using AWS managed keys and\n      customer managed keys stored in AWS KMS can be logged in AWS CloudTrail and should be reviewed periodically. Pay special\n      attention to monitoring key destruction events. To mitigate\n      accidental or malicious destruction of key material, key\n      destruction events do not delete the key material immediately.\n      Attempts to delete keys in AWS KMS are subject to a\n      waiting\n        period, which defaults to 30 days and a minimum of 7 days,\n      providing administrators time to review these actions and roll\n      back the request if necessary.\n    \n    \n      Most AWS services use AWS KMS in a way that is transparent to you\n      - your only requirement is to decide whether to use an AWS managed\n      or customer managed key. If your workload requires the direct use\n      of AWS KMS to encrypt or decrypt data, you should use\n      envelope\n        encryption to protect your data. The\n      AWS       Encryption SDK can provide your applications client-side\n      encryption primitives to implement envelope encryption and\n      integrate with AWS KMS.\n    \n     \n      \n      Implementation steps\n      \n      \n      \n         \n         \n         \n         \n         \n         \n         \n      \n          \n            Determine the appropriate\n            key\n              management options (AWS managed or customer managed)\n            for the key.\n          \n          \n             \n             \n          \n              \n                For ease of use, AWS offers AWS owned and AWS managed\n                keys for most services, which provide encryption-at-rest\n                capability without the need to manage key material or\n                key policies.\n              \n            \n              \n                When using customer managed keys, consider the default\n                key store to provide the best balance between agility,\n                security, data sovereignty, and availability. Other use\n                cases may require the use of custom key stores with\n                AWS CloudHSM or the\n                external\n                  key store.\n              \n            \n        \n          \n            Review the list of services that you are using for your\n            workload to understand how AWS KMS integrates with the\n            service. For example, EC2 instances can use encrypted EBS\n            volumes, verifying that Amazon EBS snapshots created from\n            those volumes are also encrypted using a customer managed\n            key and mitigating accidental disclosure of unencrypted\n            snapshot data.\n          \n          \n             \n             \n          \n              \n                How\n                  AWS services use AWS KMS\n              \n            \n              \n                For detailed information about the encryption options\n                that an AWS service offers, see the Encryption at Rest\n                topic in the user guide or the developer guide for the\n                service.\n              \n            \n        \n          \n            Implement AWS KMS: AWS KMS makes it simple for you to create\n            and manage keys and control the use of encryption across a\n            wide range of AWS services and in your applications.\n          \n          \n             \n             \n          \n              \n                Getting\n                  started: AWS Key Management Service (AWS KMS)\n              \n            \n              \n                Review the\n                best\n                  practices for access control to your AWS KMS\n                  keys.\n              \n            \n        \n          \n            Consider AWS Encryption SDK: Use the AWS Encryption SDK with\n            AWS KMS integration when your application needs to encrypt\n            data client-side.\n          \n          \n             \n          \n              \n                AWS                 Encryption SDK\n              \n            \n        \n          \n            Enable\n            IAM Access Analyzer to automatically review and notify if\n            there are overly broad AWS KMS key policies.\n          \n          \n             \n          \n              \n                Consider using\n                custom\n                  policy checks to verify that a resource policy\n                update does not grant public access to KMS Keys.\n              \n            \n        \n          \n            Enable\n            Security Hub to receive notifications if there are\n            misconfigured key policies, keys scheduled for deletion, or\n            keys without automated rotation enabled.\n          \n        \n          \n            Determine the logging level appropriate for your AWS KMS\n            keys. Since calls to AWS KMS, including read-only events,\n            are logged, the CloudTrail logs associated with AWS KMS can\n            become voluminous.\n          \n          \n             \n          \n              \n                Some organizations prefer to segregate the AWS KMS\n                logging activity into a separate trail. For more detail,\n                see the\n                Logging\n                  AWS KMS API calls with CloudTrail section of the\n                AWS KMS developers guide.\n              \n            \n        \n     \n   \n    \n    Resources\n    \n    \n    \n      Related documents:\n    \n    \n       \n       \n       \n       \n       \n       \n       \n    \n        \n          AWS Key Management Service\n        \n      \n        \n          AWS           cryptographic services and tools\n        \n      \n        \n          Protecting\n            Amazon S3 Data Using Encryption\n        \n      \n        \n          Envelope\n            encryption\n        \n      \n        \n          Digital sovereignty pledge\n        \n      \n        \n          Demystifying\n            AWS KMS key operations, bring your own key, custom key store, and\n            ciphertext portability\n        \n      \n        \n          AWS Key Management Service cryptographic details\n        \n      \n    \n      Related videos:\n    \n    \n       \n       \n       \n    \n        \n          How\n            Encryption Works in AWS\n        \n      \n        \n          Securing\n            Your Block Storage on AWS\n        \n      \n        \n          AWS           data protection: Using locks, keys, signatures, and\n            certificates\n        \n      \n    \n      Related examples:\n    \n    \n       \n    \n        \n          Implement\n            advanced access control mechanisms using AWS KMS\n        \n      \n  \u003cdiv\u003e\u003cdiv\u003e\u003cdiv\u003e\u003cdiv id=\"js_error_message\"\u003e\u003cp\u003e\u003cimg src=\"https://d1ge0kk1l5kms0.cloudfront.net/images/G/01/webservices/console/warning.png\" alt=\"Warning\" /\u003e \u003cstrong\u003eJavascript is disabled or is unavailable in your browser.\u003c/strong\u003e\u003c/p\u003e\u003cp\u003eTo use the Amazon Web Services Documentation, Javascript must be enabled. Please refer to your browser's Help pages for instructions.\u003c/p\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003eDocument Conventions SEC 8. How do you protect your data at rest? SEC08-BP02 Enforce encryption at rest",
  "SEC08-BP02 Enforce encryption at rest\n    Encrypt private data at rest to maintain confidentiality and provide\n    an additional layer of protection against unintended data disclosure\n    or exfiltration. Encryption protects data so that it cannot be read\n    or accessed without first being decrypted. Inventory and control\n    unencrypted data to mitigate risks associated with data exposure.\n  \n    Desired outcome: You have mechanisms that encrypt private data by default when at rest. These mechanisms help maintain the confidentiality of the data and provides an additional layer of protection against unintended data disclosure or exfiltration. You maintain an inventory of unencrypted data and understand the controls that are in place to protect it.\n  \n    Common anti-patterns:\n  \n     \n     \n     \n     \n     \n  \n      \n        Not using encrypt-by-default configurations.\n      \n    \n      \n        Providing overly permissive access to decryption keys.\n      \n    \n      \n        Not monitoring the use of encryption and decryption keys.\n      \n    \n      \n        Storing data unencrypted.\n      \n    \n      \n        Using the same encryption key for all data regardless of data usage, types, and classification.\n      \n    \n    Level of risk exposed if this best practice\n    is not established: High\n  \n\n  Implementation guidance\n    \n      Map encryption keys to data classifications within your workloads. This approach helps protect against overly permissive access when using either a single, or very small number of encryption keys for your data (see SEC07-BP01 Understand your data classification scheme).\n    \n    \n      AWS Key Management Service (AWS KMS) integrates with many AWS\n      services to make it easier to encrypt your data at rest. For\n      example, in Amazon Elastic Compute Cloud (Amazon EC2), you can set\n      default\n        encryption on accounts so that new EBS volumes are\n      automatically encrypted. When using AWS KMS, consider how tightly\n      the data needs to be restricted. Default and service-controlled\n      AWS KMS keys are managed and used on your behalf by AWS. For\n      sensitive data that requires fine-grained access to the underlying\n      encryption key, consider customer managed keys (CMKs). You have\n      full control over CMKs, including rotation and access management\n      through the use of key policies.\n    \n    \n      Additionally, services such as Amazon Simple Storage Service\n      (Amazon S3) now encrypt all new objects by default. This\n      implementation provides enhanced security with no impact on\n      performance.\n    \n    \n      Other services, such as\n      Amazon Elastic Compute Cloud (Amazon EC2) or\n      Amazon Elastic File System (Amazon EFS), support settings for\n      default encryption. You can also use\n      AWS Config Rules to automatically check that you are using\n      encryption for\n      Amazon Elastic Block Store (Amazon EBS) volumes,\n      Amazon Relational Database Service (Amazon RDS) instances,\n      Amazon S3 buckets, and other services within your organization.\n    \n    \n      AWS also provides options for client-side encryption, allowing you to encrypt data prior to uploading it to the cloud. The AWS Encryption SDK provides a way to encrypt your data using envelope encryption. You provide the wrapping key, and the AWS Encryption SDK generates a unique data key for each data object it encrypts. Consider AWS CloudHSM if you need a managed single-tenant hardware security module (HSM). AWS CloudHSM allows you to generate, import, and manage cryptographic keys on a FIPS 140-2 level 3 validated HSM. Some use cases for AWS CloudHSM include protecting private keys for issuing a certificate authority (CA), and turning on transparent data encryption (TDE) for Oracle databases. The AWS CloudHSM Client SDK provides software that allows you to encrypt data client side using keys stored inside AWS CloudHSM prior to uploading your data into AWS. The Amazon DynamoDB Encryption Client also allows you to encrypt and sign items prior to upload into a DynamoDB table.\n    \n     \n      \n      Implementation steps\n      \n      \n      \n        \n         \n         \n         \n         \n         \n      \n          \n            Configure\n            default\n              encryption for new Amazon EBS\n              volumes:\n            Specify that you want all newly created Amazon EBS volumes\n            to be created in encrypted form, with the option of using\n            the default key provided by AWS or a key that you create.\n          \n        \n          \n            Configure encrypted Amazon Machine\n              Images (AMIs): Copying an existing AMI with\n            encryption configured will automatically encrypt root\n            volumes and snapshots.\n          \n        \n          \n            Configure\n            Amazon RDS\n              encryption:\n            Configure encryption for your Amazon RDS database clusters\n            and snapshots at rest by using the encryption option.\n          \n        \n          \n            Create and configure AWS KMS keys\n              with policies that limit access to the appropriate\n              principals for each classification of data: For\n            example, create one AWS KMS key for encrypting production\n            data and a different key for encrypting development or test\n            data. You can also provide key access to other AWS accounts.\n            Consider having different accounts for your development and\n            production environments. If your production environment\n            needs to decrypt artifacts in the development account, you\n            can edit the CMK policy used to encrypt the development\n            artifacts to give the production account the ability to\n            decrypt those artifacts. The production environment can then\n            ingest the decrypted data for use in production.\n          \n        \n          \n            Configure encryption in additional\n              AWS services: For other AWS services you use,\n            review the\n            security\n              documentation for that service to determine the\n            service's encryption options.\n          \n        \n     \n   \n\n  Resources\n\n      \n    \n      Related documents:\n    \n    \n       \n       \n       \n       \n       \n       \n       \n       \n       \n       \n    \n        \n          AWS           Crypto Tools\n        \n      \n        \n          AWS Encryption SDK\n        \n      \n        \n          AWS KMS Cryptographic Details Whitepaper\n        \n      \n        \n          AWS Key Management Service\n        \n      \n        \n          AWS           cryptographic services and tools\n        \n      \n        \n          Amazon EBS Encryption\n        \n      \n        \n          Default\n          encryption for Amazon EBS volumes\n        \n      \n        \n          Encrypting\n          Amazon RDS Resources\n        \n      \n        \n          How\n          do I enable default encryption for an Amazon S3 bucket?\n        \n      \n        \n          Protecting\n          Amazon S3 Data Using Encryption\n        \n      \n    \n      Related videos:\n    \n    \n       \n       \n    \n        \n          How Encryption\n          Works in AWS\n        \n      \n        \n          Securing Your\n          Block Storage on AWS\n        \n      \n  \u003cdiv\u003e\u003cdiv\u003e\u003cdiv\u003e\u003cdiv id=\"js_error_message\"\u003e\u003cp\u003e\u003cimg src=\"https://d1ge0kk1l5kms0.cloudfront.net/images/G/01/webservices/console/warning.png\" alt=\"Warning\" /\u003e \u003cstrong\u003eJavascript is disabled or is unavailable in your browser.\u003c/strong\u003e\u003c/p\u003e\u003cp\u003eTo use the Amazon Web Services Documentation, Javascript must be enabled. Please refer to your browser's Help pages for instructions.\u003c/p\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003eDocument ConventionsSEC08-BP01 Implement secure key managementSEC08-BP03 Automate data at rest protection",
  "SEC08-BP03 Automate data at rest protection\n    Use automation to validate and enforce data at rest controls.  Use\n    automated scanning to detect misconfiguration of your data storage\n    solutions, and perform remediations through automated programmatic\n    response where possible.  Incorporate automation in your CI/CD\n    processes to detect data storage misconfigurations before they are\n    deployed to production.\n  \n    Desired outcome: Automated\n    systems scan and monitor data storage locations for misconfiguration\n    of controls, unauthorized access, and unexpected use.  Detection of\n    misconfigured storage locations initiates automated remediations.\n     Automated processes create data backups and store immutable copies\n    outside of the original environment.\n  \n    Common anti-patterns:\n  \n     \n     \n     \n     \n  \n      \n        Not considering options to enable encryption by default\n        settings, where supported.\n      \n    \n      \n        Not considering security events, in addition to operational\n        events, when formulating an automated backup and recovery\n        strategy.\n      \n    \n      \n        Not enforcing public access settings for storage services.\n      \n    \n      \n        Not monitoring and audit your controls for protecting data at\n        rest.\n      \n    \n    Benefits of establishing this best\n    practice: Automation helps to prevent the risk of\n    misconfiguring your data storage locations. It helps to prevent\n    misconfigurations from entering your production environments. This\n    best practice also helps with detecting and fixing misconfigurations\n    if they occur. \n  \n    Level of risk exposed if this best practice\n    is not established: Medium\n  \n\n  Implementation guidance \n\n      \n    \n      Automation is a theme throughout the practices for protecting your\n      data at rest.\n      SEC01-BP06 Automate\n        deployment of standard security controls describes how you\n      can capture the configuration of your resources using\n      infrastructure as code (IaC) templates, such\n      as\n      with AWS CloudFormation.  These templates are committed to a version\n      control system, and are used to deploy resources on AWS through a\n      CI/CD pipeline.  These techniques equally apply to automating the\n      configuration of your data storage solutions, such as encryption\n      settings on Amazon S3 buckets.  \n    \n    \n      You can check the settings that you define in your IaC templates\n      for misconfiguration in your CI/CD pipelines using rules in AWS CloudFormation Guard.  You\n      can monitor settings that are not yet available in CloudFormation\n      or other IaC tooling for misconfiguration with\n      AWS Config.  Alerts that Config generates for misconfigurations\n      can be remediated automatically, as described in\n      SEC04-BP04 Initiate\n        remediation for non-compliant resources.\n    \n    \n      Using automation as part of your permissions management strategy\n      is also an integral component of automated data protections.\n      SEC03-BP02\n        Grant least privilege access and\n      SEC03-BP04\n        Reduce permissions continuously describe configuring\n      least-privilege access policies that are continually monitored by the AWS Identity and Access Management Access Analyzer to generate findings when permission can be reduced.  Beyond\n      automation for monitoring permissions, you can configure\n      Amazon GuardDuty to watch for anomalous data access behavior for\n      your\n      EBS\n      volumes (by way of an EC2 instance),\n      S3\n      buckets, and supported\n      Amazon Relational Database Service databases.\n    \n    \n      Automation also plays a role in detecting when sensitive data is\n      stored in unauthorized locations.\n      SEC07-BP03 Automate\n        identification and classification describes how\n      Amazon Macie can monitor your S3 buckets for unexpected sensitive\n      data and generate alerts that can initiate an automated response.\n    \n    \n      Follow the practices in\n      REL09\n      Back up data to develop an automated data backup and\n      recovery strategy. Data backup and recovery is as important for\n      recovering from security events as it is for operational events.\n    \n\n   \n\n  Implementation steps\n\n      \n    \n       \n       \n       \n       \n       \n       \n    \n        \n          Capture data storage configuration in IaC templates.  Use\n          automated checks in your CI/CD pipelines to detect\n          misconfigurations.\n        \n        \n           \n           \n        \n            \n              You can use for AWS CloudFormation your IaC templates, and\n              AWS CloudFormation\n              Guard for checking templates for misconfiguration.\n            \n          \n            \n              Use AWS Config to run rules in a proactive evaluation mode.\n              Use this setting to check the compliance of a resource as\n              a step in your CI/CD pipeline before creating it.\n            \n          \n      \n        \n          Monitor resources for data storage misconfigurations.\n        \n        \n           \n           \n        \n            \n              Set AWS Config to monitor data storage resources for\n              changes in control configurations and generate alerts to\n              invoke remediation actions when it detects a\n              misconfiguration.\n            \n          \n            \n              See\n              SEC04-BP04 Initiate\n                remediation for non-compliant resources\n              for more guidance on automated remediations.\n            \n          \n      \n        \n          Monitor and reduce data access permissions continually through\n          automation.\n        \n        \n           \n        \n            \n              IAM Access Analyzer can run continually to generate\n              alerts when permissions can potentially be reduced.\n            \n          \n      \n        \n          Monitor and alert on anomalous data access behaviors.\n        \n        \n           \n        \n            \n              GuardDuty\n              watches for both known threat signatures and deviations\n              from baseline access behaviors for data storage resources\n              such as EBS volumes, S3 buckets, and RDS databases.\n            \n          \n      \n        \n          Monitor and alert on sensitive data being stored in unexpected\n          locations.\n        \n        \n           \n        \n            \n              Use\n              Amazon Macie to continually scan your S3 buckets for\n              sensitive data.\n            \n          \n      \n        \n          Automate secure and encrypted backups of your data.\n        \n        \n           \n        \n            \n              AWS Backup is a managed service that creates encrypted\n              and secure backups of various data sources on AWS.\n               Elastic\n              Disaster Recovery allows you to copy full server\n              workloads and maintain continuous data protection with a\n              recovery point objective (RPO) measured in seconds.  You\n              can configure both services to work together to automate\n              creating data backups and copying them to failover\n              locations.  This can help keep your data available when\n              impacted by either operational or security events.\n            \n          \n      \n   \n   \n\n  Resources\n\n      \n    \n      Related best practices:\n    \n    \n       \n       \n       \n       \n       \n       \n       \n    \n        \n          SEC01-BP06\n            Automate deployment of standard security controls\n        \n      \n        \n          SEC03-BP02\n            Grant least privilege access\n        \n      \n        \n          SEC03-BP04\n            Reduce permissions continuously\n        \n      \n        \n          SEC04-BP04 Initiate\n            remediation for non-compliant resources\n        \n      \n        \n          SEC07-BP03 Automate\n            identification and classification\n        \n      \n        \n          REL09-BP02\n          Secure and encrypt backups\n        \n      \n        \n          REL09-BP03\n          Perform data backup automatically\n        \n      \n    \n      Related documents:\n    \n    \n       \n       \n    \n        \n          AWS           Prescriptive Guidance: Automatically encrypt existing and new\n          Amazon EBS volumes\n        \n      \n        \n          Ransomware\n          Risk Management on AWS Using the NIST Cyber Security Framework\n          (CSF)\n        \n      \n    \n      Related examples:\n    \n    \n       \n       \n       \n       \n    \n        \n          How\n          to use AWS Config proactive rules and AWS CloudFormation Hooks\n          to prevent creation of noncompliant cloud resources\n        \n      \n        \n          Automate\n          and centrally manage data protection for Amazon S3 with AWS Backup\n        \n      \n        \n          AWS           re:Invent 2023 - Implement proactive data protection using\n          Amazon EBS snapshots\n        \n      \n        \n          AWS           re:Invent 2022 - Build and automate for resilience with modern\n          data protection\n        \n      \n    \n      Related tools:\n    \n    \n       \n       \n       \n       \n       \n       \n    \n        \n          AWS CloudFormation Guard\n        \n      \n        \n          AWS CloudFormation Guard Rules Registry\n        \n      \n        \n          IAM Access Analyzer\n        \n      \n        \n          Amazon Macie\n        \n      \n        \n          AWS Backup\n        \n      \n        \n          Elastic\n          Disaster Recovery\n        \n      \n  \u003cdiv\u003e\u003cdiv\u003e\u003cdiv\u003e\u003cdiv id=\"js_error_message\"\u003e\u003cp\u003e\u003cimg src=\"https://d1ge0kk1l5kms0.cloudfront.net/images/G/01/webservices/console/warning.png\" alt=\"Warning\" /\u003e \u003cstrong\u003eJavascript is disabled or is unavailable in your browser.\u003c/strong\u003e\u003c/p\u003e\u003cp\u003eTo use the Amazon Web Services Documentation, Javascript must be enabled. Please refer to your browser's Help pages for instructions.\u003c/p\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003eDocument ConventionsSEC08-BP02 Enforce encryption at restSEC08-BP04 Enforce access control",
  "SEC08-BP04 Enforce access control\n    To help protect your data at rest, enforce access control using\n    mechanisms such as isolation and versioning. Apply least privilege\n    and conditional access controls. Prevent granting public access to\n    your data.\n  \n    Desired outcome: You verify that\n    only authorized users can access data on a need-to-know basis. You\n    protect your data with regular backups and versioning to prevent\n    against intentional or inadvertent modification or deletion of data.\n    You isolate critical data from other data to protect its\n    confidentiality and data integrity.\n  Common anti-patterns:\n     \n     \n     \n     \n     \n     \n  \n       Storing data with different sensitivity requirements or classification together.\n      \n    \n       Using overly permissive permissions on decryption keys. \n    \n       Improperly classifying data. \n    \n       Not retaining detailed backups of important data. \n    \n       Providing persistent access to production data. \n    \n       Not auditing data access or regularly reviewing permissions.\n    Level of risk exposed if this best practice is not\n      established: High\n  \n    \n    Implementation guidance\n    \n    \n    \n      Protecting data at rest is important to maintain data integrity,\n      confidentiality, and compliance with regulatory requirements. You\n      can implement multiple controls to help achieve this, including\n      access control, isolation, conditional access, and versioning.\n    \n    \n      You can enforce access control with the principle of least\n      privilege, which provides only the necessary permissions to users\n      and services to perform their tasks. This includes access to\n      encryption keys. Review your\n      AWS Key Management Service (AWS KMS) policies to verify that\n      the level of access you grant is appropriate and that relevant\n      conditions apply.\n    \n    \n      You can separate data based on different classification levels by\n      using distinct AWS accounts for each level, and manage these\n      accounts using\n      AWS Organizations. This isolation can help prevent unauthorized\n      access and minimizes the risk of data exposure.\n    \n    \n      Regularly review the level of access granted in Amazon S3 bucket\n      policies. Avoid using publicly readable or writeable buckets\n      unless absolutely necessary. Consider using\n      AWS Config to detect publicly available buckets and Amazon CloudFront to serve content from Amazon S3. Verify that buckets\n      that should not allow public access are properly configured to\n      prevent it.\n    \n    \n      Implement versioning and object locking mechanisms for critical\n      data stored in Amazon S3.\n      Amazon S3 versioning preserves previous versions of objects to\n      recover data from accidental deletion or overwrites.\n      Amazon S3 Object Lock provides mandatory access control for\n      objects, which prevents them from being deleted or overwritten,\n      even by the root user, until the lock expires. Additionally,\n      Amazon S3 Glacier Vault Lock offers a similar feature for archives\n      stored in Amazon S3 Glacier.\n    \n     \n      \n      Implementation steps\n      \n      \n      \n         \n         \n         \n         \n         \n         \n         \n         \n      \n          \n            Enforce access control with the\n              principle of least privilege:\n          \n          \n             \n             \n          \n              \n                Review the access permissions granted to users and\n                services, and verify that they have only the necessary\n                permissions to perform their tasks.\n              \n            \n              \n                Review access to encryption keys by checking the\n                AWS Key Management Service (AWS KMS) policies.\n              \n            \n        \n          \n            Separate data based on different\n              classification levels:\n          \n          \n             \n             \n          \n              \n                Use distinct AWS accounts for each data classification\n                level.\n              \n            \n              \n                Manage these accounts using\n                AWS Organizations.\n              \n            \n        \n          \n            Review Amazon S3 bucket and object\n              permissions:\n          \n          \n             \n             \n             \n             \n             \n             \n          \n              \n                Regularly review the level of access granted in Amazon S3 bucket policies.\n              \n            \n              \n                Avoid using publicly readable or writeable buckets\n                unless absolutely necessary.\n              \n            \n              \n                Consider using\n                AWS Config to detect publicly available buckets.\n              \n            \n              \n                Use Amazon CloudFront to serve content from Amazon S3.\n              \n            \n              \n                Verify that buckets that should not allow public access\n                are properly configured to prevent it.\n              \n            \n              \n                You can apply the same review process for databases and\n                any other data sources that use IAM authentication, such\n                as SQS or third-party data stores.\n              \n            \n        \n          \n            Use AWS IAM Access Analyzer:\n          \n          \n             \n          \n              \n                You can configure AWS IAM Access Analyzer\n                to analyze Amazon S3 buckets and generate findings when\n                an S3 policy grants access to an external entity.\n              \n            \n        \n          \n            Implement versioning and object\n              locking mechanisms:\n          \n          \n             \n             \n             \n          \n              \n                Use\n                Amazon S3 versioning to preserve previous versions of\n                objects, which provides recovery from accidental\n                deletion or overwrites.\n              \n            \n              \n                Use\n                Amazon S3 Object Lock to provide mandatory access\n                control for objects, which prevents them from being\n                deleted or overwritten, even by the root user, until the\n                lock expires.\n              \n            \n              \n                Use\n                Amazon S3 Glacier Vault Lock for archives stored in\n                Amazon S3 Glacier.\n              \n            \n        \n          \n            Use Amazon S3 Inventory:\n          \n          \n             \n          \n              \n                You can use\n                Amazon S3 Inventory to audit and report on the\n                replication and encryption status of your S3 objects.\n              \n            \n        \n          \n            Review Amazon EBS and AMI sharing\n              permissions:\n          \n          \n             \n          \n              \n                Review your sharing permissions for\n                Amazon EBS and\n                AMI\n                  sharing to verify that your images and volumes\n                are not shared with AWS accounts that are external to\n                your workload.\n              \n            \n        \n          \n            Review AWS Resource Access Manager\n              Shares periodically:\n          \n          \n             \n             \n          \n              \n                You can use\n                AWS                 Resource Access Manager to share resources, such\n                as AWS Network Firewall policies, Amazon Route 53\n                resolver rules, and subnets, within your Amazon VPCs.\n              \n            \n              \n                Audit shared resources regularly and stop sharing\n                resources that no longer need to be shared.\n              \n            \n        \n     \n   \n\n  Resources\n\n      \n    \n      \n        Related best practices:\n      \n    \n    \n       \n       \n    \n        SEC03-BP01 Define access requirements\n        \n      \n        \n          SEC03-BP02 Grant least privilege access\n        \n      \n    \n      \n        Related documents:\n      \n    \n    \n       \n       \n       \n       \n       \n       \n       \n       \n       \n       \n       \n       \n    \n        \n          AWS KMS Cryptographic Details Whitepaper\n        \n      \n        \n          Introduction\n          to Managing Access Permissions to Your Amazon S3\n          Resources\n        \n      \n        \n          Overview\n          of managing access to your AWS KMS resources\n        \n      \n        \n          AWS Config Rules\n        \n      \n        \n          Amazon S3 + Amazon CloudFront: A Match Made in the Cloud\n        \n      \n        \n          Using\n          versioning\n        \n      \n        \n          Locking\n          Objects Using Amazon S3 Object Lock\n        \n      \n        \n          Sharing\n          an Amazon EBS Snapshot\n        \n      \n        \n          Shared\n          AMIs\n        \n      \n        \n          Hosting\n          a single-page application on Amazon S3\n        \n      \n        \n          AWS           Global Condition Keys\n        \n      \n        \n          Building\n            a Data Perimeter on AWS\n        \n      \n    \n      \n        Related videos:\n      \n    \n    \n       \n    \n        \n          Securing Your\n          Block Storage on AWS\n        \n      \n  \u003cdiv\u003e\u003cdiv\u003e\u003cdiv\u003e\u003cdiv id=\"js_error_message\"\u003e\u003cp\u003e\u003cimg src=\"https://d1ge0kk1l5kms0.cloudfront.net/images/G/01/webservices/console/warning.png\" alt=\"Warning\" /\u003e \u003cstrong\u003eJavascript is disabled or is unavailable in your browser.\u003c/strong\u003e\u003c/p\u003e\u003cp\u003eTo use the Amazon Web Services Documentation, Javascript must be enabled. Please refer to your browser's Help pages for instructions.\u003c/p\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003eDocument ConventionsSEC08-BP03 Automate data at rest protection SEC 9. How do you protect your data in transit? ",
  "SEC09-BP01 Implement secure key and certificate\n  management\n    Transport Layer Security (TLS) certificates are used to secure\n    network communications and establish the identity of websites,\n    resources, and workloads over the internet, as well as private\n    networks.\n  \n    Desired outcome: A secure certificate management system that\n    can provision, deploy, store, and renew certificates in a public key infrastructure (PKI). A\n    secure key and certificate management mechanism prevents certificate private key material from\n    disclosure and automatically renews the certificate on a periodic basis. It also integrates with other\n    services to provide secure network communications and identity for machine resources inside of\n    your workload. Key material should never be accessible to human identities. \n    Common anti-patterns:\n  \n     \n     \n     \n  \n      \n        Performing manual steps during the certificate deployment or renewal\n        processes.\n      \n    \n      \n        Paying insufficient attention to certificate authority (CA)\n        hierarchy when designing a private CA.\n      \n    \n      \n        Using self-signed certificates for public resources.\n      \n    \n    Benefits of establishing this best practice: \n     \n     \n     \n     \n  \n      \n        Simplify certificate management through automated deployment and\n        renewal\n      \n    \n      \n        Encourage encryption of data in transit using TLS certificates\n      \n    \n      \n        Increased security and auditability of certificate actions taken by\n        the certificate authority\n      \n    \n      \n        Organization of management duties at different layers of the CA\n        hierarchy\n      \n    \n    Level of risk exposed if this best practice is not established: High\n    \n    Implementation guidance \n    \n    \n    \n      Modern workloads make extensive use of encrypted network\n      communications using PKI protocols\n      such as TLS. PKI certificate management\n      can be complex, but automated certificate provisioning,\n      deployment, and renewal can reduce the friction associated with\n      certificate management.\n    \n    \n      AWS provides two services to manage general-purpose PKI\n      certificates:\n      AWS Certificate Manager and\n      AWS Private Certificate Authority (AWS Private CA). ACM is the\n      primary service that customers use to provision, manage, and\n      deploy certificates for use in both public-facing as well as\n      private AWS workloads. ACM issues private certificates using AWS Private CA and\n      integrates\n      with many other AWS managed services to provide secure TLS\n      certificates for workloads. ACM can also issue publicly trusted\n      certificates from\n      Amazon\n        Trust Services. Public certificates from ACM can be used on\n      public facing workloads, as modern browsers and operating systems\n      trust these certificates by default.\n    \n    \n      AWS Private CA allows you to\n      establish your own root or subordinate certificate authority and\n      issue TLS certificates through an API. You can use these kinds of\n      certificates in scenarios where you control and manage the trust\n      chain on the client side of the TLS connection. In addition to TLS\n      use cases, AWS Private CA can be used to issue certificates to\n      Kubernetes pods, Matter device product attestations, code signing,\n      and other use cases with a\n      custom\n        template. You can also use\n      IAM\n        Roles Anywhere to provide temporary IAM credentials to\n      on-premises workloads that have been issued X.509 certificates\n      signed by your Private CA.\n    \n    \n      In addition to ACM and AWS Private CA,\n      AWS IoT Core provides specialized support for provisioning,\n      managing and deploying PKI certificates to IoT\n      devices. AWS IoT Core provides specialized mechanisms for\n      onboarding\n        IoT devices into your public key infrastructure at scale.\n    \n    \n      Some AWS services, such as\n      Amazon API Gateway and\n      Elastic Load Balancing, offer their own capabilities for using\n      certificates to secure application connections. For example, both\n      API Gateway and Application Load Balancer (ALB) support mutual TLS\n      (mTLS) using client certificates that you create and export using\n      the AWS Management Console, CLI, or APIs.\n    \n    Considerations for establishing a private CA hierarchy\n      \n    \n      When you need to establish a private CA, it's important to take\n      special care to properly design the CA hierarchy upfront. It's a\n      best practice to deploy each level of your CA hierarchy into\n      separate AWS accounts when creating a private CA hierarchy. This\n      intentional step reduces the surface area for each level in the CA\n      hierarchy, making it simpler to discover anomalies in CloudTrail\n      log data and reducing the scope of access or impact if there is\n      unauthorized access to one of the accounts. The root CA should reside in its own separate account and should only be used to issue one or more intermediate CA certificates.\n    \n    \n      Then, create one or more intermediate CAs in accounts separate\n      from the root CA's account to issue certificates for end users,\n      devices, or other workloads. Finally, issue certificates from your\n      root CA to the intermediate CAs, which will in turn issue\n      certificates to your end users or devices. For more information on\n      planning your CA deployment and designing your CA hierarchy,\n      including planning for resiliency, cross-region replication,\n      sharing CAs across your organization, and more, see\n      Planning\n        your AWS Private CA deployment.\n    \n     \n      \n      Implementation steps\n      \n         \n         \n         \n      \n          \n            Determine the relevant AWS services required for your use\n            case:\n          \n          \n             \n             \n             \n             \n          \n              \n                Many use cases can leverage the existing AWS public key\n                infrastructure using\n                AWS Certificate Manager. ACM can\n                be used to deploy TLS certificates for web servers, load\n                balancers, or other uses for publicly trusted certificates.\n              \n            \n              \n                Consider AWS Private CA when you need\n                to establish your own private certificate authority\n                hierarchy or need access to exportable certificates. ACM can then be used to issue many types\n                  of end-entity certificates using the AWS Private CA.\n              \n            \n              \n                For use cases where certificates must be provisioned at\n                scale for embedded Internet of things (IoT) devices,\n                consider AWS IoT Core.\n              \n            \n              \n                Consider using native mTLS functionality in services\n                like\n                Amazon API Gateway or\n                Application Load Balancer.\n              \n            \n        \n          \n            Implement automated certificate renewal whenever possible:\n          \n          \n             \n          \n              \n                Use ACM managed renewal for\n                certificates issued by ACM along with\n                integrated AWS managed services.\n              \n            \n        \n          \n            Establish logging and audit trails:\n          \n          \n             \n             \n             \n          \n              \n                Enable\n                CloudTrail\n                  logs to track access to the accounts holding\n                certificate authorities. Consider configuring log file\n                integrity validation in CloudTrail to verify the\n                authenticity of the log data.\n              \n            \n              \n                Periodically generate and review\n                audit\n                  reports that list the certificates that your private\n                CA has issued or revoked. These reports can be exported to\n                an S3 bucket.\n              \n            \n              \n                When deploying a private CA, you will also need to establish\n                an S3 bucket to store the Certificate Revocation List (CRL).\n                For guidance on configuring this S3 bucket based on your\n                workload's requirements, see\n                Planning\n                  a certificate revocation list (CRL).\n              \n            \n        \n     \n   \n    \n    Resources \n    \n      Related best\n        practices:\n    \n    \n    \n       \n       \n       \n    \n        \n          SEC02-BP02 Use temporary credentials\n        \n      \n        SEC08-BP01 Implement secure key management\n      \n        \n          SEC09-BP03 Authenticate network communications        \n        \n      \n    \n    \n      Related documents:\n    \n    \n    \n       \n       \n       \n       \n    \n        \n          How\n            to host and manage an entire private certificate\n            infrastructure in AWS\n        \n      \n        \n          How\n            to secure an enterprise scale ACM Private CA hierarchy for\n            automotive and manufacturing\n        \n      \n        \n          Private\n            CA best practices\n        \n      \n        \n          How\n            to use AWS RAM to share your ACM Private CA\n            cross-account\n        \n      \n    \n    \n      Related videos:\n    \n    \n    \n       \n    \n        \n          Activating\n            AWS Certificate Manager Private CA (workshop)\n        \n      \n    \n      Related examples:\n    \n    \n       \n       \n    \n        \n          Private\n            CA workshop\n        \n      \n        \n          IOT\n            Device Management Workshop (including device\n          provisioning)\n        \n      \n    \n      Related tools:\n    \n    \n       \n    \n        \n          Plugin\n            to Kubernetes cert-manager to use AWS Private CA\n        \n      \n  \u003cdiv\u003e\u003cdiv\u003e\u003cdiv\u003e\u003cdiv id=\"js_error_message\"\u003e\u003cp\u003e\u003cimg src=\"https://d1ge0kk1l5kms0.cloudfront.net/images/G/01/webservices/console/warning.png\" alt=\"Warning\" /\u003e \u003cstrong\u003eJavascript is disabled or is unavailable in your browser.\u003c/strong\u003e\u003c/p\u003e\u003cp\u003eTo use the Amazon Web Services Documentation, Javascript must be enabled. Please refer to your browser's Help pages for instructions.\u003c/p\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003eDocument Conventions SEC 9. How do you protect your data in transit? SEC09-BP02 Enforce encryption in transit",
  "SEC09-BP02 Enforce encryption in transitEnforce your defined encryption requirements based on your organization’s policies, regulatory obligations and standards to help meet organizational, legal, and compliance requirements. Only use protocols with encryption when transmitting sensitive data outside of your virtual private cloud (VPC). Encryption helps maintain data confidentiality even when the data transits untrusted networks.\n    Desired outcome: You encrypt\n    network traffic between your resources and the internet to mitigate\n    unauthorized access to the data. You encrypt network traffic within\n    your internal AWS environment according to your security\n    requirements. You encrypt data in transit using secure TLS protocols\n    and cipher suites.\n  \n    Common anti-patterns:\n  \n     \n     \n     \n     \n  \n      \n        Using deprecated versions of SSL, TLS, and cipher suite components (for example, SSL v3.0, 1024-bit RSA keys, and RC4 cipher).\n      \n    \n      \n        Allowing unencrypted (HTTP) traffic to or from public-facing resources.\n      \n    \n      \n        Not monitoring and replacing X.509 certificates prior to expiration.\n      \n    \n      \n        Using self-signed X.509 certificates for TLS.\n      \n    \n    Level of risk exposed if this best practice\n    is not established: High\n  \n\n  Implementation guidance\n    \n      AWS services provide HTTPS endpoints using TLS for communication,\n      providing encryption in transit when communicating with the AWS\n      APIs. Insecure HTTP protocols can be audited and blocked in a\n      Virtual Private Cloud (VPC) through the use of security groups.\n      HTTP requests can also be\n      automatically\n        redirected to HTTPS in Amazon CloudFront or on an\n      Application Load Balancer. You can use an\n      Amazon Simple Storage Service (Amazon S3) bucket policy to\n      restrict the ability to upload objects through HTTP, effectively\n      enforcing the use of HTTPS for object uploads to your bucket(s).\n      You have full control over your computing resources to implement\n      encryption in transit across your services. Additionally, you can\n      use VPN connectivity into your VPC from an external network or\n      AWS Direct Connect to facilitate encryption of traffic. Verify\n      that your clients make calls to AWS APIs using at least TLS 1.2,\n      as\n      AWS       has deprecated the use of earlier versions of TLS as of February\n        2024. We recommend you use TLS 1.3. If you have special\n      requirements for encryption in transit, you can find third-party\n      solutions available in the AWS Marketplace.\n    \n     \n      \n      Implementation steps\n    \n    \n       \n       \n       \n       \n       \n       \n       \n       \n       \n    \n        \n          Enforce encryption in transit: Your defined encryption requirements should be based on the latest standards and best practices and only allow secure protocols. For example, configure a security group to only allow the HTTPS protocol to an application load balancer or Amazon EC2 instance.\n        \n      \n        \n          Configure secure protocols in edge services: Configure HTTPS with Amazon CloudFront and use a security profile appropriate for your security posture and use case.\n        \n      \n        \n          Use a VPN for external connectivity: Consider using an IPsec VPN for securing point-to-point or network-to-network connections to help provide both data privacy and integrity.\n        \n      \n        \n          Configure secure protocols in load balancers: Select a security policy that provides the strongest cipher suites supported by the clients that will be connecting to the listener. Create an HTTPS listener for your Application Load Balancer.\n        \n      \n        \n          Configure secure protocols in Amazon Redshift: Configure your cluster to require a secure socket layer (SSL) or transport layer security (TLS) connection.\n        \n      \n        \n          Configure secure protocols: Review AWS service documentation to determine encryption-in-transit capabilities.\n        \n      \n        \n          Configure secure access when uploading to Amazon S3 buckets: Use Amazon S3 bucket policy controls to enforce secure access to data.\n        \n      \n        \n          Consider using AWS Certificate Manager: ACM allows you to provision, manage, and deploy public TLS certificates for use with AWS services.\n        \n      \n        \n          Consider using AWS Private Certificate Authority for private PKI needs: AWS Private CA allows you to create private certificate authority (CA) hierarchies to issue end-entity X.509 certificates that can be used to create encrypted TLS channels.\n        \n      \n   \n   \n\n  Resources\n\n      \n    \n      Related documents:\n    \n    \n       \n       \n       \n       \n       \n       \n    \n        \n          Using HTTPS with CloudFront\n        \n      \n        \n          Connect your VPC to remote networks using AWS Virtual Private Network\n      \n        \n          Create an HTTPS listener for your Application Load Balancer\n        \n      \n        \n          Tutorial: Configure SSL/TLS on Amazon Linux 2\n        \n      \n        \n          Using SSL/TLS to encrypt a connection to a DB instance\n        \n      \n        \n          Configuring security options for connections\n        \n      \n  \u003cdiv\u003e\u003cdiv\u003e\u003cdiv\u003e\u003cdiv id=\"js_error_message\"\u003e\u003cp\u003e\u003cimg src=\"https://d1ge0kk1l5kms0.cloudfront.net/images/G/01/webservices/console/warning.png\" alt=\"Warning\" /\u003e \u003cstrong\u003eJavascript is disabled or is unavailable in your browser.\u003c/strong\u003e\u003c/p\u003e\u003cp\u003eTo use the Amazon Web Services Documentation, Javascript must be enabled. Please refer to your browser's Help pages for instructions.\u003c/p\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003eDocument ConventionsSEC09-BP01 Implement secure key and certificate\n  managementSEC09-BP03 Authenticate network communications",
  "SEC09-BP03 Authenticate network communications\n    Verify the identity of communications by using protocols that\n    support authentication, such as Transport Layer Security (TLS) or\n    IPsec.\n  \n    Design your workload to use secure, authenticated network protocols whenever communicating between services, applications, or to users. Using network protocols that support authentication and authorization provides stronger control over network flows and reduces the impact of unauthorized access.\n  \n    Desired outcome: A workload with well-defined data plane and control plane traffic flows between services. The traffic flows use authenticated and encrypted network protocols where technically feasible.\n  \n    Common anti-patterns:\n  \n     \n     \n     \n     \n     \n  \n      \n        Unencrypted or unauthenticated traffic flows within your workload.\n      \n    \n      \n        Reusing authentication credentials across multiple users or entities.\n      \n    \n      \n        Relying solely on network controls as an access control mechanism.\n      \n    \n      \n        Creating a custom authentication mechanism rather than relying on industry-standard authentication mechanisms.\n      \n    \n      \n        Overly permissive traffic flows between service components or other resources in the VPC.\n      \n    \n    Benefits of establishing this best practice:\n  \n     \n     \n     \n     \n     \n  \n      \n        Limits the scope of impact for unauthorized access to one part of the workload.\n      \n    \n      \n        Provides a higher level of assurance that actions are only performed by authenticated entities.\n      \n    \n      \n        Improves decoupling of services by clearly defining and enforcing intended data transfer interfaces.\n      \n    \n      \n        Enhances monitoring, logging, and incident response through request attribution and well-defined communication interfaces.\n      \n    \n      \n        Provides defense-in-depth for your workloads by combining network controls with authentication and authorization controls.\n      \n    \n    Level of risk exposed if this best practice\n    is not established: Low\n  \n\n  Implementation guidance\n    \n      Your workload’s network traffic patterns can be characterized into two categories: \n    \n    \n       \n       \n    \n        \n          East-west traffic represents traffic flows between services that make up a workload.\n        \n      \n        \n          North-south traffic represents traffic flows between your workload and consumers. \n        \n      \n    \n      While it is common practice to encrypt north-south traffic, securing east-west traffic using authenticated protocols is less common. Modern security practices recommend that network design alone does not grant a trusted relationship between two entities. When two services may reside within a common network boundary, it is still best practice to encrypt, authenticate, and authorize communications between those services.\n    \n    \n      As an example, AWS service APIs use the AWS Signature Version 4 (SigV4) signature protocol to authenticate the caller, no matter what network the request originates from. This authentication ensures that AWS APIs can verify the identity that requested the action, and that identity can then be combined with policies to make an authorization decision to determine whether the action should be allowed or not.\n    \n    \n      Services such as Amazon VPC Lattice and Amazon API Gateway allow you use the same SigV4 signature protocol to add authentication and authorization to east-west traffic in your own workloads. If resources outside of your AWS environment need to communicate with services that require SigV4-based authentication and authorization, you can use AWS Identity and Access Management (IAM) Roles Anywhere on the non-AWS resource to acquire temporary AWS credentials. These credentials can be used to sign requests to services using SigV4 to authorize access.\n    \n    \n      Another common mechanism for authenticating east-west traffic is\n      TLS mutual authentication (mTLS). Many Internet of Things (IoT),\n      business-to-business applications, and microservices use mTLS to\n      validate the identity of both sides of a TLS communication through\n      the use of both client and server-side X.509 certificates. These\n      certificates can be issued by AWS Private Certificate Authority\n      (AWS Private CA). You can use services such as\n      Amazon API Gateway to provide mTLS authentication for inter- or\n      intra-workload communication.\n      Application Load Balancer also supports mTLS for internal or external\n      facing workloads. While mTLS provides authentication information\n      for both sides of a TLS communication, it does not provide a\n      mechanism for authorization.\n    \n    \n      Finally, OAuth 2.0 and OpenID Connect (OIDC) are two protocols typically used for controlling access to services by users, but are now becoming popular for service-to-service traffic as well. API Gateway provides a JSON Web Token (JWT) authorizer, allowing workloads to restrict access to API routes using JWTs issued from OIDC or OAuth 2.0 identity providers. OAuth2 scopes can be used as a source for basic authorization decisions, but the authorization checks still need to be implemented in the application layer, and OAuth2 scopes alone cannot support more complex authorization needs.\n    \n     \n      \n      Implementation steps\n      \n         \n         \n         \n         \n      \n          \n            Define and document your workload network flows: The first step in implementing a defense-in-depth strategy is defining your workload’s traffic flows. \n          \n          \n             \n             \n             \n          \n              \n                Create a data flow diagram that clearly defines how data is transmitted between different services that comprise your workload. This diagram is the first step to enforcing those flows through authenticated network channels.\n              \n            \n              \n                Instrument your workload in development and testing phases to validate that the data flow diagram accurately reflects the workload’s behavior at runtime.\n              \n            \n              \n                A data flow diagram can also be useful when performing a threat modeling exercise, as described in SEC01-BP07 Identify threats and prioritize mitigations using a threat model.\n              \n            \n        \n          \n            Establish network controls: Consider AWS capabilities to establish network controls aligned to your data flows. While network boundaries should not be the only security control, they provide a layer in the defense-in-depth strategy to protect your workload.\n          \n          \n             \n             \n          \n              \n                Use security groups to establish define and restrict data flows between resources.\n              \n            \n              \n                Consider using AWS PrivateLink to communicate with both AWS and third-party services that support AWS PrivateLink. Data sent through a AWS PrivateLink interface endpoint stays within the AWS network backbone and does not traverse the public Internet.\n              \n            \n        \n          \n            Implement authentication and authorization across services in your workload: Choose the set of AWS services most appropriate to provide authenticated, encrypted traffic flows in your workload.\n          \n          \n             \n             \n             \n             \n          \n              \n                Consider Amazon VPC Lattice to secure service-to-service communication. VPC Lattice can use SigV4 authentication combined with auth policies to control service-to-service access.\n              \n            \n              \n                For service-to-service communication using mTLS,\n                consider\n                API Gateway,\n                Application Load Balancer.\n                AWS Private CA can be used to establish a private CA\n                hierarchy capable of issuing certificates for use with\n                mTLS.\n              \n            \n              \n                When integrating with services using OAuth 2.0 or OIDC, consider API Gateway using the JWT authorizer.\n              \n            \n              \n                For communication between your workload and IoT devices, consider AWS IoT Core, which provides several options for network traffic encryption and authentication.\n              \n            \n        \n          \n            Monitor for unauthorized access: Continually monitor for unintended communication channels, unauthorized principals attempting to access protected resources, and other improper access patterns.\n          \n          \n             \n             \n             \n          \n              \n                If using VPC Lattice to manage access to your services, consider enabling and monitoring VPC Lattice access logs. These access logs include information on the requesting entity, network information including source and destination VPC, and request metadata.\n              \n            \n              \n                Consider enabling VPC flow logs to capture metadata on network flows and periodically review for anomalies.\n              \n            \n              \n                Refer to the AWS Security Incident Response Guide and the Incident Response section of the AWS Well-Architected Framework security pillar for more guidance on planning, simulating, and responding to security incidents.\n              \n            \n        \n     \n   \n\n  Resources\n\n    \n      Related best practices:\n    \n    \n       \n       \n       \n    \n        \n          SEC03-BP07 Analyze public and cross-account access\n        \n      \n        \n          SEC02-BP02 Use temporary credentials\n        \n      \n        \n          SEC01-BP07 Identify threats and prioritize mitigations using a threat model\n        \n      \n    \n      Related documents:\n    \n    \n       \n       \n       \n       \n       \n    \n        \n          Evaluating access control methods to secure Amazon API Gateway APIs\n        \n      \n        \n          Configuring mutual TLS authentication for a REST API\n        \n      \n        \n          How to secure API Gateway HTTP endpoints with JWT authorizer\n        \n      \n        \n          Authorizing direct calls to AWS services using AWS IoT Core credential provider\n        \n      \n        AWS Security Incident Response Guide\n        \n      \n    \n      Related videos:\n    \n    \n       \n       \n    \n        AWS re:invent 2022: Introducing VPC Lattice\n        \n      \n        AWS re:invent 2020: Serverless API authentication for HTTP APIs on AWS\n      \n    \n      Related examples:\n    \n    \n       \n       \n    \n        \n          Amazon VPC Lattice Workshop\n        \n      \n        \n          Zero-Trust Episode 1 – The Phantom Service Perimeter workshop\n        \n      \n  \u003cdiv\u003e\u003cdiv\u003e\u003cdiv\u003e\u003cdiv id=\"js_error_message\"\u003e\u003cp\u003e\u003cimg src=\"https://d1ge0kk1l5kms0.cloudfront.net/images/G/01/webservices/console/warning.png\" alt=\"Warning\" /\u003e \u003cstrong\u003eJavascript is disabled or is unavailable in your browser.\u003c/strong\u003e\u003c/p\u003e\u003cp\u003eTo use the Amazon Web Services Documentation, Javascript must be enabled. Please refer to your browser's Help pages for instructions.\u003c/p\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003eDocument ConventionsSEC09-BP02 Enforce encryption in transitIncident response",
  "SEC10-BP01 Identify key personnel and external\n  resources\n    Identify internal and external personnel, resources, and legal\n    obligations to help your organization respond to an incident.\n  \n    Desired outcome: You have a list\n    of key personnel, their contact information, and the roles they play\n    when responding to a security event. You review this information\n    regularly and update it to reflect personnel changes from an\n    internal and external tools perspective. You consider all\n    third-party service providers and vendors while documenting this\n    information, including security partners, cloud providers, and\n    software-as-a-service (SaaS) applications. During a security event,\n    personnel are available with the appropriate level of\n    responsibility, context, and access to be able to respond and\n    recover. \n  \n    Common anti-patterns:\n  \n     \n     \n     \n     \n     \n  \n      \n        Not maintaining an updated list of key personnel with contact\n        information, their roles, and their responsibilities when\n        responding to security events.\n      \n    \n      \n        Assuming that everyone understands the people, dependencies,\n        infrastructure, and solutions when responding to and recovering\n        from an event. \n      \n    \n      \n        Not having a document or knowledge repository that represents\n        key infrastructure or application design.\n      \n    \n      \n        Not having proper onboarding processes for new employees to\n        effectively contribute to a security event response, such as\n        conducting event simulations.\n      \n    \n      \n        Not having an escalation path in place when key personnel are\n        temporarily unavailable or fail to respond during security\n        events.\n      \n    \n    Benefits of establishing this best\n      practice: This practice reduces the triage and response\n    time spent on identifying the right personnel and their roles during\n    an event. Minimize wasted time during an event by maintaining an\n    updated list of key personnel and their roles so you can bring the\n    right individuals to triage and recover from an event.\n  \n    Level of risk exposed if this best practice\n      is not established: High\n  \n    \n    Implementation guidance\n    \n    \n    \n      Identify key personnel in your organization: Maintain a contact\n      list of personnel within your organization that you need to\n      involve. Regularly review and update this information in the event\n      of personnel movement, like organizational changes, promotions,\n      and team changes. This is especially important for key roles like\n      incident managers, incident responders, and communications lead. \n    \n    \n       \n       \n       \n       \n       \n    \n        \n          Incident manager: Incident managers have overall authority\n          during the event response.\n        \n      \n        \n          Incident responders: Incident responders are responsible for\n          investigation and remediation activities. These people can\n          differ based on the type of event, but are typically\n          developers and operation teams responsible for the impacted\n          application.\n        \n      \n        \n          Communications lead: The communications lead is responsible\n          for internal and external communications, especially with\n          public agencies, regulators, and customers.\n        \n      \n        \n          Onboarding process: Regularly train and onboard new employees\n          to equip them with the necessary skills and knowledge to\n          contribute effectively to incident response efforts.\n          Incorporate simulations and hands-on exercises as part of the\n          onboarding process to facilitate their preparedness\n        \n      \n        \n          Subject matter experts (SMEs): In the case of distributed and\n          autonomous teams, we recommend you identify an SME for mission\n          critical workloads. They offer insights into the operation and\n          data classification of critical workloads involved in the\n          event.\n        \n      \n    \n      Example table format:\n    \n      | Role | Name | Contact Information | Responsibilities |\n1 | ——– | ——- | ——- | ——- |\n2 | Incident Manager | Jane Doe| jane.doe@example.com | Overall authority during response |\n3 | Incident Responder | John Smith | john.smith@example.com | Investigation and remediation |\n4 | Communications Lead | Emily Johnson | emily.johnson@example.com | Internal and external communications |\n5 | Communications Lead | Michael Brown | michael.brown@example.com | Insights on critical workloads |\n    \n      Consider using\n      the AWS Systems Manager Incident Manager feature to capture key\n      contacts, define a response plan, automate on-call schedules, and\n      create escalation plans. Automate and rotate all staff through an\n      on-call schedule, so that responsibility for the workload is\n      shared across its owners. This promotes good practices, such as\n      emitting relevant metrics and logs as well as defining alarm\n      thresholds that matter for the workload.\n    \n    \n      Identify external\n        partners: Enterprises use tools built by independent\n      software vendors (ISVs), partners, and subcontractors to build\n      differentiating solutions for their customers. Engage key\n      personnel from these parties who can help respond to and recover\n      from an incident. We recommend you sign up for the appropriate\n      level of Support in order to get prompt access to AWS subject\n      matter experts through a support case. Consider similar\n      arrangements with all critical solutions providers for the\n      workloads. Some security events require publicly listed businesses\n      to notify relevant public agencies and regulators of the event and\n      impacts. Maintain and update contact information for the relevant\n      departments and responsible individuals.\n    \n   \n    \n    Implementation steps\n    \n    \n    \n       \n       \n       \n    \n        \n          Set up an incident management solution.\n        \n        \n           \n        \n            \n              Consider deploying Incident Manager in your Security\n              Tooling account.\n            \n          \n      \n        \n          Define contacts in your incident management solution.\n        \n        \n           \n        \n            \n              Define at least two types of contact channels for each\n              contact (such as SMS, phone, or email), to ensure\n              reachability during an incident.\n            \n          \n      \n        \n          Define a response plan.\n        \n        \n           \n        \n            \n              Identify the most appropriate contacts to engage during an\n              incident. Define escalation plans aligned to the roles of\n              personnel to be engaged, rather than individual contacts.\n              Consider including contacts that may be responsible for\n              informing external entities, even if they are not directly\n              engaged to resolve the incident.  \n            \n          \n      \n   \n    \n    Resources\n    \n    \n    \n      Related best practices:\n    \n    \n       \n    \n        \n          OPS02-BP03\n            Operations activities have identified owners responsible for\n            their performance\n        \n      \n    \n      Related documents:\n    \n    \n       \n    \n        \n          AWS           Security Incident Response Guide\n        \n      \n    \n      Related examples:\n    \n    \n       \n       \n    \n        \n          AWS           customer playbook framework\n        \n      \n        \n          Prepare for\n            and respond to security incidents in your AWS\n            environment\n        \n      \n    \n      Related tools:\n    \n    \n       \n    \n        \n          AWS Systems Manager Incident Manager\n        \n      \n    \n      Related videos:\n    \n    \n       \n    \n        \n          Amazon's\n            approach to security during development\n        \n      \n  \u003cdiv\u003e\u003cdiv\u003e\u003cdiv\u003e\u003cdiv id=\"js_error_message\"\u003e\u003cp\u003e\u003cimg src=\"https://d1ge0kk1l5kms0.cloudfront.net/images/G/01/webservices/console/warning.png\" alt=\"Warning\" /\u003e \u003cstrong\u003eJavascript is disabled or is unavailable in your browser.\u003c/strong\u003e\u003c/p\u003e\u003cp\u003eTo use the Amazon Web Services Documentation, Javascript must be enabled. Please refer to your browser's Help pages for instructions.\u003c/p\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003eDocument Conventions SEC 10. How do you anticipate, respond to, and recover from incidents? SEC10-BP02 Develop incident management plans",
  "SEC10-BP02 Develop incident management plansThe first document to develop for incident response is the incident response plan. The incident response plan is designed to be the foundation for your incident response program and strategy. \n    Benefits of establishing this best practice: \n    Developing thorough and clearly defined incident response processes is key to a successful and scalable incident response program. When a security event occurs, clear steps and workflows can help you to respond in a timely manner. You might already have existing incident response processes. Regardless of your current state, it’s important to update, iterate, and test your incident response processes regularly.\n  \n    Level of risk exposed if this best practice\n    is not established: High\n  \n\n    Implementation guidance\n    \n      An incident management plan is critical to respond, mitigate, and recover from the potential impact of security incidents. An incident management plan is a structured process for identifying, remediating, and responding in a timely matter to security incidents.\n    \n    \n      The cloud has many of the same operational roles and requirements\n      found in an on-premises environment. When you create an incident\n      management plan, it is important to factor response and recovery\n      strategies that best align with your business outcome and\n      compliance requirements. For example, if you operate workloads in\n      AWS that are FedRAMP compliant in the United States, follow the\n      recommendations in\n      NIST\n        SP 800-61 Computer Security Handling Guide. Similarly, when\n      you operate workloads that store personally identifiable\n      information (PII), consider how to protect and respond to issues\n      related to data residency and use.\n    \n     When building an incident management plan for your workloads in AWS, start\n      with the AWS Shared Responsibility\n        Model for building a defense-in-depth approach towards incident response. In this\n      model, AWS manages security of the cloud, and you are responsible for security in the cloud.\n      This means that you retain control and are responsible for the security controls you choose to\n      implement. The AWS Security Incident Response Guide details key concepts and foundational\n      guidance for building a cloud-centric incident management plan.\n    \n      An effective incident management plan must be continually iterated upon, remaining current \n      with your cloud operations goal. Consider using the implementation plans detailed below as \n      you create and evolve your incident management plan.\n    \n  \n   \n\n    Implementation steps\n    \n       \n       \n       \n       \n       \n       \n       \n    \n        \n          Define roles and responsibilities within your organization\n          for handling security events. This should involve\n          representatives from various departments, including:\n        \n        \n           \n           \n           \n           \n        \n            \n              Human resources (HR)\n            \n          \n            \n              Executive team\n            \n          \n            \n              Legal department\n            \n          \n            \n              Application owners and developers (subject matter\n              experts, or SMEs)\n            \n          \n      \n        \n          Clearly outline who is responsible, accountable, consulted,\n          and informed (RACI) during an incident. Create a RACI chart\n          to facilitate quick and direct communication, and clearly\n          outline the leadership across different stages of an event.\n        \n      \n        \n          Involve application owners and developers (SMEs) during an\n          incident, as they can provide valuable information and\n          context to aid in measuring the impact. Build relationships\n          with these SMEs, and practice incident response scenarios\n          with them before an actual incident occurs.\n        \n      \n        \n          Involve trusted partners or external experts in the\n          investigation or response process, as they can provide\n          additional expertise and perspective.\n        \n      \n        \n          Align your incident management plans and roles with any\n          local regulations or compliance requirements that govern\n          your organization.\n        \n      \n        \n          Practice and test your incident response plans regularly,\n          and involve all the defined roles and responsibilities. This\n          helps streamline the process and verify you have a\n          coordinated and efficient response to security incidents.\n        \n      \n        \n          Review and update the roles, responsibilities, and RACI\n          chart periodically, or as your organizational structure or\n          requirements change.\n        \n      \n\n    \n      Understand AWS response teams and support\n    \n    \n       \n       \n       \n       \n    \n        \n          AWS Support\n        \n        \n           \n           \n        \n            \n              Support offers a range of plans that provide access to tools and expertise that support the success and operational health of your AWS solutions. If you need technical support and more resources to help plan, deploy, and optimize your AWS environment, you can select a support plan that best aligns with your AWS use case.\n            \n          \n            \n              Consider the Support Center in AWS Management Console (sign-in required) as the central point of contact to get support for issues that affect your AWS resources. Access to Support is controlled by AWS Identity and Access Management. For more information about getting access to Support features, see Getting started with Support.\n            \n          \n      \n        \n          AWS Customer Incident Response Team (CIRT)\n        \n        \n           \n           \n           \n        \n            \n              The AWS Customer Incident Response Team (CIRT) is a specialized 24/7 global AWS team that provides support to customers during active security events on the customer side of the AWS Shared Responsibility Model.\n            \n          \n            \n              When the AWS CIRT supports you, they provide assistance with triage and recovery for an active security event on AWS. They can assist in root cause analysis through the use of AWS service logs and provide you with recommendations for recovery. They can also provide security recommendations and best practices to help you avoid security events in the future.\n            \n          \n            \n              AWS customers can engage the AWS CIRT through an Support case.\n            \n          \n      \n        \n          DDoS response support\n        \n        \n           \n        \n            \n              AWS offers AWS Shield, which provides a managed distributed denial of service (DDoS) protection service that safeguards web applications running on AWS. Shield provides always-on detection and automatic inline mitigations that can minimize application downtime and latency, so there is no need to engage Support to benefit from DDoS protection. There are two tiers of Shield: AWS Shield Standard and AWS Shield Advanced. To learn about the differences between these two tiers, see Shield features documentation.\n            \n          \n      \n        \n          AWS Managed Services (AMS)\n        \n        \n           \n           \n        \n            \n              AWS Managed Services (AMS) provides ongoing management of your AWS infrastructure so you can focus on your applications. By implementing best practices to maintain your infrastructure, AMS helps reduce your operational overhead and risk. AMS automates common activities such as change requests, monitoring, patch management, security, and backup services, and provides full-lifecycle services to provision, run, and support your infrastructure.\n            \n          \n            \n              AMS takes responsibility for deploying a suite of security detective controls and provides a 24/7 first line of response to alerts. When an alert is initiated, AMS follows a standard set of automated and manual playbooks to verify a consistent response. These playbooks are shared with AMS customers during onboarding so that they can develop and coordinate a response with AMS.\n            \n          \n      \n    \n      Develop the incident response plan\n    \n    \n      The incident response plan is designed to be the foundation for your incident response program and strategy. The incident response plan should be in a formal document. An incident response plan typically includes these sections:\n    \n    \n       \n       \n       \n       \n       \n       \n    \n        \n          An incident response team overview: Outlines the goals and functions of the incident response team.\n        \n      \n        \n          Roles and responsibilities: Lists the incident response stakeholders and details their roles when an incident occurs.\n        \n      \n        \n          A communication plan: Details contact information and how you communicate during an incident.\n        \n      \n        \n          Backup communication methods: It’s a best practice to have out-of-band communication as a backup for incident communication. An example of an application that provides a secure out-of-band communications channel is AWS Wickr.\n        \n      \n        \n          Phases of incident response and actions to take: Enumerates the phases of incident response (for example, detect, analyze, eradicate, contain, and recover), including high-level actions to take within those phases.\n        \n      \n        \n          Incident severity and prioritization definitions: Details how to classify the severity of an incident, how to prioritize the incident, and then how the severity definitions affect escalation procedures.\n        \n      \n    \n      While these sections are common throughout companies of different sizes and industries, each organization’s incident response plan is unique. You need to build an incident response plan that works best for your organization.\n    \n   \n   \n\n    Resources\n    \n      Related best practices:\n    \n    \n       \n    \n        \n          SEC04 Detection\n        \n      \n\n    \n      Related documents:\n    \n    \n       \n       \n    \n        \n          AWS Security Incident Response Guide\n        \n      \n         NIST:\n            Computer Security Incident Handling Guide \n      \n  \u003cdiv\u003e\u003cdiv\u003e\u003cdiv\u003e\u003cdiv id=\"js_error_message\"\u003e\u003cp\u003e\u003cimg src=\"https://d1ge0kk1l5kms0.cloudfront.net/images/G/01/webservices/console/warning.png\" alt=\"Warning\" /\u003e \u003cstrong\u003eJavascript is disabled or is unavailable in your browser.\u003c/strong\u003e\u003c/p\u003e\u003cp\u003eTo use the Amazon Web Services Documentation, Javascript must be enabled. Please refer to your browser's Help pages for instructions.\u003c/p\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003eDocument ConventionsSEC10-BP01 Identify key personnel and external\n  resourcesSEC10-BP03 Prepare forensic capabilities",
  "SEC10-BP03 Prepare forensic capabilitiesAhead of a security incident, consider developing forensics capabilities to support security event investigations. \n    Level of risk exposed if this best practice\n    is not established: Medium\n  \n    \n      Concepts from traditional on-premises forensics apply to AWS. For key information to start building forensics capabilities in the AWS Cloud, see Forensic investigation environment strategies in the AWS Cloud.\n    \n    \n      Once you have your environment and AWS account structure set up for forensics, define the technologies required to effectively perform forensically sound methodologies across the four phases:\n    \n    \n       \n       \n       \n       \n    \n        \n          Collection: Collect relevant AWS logs, such as AWS CloudTrail, AWS Config, VPC Flow Logs, and host-level logs. Collect snapshots, backups, and memory dumps of impacted AWS resources where available.\n        \n      \n        \n          Examination: Examine the data collected by extracting and assessing the relevant information.\n        \n      \n        \n          Analysis: Analyze the data collected in order to understand the incident and draw conclusions from it.\n        \n      \n        \n          Reporting: Present the information resulting from the analysis phase.\n        \n      \n     \n      \n      Implementation steps\n      \n        Prepare your forensics environment\n      \n      \n        AWS Organizations helps you centrally manage and govern an AWS environment as you grow and scale AWS resources. An AWS organization consolidates your AWS accounts so that you can administer them as a single unit. You can use organizational units (OUs) to group accounts together to administer as a single unit.\n      \n      \n        For incident response, it’s helpful to have an AWS account structure that supports the functions of incident response, which includes a security OU and a forensics OU. Within the security OU, you should have accounts for:\n      \n      \n         \n         \n      \n          \n            Log archival: Aggregate logs in a log archival AWS account with limited permissions.\n          \n        \n          \n            Security tools: Centralize security services in a security tool AWS account. This account operates as the delegated administrator for security services.\n          \n        \n      \n        Within the forensics OU, you have the option to implement a single forensics account or accounts for each Region that you operate in, depending on which works best for your business and operational model. If you create a forensics account per Region, you can block the creation of AWS resources outside of that Region and reduce the risk of resources being copied to an unintended region. For example, if you only operate in US East (N. Virginia) Region (us-east-1) and US West (Oregon) (us-west-2), then you would have two accounts in the forensics OU: one for us-east-1 and one for us-west-2. \n      \n      \n        You can create a forensics AWS account for multiple Regions. You should exercise caution in copying AWS resources to that account to verify you’re aligning with your data sovereignty requirements. Because it takes time to provision new accounts, it is imperative to create and instrument the forensics accounts well ahead of an incident so that responders can be prepared to effectively use them for response.\n      \n      \n        The following diagram displays a sample account structure including a forensics OU with per-Region forensics accounts:\n      \n      \n         \n          \n         \n         \n        Per-Region account structure for incident response\n      \n      \n        Capture backups and snapshots\n      \n      \n        Setting up backups of key systems and databases are critical for recovering from a security incident and for forensics purposes. With backups in place, you can restore your systems to their previous safe state. On AWS, you can take snapshots of various resources. Snapshots provide you with point-in-time backups of those resources. There are many AWS services that can support you in backup and recovery. For detail on these services and approaches for backup and recovery, see Backup and Recovery Prescriptive Guidance and Use backups to recover from security incidents.\n      \n      \n        Especially when it comes to situations such as ransomware, it’s critical for your backups to be well protected. For guidance on securing your backups, see Top 10 security best practices for securing backups in AWS. In addition to securing your backups, you should regularly test your backup and restore processes to verify that the technology and processes you have in place work as expected.\n      \n      \n        Automate forensics\n      \n      \n        During a security event, your incident response team must be able to collect and analyze evidence quickly while maintaining accuracy for the time period surrounding the event (such as capturing logs related to a specific event or resource or collecting memory dump of an Amazon EC2 instance). It’s both challenging and time consuming for the incident response team to manually collect the relevant evidence, especially across a large number of instances and accounts. Additionally, manual collection can be prone to human error. For these reasons, you should develop and implement automation for forensics as much as possible.\n      \n      \n        AWS offers a number of automation resources for forensics, which are listed in the following Resources section. These resources are examples of forensics patterns that we have developed and customers have implemented. While they might be a useful reference architecture to start with, consider modifying them or creating new forensics automation patterns based on your environment, requirements, tools, and forensics processes. \n      \n     \n   \n\n  Resources\n\n      \n    \n      Related documents:\n    \n    \n       \n       \n       \n       \n       \n    \n        AWS Security Incident Response Guide - Develop Forensics Capabilities \n      \n        AWS Security Incident Response Guide - Forensics Resources \n      \n        Forensic investigation environment strategies in the AWS Cloud\n      \n        \n          How to automate forensic disk collection in AWS\n        \n      \n        AWS Prescriptive Guidance - Automate incident response and forensics \n      \n    \n      Related videos:\n    \n    \n       \n    \n        \n          Automating Incident Response and Forensics \n        \n      \n    \n      Related examples:\n    \n    \n       \n       \n    \n        \n          Automated Incident Response and Forensics Framework \n        \n      \n        \n          Automated Forensics Orchestrator for Amazon EC2\n        \n      \n  \u003cdiv\u003e\u003cdiv\u003e\u003cdiv\u003e\u003cdiv id=\"js_error_message\"\u003e\u003cp\u003e\u003cimg src=\"https://d1ge0kk1l5kms0.cloudfront.net/images/G/01/webservices/console/warning.png\" alt=\"Warning\" /\u003e \u003cstrong\u003eJavascript is disabled or is unavailable in your browser.\u003c/strong\u003e\u003c/p\u003e\u003cp\u003eTo use the Amazon Web Services Documentation, Javascript must be enabled. Please refer to your browser's Help pages for instructions.\u003c/p\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003eDocument ConventionsSEC10-BP02 Develop incident management plansSEC10-BP04 Develop and test security incident response\n  playbooks",
  "SEC10-BP04 Develop and test security incident response\n  playbooks\n    A key part of preparing your incident response processes is\n    developing playbooks. Incident response playbooks provide\n    prescriptive guidance and steps to follow when a security event\n    occurs. Having clear structure and steps simplifies the response and\n    reduces the likelihood for human error.\n  \n    Level of risk exposed if this best practice is not established: Medium\n  \n\n  Implementation guidance \n    \n      Playbooks should be created for incident scenarios such as:\n    \n    \n       \n       \n    \n        \n          Expected incidents:\n          Playbooks should be created for incidents you anticipate. This\n          includes threats like denial of service (DoS), ransomware, and\n          credential compromise.\n        \n      \n        \n          Known security findings or\n            alerts: Playbooks should be created to address your\n          known security findings and alerts, such as those from Amazon GuardDuty. When you receive a GuardDuty finding, the playbook\n          should provide clear steps to prevent mishandling or ignoring\n          the alert. For more remediation details and guidance, see\n          Remediating\n            security issues discovered by GuardDuty.\n        \n      \n    \n      Playbooks should contain technical steps for a security analyst to\n      complete in order to adequately investigate and respond to a\n      potential security incident.\n    \n     \n\n  Implementation steps \n      \n        Items to include in a playbook include:\n      \n      \n         \n         \n         \n         \n         \n      \n          \n            Playbook overview: What\n            risk or incident scenario does this playbook address? What\n            is the goal of the playbook?\n          \n        \n          \n            Prerequisites: What logs,\n            detection mechanisms, and automated tools are required for\n            this incident scenario? What is the expected notification?\n          \n        \n          \n            Communication and escalation\n            information: Who is involved and what is their\n            contact information? What are each of the stakeholders’\n            responsibilities?\n          \n        \n          \n            Response steps: Across\n            phases of incident response, what tactical steps should be\n            taken? What queries should an analyst run? What code should\n            be run to achieve the desired outcome?\n          \n          \n             \n             \n             \n             \n             \n          \n              \n                Detect: How will the\n                incident be detected?\n              \n            \n              \n                Analyze: How will the\n                scope of impact be determined?\n              \n            \n              \n                Contain: How will the\n                incident be isolated to limit scope?\n              \n            \n              \n                Eradicate: How will\n                the threat be removed from the environment?\n              \n            \n              \n                Recover: How will the\n                affected system or resource be brought back into\n                production?\n              \n            \n        \n          \n            Expected outcomes: After\n            queries and code are run, what is the expected result of the\n            playbook?\n          \n        \n     \n   \n\n  Resources\n\n      \n    \n      Related Well-Architected best\n      practices:\n    \n    \n       \n    \n        \n          SEC10-BP02 - Develop incident management plans\n        \n      \n    \n      Related documents:\n    \n    \n       \n       \n       \n       \n    \n        \n          Framework\n          for Incident Response Playbooks \n        \n      \n        \n          Develop\n          your own Incident Response Playbooks \n        \n      \n        \n          Incident\n          Response Playbook Samples \n        \n      \n        \n          Building\n          an AWS incident response runbook using Jupyter playbooks and\n          CloudTrail Lake \n        \n      \n    \n    \n  \u003cdiv\u003e\u003cdiv\u003e\u003cdiv\u003e\u003cdiv id=\"js_error_message\"\u003e\u003cp\u003e\u003cimg src=\"https://d1ge0kk1l5kms0.cloudfront.net/images/G/01/webservices/console/warning.png\" alt=\"Warning\" /\u003e \u003cstrong\u003eJavascript is disabled or is unavailable in your browser.\u003c/strong\u003e\u003c/p\u003e\u003cp\u003eTo use the Amazon Web Services Documentation, Javascript must be enabled. Please refer to your browser's Help pages for instructions.\u003c/p\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003eDocument ConventionsSEC10-BP03 Prepare forensic capabilitiesSEC10-BP05 Pre-provision access",
  "SEC10-BP05 Pre-provision accessVerify that incident responders have the correct access pre-provisioned in AWS to reduce \n    the time needed for investigation through to recovery.\n    Common anti-patterns:\n  \n     \n     \n     \n  \n      \n        Using the root account for incident response.  \n      \n    \n      \n        Altering existing accounts. \n      \n    \n      \n        Manipulating IAM permissions directly when providing just-in-time privilege elevation. \n      \n    \n    Level of risk exposed if this best practice\n    is not established: Medium\n  \n\n  Implementation guidance\n\n      \n    AWS recommends reducing or eliminating reliance on long-lived credentials wherever \n      possible, in favor of temporary credentials and just-in-time privilege escalation \n      mechanisms. Long-lived credentials are prone to security risk and increase operational \n      overhead. For most management tasks, as well as incident response tasks, we recommend you implement identity federation\n      alongside temporary \n        escalation for administrative access. In this model, a user requests elevation to a \n      higher level of privilege (such as an incident response role) and, provided the user \n      is eligible for elevation, a request is sent to an approver. If the request is approved, \n      the user receives a set of temporary AWS credentials which can be used to complete their \n      tasks. After these credentials expire, the user must submit a new elevation request.\n    \n      We recommend the use of temporary privilege escalation in the majority of incident response scenarios. The correct way to do this is to use the \n      AWS Security Token Service and session policies \n      to scope access. \n    \n    \n      There are scenarios where federated identities are unavailable, such as:\n    \n    \n       \n       \n       \n    \n        \n          Outage related to a compromised identity provider (IdP). \n        \n      \n        \n          Misconfiguration or human error causing broken federated access management system. \n        \n      \n        \n          Malicious activity such as a distributed denial of service (DDoS) event or rendering unavailability of the system. \n        \n      \n    \n      In the preceding cases, there should be emergency break glass access configured to allow \n      investigation and timely remediation of incidents. We recommend that you use a user,   \n      group, or role with appropriate permissions to perform tasks and access AWS resources.  Use the root user only for \n      tasks that require root user credentials.\n      To verify that incident responders have the correct level of access to AWS and other relevant systems, we recommend the pre-provisioning \n      of dedicated accounts. The accounts require privileged access, and must be \n      tightly controlled and monitored. The accounts must be built with the fewest privileges \n      required to perform the necessary tasks, and the level of access should be based on the \n      playbooks created as part of the incident management plan.\n    \n    \n      Use purpose-built and dedicated users and roles as a best practice. Temporarily escalating \n      user or role access through the addition of IAM policies both makes it unclear what access \n      users had during the incident, and risks the escalated privileges not being revoked. \n    \n    \n      It is important to remove as many dependencies as possible to verify that access can be \n      gained under the widest possible number of failure scenarios. To support this, create a \n      playbook to verify that incident response users are created as \n      users in a dedicated security account, and not managed through any existing Federation or \n      single sign-on (SSO) solution. Each individual responder must have their own named account. \n      The account configuration must enforce strong password policy and multi-factor authentication (MFA). \n      If the incident response playbooks only require access to the AWS Management Console, the user should not \n      have access keys configured and should be explicitly disallowed from creating access keys. \n      This can be configured with IAM policies or service control policies (SCPs) as mentioned in \n      the AWS Security Best Practices for AWS Organizations SCPs. The users should have no privileges \n      other than the ability to assume incident response roles in other accounts. \n    \n    \n      During an incident it might be necessary to grant access to other internal or external individuals \n      to support investigation, remediation, or recovery activities. In this case, use the playbook \n      mechanism mentioned previously, and there must be a process to verify that any additional access is \n      revoked immediately after the incident is complete. \n    \n    \n      To verify that the use of incident response roles can be properly monitored and audited, \n      it is essential that the IAM accounts created for this purpose are not shared between \n      individuals, and that the AWS account root user is not used unless required for a specific \n      task. If the root user is required (for example, IAM access to a specific account is unavailable), \n      use a separate process with a playbook available to verify availability of the root user sign-in credentials and \n      MFA token.\n    \n    \n      To configure the IAM policies for the incident response roles, consider using IAM Access Analyzer \n      to generate policies based on AWS CloudTrail logs. To do this, grant administrator access to the \n      incident response role on a non-production account and run through your playbooks. Once \n      complete, a policy can be created that allows only the actions taken. This policy can then be \n      applied to all the incident response roles across all accounts. You might wish to create a separate \n      IAM policy for each playbook to allow easier management and auditing. Example playbooks could \n      include response plans for ransomware, data breaches, loss of production access, and other \n      scenarios.\n    \n    \n      Use the incident response accounts to assume dedicated incident response IAM roles in \n      other AWS accounts. These roles must be configured to only be assumable by users in the \n      security account, and the trust relationship must require that the calling principal has \n      authenticated using MFA. The roles must use tightly-scoped IAM policies to control access. \n      Ensure that all AssumeRole requests for these roles are logged in CloudTrail and alerted \n      on, and that any actions taken using these roles are logged. \n    \n     It is strongly recommended that both the IAM accounts and the IAM roles are\n      clearly named to allow them to be easily found in CloudTrail logs. An example of this would be to\n      name the IAM accounts \u003cUSER_ID\u003e-BREAK-GLASS and\n      the IAM roles BREAK-GLASS-ROLE. \n    \n      CloudTrail is used to log API activity in your AWS accounts and should be used to configure \n      alerts on usage of the incident response roles. Refer to the blog post on configuring alerts \n      when root keys are used. The instructions can be modified to configure the Amazon CloudWatch metric \n      filter-to-filter on AssumeRole events related to the incident response IAM role:\n    \n    { $.eventName = \"AssumeRole\" \u0026\u0026 $.requestParameters.roleArn = \"\u003cINCIDENT_RESPONSE_ROLE_ARN\u003e\" \u0026\u0026 $.userIdentity.invokedBy NOT EXISTS \u0026\u0026 $.eventType != \"AwsServiceEvent\" }\n    \n      As the incident response roles are likely to have a high level of access, it is important \n      that these alerts go to a wide group and are acted upon promptly. \n    \n    \n      During an incident, it is possible that a responder might require access to systems which are \n      not directly secured by IAM. These could include Amazon Elastic Compute Cloud instances, Amazon Relational Database Service databases, or \n      software-as-a-service (SaaS) platforms. It is strongly recommended that rather than using \n      native protocols such as SSH or RDP, AWS Systems Manager Session Manager is used for all \n      administrative access to Amazon EC2 instances. This access can be controlled using IAM, which is \n      secure and audited. It might also be possible to automate parts of your playbooks using \n      AWS Systems Manager Run Command documents, which can reduce user error and improve \n      time to recovery. For access to databases and third-party tools, we recommend storing \n      access credentials in AWS Secrets Manager and granting access to the incident responder \n      roles. \n    \n    \n      Finally, the management of the incident response IAM accounts should be added to your \n      Joiners, Movers, and Leavers processes and reviewed and tested periodically to verify that \n      only the intended access is allowed.\n    \n   \n\n  Resources\n\n      \n    \n      Related documents:\n    \n    \n       \n       \n       \n       \n       \n       \n       \n       \n       \n       \n       \n       \n    \n        \n          Managing temporary elevated access to your AWS environment\n        \n      \n        \n          AWS Security Incident Response Guide \n      \n        \n          AWS Elastic Disaster Recovery\n        \n      \n        \n          AWS Systems Manager Incident Manager        \n        \n      \n        \n          Setting an account password policy for IAM users\n        \n      \n        \n          Using multi-factor authentication (MFA) in AWS\n        \n      \n        \n          Configuring Cross-Account Access with MFA\n        \n      \n        \n          Using IAM Access Analyzer to generate IAM policies\n        \n      \n        \n          Best Practices for AWS Organizations Service Control Policies in a Multi-Account Environment\n        \n      \n        \n          How to Receive Notifications When Your AWS Account’s Root Access Keys Are Used\n        \n      \n        \n          Create fine-grained session permissions using IAM managed policies\n        \n      \n        \n          Break\n            glass access\n        \n      \n    \n      Related videos:\n    \n    \n       \n       \n       \n    \n        \n          Automating Incident Response and Forensics in AWS\n      \n        \n          DIY guide to runbooks, incident reports, and\n            incident response\n        \n      \n        \n          Prepare for and respond to security incidents in your AWS environment\n        \n      \n  \u003cdiv\u003e\u003cdiv\u003e\u003cdiv\u003e\u003cdiv id=\"js_error_message\"\u003e\u003cp\u003e\u003cimg src=\"https://d1ge0kk1l5kms0.cloudfront.net/images/G/01/webservices/console/warning.png\" alt=\"Warning\" /\u003e \u003cstrong\u003eJavascript is disabled or is unavailable in your browser.\u003c/strong\u003e\u003c/p\u003e\u003cp\u003eTo use the Amazon Web Services Documentation, Javascript must be enabled. Please refer to your browser's Help pages for instructions.\u003c/p\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003eDocument ConventionsSEC10-BP04 Develop and test security incident response\n  playbooksSEC10-BP06 Pre-deploy tools",
  "SEC10-BP06 Pre-deploy toolsVerify that security personnel have the right tools pre-deployed to reduce the time for investigation through to recovery.\n    Level of risk exposed if this best practice\n    is not established: Medium\n  \n\n  Implementation guidance\n    \n      To automate security response and operations functions, you can use a comprehensive set of APIs and tools from AWS. You can fully automate identity management, network security, data protection, and monitoring capabilities and deliver them using popular software development methods that you already have in place. When you build security automation, your system can monitor, review, and initiate a response, rather than having people monitor your security position and manually react to events. \n    \n    \n      If your incident response teams continue to respond to alerts in the same way, they risk alert fatigue. Over time, the team can become desensitized to alerts and can either make mistakes handling ordinary situations or miss unusual alerts. Automation helps avoid alert fatigue by using functions that process the repetitive and ordinary alerts, leaving humans to handle the sensitive and unique incidents. Integrating anomaly detection systems, such as Amazon GuardDuty, AWS CloudTrail Insights, and Amazon CloudWatch Anomaly Detection, can reduce the burden of common threshold-based alerts.\n    \n    \n      You can improve manual processes by programmatically automating steps in the process. After you define the remediation pattern to an event, you can decompose that pattern into actionable logic, and write the code to perform that logic. Responders can then run that code to remediate the issue. Over time, you can automate more and more steps, and ultimately automatically handle whole classes of common incidents.\n    \n    \n      During a security investigation, you need to be able to review relevant logs to record and understand the full scope and timeline of the incident. Logs are also required for alert generation, indicating certain actions of interest have happened. It is critical to select, enable, store, and set up querying and retrieval mechanisms, and set up alerting. Additionally, an effective way to provide tools to search log data is Amazon Detective.\n    \n    \n      AWS oﬀers over 200 cloud services and thousands of features. We recommend that you review the services that can support and simplify your incident response strategy. \n    \n    \n      In addition to logging, you should develop and implement a tagging strategy. Tagging can help provide context around the purpose of an AWS resource. Tagging can also be used for automation.\n    \n     \n      \n      Implementation steps\n      \n        Select and set up logs for analysis and alerting\n      \n      \n        See the following documentation on configuring logging for incident response:\n      \n      \n         \n         \n      \n          \n            Logging strategies for security incident response\n          \n        \n          \n            SEC04-BP01 Configure service and application logging\n          \n        \n      \n        Enable security services to support detection and response\n      \n      \n        AWS provides native detective, preventative, and responsive capabilities, and other services can be used to architect custom security solutions. For a list of the most relevant services for security incident response, see Cloud capability definitions.\n      \n      \n        Develop and implement a tagging strategy\n      \n      \n        Obtaining contextual information on the business use case and relevant internal stakeholders surrounding an AWS resource can be difficult. One way to do this is in the form of tags, which assign metadata to your AWS resources and consist of a user-defined key and value. You can create tags to categorize resources by purpose, owner, environment, type of data processed, and other criteria of your choice.\n      \n      \n        Having a consistent tagging strategy can speed up response times and minimize time spent on organizational context by allowing you to quickly identify and discern contextual information about an AWS resource. Tags can also serve as a mechanism to initiate response automations. For more detail on what to tag, see Tagging your AWS resources. You’ll want to first define the tags you want to implement across your organization. After that, you’ll implement and enforce this tagging strategy. For more detail on implementation and enforcement, see Implement AWS resource tagging strategy using AWS Tag Policies and Service Control Policies (SCPs).\n      \n     \n   \n\n  Resources\n\n    \n      Related Well-Architected best practices:\n    \n    \n       \n         \n    \n        \n          SEC04-BP01 Configure service and application logging\n        \n      \n          \n            SEC04-BP02 Capture logs, findings, and metrics\n  in standardized locations\n          \n        \n    \n      Related documents:\n    \n    \n       \n       \n    \n        \n          Logging strategies for security incident response\n        \n      \n        \n          Incident response cloud capability definitions\n        \n      \n    \n      Related examples:\n    \n    \n       \n       \n       \n    \n        \n          Threat Detection and Response with Amazon GuardDuty and Amazon Detective\n        \n      \n        \n          Security Hub Workshop\n        \n      \n        \n          Vulnerability Management with Amazon Inspector\n        \n      \n  \u003cdiv\u003e\u003cdiv\u003e\u003cdiv\u003e\u003cdiv id=\"js_error_message\"\u003e\u003cp\u003e\u003cimg src=\"https://d1ge0kk1l5kms0.cloudfront.net/images/G/01/webservices/console/warning.png\" alt=\"Warning\" /\u003e \u003cstrong\u003eJavascript is disabled or is unavailable in your browser.\u003c/strong\u003e\u003c/p\u003e\u003cp\u003eTo use the Amazon Web Services Documentation, Javascript must be enabled. Please refer to your browser's Help pages for instructions.\u003c/p\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003eDocument ConventionsSEC10-BP05 Pre-provision accessSEC10-BP07 Run simulations",
  "SEC10-BP07 Run simulations\n    As organizations grow and evolve over time, so does the threat landscape, making it important to continually review your incident response capabilities. Running simulations (also known as game days) is one method that can be used to perform this assessment. Simulations use real-world security event scenarios designed to mimic a threat actor’s tactics, techniques, and procedures (TTPs) and allow an organization to exercise and evaluate their incident response capabilities by responding to these mock cyber events as they might occur in reality.\n    Benefits of establishing this best practice: Simulations have a variety of benefits:\n  \n     \n     \n     \n     \n  \n      \n        Validating cyber readiness and developing the confidence of your incident responders.\n      \n    \n      \n        Testing the accuracy and efficiency of tools and workflows.\n      \n    \n      \n        Refining communication and escalation methods aligned with your incident response plan.\n      \n    \n      \n        Providing an opportunity to respond to less common vectors.\n      \n    Level of risk exposed if this best practice\n    is not established: Medium\n\n  Implementation guidance\n    \n      There are three main types of simulations:\n    \n    \n       \n       \n       \n    \n        \n          Tabletop exercises: The tabletop approach to simulations is a discussion-based session involving the various incident response stakeholders to practice roles and responsibilities and use established communication tools and playbooks. Exercise facilitation can typically be accomplished in a full day in a virtual venue, physical venue, or a combination. Because it is discussion-based, the tabletop exercise focuses on processes, people, and collaboration. Technology is an integral part of the discussion, but the actual use of incident response tools or scripts is generally not a part of the tabletop exercise.\n        \n      \n        \n          Purple team exercises: Purple team exercises increase the level of collaboration between the incident responders (blue team) and simulated threat actors (red team). The blue team is comprised of members of the security operations center (SOC), but can also include other stakeholders that would be involved during an actual cyber event. The red team is comprised of a penetration testing team or key stakeholders that are trained in offensive security. The red team works collaboratively with the exercise facilitators when designing a scenario so that the scenario is accurate and feasible. During purple team exercises, the primary focus is on the detection mechanisms, the tools, and the standard operating procedures (SOPs) supporting the incident response efforts.\n        \n      \n        \n          Red team exercises: During a red team exercise, the offense (red team) conducts a simulation to achieve a certain objective or set of objectives from a predetermined scope. The defenders (blue team) will not necessarily have knowledge of the scope and duration of the exercise, which provides a more realistic assessment of how they would respond to an actual incident. Because red team exercises can be invasive tests, be cautious and implement controls to verify that the exercise does not cause actual harm to your environment.\n        \n      \n    \n      Consider facilitating cyber simulations at a regular interval. Each exercise type can provide unique benefits to the participants and the organization as a whole, so you might choose to start with less complex simulation types (such as tabletop exercises) and progress to more complex simulation types (red team exercises). You should select a simulation type based on your security maturity, resources, and your desired outcomes. Some customers might not choose to perform red team exercises due to complexity and cost.\n    \n   \n    \n    Implementation steps\n    \n      Regardless of the type of simulation you choose, simulations generally follow these implementation steps:\n    \n    \n       \n       \n       \n       \n       \n    \n        \n          Define core exercise elements: Define the simulation scenario and the objectives of the simulation. Both of these should have leadership acceptance.\n        \n      \n        \n          Identify key stakeholders: At a minimum, an exercise needs exercise facilitators and participants. Depending on the scenario, additional stakeholders such as legal, communications, or executive leadership might be involved.\n        \n      \n        \n          Build and test the scenario: The scenario might need to be redefined as it is being built if specific elements aren’t feasible. A finalized scenario is expected as the output of this stage.\n        \n      \n        \n          Facilitate the simulation: The type of simulation determines the facilitation used (a paper-based scenario compared to a highly technical, simulated scenario). The facilitators should align their facilitation tactics to the exercise objects and they should engage all exercise participants wherever possible to provide the most benefit.\n        \n      \n        \n          Develop the after-action report (AAR): Identify areas that went well, those that can use improvement, and potential gaps. The AAR should measure the effectiveness of the simulation as well as the team’s response to the simulated event so that progress can be tracked over time with future simulations.\n        \n      \n   \n\n  Resources\n\n      \n    \n      Related documents:\n    \n    \n       \n    \n        AWS Incident Response Guide\n        \n      \n    \n      Related videos:\n    \n    \n       \n       \n    \n        AWS GameDay - Security Edition\n      \n        \n          Running\n            effective security incident response simulations\n        \n      \n  \u003cdiv\u003e\u003cdiv\u003e\u003cdiv\u003e\u003cdiv id=\"js_error_message\"\u003e\u003cp\u003e\u003cimg src=\"https://d1ge0kk1l5kms0.cloudfront.net/images/G/01/webservices/console/warning.png\" alt=\"Warning\" /\u003e \u003cstrong\u003eJavascript is disabled or is unavailable in your browser.\u003c/strong\u003e\u003c/p\u003e\u003cp\u003eTo use the Amazon Web Services Documentation, Javascript must be enabled. Please refer to your browser's Help pages for instructions.\u003c/p\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003eDocument ConventionsSEC10-BP06 Pre-deploy toolsSEC10-BP08 Establish a framework for learning from\n  incidents",
  "SEC10-BP08 Establish a framework for learning from\n  incidents\n    Implementing a lessons learned framework and\n    root cause analysis capability can not only help improve incident\n    response capabilities, but also help prevent the incident from\n    recurring. By learning from each incident, you can help avoid\n    repeating the same mistakes, exposures, or misconfigurations, not\n    only improving your security posture, but also minimizing time lost\n    to preventable situations.\n  \n      Level of risk exposed if this best practice is not established:\n      Medium\n    \n\n  Implementation guidance \n    \n      It's important to implement a lessons learned\n      framework that establishes and achieves, at a\n      high level, the following points:\n    \n    \n       \n       \n       \n       \n       \n       \n    \n        \n          When is a lessons learned held?\n        \n      \n        \n          What is involved in the lessons learned process?\n        \n      \n        \n          How is a lessons learned performed?\n        \n      \n        \n          Who is involved in the process and how?\n        \n      \n        \n          How will areas of improvement be identified?\n        \n      \n        \n          How will you ensure improvements are effectively tracked and\n          implemented?\n        \n      \n    \n      The framework should not focus on or blame individuals, but\n      instead should focus on improving tools and processes.\n    \n     \n\n  Implementation steps\n\n      \n      \n        Aside from the preceding high-level outcomes listed, it’s important\n        to make sure that you ask the right questions to derive the most\n        value (information that leads to actionable improvements) from\n        the process. Consider these questions to help get you started in\n        fostering your lessons learned discussions:\n      \n      \n         \n         \n         \n         \n         \n         \n         \n         \n         \n         \n      \n          \n            What was the incident?\n          \n        \n          \n            When was the incident first identified?\n          \n        \n          \n            How was it identified?\n          \n        \n          \n            What systems alerted on the activity?\n          \n        \n          \n            What systems, services, and data were involved?\n          \n        \n          \n            What specifically occurred?\n          \n        \n          \n            What worked well?\n          \n        \n          \n            What didn't work well?\n          \n        \n          \n            Which process or procedures failed or failed to scale to\n            respond to the incident?\n          \n        \n          \n            What can be improved within the following areas:\n          \n          \n             \n             \n             \n          \n              \n                People\n              \n              \n                 \n                 \n                 \n              \n                  \n                    Were the people who were needed to be contacted\n                    actually available and was the contact list up to\n                    date?\n                  \n                \n                  \n                    Were people missing training or capabilities needed\n                    to effectively respond and investigate the incident?\n                  \n                \n                  \n                    Were the appropriate resources ready and available?\n                  \n                \n            \n              \n                Process\n              \n              \n                 \n                 \n                 \n                 \n              \n                  \n                    Were processes and procedures followed?\n                  \n                \n                  \n                    Were processes and procedures documented and\n                    available for this (type of) incident?\n                  \n                \n                  \n                    Were required processes and procedures missing?\n                  \n                \n                  \n                    Were the responders able to gain timely access to\n                    the required information to respond to the issue?\n                  \n                \n            \n              \n                Technology\n              \n              \n                 \n                 \n                 \n                 \n                 \n                 \n                 \n                 \n              \n                  \n                    Did existing alerting systems effectively identify\n                    and alert on the activity?\n                  \n                \n                  \n                    How could we have reduced time-to-detection by 50%?\n                  \n                \n                  \n                    Do existing alerts need improvement or new alerts\n                    need to be built for this (type of) incident?\n                  \n                \n                  \n                    Did existing tools allow for effective\n                    investigation (search/analysis) of the incident?\n                  \n                \n                  \n                    What can be done to help identify this (type of)\n                    incident sooner?\n                  \n                \n                  \n                    What can be done to help prevent this (type of)\n                    incident from occurring again?\n                  \n                \n                  \n                    Who owns the improvement plan and how will you test\n                    that it has been implemented?\n                  \n                \n                  \n                    What is the timeline for the additional\n                    monitoring or preventative controls and processes to be\n                    implemented and tested?\n                  \n                \n            \n        \n      \n        This list isn’t all-inclusive, but is intended to serve as a\n        starting point for identifying what the organization and\n        business needs are and how you can analyze them in order to most\n        effectively learn from incidents and continuously improve your\n        security posture. Most important is getting started by\n        incorporating lessons learned as a standard part of your\n        incident response process, documentation, and expectations\n        across the stakeholders.\n      \n     \n   \n\n  Resources \n      \n        Related documents:\n      \n    \n    \n       \n       \n    \n          \n            AWS             Security Incident Response Guide - Establish a framework for\n            learning from incidents\n          \n      \n          \n            NCSC\n            CAF guidance - Lessons learned\n          \n      \n  \u003cdiv\u003e\u003cdiv\u003e\u003cdiv\u003e\u003cdiv id=\"js_error_message\"\u003e\u003cp\u003e\u003cimg src=\"https://d1ge0kk1l5kms0.cloudfront.net/images/G/01/webservices/console/warning.png\" alt=\"Warning\" /\u003e \u003cstrong\u003eJavascript is disabled or is unavailable in your browser.\u003c/strong\u003e\u003c/p\u003e\u003cp\u003eTo use the Amazon Web Services Documentation, Javascript must be enabled. Please refer to your browser's Help pages for instructions.\u003c/p\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003eDocument ConventionsSEC10-BP07 Run simulationsApplication security",
  "SEC11-BP01 Train for application security\n    Provide training to your team on secure development and operation\n    practices, which helps them build secure and high-quality software.\n    This practice helps your team to prevent, detect, and remediate\n    security issues earlier in the development lifecycle. Consider\n    training that covers threat modeling, secure coding practices, and\n    using services for secure configurations and operations. Provide\n    your team access to training through self-service resources, and\n    regularly gather their feedback for continuous improvement.\n  \n    Desired outcome: You equip your\n    team with the knowledge and skills necessary to design and build\n    software with security in mind from the outset. Through training on\n    threat modeling and secure development practices, your team has a\n    deep understanding of potential security risks and how to mitigate\n    them during the software development lifecycle (SDLC). This\n    proactive approach to security is part of your team's culture, and\n    you become able to identify and remediate potential security issues\n    early on. As a result, your team delivers high-quality, secure\n    software and features more efficiently, which accelerates the\n    overall delivery timeline. You have a collaborative and inclusive\n    security culture within your organization, where the ownership of\n    security is shared across all builders.\n  \n    Common anti-patterns:\n  \n     \n     \n     \n     \n  \n      \n        You wait until a security review, and then consider the security\n        properties of a system.\n      \n    \n      \n        You leave all security decisions to a central security team.\n      \n    \n      \n        You don't communicate how the decisions taken in the SDLC relate\n        to the overall security expectations or policies of the\n        organization.\n      \n    \n      \n        You perform the security review process too late.\n      \n    \n    Benefits of establishing this best\n      practice:\n  \n     \n     \n     \n  \n      \n        Better knowledge of the organizational requirements for security\n        early in the development cycle.\n      \n    \n      \n        Being able to identify and remediate potential security issues\n        faster, resulting in a quicker delivery of features.\n      \n    \n      \n        Improved quality of software and systems.\n      \n    \n    Level of risk exposed if this best practice\n      is not established: Medium\n  \n    \n    Implementation guidance\n    \n    \n    \n      To build secure and high-quality software, provide training to\n      your team on common practices for secure development and operation\n      of applications. This practice can help your team prevent, detect,\n      and remediate security issues earlier in the development\n      lifecycle, which can accelerate your delivery timeline.\n    \n    \n      To achieve this practice, consider training your team on threat\n      modeling using AWS resources like the\n      Threat\n        Modeling Workshop. Threat modeling can help your team\n      understand potential security risks and design systems with\n      security in mind from the outset. Additionally, you can provide\n      access to\n      AWS Training and Certification, industry, or AWS Partner\n      training on secure development practices. For more detail on a\n      comprehensive approach to designing, developing, securing, and\n      efficiently operating at scale, see\n      AWS       DevOps Guidance.\n    \n    \n      Clearly define and communicate your organization's security review\n      process, and outline the responsibilities of your team, the\n      security team, and other stakeholders. Publish self-service\n      guidance, code examples, and templates that demonstrate how to\n      meet your security requirements. You can use AWS services like\n      AWS CloudFormation,\n      AWS Cloud Development Kit (AWS CDK) (AWS CDK) Constructs, and\n      Service Catalog to provide pre-approved, secure\n      configurations and reduce the need for custom setups.\n    \n    \n      Regularly gather feedback from your team on their experience with\n      the security review process and training, and use this feedback to\n      continuously improve. Conduct game days or bug bash campaigns to\n      identify and address security issues while simultaneously\n      enhancing your team's skills.\n    \n     \n      \n      Implementation steps\n      \n      \n      \n         \n         \n         \n         \n         \n         \n         \n         \n         \n      \n          \n            Identify training needs:\n            Assess the current skill level and knowledge gaps within\n            your team regarding secure development practices through\n            surveys, code reviews, or discussions with team members.\n          \n        \n          \n            Plan the training: Based\n            on the identified needs, create a training plan that covers\n            relevant topics such as threat modeling, secure coding\n            practices, security testing, and secure deployment\n            practices. Employ resources like the\n            Threat\n              Modeling Workshop,\n            AWS Training and Certification, and industry or AWS\n            Partner training programs.\n          \n        \n          \n            Schedule and deliver\n              training: Schedule regular training sessions or\n            workshops for your team. These can be instructor-led or\n            self-paced, depending on your team's preferences and\n            availability. Encourage hands-on exercises and practical\n            examples to reinforce the learning.\n          \n        \n          \n            Define a security review\n              process: Collaborate with your security team and\n            other stakeholders to clearly define the security review\n            process for your applications. Document the responsibilities\n            of each team or individual involved in the process,\n            including your development team, security team, and other\n            relevant stakeholders.\n          \n        \n          \n            Create self-service\n              resources: Develop self-service guidance, code\n            examples, and templates that demonstrate how to meet your\n            organization's security requirements. Consider AWS services\n            like\n            CloudFormation,\n            AWS CDK Constructs, and\n            Service\n              Catalog to provide pre-approved, secure\n            configurations and reduce the need for custom setups.\n          \n        \n          \n            Communicate and\n              socialize: Effectively communicate the security\n            review process and the available self-service resources to\n            your team. Conduct training sessions or workshops to\n            familiarize them with these resources, and verify that they\n            understand how to use them.\n          \n        \n          \n            Gather feedback and\n              improve: Regularly collect feedback from your\n            team on their experience with the security review process\n            and training. Use this feedback to identify areas for\n            improvement and continuously refine the training materials,\n            self-service resources, and the security review process.\n          \n        \n          \n            Conduct security\n              exercises: Organize game days or bug bash\n            campaigns to identify and address security issues within\n            your applications. These exercises not only help uncover\n            potential vulnerabilities but also serve as practical\n            learning opportunities for your team that enhance their\n            skills in secure development and operation.\n          \n        \n          \n            Continue to learn and\n              improve: Encourage your team to stay up to date\n            with the latest secure development practices, tools, and\n            techniques. Regularly review and update your training\n            materials and resources to reflect the evolving security\n            landscape and best practices.\n          \n        \n     \n   \n\n  Resources \n\n      \n    \n      Related best practices:\n    \n    \n       \n    \n        \n          SEC11-BP08 Build a program that embeds security ownership in\n  workload teams\n        \n      \n    \n      \n        Related documents:\n      \n    \n    \n       \n       \n       \n       \n       \n    \n        \n          AWS Training and Certification\n        \n      \n        \n          How\n          to think about cloud security governance\n        \n      \n        \n          How\n          to approach threat modeling\n        \n      \n        \n          Accelerating\n          training – The AWS Skills Guild\n        \n      \n        \n          AWS           DevOps Sagas\n        \n      \n    \n      \n        Related videos:\n      \n    \n    \n       \n    \n        \n          Proactive\n          security: Considerations and approaches\n        \n      \n    \n      \n        Related examples:\n      \n    \n    \n       \n       \n    \n        \n          Workshop\n          on threat modeling\n        \n      \n        \n          Industry\n          awareness for developers\n        \n      \n    \n      \n        Related services:\n      \n    \n    \n       \n       \n       \n    \n        \n          AWS CloudFormation\n        \n      \n        \n          AWS Cloud Development Kit (AWS CDK) (AWS CDK) Constructs\n        \n      \n        \n          Service Catalog\n        \n      \n  \u003cdiv\u003e\u003cdiv\u003e\u003cdiv\u003e\u003cdiv id=\"js_error_message\"\u003e\u003cp\u003e\u003cimg src=\"https://d1ge0kk1l5kms0.cloudfront.net/images/G/01/webservices/console/warning.png\" alt=\"Warning\" /\u003e \u003cstrong\u003eJavascript is disabled or is unavailable in your browser.\u003c/strong\u003e\u003c/p\u003e\u003cp\u003eTo use the Amazon Web Services Documentation, Javascript must be enabled. Please refer to your browser's Help pages for instructions.\u003c/p\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003eDocument Conventions SEC 11. How do you incorporate and validate the security properties of applications throughout the design, development, and deployment lifecycle?SEC11-BP02 Automate testing throughout the development and\n  release lifecycle",
  "SEC11-BP02 Automate testing throughout the development and\n  release lifecycle\n      Automate the testing for security properties throughout the\n      development and release lifecycle. Automation makes it easier to\n      consistently and repeatably identify potential issues in software\n      prior to release, which reduces the risk of security issues in the\n      software being provided.\n    Desired outcome:  The goal of automated testing is to\n    provide a programmatic way of detecting potential issues early and often throughout the\n    development lifecycle. When you automate regression testing, you can rerun functional and\n    non-functional tests to verify that previously tested software still performs as expected after\n    a change. When you define security unit tests to check for common misconfigurations, such as\n    broken or missing authentication, you can identify and fix these issues early in the development\n    process. \n      Test automation uses purpose-built test cases for application\n      validation, based on the application’s requirements and desired\n      functionality. The result of the automated testing is based on\n      comparing the generated test output to its respective expected\n      output, which expedites the overall testing lifecycle. Testing\n      methodologies such as regression testing and unit test suites are\n      best suited for automation. Automating the testing of security\n      properties allows builders to receive automated feedback without\n      having to wait for a security review. Automated tests in the form\n      of static or dynamic code analysis can increase code quality and\n      help detect potential software issues early in the development\n      lifecycle.\n    \n\n\n    Common anti-patterns: \n\n      \n    \n       \n       \n       \n       \n    \n        \n          Not communicating the test cases and test results of the\n          automated testing.\n        \n      \n        \n          Performing the automated testing only immediately prior to a\n          release.\n        \n      \n        \n          Automating test cases with frequently changing requirements.\n        \n      \n        \n          Failing to provide guidance on how to address the results of\n          security tests.\n        \n      \n   \n\n\n    Benefits of establishing this best practice: \n\n      \n    \n       \n       \n       \n       \n       \n    \n        \n          Reduced dependency on people evaluating the security\n          properties of systems.\n        \n      \n        \n          Having consistent findings across multiple workstreams\n          improves consistency.\n        \n      \n        \n          Reduced likelihood of introducing security issues into\n          production software.\n        \n      \n        \n          Shorter window of time between detection and remediation due\n          to catching software issues earlier.\n        \n      \n        \n          Increased visibility of systemic or repeated behavior across\n          multiple workstreams, which can be used to drive\n          organization-wide improvements.\n        \n      \n     Level of risk exposed if this best practice is not established:\n      Medium \n   \n\n  Implementation guidance \n\n      \n    \nAs you build your software, adopt various mechanisms for software testing to ensure that \n  you are testing your application for both functional requirements, based on your application’s \n  business logic, and non-functional requirements, which are focused on application reliability, \n  performance, and security.\n\nStatic application security testing (SAST) analyzes your source code for anomalous security patterns, \nand provides indications for defect prone code. SAST relies on static inputs, such as \ndocumentation (requirements specification, design documentation, and design specifications) \nand application source code to test for a range of known security issues. Static code \nanalyzers can help expedite the analysis of large volumes of code. \nThe NIST Quality Group \n  provides a comparison of Source Code Security Analyzers, \n  which includes open source tools for Byte Code Scanners and Binary Code Scanners.\n    \n      Complement your static testing with dynamic\n      analysis security testing (DAST) methodologies, which performs\n      tests against the running application to identify potentially\n      unexpected behavior. Dynamic testing can be used to detect\n      potential issues that are not detectable via static analysis.\n      Testing at the code repository, build, and pipeline stages allows\n      you to check for different types of potential issues from entering\n      into your code.\n      Amazon Q Developers provides code recommendations, including\n      security scanning, in the builder's IDE.\n      Amazon CodeGuru Security can identify critical issues, security\n      issues, and hard-to-find bugs during application development, and\n      provides recommendations to improve code quality. Extracting\n      Software Bill of Materials (SBOM) also allows you to extract a\n      formal record containing the details and relationships of the\n      various components used in building your software. This allows you\n      to inform vulnerability management, and quickly identify software\n      or component dependencies and supply chain risks.\n    \n      \n        The\n        Security\n        for Developers workshop uses AWS developer tools, such as\n        AWS CodeBuild,\n        AWS CodeCommit, and\n        AWS CodePipeline, for release pipeline automation that\n        includes SAST and DAST testing methodologies.\n      \n      \n        As you progress through your SDLC, establish an iterative\n        process that includes periodic application reviews with your\n        security team. Feedback gathered from these security reviews\n        should be addressed and validated as part of your release\n        readiness review. These reviews establish a robust application\n        security posture, and provide builders with actionable feedback\n        to address potential issues.\n      \n    \n     \n      \n      Implementation steps\n      \n      \n      \n         \n         \n         \n         \n         \n         \n         \n         \n         \n      \n          \n            Implement consistent IDE, code review, and CI/CD tools that\n            include security testing.\n          \n        \n          \n            Consider where in the SDLC it is appropriate to block\n            pipelines instead of just notifying builders that issues\n            need to be remediated.\n          \n        \n          \n            Automated\n              Security Helper (ASH) is an example for open-source\n            code security scanning tool.\n          \n        \n          \n            Performing testing or code analysis using automated tools,\n            such as\n            Amazon Q Developer integrated with developer IDEs, and\n            Amazon CodeGuru Security for scanning code on commit, helps\n            builders get feedback at the right time.\n          \n        \n          \n            When building using AWS Lambda, you can use\n            Amazon Inspector to scan the application code in your\n            functions.\n          \n        \n          \n            When automated testing is included in CI/CD pipelines, you\n            should use a ticketing system to track the notification and\n            remediation of software issues.\n          \n        \n          \n            For security tests that might generate findings, linking to\n            guidance for remediation helps builders improve code\n            quality.\n          \n        \n          \n            Regularly analyze the findings from automated tools to\n            prioritize the next automation, builder training, or\n            awareness campaign.\n          \n        \n          \n            To extract SBOM as part of your CI/CD pipelines, use\n            Amazon Inspector SBOM Generator to produce SBOMs for\n            archives, container images, directories, local systems, and\n            compiled Go and Rust binaries in the CycloneDX SBOM format.\n          \n        \n     \n   \n\n  Resources \n\n      \n    \n      Related best practices:\n    \n    \n       \n    \n        \n          DevOps\n            Guidance: DL.CR.3 Establish clear completion criteria for code\n            tasks\n        \n      \n      \n        Related documents:\n      \n    \n    \n       \n    \n        \n          Continuous\n          Delivery and Continuous Deployment\n        \n      \n    \n       \n       \n       \n       \n       \n       \n       \n       \n    \n        \n          AWS           DevOps Competency Partners\n        \n      \n        \n          AWS           Security Competency Partners for Application Security\n        \n      \n        \n          Choosing\n          a Well-Architected CI/CD approach\n        \n      \n        \n          Secrets\n            detection in Amazon CodeGuru Security\n        \n      \n        \n          Amazon CodeGuru Security Detection Library\n        \n      \n        \n          Accelerate\n          deployments on AWS with effective governance\n        \n      \n        \n          How\n          AWS approaches automating safe, hands-off\n          deployments\n        \n      \n        \n          How\n            Amazon CodeGuru Security helps you effectively balance\n            security and velocity\n        \n      \n    \n      \n      Related videos:\n    \n    \n       \n       \n       \n       \n    \n        \n          Hands-off:\n          Automating continuous delivery pipelines at Amazon\n        \n      \n        \n          Automating\n          cross-account CI/CD pipelines\n        \n      \n        \n          The\n            Software Development Prcess at Amazon\n        \n      \n        \n          Testing\n            software and systems at Amazon\n        \n      \n    \n      \n      Related examples:\n    \n    \n       \n       \n       \n    \n        \n          Industry\n          awareness for developers\n        \n      \n        \n          Automated\n            Security Helper (ASH)\n        \n      \n        \n          AWS CodePipeline Governance - Github\n        \n      \n  \u003cdiv\u003e\u003cdiv\u003e\u003cdiv\u003e\u003cdiv id=\"js_error_message\"\u003e\u003cp\u003e\u003cimg src=\"https://d1ge0kk1l5kms0.cloudfront.net/images/G/01/webservices/console/warning.png\" alt=\"Warning\" /\u003e \u003cstrong\u003eJavascript is disabled or is unavailable in your browser.\u003c/strong\u003e\u003c/p\u003e\u003cp\u003eTo use the Amazon Web Services Documentation, Javascript must be enabled. Please refer to your browser's Help pages for instructions.\u003c/p\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003eDocument ConventionsSEC11-BP01 Train for application securitySEC11-BP03 Perform regular penetration testing",
  "SEC11-BP03 Perform regular penetration testingPerform regular penetration testing of your software. This\n    mechanism helps identify potential software issues that cannot be\n    detected by automated testing or a manual code review. It can also\n    help you understand the efficacy of your detective controls.\n    Penetration testing should try to determine if the software can be\n    made to perform in unexpected ways, such as exposing data that\n    should be protected, or granting broader permissions than\n    expected.\n      \n    Desired outcome: Penetration testing is used to detect,\n    remediate, and validate your application’s security properties. Regular and scheduled\n    penetration testing should be performed as part of the software development lifecycle (SDLC).\n    The findings from penetration tests should be addressed prior to the software being released.\n    You should analyze the findings from penetration tests to identify if there are issues that\n    could be found using automation. Having a regular and repeatable penetration testing process\n    that includes an active feedback mechanism helps inform the guidance to builders and improves\n    software quality. Common anti-patterns: \n       \n       \n       \n    \n        \n          Only penetration testing for known or prevalent security\n          issues.\n        \n      \n        \n          Penetration testing applications without dependent third-party\n          tools and libraries.\n        \n      \n        \n          Only penetration testing for package security issues, and not\n          evaluating implemented business logic.\n        \n      Benefits of establishing this best practice:\n       \n       \n       \n    \n        \n          Increased confidence in the security properties of the\n          software prior to release.\n        \n      \n        \n          Opportunity to identify preferred application patterns, which\n          leads to greater software quality.\n        \n      \n        \n          A feedback loop that identifies earlier in the development\n          cycle where automation or additional training can improve the\n          security properties of software.\n        \n       Level of risk exposed if this best practice is not established:\n    High \n\n  Implementation guidance \n\n      \n    \n      Penetration testing is a structured security testing exercise\n      where you run planned security breach scenarios to detect,\n      remediate, and validate security controls. Penetration tests start\n      with reconnaissance, during which data is gathered based on the\n      current design of the application and its dependencies. A curated\n      list of security-specific testing scenarios are built and run. The\n      key purpose of these tests is to uncover security issues in your\n      application, which could be exploited for gaining unintended\n      access to your environment, or unauthorized access to data. You\n      should perform penetration testing when you launch new features,\n      or whenever your application has undergone major changes in\n      function or technical implementation.\n    \n    \n      You should identify the most appropriate stage in the development\n      lifecycle to perform penetration testing. This testing should\n      happen late enough that the functionality of the system is close\n      to the intended release state, but with enough time remaining for\n      any issues to be remediated.\n    \n     \n\n  Implementation steps \n\n      \n      \n         \n         \n         \n         \n         \n      \n          \n            Have a structured process for how penetration testing is\n            scoped, basing this process on the\n            threat\n            model is a good way of maintaining context.\n          \n        \n          \n            Identify the appropriate place in the development cycle to\n            perform penetration testing. This should be when there is\n            minimal change expected in the application, but with enough\n            time to perform remediation.\n          \n        \n          \n            Train your builders on what to expect from penetration\n            testing findings, and how to get information on remediation.\n          \n        \n          \n            Use tools to speed up the penetration testing process by\n            automating common or repeatable tests.\n          \n        \n          \n            Analyze penetration testing findings to identify systemic\n            security issues, and use this data to inform additional\n            automated testing and ongoing builder education.\n          \n        \n     \n   \n\n  Resources \n\n      \n    \n      \n        Related best practices:\n      \n    \n    \n       \n       \n    \n        \n          SEC11-BP01 Train for application security\n          \n          \n        \n      \n        SEC11-BP02 Automate testing throughout the development and\n  release lifecycle\n      \n    \n      \n        Related documents:\n      \n    \n    \n       \n       \n       \n       \n       \n    \n        \n          AWS           Penetration Testing provides detailed guidance for\n          penetration testing on AWS\n        \n      \n        \n          Accelerate\n          deployments on AWS with effective governance\n        \n      \n        \n          AWS           Security Competency Partners\n        \n      \n        \n          Modernize\n          your penetration testing architecture on AWS Fargate\n        \n      \n        \n          AWS Fault\n          injection Simulator\n        \n      \n    \n      \n        Related examples:\n      \n    \n    \n       \n       \n    \n        \n          Automate API testing with AWS CodePipeline (GitHub) \n      \n        \n          Automated security\n            helper (GitHub) \n      \n  \u003cdiv\u003e\u003cdiv\u003e\u003cdiv\u003e\u003cdiv id=\"js_error_message\"\u003e\u003cp\u003e\u003cimg src=\"https://d1ge0kk1l5kms0.cloudfront.net/images/G/01/webservices/console/warning.png\" alt=\"Warning\" /\u003e \u003cstrong\u003eJavascript is disabled or is unavailable in your browser.\u003c/strong\u003e\u003c/p\u003e\u003cp\u003eTo use the Amazon Web Services Documentation, Javascript must be enabled. Please refer to your browser's Help pages for instructions.\u003c/p\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003eDocument ConventionsSEC11-BP02 Automate testing throughout the development and\n  release lifecycleSEC11-BP04 Conduct code reviews",
  "SEC11-BP04 Conduct code reviews\n    Implement code reviews to help verify the quality and security of\n    software being developed. Code reviews involve having team members\n    other than the original code author review the code for potential\n    issues, vulnerabilities, and adherence to coding standards and best\n    practices. This process helps catch errors, inconsistencies, and\n    security flaws that might have been overlooked by the original\n    developer. Use automated tools to assist with code reviews.\n  \n    Desired outcome: You include code\n    reviews during development to increase the quality of the software\n    being written. You upskill less experienced members of the team\n    through learnings identified during the code review. You identify\n    opportunities for automation and support the code review process\n    using automated tools and testing.\n  \n    Common anti-patterns:\n  \n     \n     \n     \n     \n  \n      \n        You don't review code before deployment.\n      \n    \n      \n        The same person writes and reviews the code.\n      \n    \n      \n        You don't use automation and tools to assist or orchestrate code\n        reviews.\n      \n    \n      \n        You don't train builders on application security before they\n        review code.\n      \n    Benefits of establishing this best practice:\n       \n       \n       \n       \n    \n        \n          Increased code quality.\n        \n      \n        \n          Increased consistency of code development through reuse of\n          common approaches.\n        \n      \n        \n          Reduction in the number of issues discovered during\n          penetration testing and later stages.\n        \n      \n        \n          Improved knowledge transfer within the team.\n        \n      \n    Level of risk exposed if this best practice is not established:\n    Medium \n    \n    Implementation guidance\n    \n    \n    \n      Code reviews help to verify the quality and security of the\n      software during development. Manual reviews involve having a team\n      member other than the original code author review the code for\n      potential issues, vulnerabilities, and adherence to coding\n      standards and best practices. This process helps catch errors,\n      inconsistencies, and security flaws that might have been\n      overlooked by the original developer.\n    \n    \n      Consider Amazon CodeGuru Security\n      to help conduct automated code reviews. CodeGuru Security uses\n      machine learning and automated reasoning to analyze your code and\n      identify potential security vulnerabilities and coding issues.\n      Integrate automated code reviews with your existing code\n      repositories and continuous integration/continuous deployment\n      (CI/CD) pipelines.\n    \n     \n      \n      Implementation steps\n      \n      \n      \n         \n         \n         \n         \n         \n         \n         \n      \n          \n            Establish a code review\n              process:\n          \n          \n             \n             \n             \n          \n              \n                Define when code reviews should occur, such as before\n                merging code into the main branch or before deploying to\n                production.\n              \n            \n              \n                Determine who should be involved in the code review\n                process, such as team members, senior developers, and\n                security experts.\n              \n            \n              \n                Decide on the code review methodology, including the\n                process and tools to be used.\n              \n            \n        \n          \n            Set up code review tools:\n          \n          \n             \n             \n             \n          \n              \n                Evaluate and select code review tools that fit your\n                team's needs, such as GitHub Pull Requests or CodeGuru\n                Security\n              \n            \n              \n                Integrate the chosen tools with your existing code\n                repositories and CI/CD pipelines.\n              \n            \n              \n                Configure the tools to enforce code review requirements,\n                such as the minimum number of reviewers and approval\n                rules.\n              \n            \n        \n          \n            Define a code review checklist and\n              guidelines:\n          \n          \n             \n             \n          \n              \n                Create a code review checklist or guidelines that\n                outline what should be reviewed. Consider factors such\n                as code quality, security vulnerabilities, adherence to\n                coding standards, and performance.\n              \n            \n              \n                Share the checklist or guidelines with the development\n                team, and verify everyone understands the expectations.\n              \n            \n        \n          \n            Train developers on code review best\n              practices:\n          \n          \n             \n             \n             \n          \n              \n                Provide training to your team on how to conduct\n                effective code reviews.\n              \n            \n              \n                Educate your team on application security principles and\n                common vulnerabilities to look for during reviews.\n              \n            \n              \n                Encourage knowledge sharing and pair programming\n                sessions to upskill less experienced team members.\n              \n            \n        \n          \n            Implement the code review\n              process:\n          \n          \n             \n             \n             \n          \n              \n                Integrate the code review step into your development\n                workflow, such as creating a pull request and assigning\n                reviewers.\n              \n            \n              \n                Require that code changes undergo a code review before\n                merge or deployment.\n              \n            \n              \n                Encourage open communication and constructive feedback\n                during the review process.\n              \n            \n        \n          \n            Monitor and improve:\n          \n          \n             \n             \n             \n          \n              \n                Regularly review the effectiveness of your code review\n                process and gather feedback from the team.\n              \n            \n              \n                Identify opportunities for automation or tool\n                improvements to streamline the code review process.\n              \n            \n              \n                Continuously update and refine the code review checklist\n                or guidelines based on learnings and industry best\n                practices.\n              \n            \n        \n          \n            Foster a culture of code\n              review:\n          \n          \n             \n             \n             \n          \n              \n                Emphasize the importance of code reviews to maintain\n                code quality and security.\n              \n            \n              \n                Celebrate successes and learnings from the code review\n                process.\n              \n            \n              \n                Encourage a collaborative and supportive environment\n                where developers feel comfortable giving and receiving\n                feedback.\n              \n            \n        \n     \n   \n\n  Resources\n\n      \n    \n      \n      Related best practices:\n    \n    \n       \n    \n        \n          SEC11-BP02 Automate testing throughout the development and\n  release lifecycle\n        \n      \n    \n    \n      Related documents:\n    \n    \n       \n       \n    \n        \n          DevOps\n            Guidance: DL.CR.2 Perform peer review for code changes\n        \n      \n        \n          About\n            pull requests in GitHub\n        \n      \n\n    \n      Related examples:\n      \n        \n           \n           \n        \n            \n              Automate\n                code reviews with Amazon CodeGuru Security\n            \n          \n            \n              Automating\n                detection of security vulnerabilities and bugs in CI/CD pipelines\n                using Amazon CodeGuru Security CLI\n            \n          \n    \n      Related videos:\n    \n    \n       \n    \n        \n          Continuous\n            improvement of code quality with Amazon CodeGuru\n            Security\n        \n      \n  \u003cdiv\u003e\u003cdiv\u003e\u003cdiv\u003e\u003cdiv id=\"js_error_message\"\u003e\u003cp\u003e\u003cimg src=\"https://d1ge0kk1l5kms0.cloudfront.net/images/G/01/webservices/console/warning.png\" alt=\"Warning\" /\u003e \u003cstrong\u003eJavascript is disabled or is unavailable in your browser.\u003c/strong\u003e\u003c/p\u003e\u003cp\u003eTo use the Amazon Web Services Documentation, Javascript must be enabled. Please refer to your browser's Help pages for instructions.\u003c/p\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003eDocument ConventionsSEC11-BP03 Perform regular penetration testingSEC11-BP05 Centralize services for packages and dependencies",
  "SEC11-BP05 Centralize services for packages and dependencies\n    Provide centralized services for your teams to obtain software\n    packages and other dependencies. This allows the validation of\n    packages before they are included in the software that you write and\n    provides a source of data for the analysis of the software being\n    used in your organization.\n  \n    Desired outcome: You build your\n    workload from external software packages in addition to the code\n    that you write. This makes it simpler for you to implement\n    functionality that is repeatedly used, such as a JSON parser or an\n    encryption library. You centralize the sources for these packages\n    and dependencies so your security team can validate them before they\n    are used. You use this approach in conjunction with the manual and\n    automated testing flows to increase the confidence in the quality of\n    the software that you develop.\n  \n    Common anti-patterns:\n  \n     \n     \n  \n      \n        You pull packages from arbitrary repositories on the internet.\n      \n    \n      \n        You don't test new packages before you make them available to\n        builders.\n      \n    Benefits of establishing this best practice:\n       \n       \n       \n    \n        \n          Better understanding of what packages are being used in the\n          software being built.\n        \n      \n        \n          Being able to notify workload teams when a package needs to be\n          updated based on the understanding of who is using what.\n        \n      \n        \n          Reducing the risk of a package with issues being included in\n          your software.\n        \n      \n    Level of risk exposed if this best practice is not established:\n    Medium \n\n  Implementation guidance \n\n      \n    \n      Provide centralized services for packages and dependencies in a\n      way that is simple for builders to consume. Centralized services\n      can be logically central rather than implemented as a monolithic\n      system. This approach allows you to provide services in a way that\n      meets the needs of your builders. You should implement an\n      efficient way of adding packages to the repository when updates\n      happen or new requirements emerge. AWS services such as\n      AWS       CodeArtifact or similar AWS partner solutions provide a way\n      of delivering this capability.\n    \n     \n\n  Implementation steps\n\n      \n      \n         \n         \n         \n         \n         \n         \n      Implement a logically centralized repository service that\n          is available in all of the environments where software is\n          developed. Include access to the repository as part of the AWS account\n          vending process.Build automation to test packages before they are published\n          in a repository.Maintain metrics of the most commonly used packages,\n          languages, and teams with the highest amount of change.\n          \n            Provide an automated mechanism for builder teams to request\n            new packages and provide feedback.\n          \n        \n          \n            Regularly scan packages in your repository to identify the\n            potential impact of newly discovered issues.\n          \n        \n     \n  \n   \n\n  Resources \n\n      \n    \n      \n        Related best practices:\n      \n    \n    \n       \n    \n        \n          SEC11-BP02 Automate testing throughout the development and\n  release lifecycle\n        \n      \n    \n      \n        Related documents:\n      \n    \n       \n       \n    \n        \n          DevOps Guidance: DL.CS.2 Sign code artifacts after each build\n        \n      \n        \n          Supply chain Levels for Software Artifacts (SLSA)\n          \n        \n      \n    \n    \n      Related examples:\n    \n    \n    \n       \n       \n       \n       \n       \n       \n    \n        \n          Accelerate\n            deployments on AWS with effective governance\n        \n      \n        \n          Tighten\n            your package security with CodeArtifact Package Origin Control\n            toolkit\n        \n      \n        \n          Multi\n            Region Package Publishing Pipeline (GitHub)\n        \n      \n        \n          Publishing\n            Node.js Modules on AWS CodeArtifact using AWS CodePipeline (GitHub)\n        \n      \n        \n          AWS CDK Java CodeArtifact Pipeline Sample (GitHub)\n        \n      \n        \n          Distribute\n            private .NET NuGet packages with AWS CodeArtifact\n          (GitHub)\n        \n      \n    \n      \n        Related videos:\n      \n    \n    \n       \n       \n       \n    \n        \n          Proactive\n          security: Considerations and approaches\n        \n      \n        \n          The\n          AWS Philosophy of Security (re:Invent 2017)\n        \n      \n        \n          When\n          security, safety, and urgency all matter: Handling\n          Log4Shell\n        \n      \n  \u003cdiv\u003e\u003cdiv\u003e\u003cdiv\u003e\u003cdiv id=\"js_error_message\"\u003e\u003cp\u003e\u003cimg src=\"https://d1ge0kk1l5kms0.cloudfront.net/images/G/01/webservices/console/warning.png\" alt=\"Warning\" /\u003e \u003cstrong\u003eJavascript is disabled or is unavailable in your browser.\u003c/strong\u003e\u003c/p\u003e\u003cp\u003eTo use the Amazon Web Services Documentation, Javascript must be enabled. Please refer to your browser's Help pages for instructions.\u003c/p\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003eDocument ConventionsSEC11-BP04 Conduct code reviewsSEC11-BP06 Deploy software programmatically",
  "SEC11-BP06 Deploy software programmaticallyPerform software deployments programmatically where possible.\n    This approach reduces the likelihood that a deployment fails or an\n    unexpected issue is introduced due to human error.\n    Desired outcome: The version of\n    your workload that you test is the version that you deploy, and the\n    deployment is performed consistently every time. You externalize the\n    configuration of your workload, which helps you deploy to different\n    environments without changes. You employ cryptographic signing of\n    your software packages to verify that nothing changes between\n    environments.\n  Common anti-patterns: \n       \n       \n    \n        \n          Manually deploying software into production.\n        \n      \n        \n          Manually performing changes to software to cater to different\n          environments.\n        \n      Benefits of establishing this best practice: \n       \n       \n       \n       \n       \n    \n        \n          Increased confidence in the software release process.\n        \n      \n        \n          Reduced risk of a failed change impacting business\n          functionality.\n        \n      \n        \n          Increased release cadence due to lower change risk.\n        \n      \n        \n          Automatic rollback capability for unexpected events during\n          deployment.\n        \n      \n        \n          Ability to cryptographically prove that the software that was\n          tested is the software deployed.\n        \n      \n    Level of risk exposed if this best practice is not established:\n    High \n\n  Implementation guidance\n\n      \n    \n      To maintain a robust and reliable application infrastructure,\n      implement secure and automated deployment practices. This practice\n      involves removing persistent human access from production\n      environments, using CI/CD tools for deployments, and externalizing\n      environment-specific configuration data. By following this\n      approach, you can enhance security, reduce the risk of human\n      errors, and streamline the deployment process.\n    \n    \n      You can build your AWS account structure to remove persistent\n      human access from production environments. This practice minimizes\n      the risk of unauthorized changes or accidental modifications,\n      which improves the integrity of your production systems. Instead\n      of direct human access, you can use CI/CD tools like\n      AWS CodeBuild and\n      AWS CodePipeline to perform deployments. You can use these\n      services to automate the build, test, and deployment processes,\n      which reduces manual intervention and increases consistency.\n    \n    \n      To further enhance security and traceability, you can sign your\n      application packages after they have been tested and validate\n      these signatures during deployment. To do so, use cryptographic\n      tools such as\n      AWS Signer or\n      AWS Key Management Service (AWS KMS). By signing and verifying packages, you\n      can make sure that you deploy only authorized and validated code\n      to your environments.\n    \n    \n      Additionally, your team can architect your workload to obtain\n      environment-specific configuration data from an external source,\n      such as\n      AWS Systems Manager Parameter Store. This practice separates\n      the application code from the configuration data, which helps you\n      manage and update configurations independently without modifying\n      the application code itself.\n    \n    \n      To streamline infrastructure provisioning and management, consider\n      using infrastructure as code (IaC) tools like\n      AWS CloudFormation or\n      AWS CDK. You\n      can use these tools to define your infrastructure as code, which\n      improves the consistency and repeatability of deployments across\n      different environments.\n    \n    \n      Consider canary deployments to validate the successful deployment\n      of your software. Canary deployments involve rolling out changes\n      to a subset of instances or users before deploying to the entire\n      production environment. You can then monitor the impact of changes\n      and roll back if necessary, which minimizes the risk of widespread\n      issues.\n    \n    \n      Follow the recommendations outlined in the\n      Organizing\n      Your AWS Environment Using Multiple Accounts whitepaper.\n      This whitepaper provides guidance on separating environments (such\n      as development, staging, and production) into distinct AWS accounts, which further enhances security and isolation.\n    \n     \n\n  Implementation steps\n\n      \n      \n         \n         \n         \n         \n         \n         \n         \n         \n      \n          \n            Set up AWS account\n            structure:\n          \n          \n             \n             \n          \n              \n                Follow the guidance in the\n                Organizing\n                Your AWS Environment Using Multiple Accounts\n                whitepaper to create separate AWS accounts for different\n                environments (for exampoe, development, staging, and\n                production).\n              \n            \n              \n                Configure appropriate access controls and permissions\n                for each account to restrict direct human access to\n                production environments.\n              \n            \n        \n          \n            Implement a CI/CD\n            pipeline:\n          \n          \n             \n             \n             \n          \n              \n                Set up a CI/CD pipeline using services like\n                AWS CodeBuild and\n                AWS CodePipeline.\n              \n            \n              \n                Configure the pipeline to automatically build, test, and\n                deploy your application code to the respective\n                environments.\n              \n            \n              \n                Integrate code repositories with the CI/CD pipeline for\n                version control and code management.\n              \n            \n        \n          \n            Sign and verify application\n            packages:\n          \n          \n             \n             \n          \n              \n                Use\n                AWS Signer or\n                AWS Key Management Service (AWS KMS) to sign your\n                application packages after they have been tested and\n                validated.\n              \n            \n              \n                Configure the deployment process to verify the\n                signatures of the application packages before you deploy\n                them to the target environments.\n              \n            \n        \n          \n            Externalize configuration\n            data:\n          \n          \n             \n             \n          \n              \n                Store environment-specific configuration data in\n                AWS Systems Manager Parameter Store.\n              \n            \n              \n                Modify your application code to retrieve configuration\n                data from the Parameter Store during deployment or\n                runtime.\n              \n            \n        \n          \n            Implement infrastructure as code\n            (IaC):\n          \n          \n             \n             \n             \n          \n              \n                Use IaC tools like\n                AWS CloudFormation or\n                AWS CDK to define and manage your infrastructure as\n                code.\n              \n            \n              \n                Create CloudFormation templates or CDK scripts to\n                provision and configure the necessary AWS resources for\n                your application.\n              \n            \n              \n                Integrate IaC with your CI/CD pipeline to automatically\n                deploy infrastructure changes alongside application code\n                changes.\n              \n            \n        \n          \n            Implement canary\n            deployments:\n          \n          \n             \n             \n             \n          \n              \n                Configure your deployment process to support canary\n                deployments, where changes are rolled out to a subset of\n                instances or users before you deploy them to the entire\n                production environment.\n              \n            \n              \n                Use services like\n                AWS CodeDeploy or\n                AWS                 ECS to manage canary deployments and monitor the\n                impact of changes.\n              \n            \n              \n                Implement rollback mechanisms to revert to the previous\n                stable version if issues are detected during the canary\n                deployment.\n              \n            \n        \n          \n            Monitor and audit:\n          \n          \n             \n             \n             \n          \n              \n                Set up monitoring and logging mechanisms to track\n                deployments, application performance, and infrastructure\n                changes.\n              \n            \n              \n                Use services like\n                Amazon CloudWatch and\n                AWS CloudTrail to collect and analyze logs and\n                metrics.\n              \n            \n              \n                Implement auditing and compliance checks to verify\n                adherence to security best practices and regulatory\n                requirements.\n              \n            \n        \n          \n            Continuously improve:\n          \n          \n             \n             \n             \n          \n              \n                Regularly review and update your deployment practices,\n                and incorporate feedback and lessons learned from\n                previous deployments.\n              \n            \n              \n                Automate as much of the deployment process as possible\n                to reduce manual intervention and potential human\n                errors.\n              \n            \n              \n                Collaborate with cross-functional teams (for example,\n                operations or security) to align and continuously\n                improve deployment practices.\n              \n            \n        \n      \n        By following these steps, you can implement secure and automated\n        deployment practices in your AWS environment, which enhances\n        security, reduces the risk of human errors, and streamlines the\n        deployment process.\n      \n     \n   \n\n  Resources\n\n      \n    \n      \n        Related best practices:\n      \n    \n    \n       \n       \n    \n        \n          SEC11-BP02 Automate testing throughout the development and\n  release lifecycle\n        \n      \n        \n          DL.CI.2\n            Trigger builds automatically upon source code\n            modifications\n        \n      \n    \n      \n        Related documents:\n      \n    \n    \n       \n       \n       \n       \n    \n        \n          Accelerate\n          deployments on AWS with effective governance\n        \n      \n        \n          Automating\n          safe, hands-off deployments\n        \n      \n        \n          Code\n          signing using AWS Certificate Manager Private CA and AWS Key Management Service asymmetric keys\n        \n      \n        \n          Code\n          Signing, a Trust and Integrity Control for AWS Lambda\n        \n      \n    \n      \n        Related videos:\n      \n    \n    \n       \n    \n        \n          Hands-off:\n          Automating continuous delivery pipelines at\n          Amazon\n        \n      \n    \n      \n        Related examples:\n      \n    \n    \n       \n    \n        \n          Blue/Green\n          deployments with AWS Fargate\n        \n      \n  \u003cdiv\u003e\u003cdiv\u003e\u003cdiv\u003e\u003cdiv id=\"js_error_message\"\u003e\u003cp\u003e\u003cimg src=\"https://d1ge0kk1l5kms0.cloudfront.net/images/G/01/webservices/console/warning.png\" alt=\"Warning\" /\u003e \u003cstrong\u003eJavascript is disabled or is unavailable in your browser.\u003c/strong\u003e\u003c/p\u003e\u003cp\u003eTo use the Amazon Web Services Documentation, Javascript must be enabled. Please refer to your browser's Help pages for instructions.\u003c/p\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003eDocument ConventionsSEC11-BP05 Centralize services for packages and dependenciesSEC11-BP07 Regularly assess security properties of the\n  pipelines",
  "SEC11-BP07 Regularly assess security properties of the\n  pipelines\n      Apply the principles of the Well-Architected Security Pillar to\n      your pipelines, with particular attention to the separation of\n      permissions. Regularly assess the security properties of your\n      pipeline infrastructure. Effectively managing the security\n      of the pipelines allows you to deliver the\n      security of the software that passes through\n      the pipelines.\n    \n    Desired outcome: The pipelines\n    you use to build and deploy your software follow the same\n    recommended practices as any other workload in your environment. The\n    tests that you implement in your pipelines are not editable by the\n    teams who use them. You give the pipelines only the permissions\n    needed for the deployments they are doing using temporary\n    credentials. You implement safeguards to prevent pipelines from\n    deploying to the wrong environments. You configure your pipelines to\n    emit state so that the integrity of your build environments can be\n    validated.\n  Common anti-patterns:\n       \n       \n       \n       \n       \n    \n        \n          Security tests that can be bypassed by builders.\n        \n      \n        \n          Overly broad permissions for deployment pipelines.\n        \n      \n        \n          Pipelines not being configured to validate inputs.\n        \n      \n        \n          Not regularly reviewing the permissions associated with your\n          CI/CD infrastructure.\n        \n      \n        \n          Use of long-term or hardcoded credentials.\n        \n      Benefits of establishing this best practice:\n       \n       \n    \n        \n          Greater confidence in the integrity of the software that is\n          built and deployed through the pipelines.\n        \n      \n        \n          Ability to stop a deployment when there is suspicious\n          activity.\n        \n       Level of risk exposed if this best practice is not\n      established: High \n\n  Implementation guidance \n\n    \n      Your deployment pipelines are a critical component of your\n      software development lifecycle and should follow the same security\n      principles and practices as any other workload in your\n      environment. This includes implementing proper access controls,\n      validating inputs, and regularly reviewing and auditing the\n      permissions associated with your CI/CD infrastructure.\n    \n    \n      Verify that the teams responsible for building and deploying\n      applications do not have the ability to edit or bypass the\n      security tests and checks implemented in your pipelines. This\n      separation of concerns helps maintain the integrity of your build\n      and deployment processes.\n    \n    \n      As a starting point, consider employing the\n      AWS       Deployment Pipelines Reference Architecture. This reference\n      architecture provides a secure and scalable foundation for\n      building your CI/CD pipelines on AWS.\n    \n    \n      Additionally, you can use services like\n      AWS Identity and Access Management Access Analyzer to generate least-privilege IAM\n      policies for both your pipeline permissions and as a step in your\n      pipeline to verify workload permissions. This helps verify that\n      your pipelines and workloads have only the necessary permissions\n      required for their specific functions, which reduces the risk of\n      unauthorized access or actions.\n    \n    \n     \n\n  Implementation steps\n\n      \n      \n         \n         \n         \n      \n          \n            Start with the\n            AWS             Deployment Pipelines Reference Architecture.\n          \n        \n          \n            Consider using\n            AWS             IAM Access Analyzer to programmatically generate\n            least privilege IAM policies for the pipelines.\n          \n        \n          \n            Integrate your pipelines with monitoring and alerting so\n            that you are notified of unexpected or abnormal activity,\n            for AWS managed services\n            Amazon EventBridge allows you to route data to targets such\n            as AWS Lambda or\n            Amazon Simple Notification Service (Amazon SNS).\n          \n        \n     \n   \n\n  Resources \n\n      \n    \n      \n        Related documents:\n      \n    \n    \n       \n       \n       \n    \n        \n          AWS           Deployment Pipelines Reference Architecture\n        \n      \n        \n          Monitoring\n          AWS CodePipeline\n        \n      \n        \n          Security\n          best practices for AWS CodePipeline\n        \n      \n    \n      \n        Related examples:\n      \n    \n    \n       \n    \n        \n          DevOps\n            monitoring dashboard (GitHub) \n      \n  \u003cdiv\u003e\u003cdiv\u003e\u003cdiv\u003e\u003cdiv id=\"js_error_message\"\u003e\u003cp\u003e\u003cimg src=\"https://d1ge0kk1l5kms0.cloudfront.net/images/G/01/webservices/console/warning.png\" alt=\"Warning\" /\u003e \u003cstrong\u003eJavascript is disabled or is unavailable in your browser.\u003c/strong\u003e\u003c/p\u003e\u003cp\u003eTo use the Amazon Web Services Documentation, Javascript must be enabled. Please refer to your browser's Help pages for instructions.\u003c/p\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003eDocument ConventionsSEC11-BP06 Deploy software programmaticallySEC11-BP08 Build a program that embeds security ownership in\n  workload teams",
  "SEC11-BP08 Build a program that embeds security ownership in\n  workload teamsBuild a program or mechanism that empowers builder teams to\n    make security decisions about the software that they create. Your\n    security team still needs to validate these decisions during a\n    review, but embedding security ownership in builder teams allows for\n    faster, more secure workloads to be built. This mechanism also\n    promotes a culture of ownership that positively impacts the\n    operation of the systems you build.\n    \n    Desired outcome: You have\n    embedded security ownership and decision-making in your teams. You\n    have either trained your teams on how to think about security or\n    have augmented their team with embedded or associated security\n    people. Your teams make higher-quality security decisions earlier in\n    the development cycle as a result.\n  Common anti-patterns:\n       \n       \n       \n    \n        \n          Leaving all security design decisions to a security team.\n        \n      \n        \n          Not addressing security requirements early enough in the\n          development process.\n        \n      \n        \n          Not obtaining feedback from builders and security people on\n          the operation of the program.\n        \n      Benefits of establishing this best practice: \n       \n       \n       \n       \n       \n       \n    \n        \n          Reduced time to complete security reviews.\n        \n      \n        \n          Reduction in security issues that are only detected at the\n          security review stage.\n        \n      \n        \n          Improvement in the overall quality of the software being\n          written.\n        \n      \n        \n          Opportunity to identify and understand systemic issues or\n          areas of high value improvement.\n        \n      \n        \n          Reduction in the amount of rework required due to security\n          review findings.\n        \n      \n        \n          Improvement in the perception of the security function.\n        \n      \n    Level of risk exposed if this best practice is not established:\n    Low \n\n  Implementation guidance\n\n      \n    \n      Start with the guidance in SEC11-BP01 Train for application security. \n      Then identify the operational model for the program that\n      you think might work best for your organization. The two main\n      patterns are to train builders or to embed security people in\n      builder teams. After you have decided on the initial approach, you\n      should pilot with a single or small group of workload teams to\n      prove the model works for your organization. Leadership support\n      from the builder and security parts of the organization helps with\n      the delivery and success of the program. As you build this\n      program, it’s important to choose metrics that can be used to show\n      the value of the program. Learning from how AWS has approached\n      this problem is a good learning experience. This best practice is\n      very much focused on organizational change and culture. The tools\n      that you use should support the collaboration between the builder\n      and security communities.\n    \n     \n\n  Implementation steps\n\n      \n      \n         \n         \n         \n         \n         \n         \n      \n          \n            Start by training your builders for application security.\n          \n        \n          \n            Create a community and an onboarding program to educate\n            builders.\n          \n        \n          \n            Pick a name for the program. Guardians, Champions, or\n            Advocates are commonly used.\n          \n        \n          \n            Identify the model to use: train builders, embed security\n            engineers, or have affinity security roles.\n          \n        \n          \n            Identify project sponsors from security, builders, and\n            potentially other relevant groups.\n          \n        \n          \n            Track metrics for the number of people involved in the\n            program, the time taken for reviews, and the feedback from\n            builders and security people. Use these metrics to make\n            improvements.\n          \n        \n     \n   \n\n  Resources \n\n      \n    \n      \n        Related best practices:\n      \n    \n    \n          \n       \n    \n        \n          SEC11-BP01 Train for application security      \n        \n      \n        \n          SEC11-BP02 Automate testing throughout the development and\n  release lifecycle\n        \n      \n    \n      \n        Related documents:\n      \n    \n    \n       \n       \n       \n       \n    \n        \n          How\n          to approach threat modeling\n        \n      \n        \n          How\n          to think about cloud security governance\n        \n      \n        \n          How\n            AWS built the Security Guardians program, a mechanism to\n            distribute security ownership\n        \n      \n        \n          How to build a Security Guardians program to distribute security ownership\n        \n      \n    \n      \n        Related videos:\n      \n    \n    \n       \n       \n    \n        \n          Proactive\n          security: Considerations and approaches\n        \n      \n        \n          AppSec\n            tooling and culture tips from AWS and Toyota Motor North\n            America\n        \n      \n  \u003cdiv\u003e\u003cdiv\u003e\u003cdiv\u003e\u003cdiv id=\"js_error_message\"\u003e\u003cp\u003e\u003cimg src=\"https://d1ge0kk1l5kms0.cloudfront.net/images/G/01/webservices/console/warning.png\" alt=\"Warning\" /\u003e \u003cstrong\u003eJavascript is disabled or is unavailable in your browser.\u003c/strong\u003e\u003c/p\u003e\u003cp\u003eTo use the Amazon Web Services Documentation, Javascript must be enabled. Please refer to your browser's Help pages for instructions.\u003c/p\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003eDocument ConventionsSEC11-BP07 Regularly assess security properties of the\n  pipelinesReliability",
  "REL01-BP01 Aware of service quotas and constraints\n    Be aware of your default quotas and manage your quota increase requests for your workload architecture. \n    Know which cloud resource constraints, such as disk or network, are potentially impactful.\n  \n    Desired outcome: \n    Customers can prevent service degradation or disruption in their AWS accounts by \n    implementing proper guidelines for monitoring key metrics, infrastructure reviews, \n    and automation remediation steps to verify that services quotas and constraints are\n    not reached that could cause service degradation or disruption.\n  \n    Common anti-patterns:\n  \n     \n     \n     \n     \n     \n     \n     \n     \n     \n     \n     \n  \n      Deploying a workload without understanding the hard or soft quotas and their limits for the services used. \n    \n      Deploying a replacement workload without analyzing and reconfiguring the necessary quotas or contacting Support in advance. \n    \n      Assuming that cloud services have no limits and the services can be used without consideration to rates, limits, counts, quantities.\n    \n      \n        Assuming that quotas will automatically be increased.\n      \n    \n      \n        Not knowing the process and timeline of quota requests.\n      \n    \n      \n        Assuming that the default cloud service quota is the identical for every service compared across regions.\n      \n    \n      \n        Assuming that service constraints can be breached and the systems will auto-scale or add increase the limit beyond the resource’s constraints\n      \n    \n      \n        Not testing the application at peak traffic in order to stress the utilization of its resources.\n      \n    \n      \n        Provisioning the resource without analysis of the required resource size.\n      \n    \n      \n        Overprovisioning capacity by choosing resource types that go well beyond actual need or expected peaks.\n      \n    \n      \n        Not assessing capacity requirements for new levels of traffic in advance of a new customer event or deploying a new technology.\n      \n    \n    Benefits of establishing this best\n      practice: Monitoring and automated management of service quotas and resource constraints \n    can proactively reduce failures. Changes in traffic patterns for a customer’s service can cause a \n    disruption or degradation if best practices are not followed. By monitoring and managing these values \n    across all regions and all accounts, applications can have improved resiliency under adverse or \n    unplanned events.\n  \n    Level of risk exposed if this best practice\n    is not established: High\n  \n\n  Implementation guidance\n    \n      Service Quotas is an AWS service that helps you manage your quotas for over \n      250 AWS services from one location. Along with looking up the quota values, \n      you can also request and track quota increases from the Service Quotas console \n      or using the AWS SDK. AWS Trusted Advisor offers a service quotas check that \n      displays your usage and quotas for some aspects of some services. The default \n      service quotas per service are also in the AWS documentation per respective \n      service (for example, see Amazon VPC Quotas). \n    \n    \n      Some service limits, like rate limits on throttled APIs are set within the \n      Amazon API Gateway itself by configuring a usage plan. Some limits that are set as \n      configuration on their respective services include Provisioned IOPS, Amazon RDS \n      storage allocated, and Amazon EBS volume allocations. \n      Amazon Elastic Compute Cloud has its own service limits dashboard \n      that can help you manage your instance, Amazon Elastic Block Store, \n      and Elastic IP address limits. If you have a use case where service quotas \n      impact your application’s performance and they are not adjustable to your \n      needs, then contact Support to see if there are mitigations.\n    \n    \n      Service quotas can be Region specific or can also be global in nature. \n      Using an AWS service that reaches its quota will not act as expected in \n      normal usage and may cause service disruption or degradation. For example, \n      a service quota limits the number of DL Amazon EC2 instances used in a Region. That limit may be reached during a traffic scaling event using Auto Scaling groups (ASG).\n    \n    \n      Service quotas for each account should be assessed for usage on a regular \n      basis to determine what the appropriate service limits might be for that \n      account. These service quotas exist as operational guardrails, to prevent \n      accidentally provisioning more resources than you need. They also serve \n      to limit request rates on API operations to protect services from abuse.\n    \n    \n      Service constraints are different from service quotas. Service constraints \n      represent a particular resource’s limits as defined by that resource type. \n      These might be storage capacity (for example, gp2 has a size limit of 1 GB - 16 TB) \n      or disk throughput. It is essential that a resource type’s \n      constraint be engineered and constantly assessed for usage that might reach \n      its limit. If a constraint is reached unexpectedly, the account’s applications \n      or services may be degraded or disrupted.\n    \n    \n      If there is a use case where service quotas impact an application’s performance \n      and they cannot be adjusted to required needs, contact Support to see if \n      there are mitigations. For more detail on adjusting fixed quotas, see \n      REL01-BP03 Accommodate fixed service quotas and constraints\n  through architecture.\n    \n    \n      There are a number of AWS services and tools to help monitor and manage Service Quotas. \n      The service and tools should be leveraged to provide automated or manual checks of quota levels.\n    \n    \n       \n       \n       \n    \n        \n          AWS Trusted Advisor offers a service quota check that displays your usage and quotas \n          for some aspects of some services. It can aid in identifying services that are near quota. \n        \n      \n        \n          AWS Management Console provides methods to display services quota values, manage, \n          request new quotas, monitor status of quota requests, and display history of quotas.\n        \n      \n        \n          AWS CLI and CDKs offer programmatic methods to automatically manage and monitor service quota levels and usage.\n        \n      \n    \n      Implementation steps\n    \n    \n      For Service Quotas:\n    \n    \n       \n       \n       \n       \n       \n       \n       \n       \n       \n       \n       \n       \n    \n        \n          Review AWS Service Quotas.\n        \n      \n        \n          To be aware of your existing service quotas, determine the services (like IAM Access Analyzer) that are used. \n          There are approximately 250 AWS services controlled by service quotas. Then, determine the specific service \n          quota name that might be used within each account and Region. There are approximately 3000 service quota names per Region.\n        \n      \n        \n          Augment this quota analysis with AWS Config to find all \n          AWS resources used in your AWS accounts. \n        \n      \n        \n          Use AWS CloudFormation data to determine your AWS resources used. \n          Look at the resources that were created either in the \n          AWS Management Console or with the list-stack-resources AWS CLI command. \n          You can also see resources configured to be deployed in the template itself. \n        \n      \n        \n          Determine all the services your workload requires by looking at the deployment code.\n        \n      \n        \n          Determine the service quotas that apply. Use the programmatically accessible information from Trusted Advisor and Service Quotas.\n        \n      \n        \n          Establish an automated monitoring method \n          (see REL01-BP02 Manage service quotas across accounts and\n  regions \n          and REL01-BP04 Monitor and manage quotas) \n          to alert and inform if services quotas are near or have reached their limit.\n        \n      \n        \n          Establish an automated and programmatic method to check if a service quota has \n          been changed in one region but not in other regions in the same account \n          (see REL01-BP02 Manage service quotas across accounts and\n  regions \n          and REL01-BP04 Monitor and manage quotas). \n        \n      \n        \n          Automate scanning application logs and metrics to determine if there \n          are any quota or service constraint errors. If these errors are present, \n          send alerts to the monitoring system.\n        \n      \n        \n          Establish engineering procedures to calculate the required change in quota \n          (see REL01-BP05 Automate quota management) \n          once it has been identified that larger quotas are required for specific services.\n        \n      \n        \n          Create a provisioning and approval workflow to request changes in service quota. \n          This should include an exception workflow in case of request deny or partial approval.\n        \n      \n        \n          Create an engineering method to review service quotas prior to \n          provisioning and using new AWS services before rolling out to \n          production or loaded environments. (for example, load testing account).\n        \n      \n    \n      For service constraints:\n    \n    \n       \n       \n       \n       \n    \n        \n          Establish monitoring and metrics methods to alert for resources reading close \n          to their resource constraints. Leverage CloudWatch as appropriate for metrics \n          or log monitoring.\n        \n      \n        \n          Establish alert thresholds for each resource that has a constraint that is \n          meaningful to the application or system.\n        \n      \n        \n          Create workflow and infrastructure management procedures to change the \n          resource type if the constraint is near utilization. This workflow should \n          include load testing as a best practice to verify that new type is the \n          correct resource type with the new constraints.\n        \n      \n        \n          Migrate identified resource to the recommended new resource type, using \n          existing procedures and processes.\n        \n      \n    \n   \n\n  Resources\n    \n      Related best practices:\n    \n    \n       \n       \n       \n       \n       \n       \n       \n       \n       \n       \n    \n        \n          REL01-BP02 Manage service quotas across accounts and\n  regions\n        \n      \n        \n          REL01-BP03 Accommodate fixed service quotas and constraints\n  through architecture\n        \n      \n        \n          REL01-BP04 Monitor and manage quotas\n        \n      \n        \n          REL01-BP05 Automate quota management\n        \n      \n        \n          REL01-BP06 Ensure that a sufficient gap exists between the\n  current quotas and the maximum usage to accommodate failover\n        \n      \n        \n          REL03-BP01 Choose how to segment your workload\n        \n      \n        \n          REL10-BP01 Deploy the workload to multiple locations\n        \n      \n        \n          REL11-BP01 Monitor all components of the workload to detect\n  failures\n        \n      \n        \n          REL11-BP03 Automate healing on all layers\n        \n      \n        \n          REL12-BP04 Test resiliency using chaos engineering\n        \n      \n\n      \n    \n      Related documents:\n    \n    \n       \n       \n       \n       \n       \n       \n       \n       \n       \n       \n       \n       \n       \n       \n       \n       \n       \n       \n       \n       \n    \n        AWS Well-Architected Framework’s Reliability Pillar: Availability\n        \n      \n        \n          AWS Service Quotas (formerly referred to as service limits)\n        \n      \n        \n          AWS Trusted Advisor Best Practice Checks (see the Service Limits\n          section)\n        \n      \n        \n          AWS limit monitor on AWS answers\n        \n      \n        \n          Amazon EC2 Service Limits\n        \n      \n        \n          What\n          is Service Quotas?\n        \n      \n        \n         How to Request Quota Increase\n        \n      \n        \n          Service endpoints and quotas\n        \n      \n        \n          Service Quotas User Guide\n        \n      \n        \n          Quota Monitor for AWS\n      \n        AWS Fault Isolation Boundaries\n        \n      \n        \n          Availability with redundancy\n        \n      \n        AWS for Data\n        \n      \n        \n          What is Continuous Integration?\n        \n      \n        \n          What is Continuous Delivery?\n        \n      \n        \n          APN Partner: partners that can help with configuration management\n        \n      \n        \n          Managing the account lifecycle in account-per-tenant SaaS environments on AWS\n      \n        \n          Managing and monitoring API throttling in your workloads\n        \n      \n        \n          View AWS Trusted Advisor recommendations at scale with AWS Organizations\n      \n        \n          Automating Service Limit Increases and Enterprise Support with AWS Control Tower\n      \n    \n      Related videos:\n    \n    \n       \n       \n       \n    \n        \n          AWS Live\n          re:Inforce 2019 - Service Quotas\n        \n      \n        \n          View and Manage Quotas for AWS Services Using Service Quotas\n        \n      \n        AWS IAM Quotas Demo\n        \n      \n    \n      Related tools:\n    \n    \n       \n       \n       \n       \n       \n       \n       \n       \n       \n       \n       \n    \n        \n          Amazon CodeGuru Reviewer\n        \n      \n        AWS CodeDeploy\n      \n        AWS CloudTrail\n      \n        \n          Amazon CloudWatch\n        \n      \n        \n          Amazon EventBridge\n        \n      \n        \n          Amazon DevOps Guru\n        \n      \n        AWS Config\n      \n        AWS Trusted Advisor\n      \n        AWS CDK\n        \n      \n        AWS Systems Manager\n      \n        AWS Marketplace\n      \n  \u003cdiv\u003e\u003cdiv\u003e\u003cdiv\u003e\u003cdiv id=\"js_error_message\"\u003e\u003cp\u003e\u003cimg src=\"https://d1ge0kk1l5kms0.cloudfront.net/images/G/01/webservices/console/warning.png\" alt=\"Warning\" /\u003e \u003cstrong\u003eJavascript is disabled or is unavailable in your browser.\u003c/strong\u003e\u003c/p\u003e\u003cp\u003eTo use the Amazon Web Services Documentation, Javascript must be enabled. Please refer to your browser's Help pages for instructions.\u003c/p\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003eDocument ConventionsREL 1. How do you manage Service Quotas and constraints?REL01-BP02 Manage service quotas across accounts and\n  regions",
  "REL01-BP02 Manage service quotas across accounts and\n  regions\n    If you are using multiple accounts or Regions, \n    request the appropriate quotas in all environments \n    in which your production workloads run.\n  \n    Desired outcome: Services and applications \n    should not be affected by service quota exhaustion for configurations that \n    span accounts or Regions or that have resilience designs using zone, Region, \n    or account failover.\n  \n    Common anti-patterns:\n  \n     \n     \n     \n     \n     \n     \n  \n      Allowing resource usage in one isolation Region \n        to grow with no mechanism to maintain capacity in the \n        other ones. \n    \n      \n        Manually setting all quotas independently in isolation Regions.\n      \n    \n      \n        Not considering the effect of resiliency architectures \n        (like active or passive) in future quota needs during a \n        degradation in the non-primary Region.\n      \n    \n      \n        Not evaluating quotas regularly and making necessary \n        changes in every Region and account the workload runs.\n      \n    \n      \n        Not leveraging quota request templates \n        to request increases across multiple Regions and accounts. \n      \n    \n      \n        Not updating service quotas due to incorrectly \n        thinking that increasing quotas has cost implications \n        like compute reservation requests.\n      \n    \n    Benefits of establishing this best\n      practice: Verifying that you can handle your \n    current load in secondary regions or accounts if regional \n    services become unavailable. This can help reduce the number \n    of errors or levels of degradations that occur during region \n    loss.\n  \n    Level of risk exposed if this best practice\n    is not established: High\n  \n\n  Implementation guidance\n    \n      Service quotas are tracked per account. Unless otherwise \n      noted, each quota is AWS Region-specific. In addition to \n      the production environments, also manage quotas in all \n      applicable non-production environments so that testing \n      and development are not hindered. Maintaining a high \n      degree of resiliency requires that service quotas are \n      assessed continually (whether automated or manual).\n    \n    \n      With more workloads spanning Regions due to the implementation \n      of designs using Active/Active, \n      Active/Passive – Hot, \n      Active/Passive-Cold, and \n      Active/Passive-Pilot Light approaches, \n      it is essential to understand all Region and account quota levels. \n      Past traffic patterns are not always a good indicator if the \n      service quota is set correctly. \n    \n    \n      Equally important, the service quota name limit is not always \n      the same for every Region. In one Region, the value could be \n      five, and in another region the value could be ten.  Management \n      of these quotas must span all the same services, accounts, and \n      Regions to provide consistent resilience under load.\n    \n    \n      Reconcile all the service quota differences across different \n      Regions (Active Region or Passive Region) and create processes \n      to continually reconcile these differences. The testing plans \n      of passive Region failovers are rarely scaled to peak active \n      capacity, meaning that game day or table top exercises can \n      fail to find differences in service quotas between Regions \n      and also then maintain the correct limits.\n    \n    \n      Service quota drift, the condition where service quota limits \n      for a specific named quota is changed in one Region and not \n      all Regions, is very important to track and assess. Changing \n      the quota in Regions with traffic or potentially could carry \n      traffic should be considered.\n    \n    \n       \n       \n    \n         Select relevant accounts and Regions based on your service requirements, latency,\n          regulatory, and disaster recovery (DR) requirements. \n      \n        Identify service quotas across all relevant accounts, Regions, and\n        Availability Zones. The limits are scoped to account and Region. These\n        values should be compared for differences.\n      \n      \n    \n      Implementation steps\n    \n    \n       \n       \n       \n       \n    \n        \n          Review Service Quotas values that might have \n          breached beyond the a risk level of usage. \n          AWS Trusted Advisor provides alerts for 80% and 90% \n          threshold breaches. \n        \n      \n        \n          Review values for service quotas in any Passive \n          Regions (in an Active/Passive design). Verify \n          that load will successfully run in secondary \n          Regions in the event of a failure in the primary \n          Region. \n        \n      \n        \n          Automate assessing if any service quota drift has \n          occurred between Regions in the same account and \n          act accordingly to change the limits.\n        \n      \n        \n          If the customer Organizational Units (OU) are structured \n          in the supported manner, service quota templates should \n          be updated to reflect changes in any quotas that should \n          be applied to multiple Regions and accounts. \n        \n        \n           \n           \n        \n            \n              Create a template and associate Regions to the quota change.\n            \n          \n            \n              Review all existing service quota templates for any changes \n              required (Region, limits, and accounts).\n            \n          \n      \n  \n    \n   \n\n  Resources\n    \n      Related best practices:\n    \n    \n       \n       \n       \n       \n       \n       \n       \n       \n       \n       \n    \n        \n          REL01-BP01 Aware of service quotas and constraints\n        \n      \n        \n          REL01-BP03 Accommodate fixed service quotas and constraints\n  through architecture\n        \n      \n        \n          REL01-BP04 Monitor and manage quotas\n        \n      \n        \n          REL01-BP05 Automate quota management\n        \n      \n        \n          REL01-BP06 Ensure that a sufficient gap exists between the\n  current quotas and the maximum usage to accommodate failover\n        \n      \n        \n          REL03-BP01 Choose how to segment your workload\n        \n      \n        \n          REL10-BP01 Deploy the workload to multiple locations\n        \n      \n        \n          REL11-BP01 Monitor all components of the workload to detect\n  failures\n        \n      \n        \n          REL11-BP03 Automate healing on all layers\n        \n      \n        \n          REL12-BP04 Test resiliency using chaos engineering\n        \n      \n\n      \n    \n      Related documents:\n    \n    \n       \n       \n       \n       \n       \n       \n       \n       \n       \n       \n       \n       \n       \n       \n       \n       \n       \n       \n       \n       \n    \n        AWS Well-Architected Framework’s Reliability Pillar: Availability\n        \n      \n        \n          AWS Service Quotas (formerly referred to as service limits)\n        \n      \n        \n          AWS Trusted Advisor Best Practice Checks (see the Service Limits\n            section)\n        \n      \n        \n          AWS limit monitor on AWS answers\n        \n      \n        \n          Amazon EC2 Service Limits\n        \n      \n        \n          What\n            is Service Quotas?\n        \n      \n        \n          How to Request Quota Increase\n        \n      \n        \n          Service endpoints and quotas\n        \n      \n        \n          Service Quotas User Guide\n        \n      \n        \n          Quota Monitor for AWS\n      \n        AWS Fault Isolation Boundaries\n        \n      \n        \n          Availability with redundancy\n        \n      \n        AWS for Data\n        \n      \n        \n          What is Continuous Integration?\n        \n      \n        \n          What is Continuous Delivery?\n        \n      \n        \n          APN Partner: partners that can help with configuration management\n        \n      \n        \n          Managing the account lifecycle in account-per-tenant SaaS environments on AWS\n      \n        \n          Managing and monitoring API throttling in your workloads\n        \n      \n        \n          View AWS Trusted Advisor recommendations at scale with AWS Organizations\n      \n        \n          Automating Service Limit Increases and Enterprise Support with AWS Control Tower\n      \n    \n      Related videos:\n    \n    \n       \n       \n       \n    \n        \n          AWS Live\n            re:Inforce 2019 - Service Quotas\n        \n      \n        \n          View and Manage Quotas for AWS Services Using Service Quotas\n        \n      \n        AWS IAM Quotas Demo\n        \n      \n    \n      Related services:\n    \n    \n       \n       \n       \n       \n       \n       \n       \n       \n       \n       \n       \n    \n        \n          Amazon CodeGuru Reviewer\n        \n      \n        AWS CodeDeploy\n      \n        AWS CloudTrail\n      \n        \n          Amazon CloudWatch\n        \n      \n        \n          Amazon EventBridge\n        \n      \n        \n          Amazon DevOps Guru\n        \n      \n        AWS Config\n      \n        AWS Trusted Advisor\n      \n        AWS CDK\n        \n      \n        AWS Systems Manager\n      \n        AWS Marketplace\n      \n  \u003cdiv\u003e\u003cdiv\u003e\u003cdiv\u003e\u003cdiv id=\"js_error_message\"\u003e\u003cp\u003e\u003cimg src=\"https://d1ge0kk1l5kms0.cloudfront.net/images/G/01/webservices/console/warning.png\" alt=\"Warning\" /\u003e \u003cstrong\u003eJavascript is disabled or is unavailable in your browser.\u003c/strong\u003e\u003c/p\u003e\u003cp\u003eTo use the Amazon Web Services Documentation, Javascript must be enabled. Please refer to your browser's Help pages for instructions.\u003c/p\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003eDocument ConventionsREL01-BP01 Aware of service quotas and constraintsREL01-BP03 Accommodate fixed service quotas and constraints\n  through architecture",
  "REL01-BP03 Accommodate fixed service quotas and constraints\n  through architectureBe aware of unchangeable service quotas, service constraints, and physical resource limits. Design architectures for applications and services to prevent these limits from impacting reliability.Examples include network bandwidth, serverless function invocation payload size, throttle burst rate for of an API gateway, and concurrent user connections to a database.\n    Desired outcome: The application or service performs as expected under normal and high traffic conditions. They have been designed to work within the limitations for that resource’s fixed constraints or service quotas.\n  \n    Common anti-patterns:\n  \n     \n     \n     \n     \n  \n      Choosing a design that uses a resource of a service, unaware that there are design constraints that will cause this design to fail as you scale.\n    \n      Performing benchmarking that is unrealistic and will reach service fixed quotas during the testing. For example, running tests at a burst limit but for an extended amount of time.\n    \n      \n        Choosing a design that cannot scale or be modified if fixed service quotas are to be exceeded. For example, an SQS payload size of 256KB.\n      \n    \n      \n        Observability has not been designed and implemented to monitor and alert on thresholds for service quotas that might be at risk during high traffic events\n      \n    \n    Benefits of establishing this best\n      practice:  Verifying that the application will run under all projected services load levels without disruption or degradation.\n  \n    Level of risk exposed if this best practice\n    is not established: Medium\n  \n\n  Implementation guidance\n    \n      Unlike soft service quotas or resources that be replaced with higher capacity units, AWS services’ fixed quotas cannot be changed. This means that all these type of AWS services must be evaluated for potential hard capacity limits when used in an application design. \n    \n    \n      Hard limits are shown in the Service Quotas console. If the columns shows ADJUSTABLE = No, the service has a hard limit. Hard limits are also shown in some resources configuration pages. For example, Lambda has specific hard limits that cannot be adjusted.\n    \n    \n      As an example, when designing a python application to run in a Lambda function, the application should be evaluated to determine if there is any chance of Lambda running longer than 15 minutes. If the code may run more than this service quota limit, alternate technologies or designs must be considered. If this limit is reached after production deployment, the application will suffer degradation and disruption until it can be remediated. Unlike soft quotas, there is no method to change to these limits even under emergency Severity 1 events. \n    \n    \n      Once the application has been deployed to a testing environment, strategies should be used to find if any hard limits can be reached. Stress testing, load testing, and chaos testing should be part of the introduction test plan. \n    \n    \n      Implementation steps\n    \n    \n       \n       \n       \n       \n       \n    \n        \n          Review the complete list of AWS services that could be used in the application design phase.\n        \n      \n        \n          Review the soft quota limits and hard quota limits for all these services. Not all limits are shown in the Service Quotas console. Some services describe these limits in alternate locations.\n        \n      \n        \n          As you design your application, review your workload’s business and technology drivers, such as business outcomes, use case, dependent systems, availability targets, and disaster recovery objects. Let your business and technology drivers guide the process to identify the distributed system that is right for your workload. \n        \n      \n        \n          Analyze service load across Regions and accounts. Many hard limits are regionally based for services. However, some limits are account based.\n        \n      \n        \n          Analyze resilience architectures for resource usage during a zonal failure and Regional failure. In the progression of multi-Region designs using active/active, active/passive – hot, active/passive - cold, and active/passive - pilot light approaches, these failure cases will cause higher usage. This creates a potential use case for hitting hard limits.\n        \n      \n   \n    \n    Resources\n    \n      Related best practices:\n    \n    \n       \n       \n       \n       \n       \n       \n       \n       \n       \n       \n    \n        \n          REL01-BP01 Aware of service quotas and constraints\n        \n      \n        \n          REL01-BP02 Manage service quotas across accounts and\n  regions\n        \n      \n        \n          REL01-BP04 Monitor and manage quotas\n        \n      \n        \n          REL01-BP05 Automate quota management\n        \n      \n        \n          REL01-BP06 Ensure that a sufficient gap exists between the\n  current quotas and the maximum usage to accommodate failover\n        \n      \n        \n          REL03-BP01 Choose how to segment your workload\n        \n      \n        \n          REL10-BP01 Deploy the workload to multiple locations\n        \n      \n        \n          REL11-BP01 Monitor all components of the workload to detect\n  failures\n        \n      \n        \n          REL11-BP03 Automate healing on all layers\n        \n      \n        \n          REL12-BP04 Test resiliency using chaos engineering\n        \n      \n    \n    \n    \n      Related documents:\n    \n    \n       \n       \n       \n       \n       \n       \n       \n       \n       \n       \n       \n       \n       \n       \n       \n       \n       \n       \n       \n       \n       \n    \n        AWS Well-Architected Framework’s Reliability Pillar: Availability\n        \n      \n        \n          AWS Service Quotas (formerly referred to as service limits)\n        \n      \n        \n          AWS Trusted Advisor Best Practice Checks (see the Service Limits\n            section)\n        \n      \n        \n          AWS limit monitor on AWS answers\n        \n      \n        \n          Amazon EC2 Service Limits\n        \n      \n        \n          What\n            is Service Quotas?\n        \n      \n        \n          How to Request Quota Increase\n        \n      \n        \n          Service endpoints and quotas\n        \n      \n        \n          Service Quotas User Guide\n        \n      \n        \n          Quota Monitor for AWS\n      \n        AWS Fault Isolation Boundaries\n        \n      \n        \n          Availability with redundancy\n        \n      \n        AWS for Data\n        \n      \n        \n          What is Continuous Integration?\n        \n      \n        \n          What is Continuous Delivery?\n        \n      \n        \n          APN Partner: partners that can help with configuration management\n        \n      \n        \n          Managing the account lifecycle in account-per-tenant SaaS environments on AWS\n      \n        \n          Managing and monitoring API throttling in your workloads\n        \n      \n        \n          View AWS Trusted Advisor recommendations at scale with AWS Organizations\n      \n        \n          Automating Service Limit Increases and Enterprise Support with AWS Control Tower\n      \n        \n          Actions, resources, and condition keys for Service Quotas\n        \n      \n    \n      Related videos:\n    \n    \n       \n       \n       \n       \n    \n        \n          AWS Live\n            re:Inforce 2019 - Service Quotas\n        \n      \n        \n          View and Manage Quotas for AWS Services Using Service Quotas\n        \n      \n        AWS IAM Quotas Demo\n        \n      \n        AWS re:Invent 2018: Close Loops and Opening Minds: How to Take Control of Systems, Big and Small\n        \n      \n    \n      Related tools:\n    \n    \n      \n       \n       \n       \n       \n       \n       \n       \n       \n       \n       \n    \n        AWS CodeDeploy\n      \n        AWS CloudTrail\n      \n        \n          Amazon CloudWatch\n        \n      \n        \n          Amazon EventBridge\n        \n      \n        \n          Amazon DevOps Guru\n        \n      \n        AWS Config\n      \n        AWS Trusted Advisor\n      \n        AWS CDK\n        \n      \n        AWS Systems Manager\n      \n        AWS Marketplace\n      \n  \u003cdiv\u003e\u003cdiv\u003e\u003cdiv\u003e\u003cdiv id=\"js_error_message\"\u003e\u003cp\u003e\u003cimg src=\"https://d1ge0kk1l5kms0.cloudfront.net/images/G/01/webservices/console/warning.png\" alt=\"Warning\" /\u003e \u003cstrong\u003eJavascript is disabled or is unavailable in your browser.\u003c/strong\u003e\u003c/p\u003e\u003cp\u003eTo use the Amazon Web Services Documentation, Javascript must be enabled. Please refer to your browser's Help pages for instructions.\u003c/p\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003eDocument ConventionsREL01-BP02 Manage service quotas across accounts and\n  regionsREL01-BP04 Monitor and manage quotas",
  "REL01-BP04 Monitor and manage quotas\n    Evaluate your potential usage and increase your quotas\n    appropriately, allowing for planned growth in usage.\n  \n    Desired outcome: Active and automated systems that manage and monitor have been deployed. These operations solutions ensure that quota usage thresholds are nearing being reached. These would be proactively remediated by requested quota changes.\n  \n    Common anti-patterns:\n  \n     \n     \n     \n     \n     \n     \n     \n     \n     \n  \n      Not configuring monitoring to check for service quota thresholds\n    \n      Not configuring monitoring for hard limits, even though those values cannot be changed.\n    \n      \n        Assuming that amount of time required to request and secure a soft quota change is immediate or a short period.\n      \n    \n      \n        Configuring alarms for when service quotas are being approached, but having no process on how to respond to an alert.\n      \n    \n      \n        Only configuring alarms for services supported by AWS Service Quotas and not monitoring other AWS services.\n      \n    \n      \n        Not considering quota management for multiple Region resiliency designs, like active/active, active/passive – hot, active/passive - cold, and active/passive - pilot light approaches. \n      \n    \n      \n        Not assessing quota differences between Regions.\n      \n    \n      \n        Not assessing the needs in every Region for a specific quota increase request.\n      \n    \n      \n        Not leveraging templates for multi-Region quota management. \n      \n    \n    Benefits of establishing this best\n    practice: Automatic tracking of the AWS Service Quotas\n    and monitoring your usage against those quotas will allow you to see\n    when you are approaching a quota limit. You can also use this\n    monitoring data to help limit any degradations due to quota exhaustion.\n  \n    Level of risk exposed if this best practice\n    is not established: Medium\n  \n\n  Implementation guidance\n    \n      For supported services, you can monitor your quotas by configuring various different services that can assess and then send alerts or alarms. This can aid in monitoring usage and can alert you to approaching quotas. These alarms can be invoked from AWS Config, Lambda functions, Amazon CloudWatch, or from AWS Trusted Advisor.  You can also use metric filters on CloudWatch Logs to search and extract patterns in logs to determine if usage is approaching quota thresholds.\n    \n    \n      Implementation steps\n    \n    \n      For monitoring:\n    \n    \n       \n       \n       \n       \n       \n       \n    \n        \n          Capture current resource consumption (for example, buckets or instances). Use service API operations, such as the Amazon EC2 DescribeInstances API, to collect current resource consumption.\n        \n      \n        \n          Capture your current quotas that are essential and applicable to the services using:\n        \n        \n           \n           \n           \n           \n           \n           \n        \n            \n              AWS Service Quotas\n            \n          \n            \n              AWS Trusted Advisor\n            \n          \n            \n              AWS documentation\n            \n          \n            \n              AWS service-specific pages\n            \n          \n            \n              AWS Command Line Interface (AWS CLI)\n            \n          \n            \n              AWS Cloud Development Kit (AWS CDK)\n            \n          \n      \n        \n          Use AWS Service Quotas, an AWS service that helps you manage your quotas for over 250 AWS services from one location.\n        \n      \n        \n          Use Trusted Advisor service limits to monitor your current service limits at various thresholds. \n        \n      \n        \n          Use the service quota history (console or AWS CLI) to check on regional increases.\n        \n      \n        \n          Compare service quota changes in each Region and each account to create equivalency, if required.\n        \n      \n    \n      For management:\n    \n    \n       \n       \n       \n       \n       \n    \n        \n          Automated: Set up an AWS Config custom rule to scan service quotas across Regions and compare for differences.\n        \n      \n        \n          Automated: Set up a scheduled Lambda function to scan service quotas across Regions and compare for differences.\n        \n      \n        \n          Manual:  Scan services quota through AWS CLI, API, or AWS Console to scan service quotas across Regions and compare for differences. Report the differences.\n        \n      \n        \n          If differences in quotas are identified between Regions, request a quota change, if required. \n        \n      \n        \n          Review the result of all requests.\n        \n      \n   \n    \n    Resources\n    \n      Related best practices:\n    \n    \n       \n       \n       \n       \n       \n       \n       \n       \n       \n       \n    \n        \n          REL01-BP01 Aware of service quotas and constraints\n        \n      \n        \n          REL01-BP02 Manage service quotas across accounts and\n  regions\n        \n      \n        \n          REL01-BP03 Accommodate fixed service quotas and constraints\n  through architecture\n        \n      \n        \n          REL01-BP05 Automate quota management\n        \n      \n        \n          REL01-BP06 Ensure that a sufficient gap exists between the\n  current quotas and the maximum usage to accommodate failover\n        \n      \n        \n          REL03-BP01 Choose how to segment your workload\n        \n      \n        \n          REL10-BP01 Deploy the workload to multiple locations\n        \n      \n        \n          REL11-BP01 Monitor all components of the workload to detect\n  failures\n        \n      \n        \n          REL11-BP03 Automate healing on all layers\n        \n      \n        \n          REL12-BP04 Test resiliency using chaos engineering\n        \n      \n    \n      Related documents:\n    \n    \n       \n       \n       \n       \n       \n       \n       \n       \n       \n       \n       \n       \n       \n       \n       \n       \n       \n       \n       \n       \n       \n    \n        AWS Well-Architected Framework’s Reliability Pillar: Availability\n        \n      \n        \n          AWS Service Quotas (formerly referred to as service limits)\n        \n      \n        \n          AWS Trusted Advisor Best Practice Checks (see the Service Limits\n            section)\n        \n      \n        \n          AWS limit monitor on AWS answers\n        \n      \n        \n          Amazon EC2 Service Limits\n        \n      \n        \n          What\n            is Service Quotas?\n        \n      \n        \n          How to Request Quota Increase\n        \n      \n        \n          Service endpoints and quotas\n        \n      \n        \n          Service Quotas User Guide\n        \n      \n        \n          Quota Monitor for AWS\n      \n        AWS Fault Isolation Boundaries\n        \n      \n        \n          Availability with redundancy\n        \n      \n        AWS for Data\n        \n      \n        \n          What is Continuous Integration?\n        \n      \n        \n          What is Continuous Delivery?\n        \n      \n        \n          APN Partner: partners that can help with configuration management\n        \n      \n        \n          Managing the account lifecycle in account-per-tenant SaaS environments on AWS\n      \n        \n          Managing and monitoring API throttling in your workloads\n        \n      \n        \n          View AWS Trusted Advisor recommendations at scale with AWS Organizations\n      \n        \n          Automating Service Limit Increases and Enterprise Support with AWS Control Tower\n      \n        \n          Actions, resources, and condition keys for Service Quotas\n        \n      \n    \n      Related videos:\n    \n    \n       \n       \n       \n       \n    \n        \n          AWS Live\n            re:Inforce 2019 - Service Quotas\n        \n      \n        \n          View and Manage Quotas for AWS Services Using Service Quotas\n        \n      \n        AWS IAM Quotas Demo\n        \n      \n        AWS re:Invent 2018: Close Loops and Opening Minds: How to Take Control of Systems, Big and Small\n        \n      \n    \n      Related tools:\n    \n    \n      \n       \n       \n       \n       \n       \n       \n       \n       \n       \n       \n    \n        AWS CodeDeploy\n      \n        AWS CloudTrail\n      \n        \n          Amazon CloudWatch\n        \n      \n        \n          Amazon EventBridge\n        \n      \n        \n          Amazon DevOps Guru\n        \n      \n        AWS Config\n      \n        AWS Trusted Advisor\n      \n        AWS CDK\n        \n      \n        AWS Systems Manager\n      \n        AWS Marketplace\n      \n  \u003cdiv\u003e\u003cdiv\u003e\u003cdiv\u003e\u003cdiv id=\"js_error_message\"\u003e\u003cp\u003e\u003cimg src=\"https://d1ge0kk1l5kms0.cloudfront.net/images/G/01/webservices/console/warning.png\" alt=\"Warning\" /\u003e \u003cstrong\u003eJavascript is disabled or is unavailable in your browser.\u003c/strong\u003e\u003c/p\u003e\u003cp\u003eTo use the Amazon Web Services Documentation, Javascript must be enabled. Please refer to your browser's Help pages for instructions.\u003c/p\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003eDocument ConventionsREL01-BP03 Accommodate fixed service quotas and constraints\n  through architectureREL01-BP05 Automate quota management",
  "REL01-BP05 Automate quota management\n    Service quotas, also referred to as limits in AWS services, are the\n    maximum values for the resources in your AWS account. Each AWS\n    service defines a set of quotas and their default values. To provide\n    your workload access to all the resources it needs, you might need\n    to increase your service quota values.\n  \n    Growth in workload consumption of AWS resources can threaten\n    workload stability and impact the user experience if quotas are\n    exceeded. Implement tools to alert you when your workload approaches\n    the limits and consider creating quota increase requests\n    automatically.\n  \n    Desired outcome: Quotas are\n    appropriately configured for the workloads running in each AWS account and Region.\n  \n    Common anti-patterns:\n  \n     \n     \n     \n     \n  \n      \n        You fail to consider and adjust quotas appropriately to meet\n        workload requirements.\n      \n    \n      \n        You track quotas and usage using methods that can become\n        outdated, such as with spreadsheets.\n      \n    \n      \n        You only update service limits on periodic schedules.\n      \n    \n      \n        Your organization lacks operational processes to review existing\n        quotas and request service quota increases when necessary.\n      \n    \n    Benefits of establishing this best\n      practice:\n  \n     \n     \n  \n      \n        Enhanced workload resiliency: You prevent errors caused by\n        exceeding AWS resource quotas.\n      \n    \n      \n        Simplified disaster recovery: You can reuse automated quota\n        management mechanisms built in the primary Region during DR\n        setup in another AWS Region.\n      \n    \n    Level of risk exposed if this best practice\n      is not established: Medium\n  \n    \n    Implementation guidance\n    \n    \n    \n      View current quotas and track ongoing quota consumption through\n      mechanisms such as AWS Service Quotas console, AWS Command Line Interface (AWS CLI), and AWS SDKs. You can also integrate your\n      configuration management databases (CMDB) and IT service\n      management (ITSM) systems with the AWS Service Quota APIs.\n    \n    \n      Generate automated alerts if quota usage reaches your defined\n      thresholds, and define a process for submitting quota increase\n      requests when you receive alerts. If the underlying workload is\n      critical to your business, you can automate quota increase\n      requests, but carefully test the automation to avoid the risk of\n      runaway action such as a growth feedback loop.\n    \n    \n      Smaller quota increases are often automatically approved. Larger\n      quota requests may need to be manually processed by AWS support\n      and can take additional time to review and process. Allow for\n      additional time to process multiple requests or large increase\n      requests.\n    \n     \n      \n      Implementation steps\n      \n      \n      \n         \n         \n         \n         \n         \n      \n          \n            Implement automated monitoring of service quotas, and issue\n            alerts if your workload's resource utilization growth\n            approaches your quota limits. For example,\n            Quota\n              Monitor for AWS can provide automated monitoring of\n            service quotas. This tool integrates with AWS Organizations\n            and deploys using Cloudformation StackSets so that new\n            accounts are automatically monitored on creation.\n          \n        \n          \n            Use features such as\n            Service Quotas request templates or\n            AWS Control Tower to simplify Service Quotas setup for\n            new accounts.\n          \n        \n          \n            Build dashboards of your current service quota use across\n            all AWS accounts and regions and reference them as necessary\n            to prevent exceeding your quotas.\n            Trusted\n              Advisor Organizational (TAO) Dashboard, part of the\n            Cloud\n              Intelligence Dashboards, can get you quickly started\n            with such a dashboard.\n          \n        \n          \n            Track service limit increase requests.\n            Consolidated\n              Insights from Multiple Accounts(CIMA) can provide an\n            Organization-level view of all your requests.\n          \n        \n          \n            Test alert generation and any quota increase request\n            automation by setting lower quota thresholds in\n            non-production accounts. Do not conduct these tests in a\n            production account.\n          \n        \n     \n   \n    \n    Resources\n    \n    \n    \n      Related best practices:\n    \n    \n       \n    \n        \n          OPS10-BP07\n            Automate responses to events\n        \n      \n    \n      Related documents:\n    \n    \n       \n       \n       \n       \n       \n       \n       \n    \n        \n          APN\n            Partner: partners that can help with configuration\n            management\n        \n      \n        \n          AWS Marketplace: CMDB products that help track limits\n        \n      \n        \n          AWS           Service Quotas (formerly referred to as service limits)\n        \n      \n        \n          AWS Trusted Advisor Best Practice Checks (see the Service Limits\n            section)\n        \n      \n        \n          Quota\n            Monitor Solution on AWS - AWS Solution\n        \n      \n        \n          What\n            is Service Quotas?\n        \n      \n        \n          What\n            is Service Quotas request templates?\n        \n      \n    \n      Related videos:\n    \n    \n       \n       \n    \n        \n          AWS Live\n            re:Inforce 2019 - Service Quotas\n        \n      \n        \n          Automating\n            Service Limit Increases and Enterprise Support with AWS Control Tower\n        \n      \n    \n      Related tools:\n    \n    \n       \n    \n        \n          Quota\n            Monitor for AWS\n        \n      \n  \u003cdiv\u003e\u003cdiv\u003e\u003cdiv\u003e\u003cdiv id=\"js_error_message\"\u003e\u003cp\u003e\u003cimg src=\"https://d1ge0kk1l5kms0.cloudfront.net/images/G/01/webservices/console/warning.png\" alt=\"Warning\" /\u003e \u003cstrong\u003eJavascript is disabled or is unavailable in your browser.\u003c/strong\u003e\u003c/p\u003e\u003cp\u003eTo use the Amazon Web Services Documentation, Javascript must be enabled. Please refer to your browser's Help pages for instructions.\u003c/p\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003eDocument ConventionsREL01-BP04 Monitor and manage quotasREL01-BP06 Ensure that a sufficient gap exists between the\n  current quotas and the maximum usage to accommodate failover",
  "REL01-BP06 Ensure that a sufficient gap exists between the\n  current quotas and the maximum usage to accommodate failoverThis article explains how to maintain space between the resource quota and your usage, and how it can benefit your organization. After you finish using a resource, the usage quota may continue to account for that resource. This can result in a failing or inaccessible resource. Prevent resource failure by verifying that your quotas cover the overlap of inaccessible resources and their replacements. Consider cases like network failure, Availability Zone failure, or Region failures when calculating this gap.\n    Desired outcome: Small or large failures in resources or resource accessibility can be covered within the current service thresholds. Zone failures, network failures, or even Regional failures have been considered in the resource planning. \n  \n    Common anti-patterns:\n  \n     \n     \n     \n     \n  \n      \n        Setting service quotas based on current needs without accounting for failover scenarios.\n      \n    \n      \n        Not considering the principals of static stability when calculating the peak quota for a service.\n      \n    \n      \n        Not considering the potential of inaccessible resources in calculating total quota needed for each Region.\n      \n    \n      \n        Not considering AWS service fault isolation boundaries for some services and their potential abnormal usage patterns. \n      \n    \n    Benefits of establishing this best\n      practice: When service disruption events impact application availability, use the cloud to implement strategies to recover from these events. An example strategy is creating additional resources to replace inaccessible resources to accommodate failover conditions without exhausting your service limit.\n  \n    Level of risk exposed if this best practice\n    is not established: Medium\n  \n\n  Implementation guidance\n    \n      When evaluating a quota limit, consider failover cases that might occur due to some degradation. Consider the following failover cases.\n    \n    \n       \n       \n       \n       \n       \n       \n    \n        \n          A disrupted or inaccessible VPC.\n        \n      \n        \n          An inaccessible subnet.\n        \n      \n        \n          A degraded Availability Zone that impacts resource accessibility.\n        \n      \n        \n          Networking routes or ingress and egress points are blocked or changed.\n        \n      \n        \n          A degraded Region that impacts resource accessibility.\n        \n      \n        \n          A subset of resources affected by a failure in a Region or an Availability Zone.\n        \n      \n    \n      The decision to failover is unique for each situation, as the business impact can vary. Address resource capacity planning in the failover location and the resources’ quotas before deciding to failover an application or service.\n    \n    \n      Consider higher than normal peaks of activity when reviewing quotas for each service. These peaks might be related to resources that are inaccessible due to networking or permissions, but are still active. Unterminated active resources count against the service quota limit.\n    \n    \n      Implementation steps\n    \n    \n       \n       \n       \n       \n       \n       \n       \n       \n       \n       \n       \n    \n        \n          Maintain space between your service quota and your maximum usage to accommodate for a failover or loss of accessibility.\n        \n      \n        \n          Determine your service quotas. Account for typical deployment patterns, availability requirements, and consumption growth.\n        \n      \n        \n          Request quota increases if necessary. Anticipate a wait time for the quota increase request.\n        \n      \n        \n          Determine your reliability requirements (also known as your number of nines).\n        \n      \n        \n          Understand potential fault scenarios such as loss of a component, an Availability Zone, or a Region.\n        \n      \n        \n          Establish your deployment methodology (examples include canary, blue/green, red/black, and rolling).\n        \n      \n        \n          Include an appropriate buffer to the current quota limit. An example buffer could be 15%.\n        \n      \n        \n          Include calculations for static stability (Zonal and Regional) where appropriate.\n        \n      \n        \n          Plan consumption growth and monitor your consumption trends.\n        \n      \n        \n          Consider the static stability impact for your most critical workloads. Assess resources\n          conforming to a statically stable system in all Regions and Availability Zones.\n        \n      \n        \n          Consider using On-Demand Capacity Reservations to schedule capacity ahead of any failover. This is a useful strategy to implement for critical business schedules to reduce potential risks of obtaining the correct quantity and type of resources during failover.\n        \n      \n    \n   \n    \n    Resources\n    \n      Related best practices:\n    \n    \n       \n       \n       \n       \n       \n       \n       \n       \n       \n       \n    \n        \n          REL01-BP01 Aware of service quotas and constraints\n        \n      \n        \n          REL01-BP02 Manage service quotas across accounts and\n  regions\n        \n      \n        \n          REL01-BP03 Accommodate fixed service quotas and constraints\n  through architecture\n        \n      \n        \n          REL01-BP04 Monitor and manage quotas\n        \n      \n        \n          REL01-BP05 Automate quota management\n        \n      \n        \n          REL03-BP01 Choose how to segment your workload\n        \n      \n        \n          REL10-BP01 Deploy the workload to multiple locations\n        \n      \n        \n          REL11-BP01 Monitor all components of the workload to detect\n  failures\n        \n      \n        \n          REL11-BP03 Automate healing on all layers\n        \n      \n        \n          REL12-BP04 Test resiliency using chaos engineering\n        \n      \n    \n      Related documents:\n    \n    \n       \n       \n       \n       \n       \n       \n       \n       \n       \n       \n       \n       \n       \n       \n       \n       \n       \n       \n       \n       \n       \n    \n        AWS Well-Architected Framework’s Reliability Pillar: Availability\n        \n      \n        \n          AWS Service Quotas (formerly referred to as service limits)\n        \n      \n        \n          AWS Trusted Advisor Best Practice Checks (see the Service Limits\n            section)\n        \n      \n        \n          AWS limit monitor on AWS answers\n        \n      \n        \n          Amazon EC2 Service Limits\n        \n      \n        \n          What\n            is Service Quotas?\n        \n      \n        \n          How to Request Quota Increase\n        \n      \n        \n          Service endpoints and quotas\n        \n      \n        \n          Service Quotas User Guide\n        \n      \n        \n          Quota Monitor for AWS\n      \n        AWS Fault Isolation Boundaries\n        \n      \n        \n          Availability with redundancy\n        \n      \n        AWS for Data\n        \n      \n        \n          What is Continuous Integration?\n        \n      \n        \n          What is Continuous Delivery?\n        \n      \n        \n          APN Partner: partners that can help with configuration management\n        \n      \n        \n          Managing the account lifecycle in account-per-tenant SaaS environments on AWS\n      \n        \n          Managing and monitoring API throttling in your workloads\n        \n      \n        \n          View AWS Trusted Advisor recommendations at scale with AWS Organizations\n      \n        \n          Automating Service Limit Increases and Enterprise Support with AWS Control Tower\n      \n        \n          Actions, resources, and condition keys for Service Quotas\n        \n      \n    \n      Related videos:\n    \n    \n       \n       \n       \n       \n    \n        \n          AWS Live\n            re:Inforce 2019 - Service Quotas\n        \n      \n        \n          View and Manage Quotas for AWS Services Using Service Quotas\n        \n      \n        AWS IAM Quotas Demo\n        \n      \n        AWS re:Invent 2018: Close Loops and Opening Minds: How to Take Control of Systems, Big and Small\n        \n      \n    \n      Related tools:\n    \n    \n      \n       \n       \n       \n       \n       \n       \n       \n       \n       \n       \n    \n        AWS CodeDeploy\n      \n        AWS CloudTrail\n      \n        \n          Amazon CloudWatch\n        \n      \n        \n          Amazon EventBridge\n        \n      \n        \n          Amazon DevOps Guru\n        \n      \n        AWS Config\n      \n        AWS Trusted Advisor\n      \n        AWS CDK\n        \n      \n        AWS Systems Manager\n      \n        AWS Marketplace\n      \n  \u003cdiv\u003e\u003cdiv\u003e\u003cdiv\u003e\u003cdiv id=\"js_error_message\"\u003e\u003cp\u003e\u003cimg src=\"https://d1ge0kk1l5kms0.cloudfront.net/images/G/01/webservices/console/warning.png\" alt=\"Warning\" /\u003e \u003cstrong\u003eJavascript is disabled or is unavailable in your browser.\u003c/strong\u003e\u003c/p\u003e\u003cp\u003eTo use the Amazon Web Services Documentation, Javascript must be enabled. Please refer to your browser's Help pages for instructions.\u003c/p\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003eDocument ConventionsREL01-BP05 Automate quota management REL 2. How do you plan your network topology? ",
  "REL02-BP01 Use highly available network connectivity for your\n  workload public endpoints\n    Building highly available network connectivity to public endpoints of your workloads can help you reduce downtime due to loss of connectivity and improve the availability and SLA of your workload. To achieve this, use highly available DNS, content delivery networks (CDNs), API gateways, load balancing, or reverse proxies.\n  \n    Desired outcome: It is critical to plan, build, and operationalize highly available network connectivity for your public endpoints. If your workload becomes unreachable due to a loss in connectivity, even if your workload is running and available, your customers will see your system as down. By combining the highly available and resilient network connectivity for your workload’s public endpoints, along with a resilient architecture for your workload itself, you can provide the best possible availability and service level for your customers.\n  \n    AWS Global Accelerator, Amazon CloudFront, Amazon API Gateway, AWS Lambda Function URLs, AWS AppSync APIs, and Elastic Load Balancing (ELB) all provide highly available public endpoints. Amazon Route 53 provides a highly available DNS service for domain name resolution to verify that your public endpoint addresses can be resolved.\n  \n    You can also evaluate AWS Marketplace software appliances for load balancing and proxying.\n  \n    Common anti-patterns:\n  \n     \n     \n     \n     \n     \n     \n     \n     \n  \n      Designing a highly available workload without planning out DNS and network connectivity for high availability.\n    \n       Using public internet addresses on individual instances or containers and managing the connectivity to them with DNS.\n    \n       Using IP addresses instead of domain names for locating services.\n    \n      \n        Not testing out scenarios where connectivity to your public endpoints is lost.\n      \n    \n      \n        Not analyzing network throughput needs and distribution patterns.\n      \n    \n      \n        Not testing and planning for scenarios where internet network connectivity to your public endpoints of your workload might be interrupted.\n      \n    \n      \n        Providing content (like web pages, static assets, or media files) to a large geographic area and not using a content delivery network. \n      \n    \n      \n        Not planning for distributed denial of service (DDoS) attacks. DDoS attacks risk shutting out legitimate traffic and lowering availability for your users.\n      \n    \n    Benefits of establishing this best\n      practice: Designing for highly available and resilient network connectivity ensures that your workload is accessible and available to your users.\n  \n    Level of risk exposed if this best practice\n    is not established: High\n  \n\n  Implementation guidance\n    \n      At the core of building highly available network connectivity to your public endpoints is the routing of the traffic. To verify your traffic is able to reach the endpoints, the DNS must be able to resolve the domain names to their corresponding IP addresses. Use a highly available and scalable Domain Name System (DNS) such as Amazon Route 53 to manage your domain’s DNS records. You can also use health checks provided by Amazon Route 53. The health checks verify that your application is reachable, available, and functional, and they can be set up in a way that they mimic your user’s behavior, such as requesting a web page or a specific URL. In case of failure, Amazon Route 53 responds to DNS resolution requests and directs the traffic to only healthy endpoints. You can also consider using Geo DNS and Latency Based Routing capabilities offered by Amazon Route 53.\n    \n    \n      To verify that your workload itself is highly available, use Elastic Load Balancing (ELB). Amazon Route 53 can be used to target traffic to ELB, which distributes the traffic to the target compute instances. You can also use Amazon API Gateway along with AWS Lambda for a serverless solution. Customers can also run workloads in multiple AWS Regions. With multi-site active/active pattern, the workload can serve traffic from multiple Regions. With a multi-site active/passive pattern, the workload serves traffic from the active region while data is replicated to the secondary region and becomes active in the event of a failure in the primary region. Route 53 health checks can then be used to control DNS failover from any endpoint in a primary Region to an endpoint in a secondary Region, verifying that your workload is reachable and available to your users.\n    \n    \n      Amazon CloudFront provides a simple API for distributing content with low latency and high data transfer rates by serving requests using a network of edge locations around the world. Content delivery networks (CDNs) serve customers by serving content located or cached at a location near to the user. This also improves availability of your application as the load for content is shifted away from your servers over to CloudFront’s edge locations. The edge locations and regional edge caches hold cached copies of your content close to your viewers resulting in quick retrieval and increasing reachability and availability of your workload.\n    \n    \n      For workloads with users spread out geographically, AWS Global Accelerator helps you improve the availability and performance of the applications. AWS Global Accelerator provides Anycast static IP addresses that serve as a fixed entry point to your application hosted in one or more AWS Regions. This allows traffic to ingress onto the AWS global network as close to your users as possible, improving reachability and availability of your workload. AWS Global Accelerator also monitors the health of your application endpoints by using TCP, HTTP, and HTTPS health checks. Any changes in the health or configuration of your endpoints permit redirection of user traffic to healthy endpoints that deliver the best performance and availability to your users. In addition, AWS Global Accelerator has a fault-isolating design that uses two static IPv4 addresses that are serviced by independent network zones increasing the availability of your applications.\n    \n    \n      To help protect customers from DDoS attacks, AWS provides AWS Shield Standard. Shield Standard comes automatically turned on and protects from common infrastructure (layer 3 and 4) attacks like SYN/UDP floods and reflection attacks to support high availability of your applications on AWS. For additional protections against more sophisticated and larger attacks (like UDP floods), state exhaustion attacks (like TCP SYN floods), and to help protect your applications running on Amazon Elastic Compute Cloud (Amazon EC2), Elastic Load Balancing (ELB), Amazon CloudFront, AWS Global Accelerator, and Route 53, you can consider using AWS Shield Advanced. For protection against Application layer attacks like HTTP POST or GET floods, use AWS WAF. AWS WAF can use IP addresses, HTTP headers, HTTP body, URI strings, SQL injection, and cross-site scripting conditions to determine if a request should be blocked or allowed.\n    \n    \n      Implementation steps\n    \n    \n       \n       \n       \n       \n       \n       \n    \n        \n          Set up highly available DNS: Amazon Route 53 is a highly available and scalable domain name system (DNS) web service. Route 53 connects user requests to internet applications running on AWS or on-premises. For more information, see configuring Amazon Route 53 as your DNS service. \n        \n      \n        \n          Setup health checks: When using Route 53, verify that only healthy targets are resolvable. Start by creating Route 53 health checks and configuring DNS failover. The following aspects are important to consider when setting up health checks: \n        \n        \n           \n           \n           \n           \n        \n            \n              How Amazon Route 53 determines whether a health check is healthy\n            \n          \n            \n              Creating, updating, and deleting health checks\n            \n          \n            \n              Monitoring health check status and getting notifications\n            \n          \n            \n              Best practices for Amazon Route 53 DNS\n            \n          \n      \n        \n          Connect your DNS service to your endpoints.\n        \n        \n           \n           \n        \n            \n              When using Elastic Load Balancing as a target for your traffic, create an alias record using Amazon Route 53 that points to your load balancer’s regional endpoint. During the creation of the alias record, set the Evaluate target health option to Yes.\n            \n          \n            \n              For serverless workloads or private APIs when API Gateway is used, use Route 53 to direct traffic to API Gateway.\n            \n          \n      \n        \n          Decide on a content delivery network.\n        \n        \n           \n           \n        \n            \n              For delivering content using edge locations closer to the user, start by understanding how CloudFront delivers content.\n            \n          \n            \n              Get started with a simple CloudFront distribution. CloudFront then knows where you want the content to be delivered from, and the details about how to track and manage content delivery. The following aspects are important to understand and consider when setting up CloudFront distribution:\n            \n            \n               \n               \n               \n               \n            \n                \n                  How caching works with CloudFront edge locations\n                \n              \n                \n                  Increasing the proportion of requests that are served directly from the CloudFront caches (cache hit ratio)\n                \n              \n                \n                  Using Amazon CloudFront Origin Shield\n                \n              \n                \n                  Optimizing high availability with CloudFront origin failover\n                \n              \n          \n      \n        \n          Set up application layer protection: AWS WAF helps you protect against common web exploits and bots that can affect availability, compromise security, or consume excessive resources. To get a deeper understanding, review how AWS WAF works and when you are ready to implement protections from application layer HTTP POST AND GET floods, review Getting started with AWS WAF. You can also use AWS WAF with CloudFront see the documentation on how AWS WAF works with Amazon CloudFront features.\n        \n      \n        \n          Set up additional DDoS protection: By default, all AWS customers receive protection from common, most frequently occurring network and transport layer DDoS attacks that target your web site or application with AWS Shield Standard at no additional charge. For additional protection of internet-facing applications running on Amazon EC2, Elastic Load Balancing, Amazon CloudFront, AWS Global Accelerator, and Amazon Route 53 you can consider AWS Shield Advanced and review examples of DDoS resilient architectures. To protect your workload and your public endpoints from DDoS attacks review Getting started with AWS Shield Advanced.\n        \n      \n   \n\n  Resources\n    \n      Related best practices:\n    \n    \n       \n       \n       \n    \n        \n          REL10-BP01 Deploy the workload to multiple locations\n        \n      \n        \n          REL11-BP04 Rely on the data plane and not the control plane\n  during recovery\n        \n      \n        \n          REL11-BP06 Send notifications when events impact\n  availability\n        \n      \n    \n      Related documents:\n    \n    \n       \n       \n       \n       \n       \n       \n       \n       \n       \n       \n       \n    \n        \n          APN\n          Partner: partners that can help plan your networking\n        \n      \n        \n          AWS Marketplace for Network Infrastructure\n        \n      \n        \n          What\n          Is AWS Global Accelerator?\n        \n      \n        \n          What\n          is Amazon CloudFront?\n        \n      \n        \n          What\n          is Amazon Route 53?\n        \n      \n        \n          What\n          is Elastic Load Balancing?\n        \n      \n        \n          Network Connectivity capability - Establishing Your Cloud Foundations\n        \n      \n        \n          What is Amazon API Gateway?\n        \n      \n        \n          What are AWS WAF, AWS Shield, and AWS Firewall Manager?\n        \n      \n        \n          What is Amazon Application Recovery Controller?\n        \n      \n        \n          Configure custom health checks for DNS failover\n        \n      \n    \n      Related videos:\n    \n    \n       \n       \n       \n       \n       \n    \n        AWS re:Invent 2022 - Improve performance and availability with AWS Global Accelerator\n      \n        AWS re:Invent 2020: Global traffic management with Amazon Route 53\n        \n      \n        AWS re:Invent 2022 - Operating highly available Multi-AZ applications\n        \n      \n        AWS re:Invent 2022 - Dive deep on AWS networking infrastructure\n        \n      \n        AWS re:Invent 2022 - Building resilient networks \n        \n      \n    \n      Related examples:\n    \n    \n       \n       \n    \n        \n          Disaster Recovery with Amazon Application Recovery Controller (ARC)\n        \n      \n        AWS Global Accelerator Workshop\n        \n      \n  \u003cdiv\u003e\u003cdiv\u003e\u003cdiv\u003e\u003cdiv id=\"js_error_message\"\u003e\u003cp\u003e\u003cimg src=\"https://d1ge0kk1l5kms0.cloudfront.net/images/G/01/webservices/console/warning.png\" alt=\"Warning\" /\u003e \u003cstrong\u003eJavascript is disabled or is unavailable in your browser.\u003c/strong\u003e\u003c/p\u003e\u003cp\u003eTo use the Amazon Web Services Documentation, Javascript must be enabled. Please refer to your browser's Help pages for instructions.\u003c/p\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003eDocument Conventions REL 2. How do you plan your network topology? REL02-BP02 Provision redundant connectivity between private\n  networks in the cloud and on-premises environments",
  "REL02-BP02 Provision redundant connectivity between private\n  networks in the cloud and on-premises environments\n    Implement redundancy in your connections between private networks in\n    the cloud and on-premises environments to achieve connectivity\n    resilience. This can be accomplished by deploying two or more links\n    and traffic paths, preserving connectivity in the event of network\n    failures.\n  \n    Common anti-patterns:\n  \n     \n     \n     \n     \n     \n  \n      \n        You depend on just one network connection, which creates a single\n        point of failure.\n      \n    \n      \n        You use only one VPN tunnel or multiple tunnels that end in the same\n        Availability Zone.\n      \n    \n      \n        You rely on one ISP for VPN connectivity, which can lead to\n        complete failures during ISP outages.\n      \n    \n      \n        Not implementing dynamic routing protocols like BGP, which are\n        crucial for rerouting traffic during network disruptions.\n      \n    \n      \n        You ignore the bandwidth limitations of VPN tunnels and\n        overestimate their backup capabilities.\n      \n    \n    Benefits of establishing this best\n    practice: By implementing redundant connectivity between\n    your cloud environment and your corporate or on-premises\n    environment, the dependent services between the two environments can\n    communicate reliably.\n  \n    Level of risk exposed if this best practice\n    is not established: High\n  \n\n  Implementation guidance \n\n      \n    \n      When using AWS Direct Connect to connect your on-premises network\n      to AWS, you can achieve maximum network resiliency (SLA of 99.99%)\n      by using separate connections that end on distinct devices in more\n      than one on-premises location and more than one AWS Direct Connect\n      location. This topology offers resilience against device failures,\n      connectivity issues, and complete location outages. Alternatively,\n      you can achieve high resiliency (SLA of 99.9%) by using two\n      individual connections to multiple locations (each on-premises\n      location connected to a single Direct Connect location). This\n      approach protects against connectivity disruptions caused by fiber\n      cuts or device failures and helps mitigate complete location\n      failures. The AWS Direct Connect Resiliency Toolkit can assist in\n      designing your AWS Direct Connect topology.\n    \n    \n      You can also consider AWS Site-to-Site VPN ending on an AWS Transit Gateway as a cost-effective backup to your primary AWS Direct Connect connection. This setup enables equal-cost multipath\n      (ECMP) routing across multiple VPN tunnels, allowing for\n      throughput of up to 50Gbps, even though each VPN tunnel is capped\n      at 1.25 Gbps. It's important to note, however, that AWS Direct Connect is still the most effective choice for minimizing network\n      disruptions and providing stable connectivity.\n    \n    \n      When using VPNs over the internet to connect your cloud\n      environment to your on-premises data center, configure two VPN\n      tunnels as part of a single site-to-site VPN connection. Each\n      tunnel should end in a different Availability Zone for high\n      availability and use redundant hardware to prevent on-premises\n      device failure. Additionally, consider multiple internet\n      connections from various internet service providers (ISPs) at your\n      on-premises location to avoid complete VPN connectivity disruption\n      due to a single ISP outage. Selecting ISPs with diverse routing\n      and infrastructure, especially those with separate physical paths\n      to AWS endpoints, provides high connectivity availability.\n    \n    \n      In addition to physical redundancy with multiple AWS Direct Connect connections and multiple VPN tunnels (or a combination of\n      both), implementing Border Gateway Protocol (BGP) dynamic routing\n      is also crucial. Dynamic BGP provides automatic rerouting of\n      traffic from one path to another based on real-time network\n      conditions and configured policies. This dynamic behavior is\n      especially beneficial in maintaining network availability and\n      service continuity in the event of link or network failures. It\n      quickly selects alternative paths, enhancing the network's\n      resilience and reliability.\n    \n     \n\n  Implementation steps\n\n      \n      \n         \n         \n         \n         \n      \n          \n            Acquisition highly-available connectivity between AWS and\n            your on-premises environment.\n          \n          \n             \n             \n             \n          \n              \n                Use multiple AWS Direct Connect connections or VPN\n                tunnels between separately deployed private networks.\n              \n            \n              \n                Use multiple AWS Direct Connect locations for high\n                availability.\n              \n            \n              \n                If using multiple AWS Regions, create redundancy in at\n                least two of them.\n              \n            \n        \n          \n            Use AWS Transit Gateway, when possible, to end your\n            VPN\n            connection.\n          \n        \n          \n            Evaluate AWS Marketplace appliances to end VPNs or \n            extend\n              your SD-WAN to AWS.\n            If you use AWS Marketplace appliances, deploy redundant\n            instances for high availability in different Availability\n            Zones.\n          \n        \n          \n            Provide a redundant connection to your on-premises\n            environment.\n          \n          \n             \n             \n          \n              \n                You may need redundant connections to multiple AWS Regions to achieve your availability needs.\n              \n            \n              \n                Use the\n                AWS Direct Connect Resiliency Toolkit to get started.\n              \n            \n        \n     \n   \n\n  Resources\n\n      \n    \n      Related documents:\n    \n    \n       \n       \n       \n       \n       \n       \n       \n       \n       \n       \n       \n       \n       \n       \n       \n    \n        \n          AWS Direct Connect Resiliency Recommendations\n        \n      \n        \n          Using\n          Redundant Site-to-Site VPN Connections to Provide\n          Failover\n        \n      \n        \n          Routing\n          policies and BGP communities\n        \n      \n        \n          Active/Active\n          and Active/Passive Configurations in AWS Direct Connect\n        \n      \n        \n          APN\n          Partner: partners that can help plan your networking\n        \n      \n        \n          AWS Marketplace for Network Infrastructure\n        \n      \n        \n          Amazon Virtual Private Cloud Connectivity Options Whitepaper\n        \n      \n        \n          Building\n          a Scalable and Secure Multi-VPC AWS Network\n          Infrastructure\n        \n      \n        \n          Using redundant\n            Site-to-Site VPN connections to provide failover\n        \n      \n        \n          Using\n          the AWS Direct Connect Resiliency Toolkit to get started\n        \n      \n        \n          VPC\n          Endpoints and VPC Endpoint Services (AWS PrivateLink)\n        \n      \n        \n          What\n          Is Amazon VPC?\n        \n      \n        \n          What\n          is a transit gateway?\n        \n      \n        \n          What\n          is AWS Site-to-Site VPN?\n        \n      \n        \n          Working with Direct\n            Connect gateways\n        \n      \n    \n      Related videos:\n    \n    \n       \n       \n    \n        \n          AWS re:Invent\n          2018: Advanced VPC Design and New Capabilities for Amazon VPC\n         \n        \n      \n        \n          AWS re:Invent\n          2019: AWS Transit Gateway reference architectures for many\n          VPCs\n        \n      \n  \u003cdiv\u003e\u003cdiv\u003e\u003cdiv\u003e\u003cdiv id=\"js_error_message\"\u003e\u003cp\u003e\u003cimg src=\"https://d1ge0kk1l5kms0.cloudfront.net/images/G/01/webservices/console/warning.png\" alt=\"Warning\" /\u003e \u003cstrong\u003eJavascript is disabled or is unavailable in your browser.\u003c/strong\u003e\u003c/p\u003e\u003cp\u003eTo use the Amazon Web Services Documentation, Javascript must be enabled. Please refer to your browser's Help pages for instructions.\u003c/p\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003eDocument ConventionsREL02-BP01 Use highly available network connectivity for your\n  workload public endpointsREL02-BP03 Ensure IP subnet allocation accounts for expansion and\n      availability",
  "REL02-BP03 Ensure IP subnet\n      allocation accounts for expansion and availability Amazon VPC IP address ranges must be large enough to accommodate workload requirements,\n    including factoring in future expansion and allocation of IP addresses to subnets across\n    Availability Zones. This includes load balancers, EC2 instances, and container-based\n    applications.  When you plan your network topology, the first step is to define the IP address space\n    itself. Private IP address ranges (following RFC 1918 guidelines) should be allocated for each\n    VPC. Accommodate the following requirements as part of this process: \n     \n     \n     \n     \n     \n     \n     \n     \n  \n      Allow IP address space for more than one VPC per Region. \n    \n      Within a VPC, allow space for multiple subnets so that you can cover multiple\n        Availability Zones. \n    \n      Consider leaving unused CIDR block space within a VPC for future expansion.\n    \n      Ensure that there is IP address space to meet the needs of any transient fleets of Amazon EC2\n        instances that you might use, such as Spot Fleets for machine learning, Amazon EMR clusters,\n        or Amazon Redshift clusters. Similar consideration should be given to Kubernetes clusters, such as\n        Amazon Elastic Kubernetes Service (Amazon EKS), as each Kubernetes pod is assigned a routable address from the VPC CIDR\n        block by default.\n    \n      Note that the first four IP addresses and the last IP address in each subnet CIDR block\n        are reserved and not available for your use.\n    \n      Note that the initial VPC CIDR block allocated to your VPC cannot be changed or deleted,\n        but you can add additional non-overlapping CIDR blocks to the VPC. Subnet IPv4 CIDRs cannot\n        be changed, however IPv6 CIDRs can.\n    \n      The largest possible VPC CIDR block is a /16, and the smallest is a /28.\n    \n      Consider other connected networks (VPC, on-premises, or other cloud providers) and\n        ensure non-overlapping IP address space. For more information, see REL02-BP05 Enforce non-overlapping private IP address ranges in all private address\n          spaces where they are connected.\n    \n    Desired outcome: A scalable IP subnet can help you accomodate\n    for future growth and avoid unnecessary waste.\n    Common anti-patterns:\n  \n     \n     \n     \n     \n     \n  \n      Failing to consider future growth, resulting in CIDR blocks that are too small and\n        requiring reconfiguration, potentially causing downtime.\n    \n      Incorrectly estimating how many IP addresses an elastic load balancer can use.\n    \n      Deploying many high traffic load balancers into the same subnets\n    \n      Using automated scaling mechanisms whilst failing to monitor IP address\n        consumption.\n    \n      Defining excessively large CIDR ranges well beyond future growth expectations, which can\n        lead to difficulty peering with other networks with overlapping address ranges.\n    \n    Benefits of establishing this best practice: This ensures that\n    you can accommodate the growth of your workloads and continue to provide availability as you\n    scale up. \n    Level of risk exposed if this best practice is not established:\n    Medium \n\n    Implementation guidance\n\n    Plan your network to accommodate for growth, regulatory compliance, and integration with\n      others. Growth can be underestimated, regulatory compliance can change, and acquisitions or\n      private network connections can be difficult to implement without proper planning. \n\n    \n       \n       \n       \n    \n         Select relevant AWS accounts and Regions based on your service requirements,\n          latency, regulatory, and disaster recovery (DR) requirements. \n      \n         Identify your needs for regional VPC deployments. \n      \n         Identify the size of the VPCs. \n        \n           \n           \n           \n           \n        \n             Determine if you are going to deploy multi-VPC connectivity. \n            \n               \n               \n            \n                \n                  What\n                    Is a Transit Gateway?\n                \n              \n                \n                  Single Region\n                    Multi-VPC Connectivity\n                \n              \n          \n             Determine if you need segregated networking for regulatory requirements. \n          \n            Make VPCs with appropriately-sized CIDR blocks to accommodate your current and\n              future needs.\n            \n               \n            \n                If you have unknown growth projections, you may wish to err on the side of\n                  larger CIDR blocks to reduce the potential for future reconfiguration\n              \n          \n            Consider using IPv6 addressing for\n              subnets as part of a dual-stack VPC. IPv6 is well suited to being used in private\n              subnets containing fleets of ephemeral instances or containers that would otherwise\n              require large numbers of IPv4 addresses.\n          \n      \n\n   \n\n    Resources\n\n    \n      Related Well-Architected best practices:\n    \n    \n       \n    \n        \n          REL02-BP05 Enforce non-overlapping private IP address ranges in all private address\n            spaces where they are connected\n        \n      \n\n    \n      Related documents:\n    \n    \n       \n       \n       \n       \n       \n       \n       \n       \n       \n       \n       \n       \n    \n        \n          APN Partner: partners\n            that can help plan your networking\n        \n      \n        \n          AWS Marketplace for Network\n            Infrastructure\n        \n      \n        \n          Amazon Virtual Private Cloud\n            Connectivity Options Whitepaper\n        \n      \n        \n          Multiple data\n            center HA network connectivity\n        \n      \n        \n          Single Region Multi-VPC Connectivity\n        \n      \n        \n          What Is\n            Amazon VPC?\n        \n      \n        \n          IPv6 on AWS\n        \n      \n        \n          IPv6 on reference architectures\n        \n      \n        \n          Amazon Elastic Kubernetes Service\n            launches IPv6 support\n        \n      \n        \n          Recommendations for your VPC - Classic Load Balancers\n        \n      \n        \n          Availability Zone subnets - Application Load Balancers\n        \n      \n        \n          Availability Zones - Network Load Balancers\n        \n      \n    \n      Related videos:\n    \n    \n       \n       \n       \n    \n        \n          AWS re:Invent 2018: Advanced VPC Design and\n            New Capabilities for Amazon VPC (NET303)\n        \n      \n        \n          AWS re:Invent 2019: AWS Transit Gateway reference\n            architectures for many VPCs (NET406-R1)\n        \n      \n        \n          AWS re:Invent 2023: AWS Ready\n            for what's next? Designing networks for growth and flexibility (NET310)\n        \n      \n  \u003cdiv\u003e\u003cdiv\u003e\u003cdiv\u003e\u003cdiv id=\"js_error_message\"\u003e\u003cp\u003e\u003cimg src=\"https://d1ge0kk1l5kms0.cloudfront.net/images/G/01/webservices/console/warning.png\" alt=\"Warning\" /\u003e \u003cstrong\u003eJavascript is disabled or is unavailable in your browser.\u003c/strong\u003e\u003c/p\u003e\u003cp\u003eTo use the Amazon Web Services Documentation, Javascript must be enabled. Please refer to your browser's Help pages for instructions.\u003c/p\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003eDocument ConventionsREL02-BP02 Provision redundant connectivity between private\n  networks in the cloud and on-premises environmentsREL02-BP04 Prefer hub-and-spoke topologies over many-to-many\n  mesh",
  "REL02-BP04 Prefer hub-and-spoke topologies over many-to-many\n  mesh\n    When connecting multiple private networks, such as Virtual Private\n    Clouds (VPCs) and on-premises networks, opt for a hub-and-spoke\n    topology over a meshed one. Unlike meshed topologies, where each\n    network connects directly to the others and increases the complexity\n    and management overhead, the hub-and-spoke architecture centralizes\n    connections through a single hub. This centralization simplifies the\n    network structure and enhances its operability, scalability, and\n    control.\n  \n    AWS Transit Gateway is a managed, scalable, and highly-available\n    service designed for construction of hub-and-spoke networks on AWS.\n    It serves as the central hub of your network that provides network\n    segmentation, centralized routing, and the simplified connection to\n    both cloud and on-premises environments. The following figure\n    illustrates how you can use AWS Transit Gateway to build your\n    hub-and-spoke topology.\n  \n     \n      \n     \n   \n    Desired outcome: You have\n    connected your Virtual Private Clouds (VPCs) and on-premises\n    networks through a central hub. You configure your peering\n    connections through the hub, which acts as a highly scalable cloud\n    router. Routing is simplified because you do not have to work with\n    complex peering relationships. Traffic between networks is\n    encrypted, and you have the ability to isolate networks.\n  \n    Common anti-patterns:\n  \n     \n     \n     \n  \n      \n        You build complex network peering rules.\n      \n    \n      \n        You provide routes between networks that should not communicate\n        with one another (for example, separate workloads that have no\n        interdependencies).\n      \n    \n      \n        There is ineffective governance of the hub instance.\n      \n    \n    Benefits of establishing this best\n      practice: As the number of connected networks increases,\n    management and expansion of meshed connectivity becomes increasingly\n    challenging. A mesh architecture introduces additional challenges,\n    such as additional infrastructure components, configuration\n    requirements, and deployment considerations. The mesh also\n    introduces additional overhead to manage and monitor the data plane\n    and control plane components. You must think about how to provide\n    high availability of the mesh architecture, how to monitor the mesh\n    health and performance, and how to handle upgrades of the mesh\n    components.\n  \n    A hub-and-spoke model, on the other hand, establishes centralized\n    traffic routing across multiple networks. It provides a simpler\n    approach to management and monitoring of the data plane and control\n    plane components.\n  \n    Level of risk exposed if this best practice\n      is not established: Medium\n  \n    \n    Implementation guidance\n    \n    \n    \n      Create a Network Services account if one does not exist. Place the\n      hub in the organization's Network Services account. This approach\n      allows the hub to be centrally managed by network engineers.\n    \n    \n      The hub of the hub-and-spoke model acts as a virtual router for\n      traffic flowing between your Virtual Private Clouds (VPCs) and\n      on-premises networks. This approach reduces network complexity and\n      makes it easier to troubleshoot networking issues.\n    \n    \n      Consider your network design, including the VPCs, AWS Direct Connect, and Site-to-Site VPN connections you want to\n      interconnect.\n    \n    \n      Consider using a separate subnet for each transit gateway VPC\n      attachment. For each subnet, use a small CIDR (for example\n      /28) so that you have more address space for\n      compute resources. Additionally, create one network ACL, and\n      associate it with all of the subnets that are associated with the\n      hub. Keep the network ACL open in both the inbound and outbound\n      directions.\n    \n    \n      Design and implement your routing tables such that routes are\n      provided only between networks that should communicate. Omit\n      routes between networks that should not communicate with one\n      another (for example, between separate workloads that have no\n      inter-dependencies).\n    \n     \n      \n      Implementation steps\n      \n      \n      \n         \n         \n         \n         \n         \n      \n          \n            Plan your network. Determine which networks you want to\n            connect, and verify that they don't share overlapping CIDR\n            ranges.\n          \n        \n          \n            Create an AWS Transit Gateway and attach your VPCs.\n          \n        \n          \n            If needed, create VPN connections or Direct Connect\n            gateways, and associate them with the Transit Gateway.\n          \n        \n          \n            Define how traffic is routed between the connected VPCs and\n            other connections through configuration of your Transit\n            Gateway route tables.\n          \n        \n          \n            Use Amazon CloudWatch to monitor and adjust configurations\n            as necessary for performance and cost optimization.\n          \n        \n     \n   \n    \n    Resources\n    \n    \n    \n      Related best practices:\n    \n    \n       \n       \n    \n        \n          REL02-BP03\n            Ensure IP subnet allocation accounts for expansion and\n            availability\n        \n      \n        \n          REL02-BP05\n            Enforce non-overlapping private IP address ranges in all\n            private address spaces where they are connected\n        \n      \n    \n      Related documents:\n    \n    \n       \n       \n       \n       \n       \n       \n       \n    \n        \n          What\n            Is a Transit Gateway?\n        \n      \n        \n          Transit\n            gateway design best practices\n        \n      \n        \n          Building\n            a Scalable and Secure Multi-VPC AWS Network\n            Infrastructure\n        \n      \n        \n          Building\n            a global network using AWS Transit Gateway Inter-Region\n            peering\n        \n      \n        \n          Amazon Virtual Private Cloud Connectivity Options\n        \n      \n        \n          APN\n            Partner: partners that can help plan your networking\n        \n      \n        \n          AWS Marketplace for Network Infrastructure\n        \n      \n    \n      Related videos:\n    \n    \n       \n       \n    \n        \n          AWS           re:Invent 2023 - AWS networking foundations\n        \n      \n        \n          AWS           re:Invent 2023 - Advanced VPC designs and new\n            capabilities\n        \n      \n    \n      Related workshops:\n    \n    \n       \n    \n        \n          AWS Transit Gateway Workshop\n        \n      \n  \u003cdiv\u003e\u003cdiv\u003e\u003cdiv\u003e\u003cdiv id=\"js_error_message\"\u003e\u003cp\u003e\u003cimg src=\"https://d1ge0kk1l5kms0.cloudfront.net/images/G/01/webservices/console/warning.png\" alt=\"Warning\" /\u003e \u003cstrong\u003eJavascript is disabled or is unavailable in your browser.\u003c/strong\u003e\u003c/p\u003e\u003cp\u003eTo use the Amazon Web Services Documentation, Javascript must be enabled. Please refer to your browser's Help pages for instructions.\u003c/p\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003eDocument ConventionsREL02-BP03 Ensure IP subnet allocation accounts for expansion and\n      availabilityREL02-BP05 Enforce non-overlapping private IP address ranges in\n  all private address spaces where they are connected",
  "REL02-BP05 Enforce non-overlapping private IP address ranges in\n  all private address spaces where they are connectedThe IP address ranges of each of your VPCs must not overlap when peered, connected via Transit Gateway, or connected over VPN. Avoid IP address conflicts between a VPC and on-premises environments or with other cloud providers that you use. You must also have a way to allocate private IP address ranges when needed. An IP address management (IPAM) system can help with automating this. \n    Desired outcome:\n  \n     \n     \n  \n      \n        No IP address range conflicts between VPCs, on-premises environments, or other cloud providers.\n      \n    \n      \n        Proper IP address management allows for easier scaling of network infrastructure to accommodate growth and changes in network requirements.\n      \n    \n    Common anti-patterns:\n  \n     \n     \n     \n     \n  \n      \n        Using the same IP range in your VPC as you have on premises,\n        in your corporate network, or other cloud providers\n      \n    \n      \n        Not tracking IP ranges of VPCs used to deploy your workloads.\n      \n    \n      \n        Relying on manual IP address management processes, such as spreadsheets.\n      \n    \n      \n        Over- or under-sizing CIDR blocks, which results in IP address waste or insufficient address space for your workload.\n      \n    \n    Benefits of establishing this best\n      practice: Active planning of your network will ensure that you do not have multiple occurrences of the same IP address in interconnected networks. This prevents routing problems from occurring in parts of the workload that are using the different applications.\n  \n    Level of risk exposed if this best practice\n    is not established: Medium\n  \n\n  Implementation guidance\n\n      \n    \n      Make use of an IPAM, such as the Amazon VPC IP Address Manager, to monitor and manage your CIDR use. Several IPAMs are also available from the AWS Marketplace. Evaluate your potential usage on AWS, add CIDR ranges to existing VPCs, and create VPCs to allow planned growth in usage. \n    \n     \n      \n      Implementation steps\n    \n           \n       \n       \n       \n       \n       \n       \n    \n        \n          Capture current CIDR consumption (for example, VPCs and subnets).\n        \n        \n           \n           \n        \n            \n              Use service API operations to collect current CIDR consumption.\n            \n          \n            \n              Use the Amazon VPC IP Address Manager to discover resources.\n            \n          \n      \n        \n          Capture your current subnet usage.\n        \n        \n           \n           \n        \n            \n              Use service API operations to collect subnets per VPC in each Region.\n            \n          \n            \n              Use the Amazon VPC IP Address Manager to discover resources.\n            \n          \n      \n        \n          Record the current usage.\n        \n      \n        \n          Determine if you created any overlapping IP ranges.\n        \n      \n        \n          Calculate the spare capacity.\n        \n      \n        \n          Identify overlapping IP ranges. You can either migrate to a new range of\n          addresses or consider using techniques like private NAT Gateway or AWS PrivateLink if you need to connect the overlapping ranges.\n        \n      \n     \n   \n\n  Resources\n\n    \n      Related best practices:\n    \n    \n       \n    \n        \n          Protecting networks\n        \n      \n    \n      Related documents:\n    \n    \n       \n       \n       \n       \n       \n       \n       \n    \n        \n          APN\n          Partner: partners that can help plan your networking\n        \n      \n        \n          AWS Marketplace for Network Infrastructure\n        \n      \n        \n          Amazon Virtual Private Cloud Connectivity Options Whitepaper\n        \n      \n        \n          Multiple\n          data center HA network connectivity\n        \n      \n        \n          Connecting Networks with Overlapping IP Ranges\n        \n      \n        \n          What\n          Is Amazon VPC?\n        \n      \n        \n          What\n          is IPAM?\n        \n      \n    \n      Related videos:\n    \n    \n       \n       \n       \n       \n    \n        AWS re:Invent 2023 - Advanced VPC designs and new capabilities\n        \n      \n        \n          AWS re:Invent\n          2019: AWS Transit Gateway reference architectures for many\n          VPCs\n        \n      \n        AWS re:Invent 2023 - Ready for what’s next? Designing networks for growth and flexibility\n        \n      \n        AWS re:Invent 2021 - {New Launch} Manage your IP addresses at scale on AWS\n      \n  \u003cdiv\u003e\u003cdiv\u003e\u003cdiv\u003e\u003cdiv id=\"js_error_message\"\u003e\u003cp\u003e\u003cimg src=\"https://d1ge0kk1l5kms0.cloudfront.net/images/G/01/webservices/console/warning.png\" alt=\"Warning\" /\u003e \u003cstrong\u003eJavascript is disabled or is unavailable in your browser.\u003c/strong\u003e\u003c/p\u003e\u003cp\u003eTo use the Amazon Web Services Documentation, Javascript must be enabled. Please refer to your browser's Help pages for instructions.\u003c/p\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003eDocument ConventionsREL02-BP04 Prefer hub-and-spoke topologies over many-to-many\n  meshWorkload architecture",
  "REL03-BP01 Choose how to segment your workload\n    Workload segmentation is important when determining the resilience requirements of your application. \n    Monolithic architecture should be avoided whenever possible. Instead, carefully consider which \n    application components can be broken out into microservices. Depending on your application requirements, \n    this may end up being a combination of a service-oriented architecture (SOA) with microservices where \n    possible. Workloads that are capable of statelessness are more capable of being deployed as microservices.\n  \n    Desired outcome: Workloads should be supportable, scalable, and as \n    loosely coupled as possible. \n  \n    When making choices about how to segment your workload, balance the benefits against the complexities. \n    What is right for a new product racing to first launch is different than what a workload built to scale \n    from the start needs. When refactoring an existing monolith, you will need to consider how well the application \n    will support a decomposition towards statelessness. Breaking services into smaller pieces allows small, \n    well-defined teams to develop and manage them. However, smaller services can introduce complexities which \n    include possible increased latency, more complex debugging, and increased operational burden.\n  \n    Common anti-patterns:\n  \n     \n  \n       The microservice Death Star is a situation in\n        which the atomic components become so highly interdependent that a failure of one results in\n        a much larger failure, making the components as rigid and fragile as a monolith. \n    \n    Benefits of establishing this practice:\n  \n     \n     \n     \n     \n  \n      \n        More specific segments lead to greater agility, organizational flexibility, and scalability.\n      \n    \n      \n        Reduced impact of service interruptions.\n      \n    \n      \n        Application components may have different availability requirements, which can be supported by a more atomic segmentation.\n      \n    \n      \n        Well-defined responsibilities for teams supporting the workload.\n      \n    \n    Level of risk exposed if this best practice\n      is not established: High\n  \n    \n    Implementation guidance\n  \n    Choose your architecture type based on how you will segment your workload. Choose an SOA \n    or microservices architecture (or in some rare cases, a monolithic architecture). Even if you choose to start with a \n    monolith architecture, you must ensure that it’s modular and can ultimately evolve to SOA or microservices as your \n    product scales with user adoption. SOA and microservices offer respectively smaller segmentation, which is preferred as a \n    modern scalable and reliable architecture, but there are trade-offs to consider, especially when deploying a microservice \n    architecture.\n  \n  \n    One primary trade-off is that you now have a distributed compute architecture that can make it harder to achieve user \n    latency requirements and there is additional complexity in the debugging and tracing of user interactions. You can use \n    AWS X-Ray to assist you in solving this problem. Another effect to consider is increased operational complexity as you \n    increase the number of applications that you are managing, which requires the deployment of multiple independency \n    components.\n  \n    \n       \n        \n       \n       \n      Monolithic, service-oriented, and microservices architectures\n   \n\n \n\n  Implementation steps  \n    \n       \n       \n       \n       \n       \n       \n    \n        \n        Determine the appropriate architecture to refactor or build your application.\n        SOA and microservices offer respectively smaller segmentation, which is \n        preferred as a modern scalable and reliable architecture. SOA can be a \n        good compromise for achieving smaller segmentation while avoiding some of \n        the complexities of microservices. For more details, see Microservice Trade-Offs.      \n        \n      \n         \n          If your workload is amenable to it, and your organization can support it, you \n          should use a microservices architecture to achieve the best agility and reliability.\n          For more details, see Implementing\n            Microservices on AWS.\n        \n      \n         Consider following the Strangler Fig pattern to refactor a monolith into\n          smaller components. This involves gradually replacing specific application components with\n          new applications and services. AWS Migration Hub Refactor Spaces acts as the starting point for incremental refactoring. For\n          more details, see Seamlessly migrate on-premises legacy workloads using a strangler pattern. \n      \n         Implementing microservices may require a service discovery mechanism to allow these\n          distributed services to communicate with each other. AWS App Mesh can be used\n          with service-oriented architectures to provide reliable discovery and access of services.\n            AWS Cloud Map can also be used for dynamic, DNS-based\n          service discovery. \n      \n         If you’re migrating from a monolith to SOA, Amazon MQ can help bridge\n          the gap as a service bus when redesigning legacy applications in the cloud.\n      \n         For existing monoliths with a single, shared database, choose how to reorganize the\n          data into smaller segments. This could be by business unit, access pattern, or data\n          structure. At this point in the refactoring process, you should choose to move forward\n          with a relational or non-relational (NoSQL) type of database. For more details, see From SQL to NoSQL. \n      \n    \n      Level of effort for the implementation plan: High\n    \n    \n   \n\n  Resources\n\n    \n      Related best practices:\n      \n    \n       \n    \n        \n          REL03-BP02 Build services focused on specific business domains\n  and functionality\n        \n      \n    \n      Related documents:\n    \n    \n       \n       \n       \n       \n       \n       \n       \n       \n    \n        \n          Amazon API Gateway: Configuring a REST API Using OpenAPI\n        \n      \n        \n          What is Service-Oriented\n            Architecture?\n        \n      \n        \n          Bounded\n          Context (a central pattern in Domain-Driven Design)\n        \n      \n        \n          Implementing\n          Microservices on AWS\n        \n      \n        \n          Microservice\n          Trade-Offs\n        \n      \n        \n          Microservices\n          - a definition of this new architectural term\n        \n      \n        \n          Microservices\n          on AWS\n        \n      \n        \n          What\n          is AWS App Mesh?\n        \n      \n    \n      Related examples:\n    \n    \n       \n    \n        \n          Iterative App Modernization Workshop\n        \n      \n    \n      Related videos:\n    \n    \n       \n    \n        \n          Delivering Excellence with\n            Microservices on AWS\n        \n      \n  \u003cdiv\u003e\u003cdiv\u003e\u003cdiv\u003e\u003cdiv id=\"js_error_message\"\u003e\u003cp\u003e\u003cimg src=\"https://d1ge0kk1l5kms0.cloudfront.net/images/G/01/webservices/console/warning.png\" alt=\"Warning\" /\u003e \u003cstrong\u003eJavascript is disabled or is unavailable in your browser.\u003c/strong\u003e\u003c/p\u003e\u003cp\u003eTo use the Amazon Web Services Documentation, Javascript must be enabled. Please refer to your browser's Help pages for instructions.\u003c/p\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003eDocument Conventions REL 3. How do you design your workload service architecture? REL03-BP02 Build services focused on specific business domains\n  and functionality",
  "REL03-BP02 Build services focused on specific business domains\n  and functionalityService-oriented architectures (SOA) define services with well-delineated functions defined by business needs. Microservices use domain models and bounded context to draw service boundaries along business context boundaries. Focusing on business domains and functionality helps teams define independent reliability requirements for their services. Bounded contexts isolate and encapsulate business logic, allowing teams to better reason about how to handle failures.\n    Desired outcome: Engineers and business stakeholders jointly define bounded contexts and use them to design systems as services that fulfill specific business functions. These teams use established practices like event storming to define requirements. New applications are designed as services well-defined boundaries and loosely coupling. Existing monoliths are decomposed into bounded contexts and system designs move towards SOA or microservice architectures. When monoliths are refactored, established approaches like bubble contexts and monolith decomposition patterns are applied.\n  \n    Domain-oriented services are executed as one or more processes that don’t share state. They independently respond to fluctuations in demand and handle fault scenarios in light of domain specific requirements.\n  \n    Common anti-patterns:\n  \n     \n     \n     \n     \n  \n      \n        Teams are formed around specific technical domains like UI and UX, middleware, or database instead of specific business domains.\n      \n    \n      \n        Applications span domain responsibilities. Services that span bounded contexts can be more difficult to maintain, require larger testing efforts, and require multiple domain teams to participate in software updates.\n      \n    \n      \n        Domain dependencies, like domain entity libraries, are shared across services such that changes for one service domain require changes to other service domains\n      \n    \n      \n        Service contracts and business logic don’t express entities in a common and consistent domain language, resulting in translation layers that complicate systems and increase debugging efforts.\n      \n    \n    Benefits of establishing this best practice: Applications are designed as independent services bounded by business domains and use a common business language. Services are independently testable and deployable. Services meet domain specific resiliency requirements for the domain implemented. \n  \n    Level of risk exposed if this best practice\n    is not established: High\n  \n\n  Implementation guidance\n    \n      Domain-driven design (DDD) is the foundational approach of designing and building software around business domains. It’s helpful to work with an existing framework when building services focused on business domains. When working with existing monolithic applications, you can take advantage of decomposition patterns that provide established techniques to modernize applications into services.   \n    \n    \n       \n        \n       \n       \n      Domain-driven design\n    \n     \n   \n    \n    Implementation steps\n    \n       \n       \n       \n       \n    \n        \n          Teams can hold event storming workshops to quickly identify events, commands, aggregates and domains in a lightweight sticky note format.\n          \n        \n      \n        \n          Once domain entities and functions have been formed in a domain context, you can divide your domain into services using bounded context, where entities that share similar features and attributes are grouped together. With the model divided into contexts, a template for how to boundary microservices emerges.\n        \n        \n           \n           \n        \n            \n              For example, the Amazon.com website entities might include package, delivery, schedule, price, discount, and currency.\n            \n          \n            \n              Package, delivery, and schedule are grouped into the shipping context, while price, discount, and currency are grouped into the pricing context.\n            \n          \n      \n        \n          Decomposing monoliths into microservices outlines patterns for refactoring microservices. Using patterns for decomposition by business capability, subdomain, or transaction aligns well with domain-driven approaches.\n        \n      \n        \n          Tactical techniques such as the bubble context allow you to introduce DDD in existing or legacy applications without up-front rewrites and full commitments to DDD. In a bubble context approach, a small bounded context is established using a service mapping and coordination, or anti-corruption layer, which protects the newly defined domain model from external influences. \n        \n      \n    \n      After teams have performed domain analysis and defined entities and service contracts, they can take advantage of AWS services to implement their domain-driven design as cloud-based services.\n    \n    \n       \n       \n       \n    \n        \n          Start your development by defining tests that exercise business rules of your domain. Test-driven development (TDD) and behavior-driven development (BDD) help teams keep services focused on solving business problems.\n        \n      \n        \n          Select the AWS services that best meet your business domain requirements and microservice architecture:\n        \n        \n           \n           \n           \n        \n            \n              AWS Serverless allows your team focus on specific domain logic instead of managing servers and infrastructure.\n            \n          \n            \n              Containers at AWS simplify the management of your infrastructure, so you can focus on your domain requirements. \n            \n          \n            \n              Purpose built databases help you match your domain requirements to the best fit database type.\n            \n          \n      \n        \n          Building hexagonal architectures on AWS outlines a framework to build business logic into services working backwards from a business domain to fulfill functional requirements and then attach integration adapters. Patterns that separate interface details from business logic with AWS services help teams focus on domain functionality and improve software quality.\n        \n      \n   \n\n  Resources\n\n    \n      Related best practices:\n    \n    \n       \n       \n    \n        \n          REL03-BP01 Choose how to segment your workload\n        \n      \n        \n          REL03-BP03 Provide service contracts per API\n        \n      \n    \n      Related documents:\n    \n    \n       \n       \n       \n       \n       \n       \n       \n       \n       \n       \n       \n       \n    \n        AWS Microservices\n      \n        \n          Implementing\n            Microservices on AWS\n        \n      \n        \n          How\n            to break a Monolith into Microservices\n        \n      \n        \n          Getting\n          Started with DDD when Surrounded by Legacy Systems\n        \n      \n        \n          Domain-Driven Design: Tackling Complexity in the Heart of Software\n        \n      \n        \n          Building hexagonal architectures on AWS\n      \n        \n          Decomposing monoliths into microservices\n        \n      \n        \n          Event Storming\n        \n      \n        \n          Messages Between Bounded Contexts\n        \n      \n         \n          Microservices\n        \n      \n        \n          Test-driven development\n        \n      \n        \n          Behavior-driven development\n        \n      \n    \n      Related examples:\n    \n    \n       \n    \n        \n          Designing Cloud Native Microservices on AWS (from DDD/EventStormingWorkshop)\n        \n      \n    \n      Related tools:\n    \n    \n       \n       \n       \n    \n        AWS Cloud Databases\n        \n      \n        \n          Serverless on AWS\n      \n        \n          Containers at AWS\n      \n  \u003cdiv\u003e\u003cdiv\u003e\u003cdiv\u003e\u003cdiv id=\"js_error_message\"\u003e\u003cp\u003e\u003cimg src=\"https://d1ge0kk1l5kms0.cloudfront.net/images/G/01/webservices/console/warning.png\" alt=\"Warning\" /\u003e \u003cstrong\u003eJavascript is disabled or is unavailable in your browser.\u003c/strong\u003e\u003c/p\u003e\u003cp\u003eTo use the Amazon Web Services Documentation, Javascript must be enabled. Please refer to your browser's Help pages for instructions.\u003c/p\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003eDocument ConventionsREL03-BP01 Choose how to segment your workloadREL03-BP03 Provide service contracts per API",
  "REL03-BP03 Provide service contracts per APIService contracts are documented agreements between API producers and consumers defined in a machine-readable API definition. A contract versioning strategy allows consumers to continue using the existing API and migrate their applications to a newer API when they are ready. Producer deployment can happen any time as long as the contract is followed. Service teams can use the technology stack of their choice to satisfy the API contract. \n    Desired outcome: Applications built with service-oriented or microservice architectures are able to operate independently while having integrated runtime dependency. Changes deployed to an API consumer or producer do not interrupt the stability of the overall system when both sides follow a common API contract. Components that communicate over service APIs can perform independent functional releases, upgrades to runtime dependencies, or fail over to a disaster recovery (DR) site with little or no impact to each other. In addition, discrete services are able to independently scale absorbing resource demand without requiring other services to scale in unison. \n  \n    Common anti-patterns:\n  \n     \n     \n     \n     \n  \n      \n        Creating service APIs without strongly typed schemas. This results in APIs that cannot be used to generate API bindings and payloads that can’t be programmatically validated.\n      \n    \n      \n        Not adopting a versioning strategy, which forces API consumers to update and release or fail when service contracts evolve.\n      \n    \n      \n        Error messages that leak details of the underlying service implementation rather than describe integration failures in the domain context and language.\n      \n    \n      \n        Not using API contracts to develop test cases and mock API implementations to allow for independent testing of service components. \n      \n    \n    Benefits of establishing this best practice: Distributed systems composed of components that communicate over API service contracts can improve reliability. Developers can catch potential issues early in the development process with type checking during compilation to verify that requests and responses follow the API contract and required fields are present. API contracts provide a clear self-documenting interface for APIs and provider better interoperability between different systems and programming languages.  \n  \n    Level of risk exposed if this best practice\n      is not established: Medium\n  \n\n  Implementation guidance\n    \n      Once you have identified business domains and determined your workload segmentation, you can develop your service APIs. First, define machine-readable service contracts for APIs, and then implement an API versioning strategy. When you are ready to integrate services over common protocols like REST, GraphQL, or asynchronous events, you can incorporate AWS services into your architecture to integrate your components with strongly-typed API contracts. \n    \n    \n      AWS services for service API contrats\n    \n    \n      Incorporate AWS services including Amazon API Gateway, AWS AppSync, and Amazon EventBridge into your architecture to use API service contracts in your application. Amazon API Gateway helps you integrate with directly native AWS services and other web services. API Gateway supports the OpenAPI specification and versioning. AWS AppSync is a managed GraphQL endpoint you configure by defining a GraphQL schema to define a service interface for queries, mutations and subscriptions. Amazon EventBridge uses event schemas to define events and generate code bindings for your events.\n    \n   \n    \n    Implementation steps\n    \n       \n       \n       \n       \n       \n       \n       \n    \n        \n          First, define a contract for your API. A contract will express the capabilities of an API as well as define strongly typed data objects and fields for the API input and output.\n        \n      \n        \n          When you configure APIs in API Gateway, you can import and export OpenAPI Specifications for your endpoints. \n        \n        \n           \n           \n        \n            \n              Importing an OpenAPI definition simplifies the creation of your API and can be integrated with AWS infrastructure as code tools like the AWS Serverless Application Model and AWS Cloud Development Kit (AWS CDK). \n            \n          \n            \n              Exporting an API definition simplifies integrating with API testing tools and provides services consumer an integration specification. \n            \n          \n      \n        \n          You can define and manage GraphQL APIs with AWS AppSync by defining a GraphQL schema file to generate your contract interface and simplify interaction with complex REST models, multiple database tables or legacy services. \n        \n      \n        \n          AWS Amplify projects that are integrated with AWS AppSync generate strongly typed JavaScript query files for use in your application as well as an AWS AppSync GraphQL client library for Amazon DynamoDB tables. \n        \n      \n        \n          When you consume service events from Amazon EventBridge, events adhere to schemas that already exist in the schema registry or that you define with the OpenAPI Spec. With a schema defined in the registry, you can also generate client bindings from the schema contract to integrate your code with events. \n        \n      \n        \n          Extending or version your API. Extending an API is a simpler option when adding fields that can be configured with optional fields or default values for required fields.\n        \n        \n           \n           \n        \n            \n              JSON based contracts for protocols like REST and GraphQL can be a good fit for contract extension.\n            \n          \n            \n              XML based contracts for protocols like SOAP should be tested with service consumers to determine the feasibility of contract extension. \n            \n          \n      \n        \n          When versioning an API, consider implementing proxy versioning where a facade is used to support versions so that logic can be maintained in a single codebase.\n        \n        \n           \n        \n            \n              With API Gateway you can use request and response mappings to simplify absorbing contract changes by establishing a facade to provide default values for new fields or to strip removed fields from a request or response. With this approach the underlying service can maintain a single codebase.\n            \n          \n      \n   \n\n  Resources\n\n    \n      Related best practices:\n    \n    \n       \n       \n       \n       \n       \n    \n        \n          REL03-BP01 Choose how to segment your workload\n        \n      \n        \n          REL03-BP02 Build services focused on specific business domains\n  and functionality\n        \n      \n        \n          REL04-BP02 Implement loosely coupled dependencies\n        \n      \n        \n          REL05-BP03 Control and limit retry calls\n        \n      \n        \n          REL05-BP05 Set client timeouts\n        \n      \n    \n      Related documents:\n    \n    \n       \n       \n       \n       \n       \n       \n       \n       \n       \n    \n        \n          What Is An API (Application Programming Interface)?\n        \n      \n        \n          Implementing Microservices on AWS\n      \n        \n          Microservice Trade-Offs\n        \n      \n        \n          Microservices - a definition of this new architectural term\n        \n      \n        \n          Microservices on AWS\n      \n        \n          Working with API Gateway extensions to OpenAPI\n        \n      \n        \n          OpenAPI-Specification\n        \n      \n        \n          GraphQL: Schemas and Types\n        \n      \n        \n          Amazon EventBridge code bindings\n        \n      \n    \n      Related examples:\n    \n    \n       \n       \n       \n       \n       \n    \n        \n          Amazon API Gateway: Configuring a REST API Using OpenAPI\n        \n      \n        \n          Amazon API Gateway to Amazon DynamoDB CRUD application using OpenAPI\n        \n      \n        \n          Modern application integration patterns in a serverless age: API Gateway Service Integration\n        \n      \n        \n          Implementing header-based API Gateway versioning with Amazon CloudFront\n        \n      \n        AWS AppSync: Building a client application\n        \n      \n    \n      Related videos:\n    \n    \n       \n    \n        \n          Using OpenAPI in AWS SAM to manage API Gateway\n        \n      \n    \n      Related tools:\n    \n    \n       \n       \n       \n    \n        \n          Amazon API Gateway \n        \n      \n        AWS AppSync\n      \n        \n          Amazon EventBridge\n        \n      \n  \u003cdiv\u003e\u003cdiv\u003e\u003cdiv\u003e\u003cdiv id=\"js_error_message\"\u003e\u003cp\u003e\u003cimg src=\"https://d1ge0kk1l5kms0.cloudfront.net/images/G/01/webservices/console/warning.png\" alt=\"Warning\" /\u003e \u003cstrong\u003eJavascript is disabled or is unavailable in your browser.\u003c/strong\u003e\u003c/p\u003e\u003cp\u003eTo use the Amazon Web Services Documentation, Javascript must be enabled. Please refer to your browser's Help pages for instructions.\u003c/p\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003eDocument ConventionsREL03-BP02 Build services focused on specific business domains\n  and functionality REL 4. How do you design interactions in a distributed system to prevent\n                  failures? ",
  "REL04-BP01 Identify the kind of distributed systems you depend\n    on\n    Distributed systems can be synchronous, asynchronous, or batch.\n    Synchronous systems must process requests as quickly as possible and\n    communicate with each other by making synchronous request and\n    response calls using HTTP/S, REST, or remote procedure call (RPC)\n    protocols. Asynchronous systems communicate with each other by\n    exchanging data asynchronously through an intermediary service\n    without coupling individual systems. Batch systems receive a large\n    volume of input data, run automated data processes without human\n    intervention, and generate output data.\n  \n    Desired outcome: Design a\n    workload that effectively interacts with synchronous, asynchronous,\n    and batch dependencies.\n  \n    Common anti-patterns:\n  \n     \n     \n     \n     \n  \n      \n        Workload waits indefinitely for a response from its\n        dependencies, which could lead to workload clients timing out,\n        not knowing if their request has been received.\n      \n    \n      \n        Workload uses a chain of dependent systems that call each other\n        synchronously. This requires each system to be available and to\n        successfully process a request before the whole chain can\n        succeed, leading to potentially brittle behavior and overall\n        availability.\n      \n    \n      \n        Workload communicates with its dependencies asynchronously and\n        rely on the concept of exactly-once guaranteed delivery of\n        messages, when often it is still possible to receive duplicate\n        messages.\n      \n    \n      \n        Workload does not use proper batch scheduling tools and allows\n        concurrent execution of the same batch job.\n      \n    \n    Benefits of establishing this best\n      practice: It is common for a given workload to implement\n    one or more style of communication between synchronous,\n    asynchronous, and batch. This best practice helps you identify the\n    different trade-offs associated with each style of communication to\n    make your workload able to tolerate disruptions in any of its\n    dependencies.\n  \n    Level of risk exposed if this best practice\n      is not established: High\n  \n    \n    Implementation guidance\n    \n      The following sections contain both general and specific\n      implementation guidance for each kind of dependency.\n    \n    \n      General guidance\n    \n    \n       \n       \n       \n       \n    \n        \n          Make sure that the performance and reliability service-level\n          objectives (SLOs) that your dependencies offer meet the\n          performance and reliability requirements of your workload.\n        \n      \n        \n          Use\n          AWS         observability services to\n          monitor\n            response times and error rates to make sure your\n          dependency is providing service at the levels needed by your\n          workload.\n        \n      \n        \n          Identify the potential challenges that your workload may face\n          when communicating with its dependencies. Distributed systems\n          come\n            with a wide range of challenges that might increase\n          architectural complexity, operational burden, and cost. Common\n          challenges include latency, network disruptions, data loss,\n          scaling, and data replication lag.\n        \n      \n        \n          Implement robust error handling and\n          logging\n          to help you troubleshoot problems when your dependency\n          experiences issues.\n        \n      \n    \n      Synchronous dependency\n    \n    \n      In synchronous communications, your workload sends a request to its\n      dependency and blocks the operation waiting for a response. When its\n      dependency receives the request, it tries to handle it as soon as\n      possible and sends a response back to your workload. A significant\n      challenge with synchronous communication is that it causes temporal\n      coupling, which requires your workload and its dependencies to be\n      available at the same time. When your workload needs to communicate\n      synchronously with its dependencies, consider the following\n      guidance:\n    \n    \n       \n       \n       \n       \n       \n    \n        \n          Your workload should not rely on multiple synchronous\n          dependencies to perform a single function. This chain of\n          dependencies increases overall brittleness because all\n          dependencies in the pathway need to be available in order for\n          the request to complete successfully.\n        \n      \n        \n          When a dependency is unhealthy or unavailable, determine your\n          error handling and retry strategies. Avoid using bimodal\n          behavior. Bimodal behavior is when your workload exhibits\n          different behavior under normal and failure modes. For more\n          details on bimodal behavior, see\n          REL11-BP05\n            Use static stability to prevent bimodal behavior.\n        \n      \n        \n          Keep in mind that failing fast is better than making your\n          workload wait. For instance, the\n          AWS Lambda Developer Guide describes how to handle retries\n          and failures when you invoke Lambda functions.\n        \n      \n        \n          Set timeouts when your workload calls its dependency. This\n          technique avoids waiting too long or waiting indefinitely for a\n          response. For helpful discussion of this issue, see\n          Tuning\n            AWS Java SDK HTTP request settings for latency-aware Amazon DynamoDB applications.\n        \n      \n        \n          Minimize the number of calls made from your workload to its\n          dependency to fulfill a single request. Having chatty calls\n          between them increases coupling and latency.\n        \n      \n    \n      Asynchronous dependency\n    \n    \n      To temporally decouple your workload from its dependency, they\n      should communicate asynchronously. Using an asynchronous approach,\n      your workload can continue with any other processing without having\n      to wait for its dependency, or chain of dependencies, to send a\n      response.\n    \n    \n      When your workload needs to communicate asynchronously with its\n      dependency, consider the following guidance:\n    \n    \n       \n    \n        \n          Determine whether to use messaging or event streaming based on\n          your use case and requirements.\n          Messaging\n          allows your workload to communicate with its dependency by\n          sending and receiving messages through a message broker.\n          Event\n            streaming allows your workload and its dependency to use\n          a streaming service to publish and subscribe to events,\n          delivered as continuous streams of data, that need to be\n          processed as soon as possible.\n        \n      \n    \n       \n       \n       \n       \n       \n    \n        \n          Messaging and event streaming handle messages differently so you\n          need to make trade-off decisions based on:\n        \n        \n           \n           \n           \n           \n        \n            \n              Message priority: message\n              brokers can process high-priority messages ahead of normal\n              messages. In event streaming, all messages have the same\n              priority.\n            \n          \n            \n              Message consumption:\n              message brokers ensure that consumers receive the message.\n              Event streaming consumers must keep track of the last\n              message they have read.\n            \n          \n            \n              Message ordering: with\n              messaging, receiving messages in the exact order they are\n              sent is not guaranteed unless you use a first-in-first-out\n              (FIFO) approach. Event streaming always preserves the order\n              in which the data was produced.\n            \n          \n            \n              Message deletion: with\n              messaging, the consumer must delete the message after\n              processing it. The event streaming service appends the\n              message to a stream and remains in there until the message's\n              retention period expires. This deletion policy makes event\n              streaming suitable for replaying messages.\n            \n          \n      \n        \n          Define how your workload knows when its dependency completes its\n          work. For instance, when your workload invokes a\n          Lambda\n            function asynchronously, Lambda places the event in a\n          queue and returns a success response without additional\n          information. After processing is complete, the Lambda function\n          can\n          send\n            the result to a destination, configurable based on\n          success or failure.\n        \n      \n        \n          Build your workload to handle duplicate messages by leveraging\n          idempotency. Idempotency means that the results of your workload\n          do not change even if your workload is generated more than once\n          for the same message. It is important to point out that\n          messaging\n          or\n          streaming\n          services will redeliver a message if a network failure occurs or\n          if an acknowledgement has not been received.\n        \n      \n        \n          If your workload does not get a response from its dependency, it\n          needs to resubmit the request. Consider limiting the number of\n          retries to preserve your workload's CPU, memory, and network\n          resources to handle other requests. The\n          AWS Lambda documentation shows how to handle errors for\n          asynchronous invocation.\n        \n      \n        \n          Leverage suitable observability, debugging, and tracing tools to\n          manage and operate your workload's asynchronous communication\n          with its dependency. You can use\n          Amazon CloudWatch to monitor\n          messaging\n          and\n          event\n            streaming services. You can also instrument your workload\n          with AWS X-Ray to quickly\n          gain\n            insights for troubleshooting problems.\n        \n      \n    \n      Batch dependency\n    \n    \n      Batch systems take input data, initiate a series of jobs to process\n      it, and produce some output data, without manual intervention.\n      Depending on the data size, jobs could run from minutes to, in some\n      cases, several days. When your workload communicates with its batch\n      dependency, consider the following guidance:\n    \n    \n       \n       \n       \n       \n    \n        \n          Define the time window when your workload should run the batch\n          job. Your workload can set up a recurrence pattern to invoke a\n          batch system, for example, every hour or at the end of every\n          month.\n        \n      \n        \n          Determine the location of the data input and the processed data\n          output. Choose a storage service, such as\n          Amazon Simple Storage Services (Amazon S3),\n          Amazon Elastic File System (Amazon EFS), and\n          Amazon FSx for Lustre, that allows your workload to read and\n          write files at scale.\n        \n      \n        \n          If your workload needs to invoke multiple batch jobs, you could\n          leverage\n          AWS Step Functions to simplify the orchestration of batch\n          jobs that run in AWS or on-premises. This\n          sample\n            project demonstrates orchestration of batch jobs using\n          Step Functions,\n          AWS Batch, and Lambda.\n        \n      \n        \n          Monitor batch jobs to look for abnormalities, such as a job\n          taking longer than it should to complete. You could use tools\n          like\n          CloudWatch\n            Container Insights to monitor AWS Batch environments and\n          jobs. In this instance, your workload would stop the next job\n          from beginning and inform the relevant staff of the exception.\n        \n      \n   \n    \n    Resources\n    \n    \n    \n      Related documents:\n    \n    \n       \n       \n       \n       \n       \n       \n       \n       \n       \n       \n       \n       \n       \n       \n       \n    \n        \n          AWS Cloud Operations: Monitoring and Observability\n        \n      \n        \n          The\n            Amazon's Builder Library: Challenges with distributed\n            systems\n        \n      \n        \n          REL11-BP05\n            Use static stability to prevent bimodal behavior\n        \n      \n        \n          AWS Lambda Developer Guide: Error handling and automatic retries in\n            AWS Lambda\n        \n      \n        \n          Tuning\n            AWS Java SDK HTTP request settings for latency-aware Amazon DynamoDB applications\n        \n      \n        \n          AWS         Messaging\n        \n      \n        \n          What\n            is streaming data?\n        \n      \n        \n          AWS Lambda Developer Guide: Asynchronous invocation\n        \n      \n        \n          Amazon Simple Queue Service FAQ: FIFO queues\n        \n      \n        \n          Amazon Kinesis Data Streams Developer Guide: Handling Duplicate\n            Records\n        \n      \n        \n          Amazon Simple Queue Service Developer Guide: Available CloudWatch\n            metrics for Amazon SQS\n        \n      \n        \n          Amazon Kinesis Data Streams Developer Guide: Monitoring the Amazon Kinesis Data Streams Service with Amazon CloudWatch\n        \n      \n        \n          AWS X-Ray Developer Guide: AWS X-Ray concepts\n        \n      \n        \n          AWS         Samples on GitHub: AWS Step functions Complex Orchestrator\n            App\n        \n      \n        \n          AWS Batch User Guide: AWS Batch CloudWatch Container Insights\n        \n      \n    \n      Related videos:\n    \n    \n       \n    \n        \n          AWS         Summit SF 2022 - Full-stack observability and application\n            monitoring with AWS (COP310)\n        \n      \n    \n      Related tools:\n    \n    \n       \n       \n       \n       \n       \n       \n       \n       \n    \n        \n          Amazon CloudWatch\n        \n      \n        \n          Amazon CloudWatch Logs\n        \n      \n        \n          AWS X-Ray\n        \n      \n        \n          Amazon Simple Storage Services (Amazon S3)\n        \n      \n        \n          Amazon Elastic File System (Amazon EFS)\n        \n      \n        \n          Amazon FSx for Lustre\n        \n      \n        \n          AWS Step Functions\n        \n      \n        \n          AWS Batch\n        \n      \n  \u003cdiv\u003e\u003cdiv\u003e\u003cdiv\u003e\u003cdiv id=\"js_error_message\"\u003e\u003cp\u003e\u003cimg src=\"https://d1ge0kk1l5kms0.cloudfront.net/images/G/01/webservices/console/warning.png\" alt=\"Warning\" /\u003e \u003cstrong\u003eJavascript is disabled or is unavailable in your browser.\u003c/strong\u003e\u003c/p\u003e\u003cp\u003eTo use the Amazon Web Services Documentation, Javascript must be enabled. Please refer to your browser's Help pages for instructions.\u003c/p\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003eDocument Conventions REL 4. How do you design interactions in a distributed system to prevent\n                  failures? REL04-BP02 Implement loosely coupled dependencies",
  "REL04-BP02 Implement loosely coupled dependencies\n    Dependencies such as queuing systems, streaming systems, workflows,\n    and load balancers are loosely coupled. Loose coupling helps isolate\n    behavior of a component from other components that depend on it,\n    increasing resiliency and agility.\n  \n    Decoupling dependencies, such as queuing systems, streaming systems, and workflows, help minimize the impact of changes or failure on a system. This separation isolates a component's behavior from affecting others that depend on it, improving resilience and agility. \n  \n    In tightly coupled systems, changes to one component can necessitate changes in other components that rely on it, resulting in degraded performance across all components. Loose coupling breaks this dependency so that dependent components only need to know the versioned and published interface. Implementing loose coupling between dependencies isolates a failure in one from impacting another.\n  \n    Loose coupling allows you to modify code or add features to a component while minimizing risk to other components that depend on it. It also allows for granular resilience at a component level where you can scale out or even change underlying implementation of the dependency.\n  \n    To further improve resiliency through loose coupling, make component\n    interactions asynchronous where possible. This model is suitable for\n    any interaction that does not need an immediate response and where\n    an acknowledgment that a request has been registered will suffice.\n    It involves one component that generates events and another that\n    consumes them. The two components do not integrate through direct\n    point-to-point interaction but usually through an intermediate\n    durable storage layer, such as an Amazon SQS queue, a streaming data\n    platform such as Amazon Kinesis, or AWS Step Functions.\n  \n       \n\n       \n       \n      Figure 4: Dependencies such as queuing systems and load\n        balancers are loosely coupled\n   \n\n    Amazon SQS queues and AWS Step Functions are just two ways to\n    add an intermediate layer for loose coupling. Event-driven\n    architectures can also be built in the AWS Cloud using Amazon EventBridge, which can abstract clients (event producers) from the\n    services they rely on (event consumers). Amazon Simple Notification Service (Amazon SNS) is an effective solution when you need\n    high-throughput, push-based, many-to-many messaging. Using Amazon SNS topics, your publisher systems can fan out messages to a large\n    number of subscriber endpoints for parallel processing.\n  \n    While queues offer several advantages, in most hard real-time\n    systems, requests older than a threshold time (often seconds) should\n    be considered stale (the client has given up and is no longer\n    waiting for a response), and not processed. This way more recent\n    (and likely still valid requests) can be processed instead.\n  \n    Desired outcome: Implementing loosely coupled dependencies allows you to minimize the surface area for failure to a component level, which helps diagnose and resolve issues. It also simplifies development cycles, allowing teams to implement changes at a modular level without affecting the performance of other components that depend on it. This approach provides the capability to scale out at a component level based on resource needs, as well as utilization of a component contributing to cost-effectiveness.\n  \n    Common anti-patterns:\n  \n     \n     \n     \n     \n  \n      \n        Deploying a monolithic workload.      \n      \n    \n      \n        Directly invoking APIs between workload tiers with no capability\n        of failover or asynchronous processing of the request.\n      \n    \n      \n        Tight coupling using shared data. Loosely coupled systems should avoid sharing data through shared databases or other forms of tightly coupled data storage, which can reintroduce tight coupling and hinder scalability.\n      \n    \n      \n        Ignoring back pressure. Your workload should have the ability to slow down or stop incoming data when a component can't process it at the same rate.\n      \n    \n    Benefits of establishing this best\n    practice: Loose coupling helps isolate behavior of a\n    component from other components that depend on it, increasing\n    resiliency and agility. Failure in one component is isolated from\n    others.\n  \n    Level of risk exposed if this best practice\n    is not established: High\n  \n\n  Implementation guidance\n    \n      Implement loosely coupled dependencies. There are various solutions that allow you to build loosely coupled applications. These include services for implementing fully managed queues, automated workflows, react to events,  and APIs among others which can help isolate behavior of components from other components, and as such increasing resilience and agility.\n    \n    \n       \n       \n       \n       \n       \n    \n        \n          Build event-driven architectures: Amazon EventBridge helps you build loosely coupled and distributed event-driven architectures.\n        \n      \n        \n          Implement queues in distributed systems: You can use Amazon Simple Queue Service (Amazon SQS) to integrate and decouple distributed systems.\n        \n      \n        \n          Containerize components as microservices: Microservices allow teams to build applications composed of small independent components which communicate over well-defined APIs. Amazon Elastic Container Service (Amazon ECS), and Amazon Elastic Kubernetes Service (Amazon EKS) can help you get started faster with containers.\n        \n      \n        \n          Manage workflows with Step Functions: Step Functions help you coordinate multiple AWS services into flexible workflows.\n        \n      \n        \n          Leverage publish-subscribe (pub/sub) messaging architectures: Amazon Simple Notification Service (Amazon SNS) provides message delivery from publishers to subscribers (also known as producers and consumers). \n        \n      \n     \n      \n      Implementation steps\n      \n         \n         \n         \n         \n      \n          \n            Components in an event-driven architecture are initiated by events. Events are actions that happen in a system, such as a user adding an item to a cart. When an action is successful, an event is generated that actuates the next component of the system. \n          \n          \n             \n             \n          \n              \n                Building Event-driven Applications with Amazon EventBridge\n              \n            \n              AWS re:Invent 2022 - Designing Event-Driven Integrations using Amazon EventBridge\n              \n            \n        \n          \n            Distributed messaging systems have three main parts that need to be implemented for a queue based architecture. They include components of the distributed system, the queue that is used for decoupling (distributed on Amazon SQS servers), and the messages in the queue. A typical system has producers which initiate the message into the queue, and the consumer which receives the message from the queue. The queue stores messages across multiple Amazon SQS servers for redundancy.\n          \n          \n             \n             \n          \n              \n                Basic Amazon SQS architecture\n              \n            \n              \n                Send Messages Between Distributed Applications with Amazon Simple Queue Service\n              \n            \n        \n          \n            Microservices, when well-utilized, enhance maintainability and boost scalability, as loosely coupled components are managed by independent teams. It also allows for the isolation of behaviors to a single component in case of changes.\n          \n          \n             \n             \n          \n              \n                Implementing Microservices on AWS\n            \n              \n                Let's Architect! Architecting microservices with containers\n              \n            \n        \n          \n            With AWS Step Functions you can build distributed applications, automate processes, orchestrate microservices, among other things. The orchestration of multiple components into an automated workflow allows you to decouple dependencies in your application.\n          \n          \n             \n             \n          \n              \n                Create a Serverless Workflow with AWS Step Functions and AWS Lambda\n            \n              \n                Getting Started with AWS Step Functions\n            \n        \n     \n   \n\n  Resources\n\n      \n    \n      Related documents:\n    \n    \n       \n       \n       \n       \n       \n       \n       \n       \n       \n    \n        \n          Amazon EC2: Ensuring Idempotency\n        \n      \n        \n          The\n          Amazon Builders' Library: Challenges with distributed\n          systems\n        \n      \n        \n          The\n          Amazon Builders' Library: Reliability, constant work, and a\n          good cup of coffee\n        \n      \n        \n          What\n          Is Amazon EventBridge?\n        \n      \n        \n          What\n          Is Amazon Simple Queue Service?\n        \n      \n        \n          Break up with your monolith\n        \n      \n        \n          Orchestrate Queue-based Microservices with AWS Step Functions and Amazon SQS\n        \n      \n        \n          Basic Amazon SQS architecture\n        \n      \n        \n          Queue-Based Architecture\n        \n      \n    \n      Related videos:\n    \n    \n       \n       \n       \n       \n       \n       \n    \n        \n          AWS New York\n          Summit 2019: Intro to Event-driven Architectures and Amazon EventBridge (MAD205)\n        \n      \n        \n          AWS re:Invent\n          2018: Close Loops and Opening Minds: How to Take Control of Systems, Big and Small ARC337 (includes loose coupling,\n          constant work, static stability)\n        \n      \n        \n          AWS re:Invent\n          2019: Moving to event-driven architectures (SVS308)\n        \n      \n        AWS re:Invent 2019: Scalable serverless event-driven applications using Amazon SQS and Lambda\n        \n      \n        AWS re:Invent 2022 - Designing event-driven integrations using Amazon EventBridge\n        \n      \n        AWS re:Invent 2017: Elastic Load Balancing Deep Dive and Best Practices\n        \n      \n  \u003cdiv\u003e\u003cdiv\u003e\u003cdiv\u003e\u003cdiv id=\"js_error_message\"\u003e\u003cp\u003e\u003cimg src=\"https://d1ge0kk1l5kms0.cloudfront.net/images/G/01/webservices/console/warning.png\" alt=\"Warning\" /\u003e \u003cstrong\u003eJavascript is disabled or is unavailable in your browser.\u003c/strong\u003e\u003c/p\u003e\u003cp\u003eTo use the Amazon Web Services Documentation, Javascript must be enabled. Please refer to your browser's Help pages for instructions.\u003c/p\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003eDocument ConventionsREL04-BP01 Identify the kind of distributed systems you depend\n      onREL04-BP03 Do constant work",
  "REL04-BP03 Do constant work\n    Systems can fail when there are large, rapid changes in load. For\n    example, if your workload is doing a health check that monitors the\n    health of thousands of servers, it should send the same size payload\n    (a full snapshot of the current state) each time. Whether no servers\n    are failing, or all of them, the health check system is doing\n    constant work with no large, rapid changes.\n  \n    For example, if the health check system is monitoring 100,000\n    servers, the load on it is nominal under the normally light server\n    failure rate. However, if a major event makes half of those servers\n    unhealthy, then the health check system would be overwhelmed trying\n    to update notification systems and communicate state to its clients.\n    So instead the health check system should send the full snapshot of\n    the current state each time. 100,000 server health states, each\n    represented by a bit, would only be a 12.5-KB payload. Whether no\n    servers are failing, or all of them are, the health check system is\n    doing constant work, and large, rapid changes are not a threat to\n    the system stability. This is actually how Amazon Route 53 handles\n    health checks for endpoints (such as IP addresses) to determine how\n    end users are routed to them.\n  \n    Level of risk exposed if this best practice\n    is not established: Low\n  \n\n  Implementation guidance\n\n\n    \n       \n       \n     \n        Do constant work so that systems do not fail when there are large,\n        rapid changes in load.\n      \n        Implement loosely coupled dependencies. Dependencies such as\n        queuing systems, streaming systems, workflows, and load balancers\n        are loosely coupled. Loose coupling helps isolate behavior of a\n        component from other components that depend on it, increasing\n        resiliency and agility.\n      \n        \n           \n           \n        \n            \n              The\n                Amazon Builders' Library: Reliability, constant work, and a good cup of\n                coffee\n            \n          \n            \n              AWS re:Invent 2018: Close Loops and\n                Opening Minds: How to Take Control of Systems, Big and Small ARC337 (includes\n                constant work)\n            \n            \n               \n            \n                 For the example of a health check system monitoring 100,000 servers, engineer\n                  workloads so that payload sizes remain constant regardless of number of successes\n                  or failures. \n              \n          \n\n   \n\n  Resources\n\n      \n    \n      Related documents:\n    \n    \n       \n       \n       \n    \n        \n          Amazon EC2: Ensuring Idempotency\n        \n      \n        \n          The\n          Amazon Builders' Library: Challenges with distributed\n          systems\n        \n      \n        \n          The\n          Amazon Builders' Library: Reliability, constant work, and a\n          good cup of coffee\n        \n      \n    \n      Related videos:\n    \n    \n       \n       \n       \n       \n    \n        \n          AWS New York\n          Summit 2019: Intro to Event-driven Architectures and Amazon EventBridge (MAD205)\n        \n      \n        \n          AWS           re:Invent 2018: Close Loops and Opening Minds: How to Take\n          Control of Systems, Big and Small ARC337 (includes constant\n          work)\n        \n      \n        \n          AWS re:Invent\n          2018: Close Loops and Opening Minds: How to Take Control of\n          Systems, Big and Small ARC337 (includes loose coupling, constant work, static stability)\n        \n      \n        \n          AWS re:Invent\n          2019: Moving to event-driven architectures (SVS308)\n        \n      \n  \u003cdiv\u003e\u003cdiv\u003e\u003cdiv\u003e\u003cdiv id=\"js_error_message\"\u003e\u003cp\u003e\u003cimg src=\"https://d1ge0kk1l5kms0.cloudfront.net/images/G/01/webservices/console/warning.png\" alt=\"Warning\" /\u003e \u003cstrong\u003eJavascript is disabled or is unavailable in your browser.\u003c/strong\u003e\u003c/p\u003e\u003cp\u003eTo use the Amazon Web Services Documentation, Javascript must be enabled. Please refer to your browser's Help pages for instructions.\u003c/p\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003eDocument ConventionsREL04-BP02 Implement loosely coupled dependenciesREL04-BP04 Make mutating operations idempotent",
  "REL04-BP04 Make mutating operations idempotent\n    An idempotent service promises that each request is processed\n    exactly once, such that making multiple identical requests has the\n    same effect as making a single request. This makes it easier for a\n    client to implement retries without fear that a request is\n    erroneously processed multiple times. To do this, clients can issue\n    API requests with an idempotency token, which is used whenever the\n    request is repeated. An idempotent service API uses the token to\n    return a response identical to the response that was returned the\n    first time that the request was completed, even if the underlying\n    state of the system has changed.\n  \n    In a distributed system, it is relatively simple to perform an\n    action at most once (client makes only one request) or at least once\n    (keep requesting until client gets confirmation of success). It is\n    more difficult to guarantee an action is performed exactly\n    once, such that making multiple identical requests has\n    the same effect as making a single request. Using idempotency tokens\n    in APIs, services can receive a mutating request one or more times\n    without the need to create duplicate records or side effects.\n  \n    Desired outcome: You have a\n    consistent, well-documented, and widely adopted approach for\n    ensuring idempotency across all components and services.\n  \n    Common anti-patterns:\n  \n     \n     \n     \n     \n     \n  \n      \n        You apply idempotency indiscriminately, even when not needed.\n      \n    \n      \n        You introduce overly complex logic for implementing idempotency.\n      \n    \n      \n        You use timestamps as keys for idempotency. This can cause\n        inaccuracies due to clock skew or due to multiple clients that\n        use the same timestamps to apply changes.\n      \n    \n      \n        You store entire payloads for idempotency. In this approach, you\n        save complete data payloads for every request and overwrite it\n        at each new request. This can degrade performance and affect\n        scalability.\n      \n    \n      \n        You generate keys inconsistently across services. Without\n        consistent keys, services may fail to recognize duplicate\n        requests, which results in unintended results.\n      \n    \n    Benefits of establishing this best\n    practice:\n  \n     \n     \n     \n     \n     \n     \n  \n      \n        Greater scalability: The system can handle retries and duplicate\n        requests without having to perform additional logic or complex\n        state management.\n      \n    \n      \n        Enhanced reliability: Idempotency helps services handle multiple\n        identical requests in a consistent manner, which reduces the\n        risk of unintended side effects or duplicate records. This is\n        especially crucial in distributed systems, where network\n        failures and retries are common.\n      \n    \n      \n        Improved data consistency: Because the same request produces the\n        same response, idempotency helps maintain data consistency\n        across distributed systems. This is essential to maintain the\n        integrity of transactions and operations.\n      \n    \n      \n        Error handling: Idempotency tokens make error handling more\n        straightforward. If a client does not receive a response due to\n        an issue, it can safely resend the request with the same\n        idempotency token.\n      \n    \n      \n        Operational transparency: Idempotency allows for better\n        monitoring and logging. Services can log requests with their\n        idempotency tokens, which makes it easier to trace and debug\n        issues.\n      \n    \n      \n        Simplified API contract: It can simplify the contract between\n        the client and server side systems and reduce the fear of\n        erroneous data processing.\n      \n    \n    Level of risk exposed if this best practice\n    is not established: Medium\n  \n\n  Implementation guidance\n\n      \n    \n      In a distributed system, performing an action at most once (the\n      client makes only one request) or at least once (the client keeps\n      requesting until success is confirmed) is relatively\n      straightforward. However, it's challenging to implement\n      exactly once behavior. To achieve this, your\n      clients should generate and provide an idempotency token for each\n      request.\n    \n    \n      By using idempotency tokens, a service can distinguish between new\n      requests and repeated ones. When a service receives a request with\n      an idempotency token, it checks if the token has already been\n      used. If the token has been used, the service retrieves and\n      returns the stored response. If the token is new, the service\n      processes the request, stores the response along with the token,\n      and then returns the response. This mechanism makes all responses\n      idempotent, which enhances the reliability and consistency of the\n      distributed system.\n    \n    \n      Idempotency is also an important behavior of event-driven\n      architectures. These architectures are typically backed by a\n      message queue such as Amazon SQS, Amazon MQ, Amazon Kinesis\n      Streams, or Amazon Managed Streaming for Apache Kafka (MSK). In some\n      circumstances, a message that was published only once may be\n      accidentally delivered more than once. When a publisher generates\n      and includes idempotency tokens in messages, it requests that the\n      processing of any duplicate message received doesn't result in a\n      repeated action for the same message. Consumers should keep track\n      of each token received and ignore messages that contain duplicate\n      tokens.\n    \n    \n      Services and consumers should also pass the received idempotency\n      token to any downstream services that it calls. Every downstream\n      service in the processing chain is similarly responsible for\n      making sure that idempotency is implemented to avoid the side\n      effect of processing a message more than once.\n    \n     \n\n  Implementation steps\n\n      \n      \n         \n         \n         \n         \n         \n         \n      \n          \n            Identify idempotent\n            operations\n          \n          \n            Determine which operations require idempotency. These\n            typically include POST,\n            PUT, and DELETE HTTP\n            methods and database insert, update, or delete operations.\n            Operations that do not mutate state, such as read-only\n            queries, usually do not require idempotency unless they have\n            side effects.\n          \n        \n          \n            Use unique identifiers\n          \n          \n            Include a unique token in each idempotent operation request\n            sent by the sender, either directly in the request or as\n            part of its metadata (for example, an HTTP header). This\n            allows the receiver to recognize and handle duplicate\n            requests or operations. Identifiers commonly used for tokens\n            include\n            Universally\n            Unique Identifiers (UUIDs) and\n            \n            \n            \n            K-Sortable\n            Unique Identifiers (KSUIDs).\n          \n        \n          \n            Track and manage state\n          \n          \n            Maintain the state of each operation or request in your\n            workload. This can be achieved by storing the idempotency\n            token and the corresponding state (such as\n            pending, completed, or\n            failed) in a database, cache, or other\n            persistent store. This state information allows the workload\n            to identify and handle duplicate requests or operations.\n          \n          \n            Maintain consistency and atomicity by using appropriate\n            concurrency control mechanisms if needed, such as locks,\n            transactions, or optimistic concurrency controls. This\n            includes the process of recording the idempotent token and\n            running all mutating operations associated with servicing\n            the request. This helps prevent race conditions and verifies\n            that idempotent operations run correctly.\n          \n          \n            Regularly remove old idempotency tokens from the datastore\n            to manage storage and performance. If your storage system\n            supports it, consider using expiration timestamps for data\n            (often known as time to live, or TTL values). The likelihood\n            of idempotency token reuse diminishes over time.\n          \n          \n            Common AWS storage options typically used for storing\n            idempotency tokens and related state include:\n          \n          \n             \n             \n             \n             \n          \n              \n                Amazon DynamoDB:\n                DynamoDB is a NoSQL database service that provides\n                low-latency performance and high availability, which\n                makes it well-suited for the storage of\n                idempotency-related data. The key-value and document\n                data model of DynamoDB allows for efficient storage and\n                retrieval of idempotency tokens and associated state\n                information. DynamoDB can also expire idempotency tokens\n                automatically if your application sets a TTL value when\n                it inserts them.\n              \n            \n              \n                Amazon ElastiCache:\n                ElastiCache can store idempotency tokens with high\n                throughput, low latency, and at low cost. Both\n                ElastiCache (Redis) and ElastiCache (Memcached) can also\n                expire idempotency tokens automatically if your\n                application sets a TTL value when it inserts them.\n              \n            \n              \n                Amazon Relational Database Service\n                  (RDS): You can use Amazon RDS to store idempotency\n                tokens and related state information, especially if your\n                application already uses a relational database for other\n                purposes.\n              \n            \n              \n                Amazon Simple Storage Service (S3): Amazon S3 is a highly scalable and durable object storage service that can be used to store idempotency tokens and related metadata. The versioning capabilities of S3 can be particularly useful for maintenance of the state of idempotent operations.\n                \n                The choice of storage service typically depends on factors such as the volume of idempotency-related data, the required performance characteristics, the need for durability and availability, and how the idempotency mechanism integrates with the overall workload architecture.\n              \n            \n        \n          \n            Implement idempotent\n            operations\n          \n          \n            Design your API and workload components to be idempotent.\n            Incorporate idempotency checks into your workload\n            components. Before you process a request or perform an\n            operation, check if the unique identifier has already been\n            processed. If it has, return the previous result instead of\n            executing the operation again. For example, if a client\n            sends a request to create a user, check if a user with the\n            same unique identifier already exists. If the user exists,\n            it should return the existing user information instead of\n            creating a new one. Similarly, if a queue consumer receives\n            a message with a duplicate idempotency token, the consumer\n            should ignore the message.\n          \n          \n            Create comprehensive test suites that validate the\n            idempotency of requests. They should cover a wide range of\n            scenarios, such as successful requests, failed requests, and\n            duplicate requests.\n          \n          \n            If your workload leverages AWS Lambda functions, consider\n            Powertools for AWS Lambda. Powertools for AWS Lambda is a\n            developer toolkit that helps implement serverless best\n            practices and increase developer velocity when you work with\n            AWS Lambda functions. In particular, it provides a utility\n            to convert your Lambda functions into idempotent operations\n            which are safe to retry.\n          \n        \n          \n            Communicate idempotency\n            clearly\n          \n          \n            Document your API and workload components to clearly\n            communicate the idempotent nature of the operations. This\n            helps clients understand the expected behavior and how to\n            interact with your workload reliably.\n          \n        \n          \n            Monitor and audit\n          \n          \n            Implement monitoring and auditing mechanisms to detect any\n            issues related to the idempotency of responses, such as\n            unexpected response variations or excessive duplicate\n            request handling. This can help you detect and investigate\n            any issues or unexpected behaviors in your workload.\n          \n        \n     \n   \n\n  Resources\n\n      \n    \n      Related best practices:\n    \n    \n       \n       \n       \n       \n    \n        \n          REL05-BP03\n          Control and limit retry calls\n        \n      \n        \n          REL06-BP01\n          Monitor all components for the workload (Generation)\n        \n      \n        \n          REL06-BP03\n          Send notifications (Real-time processing and alarming))\n        \n      \n        \n          REL08-BP02\n          Integrate functional testing as part of your deployment\n        \n      \n    \n      Related documents:\n    \n    \n       \n       \n       \n       \n       \n       \n    \n        \n          The\n          Amazon Builders' Library: Making retries safe with idempotent APIs\n        \n      \n        \n          The\n          Amazon Builders' Library: Challenges with distributed systems\n        \n      \n        \n          The\n          Amazon Builders' Library: Reliability, constant work, and a good cup of coffee\n        \n      \n        \n          Amazon Elastic Container Service: Ensuring idempotency\n        \n      \n        \n          How\n          do I make my Lambda function idempotent?\n        \n      \n        \n          Ensuring\n          idempotency in Amazon EC2 API requests\n        \n      \n    \n      Related videos:\n    \n    \n       \n       \n       \n       \n       \n       \n    \n        \n          Building\n          Distributed Applications with Event-driven Architecture - AWS Online Tech Talks\n        \n      \n        \n          AWS           re:Invent 2023 - Building next-generation applications with event-driven architecture\n        \n      \n        \n          AWS           re:Invent 2023 - Advanced integration patterns \u0026 trade-offs for loosely coupled systems\n        \n      \n        \n          AWS           re:Invent 2023 - Advanced event-driven patterns with Amazon EventBridge\n        \n      \n        \n          AWS re:Invent\n          2018 - Close Loops and Opening Minds: How to Take Control of Systems, Big and Small ARC337 (includes loose coupling, constant work, static stability)\n        \n      \n        \n          AWS re:Invent\n          2019 - Moving to event-driven architectures (SVS308)\n        \n      \n    \n      Related tools:\n    \n    \n       \n       \n       \n    \n        \n          Idempotency\n          with AWS Lambda Powertools (Java)\n        \n      \n        \n          Idempotency\n          with AWS Lambda Powertools (Python)\n        \n      \n        \n          AWS Lambda Powertools GitHub page\n        \n      \n  \u003cdiv\u003e\u003cdiv\u003e\u003cdiv\u003e\u003cdiv id=\"js_error_message\"\u003e\u003cp\u003e\u003cimg src=\"https://d1ge0kk1l5kms0.cloudfront.net/images/G/01/webservices/console/warning.png\" alt=\"Warning\" /\u003e \u003cstrong\u003eJavascript is disabled or is unavailable in your browser.\u003c/strong\u003e\u003c/p\u003e\u003cp\u003eTo use the Amazon Web Services Documentation, Javascript must be enabled. Please refer to your browser's Help pages for instructions.\u003c/p\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003eDocument ConventionsREL04-BP03 Do constant work REL 5. How do you design interactions in a distributed system to mitigate\n                  or withstand failures? ",
  "REL05-BP01 Implement graceful degradation to transform\n  applicable hard dependencies into soft dependenciesApplication components should continue to perform their core function even if dependencies become unavailable. They might be serving slightly stale data, alternate data, or even no data. This ensures overall system function is only minimally impeded by localized failures while delivering the central business value.\n    Desired outcome: When a component's dependencies are unhealthy, the component itself can still function, although in a degraded manner. Failure modes of components should be seen as normal operation. Workflows should be designed in such a way that such failures do not lead to complete failure or at least to predictable and recoverable states. \n  \n    Common anti-patterns:\n  \n     \n     \n     \n     \n     \n  \n      \n        Not identifying the core business functionality needed. Not testing that components are functional even during dependency failures.\n      \n    \n      \n        Serving no data on errors or when only one out of multiple dependencies is unavailable and partial results can still be returned.\n      \n    \n      \n        Creating an inconsistent state when a transaction partially fails.\n      \n    \n      \n        Not having an alternative way to access a central parameter store.\n      \n    \n      \n        Invalidating or emptying local state as a result of a failed refresh without considering the consequences of doing so.\n      \n    \n    Benefits of establishing this best practice: Graceful degradation improves the availability of the system as a whole and maintains the functionality of the most important functions even during failures. \n  \n    Level of risk exposed if this best practice\n    is not established: High\n  \n\n  Implementation guidance\n\n  Implementing graceful degradation helps minimize the impact of dependency failures on component function. Ideally, a component detects dependency failures and works around them in a way that minimally impacts other components or customers.\n\n    \n      Architecting for graceful degradation means considering potential failure modes during dependency design. For each failure mode, have a way to deliver most or at least the most critical functionality of the component to callers or customers. These considerations can become additional requirements that can be tested and verified. Ideally, a component is able to perform its core function in an acceptable manner even when one or multiple dependencies fail.\n    \n    \n      This is as much a business discussion as a technical one. All business requirements are important and should be fulfilled if possible. However, it still makes sense to ask what should happen when not all of them can be fulfilled. A system can be designed to be available and consistent, but under circumstances where one requirement must be dropped, which one is more important? For payment processing, it might be consistency. For a real-time application, it might be availability. For a customer facing website, the answer may depend on customer expectations.\n    \n    \n      What this means depends on the requirements of the component and what should be considered its core function. For example:\n    \n    \n       \n       \n       \n       \n       \n    \n        \n          An ecommerce website might display data from multiple different systems like personalized recommendations, highest ranked products, and status of customer orders on the landing page. When one upstream system fails, it still makes sense to display everything else instead of showing an error page to a customer.\n        \n      \n        \n          A component performing batch writes can still continue processing a batch if one of the individual operations fails. It should be simple to implement a retry mechanism. This can be done by returning information on which operations succeeded, which failed, and why they failed to the caller, or putting failed requests into a dead letter queue to implement asynchronous retries. Information about failed operations should be logged as well.\n        \n      \n        \n          A system that processes transactions must verify that either all or no individual updates are executed. For distributed transactions, the saga pattern can be used to roll back previous operations in case a later operation of the same transaction fails. Here, the core function is maintaining consistency.\n        \n      \n        \n          Time critical systems should be able to deal with dependencies not responding in a timely manner. In these cases, the circuit breaker pattern can be used. When responses from a dependency start timing out, the system can switch to a closed state where no additional call are made.\n        \n      \n        \n          An application may read parameters from a parameter store. It can be useful to create container images with a default set of parameters and use these in case the parameter store is unavailable.\n        \n      \n    \n      Note that the pathways taken in case of component failure need to be tested and should be significantly simpler than the primary pathway. Generally, fallback strategies should be avoided.\n    \n   \n    \n    Implementation steps\n    \n      Identify external and internal dependencies. Consider what kinds of failures can occur in them. Think about ways that minimize negative impact on upstream and downstream systems and customers during those failures.\n    \n    \n      The following is a list of dependencies and how to degrade gracefully when they fail:\n    \n    \n       \n       \n       \n       \n       \n    \n        \n          Partial failure of dependencies: A component may make multiple requests to downstream systems, either as multiple requests to one system or one request to multiple systems each. Depending on the business context, different ways of handling for this may be appropriate (for more detail, see previous examples in Implementation guidance).\n        \n      \n        \n          A downstream system is unable to process requests due to high load: If requests to a downstream system are consistently failing, it does not make sense to continue retrying. This may create additional load on an already overloaded system and make recovery more difficult. The circuit breaker pattern can be utilized here, which monitors failing calls to a downstream system. If a high number of calls are failing, it will stop sending more requests to the downstream system and only occasionally let calls through to test whether the downstream system is available again.\n        \n      \n        \n          A parameter store is unavailable: To transform a parameter store, soft dependency caching or sane defaults included in container or machine images may be used. Note that these defaults need to be kept up-to-date and included in test suites.\n        \n      \n        \n          A monitoring service or other non-functional dependency is unavailable: If a component is intermittently unable to send logs, metrics, or traces to a central monitoring service, it is often best to still execute business functions as usual. Silently not logging or pushing metrics for a long time is often not acceptable. Also, some use cases may require complete auditing entries to fulfill compliance requirements.\n        \n      \n        \n          A primary instance of a relational database may be unavailable: Amazon Relational Database Service, like almost all relational databases, can only have one primary writer instance. This creates a single point of failure for write workloads and makes scaling more difficult. This can partially be mitigated by using a Multi-AZ configuration for high availability or Amazon Aurora Serverless for better scaling. For very high availability requirements, it can make sense to not rely on the primary writer at all. For queries that only read, read replicas can be used, which provide redundancy and the ability to scale out, not just up. Writes can be buffered, for example in an Amazon Simple Queue Service queue, so that write requests from customers can still be accepted even if the primary is temporarily unavailable. \n        \n      \n   \n\n  Resources\n\n      \n    \n      Related documents:\n    \n    \n       \n       \n       \n       \n       \n       \n       \n       \n    \n        \n          Amazon API Gateway: Throttle API Requests for Better\n          Throughput\n        \n      \n        \n          CircuitBreaker\n          (summarizes Circuit Breaker from “Release It!” book)\n        \n      \n        \n          Error\n          Retries and Exponential Backoff in AWS\n        \n      \n        \n          Michael\n          Nygard “Release It! Design and Deploy Production-Ready\n          Software”\n        \n      \n        \n          The\n          Amazon Builders' Library: Avoiding fallback in distributed\n          systems\n        \n      \n        \n          The\n          Amazon Builders' Library: Avoiding insurmountable queue\n          backlogs\n        \n      \n        \n          The\n          Amazon Builders' Library: Caching challenges and\n          strategies\n        \n      \n        \n          The\n          Amazon Builders' Library: Timeouts, retries, and backoff with\n          jitter\n        \n      \n    \n      Related videos:\n    \n    \n       \n    \n        \n          Retry,\n          backoff, and jitter: AWS re:Invent 2019: Introducing The\n          Amazon Builders’ Library (DOP328)\n        \n      \n  \u003cdiv\u003e\u003cdiv\u003e\u003cdiv\u003e\u003cdiv id=\"js_error_message\"\u003e\u003cp\u003e\u003cimg src=\"https://d1ge0kk1l5kms0.cloudfront.net/images/G/01/webservices/console/warning.png\" alt=\"Warning\" /\u003e \u003cstrong\u003eJavascript is disabled or is unavailable in your browser.\u003c/strong\u003e\u003c/p\u003e\u003cp\u003eTo use the Amazon Web Services Documentation, Javascript must be enabled. Please refer to your browser's Help pages for instructions.\u003c/p\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003eDocument Conventions REL 5. How do you design interactions in a distributed system to mitigate\n                  or withstand failures? REL05-BP02 Throttle requests",
  "REL05-BP02 Throttle requestsThrottle requests to mitigate resource exhaustion due to unexpected increases in demand. Requests below throttling rates are processed while those over the defined limit are rejected with a return message indicating the request was throttled. \n    Desired outcome: Large volume spikes either from sudden customer traffic increases, flooding attacks, or retry storms are mitigated by request throttling, allowing workloads to continue normal processing of supported request volume. \n  \n    Common anti-patterns:\n  \n     \n     \n     \n     \n     \n     \n     \n     \n  \n      \n        API endpoint throttles are not implemented or are left at default values without considering expected volumes.\n      \n    \n      \n        API endpoints are not load tested or throttling limits are not tested.\n      \n    \n      \n        Throttling request rates without considering request size or complexity.\n      \n    \n      \n        Testing maximum request rates or maximum request size, but not testing both together.\n      \n    \n      \n        Resources are not provisioned to the same limits established in testing.\n      \n    \n      \n        Usage plans have not been configured or considered for application to application (A2A) API consumers.\n      \n    \n      \n        Queue consumers that horizontally scale do not have maximum concurrency settings configured.\n      \n    \n      \n        Rate limiting on a per IP address basis has not been implemented.\n      \n    \n    Benefits of establishing this best practice: Workloads that set throttle limits are able to operate normally and process accepted request load successfully under unexpected volume spikes. Sudden or sustained spikes of requests to APIs and queues are throttled and do not exhaust request processing resources. Rate limits throttle individual requestors so that high volumes of traffic from a single IP address or API consumer will not exhaust resources impact other consumers. \n  \n    Level of risk exposed if this best practice\n    is not established: High\n  \n\n  Implementation guidance\n\n  Services should be designed to process a known capacity of requests; this capacity can be established through load testing. If request arrival rates exceed limits, the appropriate response signals that a request has been throttled. This allows the consumer to handle the error and retry later. \n\n    \n      When your service requires a throttling implementation, consider implementing the token bucket algorithm, where a token counts for a request. Tokens are refilled at a throttle rate per second and emptied asynchronously by one token per request.\n    \n    \n       \n        \n       \n       \n      The token bucket algorithm.\n    \n     \n    \n      Amazon API Gateway implements the token bucket algorithm according to account and region limits and can be configured per-client with usage plans. Additionally, Amazon Simple Queue Service (Amazon SQS) and Amazon Kinesis can buffer requests to smooth out the request rate, and allow higher throttling rates for requests that can be addressed. Finally, you can implement rate limiting with AWS WAF to throttle specific API consumers that generate unusually high load.\n    \n   \n    \n    Implementation steps\n    \n      You can configure API Gateway with throttling limits for your APIs and return 429 Too Many Requests errors when limits are exceeded. You can use AWS WAF with your AWS AppSync and API Gateway endpoints to enable rate limiting on a per IP address basis. Additionally, where your system can tolerate asynchronous processing, you can put messages into a queue or stream to speed up responses to service clients, which allows you to burst to higher throttle rates. \n    \n    \n      With asynchronous processing, when you’ve configured Amazon SQS as an event source for AWS Lambda, you can configure maximum concurrency to avoid high event rates from consuming available account concurrent execution quota needed for other services in your workload or account.\n    \n    \n      While API Gateway provides a managed implementation of the token bucket, in cases where you cannot use API Gateway, you can take advantage of language specific open-source implementations (see related examples in Resources) of the token bucket for your services. \n    \n    \n       \n       \n       \n       \n       \n       \n       \n       \n    \n        \n          Understand and configure API Gateway throttling limits at the account level per region, API per stage, and API key per usage plan levels.\n        \n      \n        \n          Apply AWS WAF rate limiting rules to API Gateway and AWS AppSync endpoints to protect against floods and block malicious IPs. Rate limiting rules can also be configured on AWS AppSync API keys for A2A consumers. \n        \n      \n        \n          Consider whether you require more throttling control than rate limiting for AWS AppSync APIs, and if so, configure an API Gateway in front of your AWS AppSync endpoint.\n        \n      \n        \n          When Amazon SQS queues are set up as triggers for Lambda queue consumers, set maximum concurrency to a value that processes enough to meet your service level objectives but does not consume concurrency limits impacting other Lambda functions. Consider setting reserved concurrency on other Lambda functions in the same account and region when you consume queues with Lambda. \n        \n      \n        \n          Use API Gateway with native service integrations to Amazon SQS or Kinesis to buffer requests. \n        \n      \n        \n          If you cannot use API Gateway, look at language specific libraries to implement the token bucket algorithm for your workload. Check the examples section and do your own research to find a suitable library.\n        \n      \n        \n          Test limits that you plan to set, or that you plan to allow to be increased, and document the tested limits.\n        \n      \n        \n          Do not increase limits beyond what you establish in testing. When increasing a limit, verify that provisioned resources are already equivalent to or greater than those in test scenarios before applying the increase.\n        \n      \n   \n\n  Resources\n\n    \n      Related best practices:\n    \n    \n       \n       \n    \n        \n          REL04-BP03 Do constant work\n        \n      \n        \n          REL05-BP03 Control and limit retry calls\n        \n      \n    \n      Related documents:\n    \n    \n       \n       \n       \n       \n    \n        \n          Amazon API Gateway: Throttle API Requests for Better\n          Throughput\n        \n      \n        AWS WAF: Rate-based rule statement\n        \n      \n        \n          Introducing maximum concurrency of AWS Lambda when using Amazon SQS as an event source\n        \n      \n        AWS Lambda: Maximum Concurrency\n        \n      \n    \n      Related examples:\n    \n    \n       \n       \n       \n       \n       \n    \n        \n          The three most important AWS WAF rate-based rules\n        \n      \n        \n          Java Bucket4j\n        \n      \n        \n          Python token-bucket\n        \n      \n         \n          Node token-bucket\n        \n      \n        \n          .NET System Threading Rate Limiting\n        \n      \n    \n      Related videos:\n    \n    \n       \n    \n        \n          Implementing GraphQL API security best practices with AWS AppSync\n      \n    \n      Related tools:\n    \n    \n       \n       \n       \n       \n       \n       \n    \n        \n          Amazon API Gateway\n        \n      \n        AWS AppSync\n      \n         \n          Amazon SQS\n        \n      \n        \n          Amazon Kinesis\n        \n      \n        AWS WAF\n      \n        \n          Virtual Waiting Room on AWS\n      \n  \u003cdiv\u003e\u003cdiv\u003e\u003cdiv\u003e\u003cdiv id=\"js_error_message\"\u003e\u003cp\u003e\u003cimg src=\"https://d1ge0kk1l5kms0.cloudfront.net/images/G/01/webservices/console/warning.png\" alt=\"Warning\" /\u003e \u003cstrong\u003eJavascript is disabled or is unavailable in your browser.\u003c/strong\u003e\u003c/p\u003e\u003cp\u003eTo use the Amazon Web Services Documentation, Javascript must be enabled. Please refer to your browser's Help pages for instructions.\u003c/p\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003eDocument ConventionsREL05-BP01 Implement graceful degradation to transform\n  applicable hard dependencies into soft dependenciesREL05-BP03 Control and limit retry calls",
  "REL05-BP03 Control and limit retry callsUse exponential backoff to retry requests at progressively longer intervals between each retry. Introduce jitter between retries to randomize retry intervals. Limit the maximum number of retries.\n    Desired outcome: Typical components in a distributed software system include servers, load balancers, databases, and DNS servers. During normal operation, these components can respond to requests with errors that are temporary or limited, and also errors that would be persistent regardless of retries. When clients make requests to services, the requests consume resources including memory, threads, connections, ports, or any other limited resources. Controlling and limiting retries is a strategy to release and minimize consumption of resources so that system components under strain are not overwhelmed. \n  \n    When client requests time out or receive error responses, they should determine whether or not to retry. If they do retry, they do so with exponential backoff with jitter and a maximum retry value. As a result, backend services and processes are given relief from load and time to self-heal, resulting in faster recovery and successful request servicing. \n  \n    Common anti-patterns: \n  \n     \n     \n     \n     \n     \n     \n     \n  \n      \n        Implementing retries without adding exponential backoff, jitter, and maximum retry values. Backoff and jitter help avoid artificial traffic spikes due to unintentionally coordinated retries at common intervals.\n      \n    \n      \n        Implementing retries without testing their effects or assuming retries are already built into an SDK without testing retry scenarios. \n      \n    \n      \n        Failing to understand published error codes from dependencies, leading to retrying all errors, including those with a clear cause that indicates lack of permission, configuration error, or another condition that predictably will not resolve without manual intervention.\n      \n    \n      \n        Not addressing observability practices, including monitoring and alerting on repeated service failures so that underlying issues are made known and can be addressed.\n      \n    \n      \n        Developing custom retry mechanisms when built-in or third-party retry capabilities suffice.\n      \n    \n      \n        Retrying at multiple layers of your application stack in a manner which compounds retry attempts further consuming resources in a retry storm. Be sure to understand how these errors affect your application the dependencies you rely on, then implement retries at only one level.\n      \n    \n      \n        Retrying service calls that are not idempotent, causing unexpected side effects like duplicated results.\n      \n    \n    Benefits of establishing this best practice: Retries help clients acquire desired results when requests fail but also consume more of a server’s time to get the successful responses they want. When failures are rare or transient, retries work well. When failures are caused by resource overload, retries can make things worse. Adding exponential backoff with jitter to client retries allows servers to recover when failures are caused by resource overload. Jitter avoids alignment of requests into spikes, and backoff diminishes load escalation caused by adding retries to normal request load. Finally, it’s important to configure a maximum number of retries or elapsed time to avoid creating backlogs that produce metastable failures.\n  \n    Level of risk exposed if this best practice\n    is not established: High\n  \n\n  Implementation guidance\n    \n      Control and limit retry calls. Use exponential backoff to retry after progressively longer intervals. Introduce jitter to randomize retry intervals and limit the maximum number of retries.\n    \n    \n      Some AWS SDKs implement retries and exponential backoff by default. Use these built-in AWS implementations where applicable in your workload. Implement similar logic in your workload when calling services that are idempotent and where retries improve your client availability. Decide what the timeouts are and when to stop retrying based on your use case. Build and exercise testing scenarios for those retry use cases.\n    \n\n   \n    \n    Implementation steps\n    \n       \n       \n       \n       \n    \n        \n          Determine the optimal layer in your application stack to implement retries for the services your application relies on. \n        \n      \n        \n          Be aware of existing SDKs that implement proven retry strategies with exponential backoff and jitter for your language of choice, and favor these over writing your own retry implementations. \n        \n      \n        \n          Verify that services are idempotent before implementing retries. Once retries are implemented, be sure they are both tested and regularly exercise in production. \n        \n      \n        \n          When calling AWS service APIs, use the AWS SDKs and AWS CLI and understand the retry configuration options. Determine if the defaults work for your use case, test, and adjust as needed. \n        \n      \n   \n\n  Resources\n\n    \n      Related best practices:\n    \n    \n       \n       \n       \n       \n       \n    \n        \n          REL04-BP04 Make mutating operations idempotent\n        \n      \n        \n          REL05-BP02 Throttle requests\n        \n      \n        \n          REL05-BP04 Fail fast and limit queues\n        \n      \n        \n          REL05-BP05 Set client timeouts\n        \n      \n        \n          REL11-BP01 Monitor all components of the workload to detect\n  failures\n        \n      \n    \n      Related documents:\n    \n    \n       \n       \n       \n       \n    \n        \n          Error\n          Retries and Exponential Backoff in AWS\n        \n      \n        \n          The\n          Amazon Builders' Library: Timeouts, retries, and backoff with\n          jitter\n        \n      \n        \n          Exponential Backoff and Jitter\n        \n      \n         \n          Making retries safe with idempotent APIs\n        \n      \n    \n      Related examples:\n    \n    \n       \n       \n    \n        \n          Spring Retry\n        \n      \n        \n          Resilience4j Retry\n        \n      \n    \n      Related videos:\n    \n    \n       \n    \n        \n          Retry,\n          backoff, and jitter: AWS re:Invent 2019: Introducing The\n          Amazon Builders’ Library (DOP328)\n        \n      \n    \n      Related tools:\n    \n    \n       \n       \n    \n        AWS SDKs and Tools: Retry behavior\n        \n      \n        AWS Command Line Interface: AWS CLI retries\n        \n      \n  \u003cdiv\u003e\u003cdiv\u003e\u003cdiv\u003e\u003cdiv id=\"js_error_message\"\u003e\u003cp\u003e\u003cimg src=\"https://d1ge0kk1l5kms0.cloudfront.net/images/G/01/webservices/console/warning.png\" alt=\"Warning\" /\u003e \u003cstrong\u003eJavascript is disabled or is unavailable in your browser.\u003c/strong\u003e\u003c/p\u003e\u003cp\u003eTo use the Amazon Web Services Documentation, Javascript must be enabled. Please refer to your browser's Help pages for instructions.\u003c/p\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003eDocument ConventionsREL05-BP02 Throttle requestsREL05-BP04 Fail fast and limit queues",
  "REL05-BP04 Fail fast and limit queuesWhen a service is unable to respond successfully to a request, fail fast. This allows resources associated with a request to be released, and permits a service to recover if it’s running out of resources. Failing fast is a well-established software design pattern that can be leveraged to build highly reliable workloads in the cloud. Queuing is also a well-established enterprise integration pattern that can smooth load and allow clients to release resources when asynchronous processing can be tolerated. When a service is able to respond successfully under normal conditions but fails when the rate of requests is too high, use a queue to buffer requests. However, do not allow a buildup of long queue backlogs that can result in processing stale requests that a client has already given up on.\n    Desired outcome: When systems experience resource contention, timeouts, exceptions, or grey failures that make service level objectives unachievable, fail fast strategies allow for faster system recovery. Systems that must absorb traffic spikes and can accommodate asynchronous processing can improve reliability by allowing clients to quickly release requests by using queues to buffer requests to backend services. When buffering requests to queues, queue management strategies are implemented to avoid insurmountable backlogs. \n  \n    Common anti-patterns:\n  \n     \n     \n     \n     \n     \n     \n     \n     \n     \n  \n      \n        Implementing message queues but not configuring dead letter queues (DLQ) or alarms on DLQ volumes to detect when a system is in failure.\n      \n    \n      \n        Not measuring the age of messages in a queue, a measurement of latency to understand when queue consumers are falling behind or erroring out causing retrying.\n      \n    \n      \n        Not clearing backlogged messages from a queue, when there is no value in processing these messages if the business need no longer exists.\n      \n    \n      \n        Configuring first in first out (FIFO) queues when last in first out (LIFO) queues would better serve client needs, for example when strict ordering is not required and backlog processing is delaying all new and time sensitive requests resulting in all clients experiencing breached service levels.\n      \n    \n      \n        Exposing internal queues to clients instead of exposing APIs that manage work intake and place requests into internal queues.\n      \n    \n      \n        Combining too many work request types into a single queue which can exacerbate backlog conditions by spreading resource demand across request types.\n      \n    \n      \n        Processing complex and simple requests in the same queue, despite needing different monitoring, timeouts and resource allocations.\n      \n    \n      \n        Not validating inputs or using assertions to implement fail fast mechanisms in software that bubble up exceptions to higher level components that can handle errors gracefully.\n      \n    \n      \n        Not removing faulty resources from request routing, especially when failures are grey emitting both successes and failures due to crashing and restarting, intermittent dependency failure, reduced capacity, or network packet loss.\n      \n    \n    Benefits of establishing this best practice: Systems that fail fast are easier to debug and fix, and often expose issues in coding and configuration before releases are published into production. Systems that incorporate effective queueing strategies provide greater resilience and reliability to traffic spikes and intermittent system fault conditions. \n  \n    Level of risk exposed if this best practice\n    is not established: High\n  \n\n  Implementation guidance\n    \n      Fail fast strategies can be coded into software solutions as well as configured into infrastructure. In addition to failing fast, queues are a straightforward yet powerful architectural technique to decouple system components smooth load. Amazon CloudWatch provides capabilities to monitor for and alarm on failures. Once a system is known to be failing, mitigation strategies can be invoked, including failing away from impaired resources. When systems implement queues with Amazon SQS and other queue technologies to smooth load, they must consider how to manage queue backlogs, as well as message consumption failures. \n    \n   \n    \n    Implementation steps\n    \n       \n       \n       \n       \n       \n       \n       \n    \n        \n          Implement programmatic assertions or specific metrics in your software and use them to explicitly alert on system issues. Amazon CloudWatch helps you create metrics and alarms based on application log pattern and SDK instrumentation.\n        \n      \n        \n          Use CloudWatch metrics and alarms to fail away from impaired resources that are adding latency to processing or repeatedly failing to process requests. \n        \n      \n        \n          Use asynchronous processing by designing APIs to accept requests and append requests to internal queues using Amazon SQS and then respond to the message-producing client with a success message so the client can release resources and move on with other work while backend queue consumers process requests.\n        \n      \n        \n          Measure and monitor for queue processing latency by producing a CloudWatch metric each time you take a message off a queue by comparing now to message timestamp.\n        \n      \n        \n          When failures prevent successful message processing or traffic spikes in volumes that cannot be processed within service level agreements, sideline older or excess traffic to a spillover queue. This allows priority processing of new work, and older work when capacity is available. This technique is an approximation of LIFO processing and allows normal system processing for all new work. \n        \n      \n        \n          Use dead letter or redrive queues to move messages that can’t be processed out of the backlog into a location that can be researched and resolved later\n        \n      \n        \n          Either retry or, when tolerable, drop old messages by comparing now to the message timestamp and discarding messages that are no longer relevant to the requesting client.\n        \n      \n   \n\n  Resources\n\n    \n      Related best practices:\n    \n    \n       \n       \n       \n       \n       \n    \n        \n          REL04-BP02 Implement loosely coupled dependencies\n        \n      \n        \n          REL05-BP02 Throttle requests\n        \n      \n        \n          REL05-BP03 Control and limit retry calls\n        \n      \n        \n          REL06-BP02 Define and calculate metrics (Aggregation)\n        \n      \n        \n          REL06-BP07 Monitor end-to-end tracing of requests through your\n  system\n        \n      \n    \n      Related documents:\n    \n    \n       \n       \n       \n       \n       \n    \n        \n          Avoiding insurmountable queue backlogs\n        \n      \n        \n          Fail\n          Fast\n        \n      \n        \n          How can I prevent an increasing backlog of messages in my Amazon SQS queue?\n        \n      \n        \n          Elastic Load Balancing: Zonal Shift\n        \n      \n        \n          Amazon Application Recovery Controller: Routing control for traffic failover\n        \n      \n    \n      Related examples:\n    \n    \n       \n    \n        \n          Enterprise Integration Patterns: Dead Letter Channel\n        \n      \n    \n      Related videos:\n    \n    \n       \n    \n        \n          AWS re:Invent 2022 - Operating highly available Multi-AZ applications\n        \n      \n    \n      Related tools:\n    \n    \n       \n       \n       \n       \n    \n        \n          Amazon SQS\n        \n      \n        \n          Amazon MQ\n        \n      \n        AWS IoT Core\n      \n        \n          Amazon CloudWatch\n        \n      \n  \u003cdiv\u003e\u003cdiv\u003e\u003cdiv\u003e\u003cdiv id=\"js_error_message\"\u003e\u003cp\u003e\u003cimg src=\"https://d1ge0kk1l5kms0.cloudfront.net/images/G/01/webservices/console/warning.png\" alt=\"Warning\" /\u003e \u003cstrong\u003eJavascript is disabled or is unavailable in your browser.\u003c/strong\u003e\u003c/p\u003e\u003cp\u003eTo use the Amazon Web Services Documentation, Javascript must be enabled. Please refer to your browser's Help pages for instructions.\u003c/p\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003eDocument ConventionsREL05-BP03 Control and limit retry callsREL05-BP05 Set client timeouts",
  "REL05-BP05 Set client timeoutsSet timeouts appropriately on connections and requests, verify them systematically, and do not rely on default values as they are not aware of workload specifics.\n    Desired outcome: Client timeouts should consider the cost to the client, server, and workload associated with waiting for requests that take abnormal amounts of time to complete. Since it is not possible to know the exact cause of any timeout, clients must use knowledge of services to develop expectations of probable causes and appropriate timeouts\n  \n    Client connections time out based on configured values.  After encountering a timeout, clients make decisions to back off and retry or open a circuit breaker. These patterns avoid issuing requests that may exacerbate an underlying error condition. \n  \n    Common anti-patterns:\n  \n     \n     \n     \n     \n     \n     \n     \n     \n     \n  \n      \n        Not being aware of system timeouts or default timeouts.\n      \n    \n      \n        Not being aware of normal request completion timing.\n      \n    \n      \n        Not being aware of possible causes for requests to take abnormally long to complete, or the costs to client, service, or workload performance associated with waiting on these completions.\n      \n    \n      \n        Not being aware of the probability of impaired network causing a request to fail only once timeout is reached, and the costs to client and workload performance for not adopting a shorter timeout.\n      \n    \n      \n        Not testing timeout scenarios both for connections and requests.\n      \n    \n      \n        Setting timeouts too high, which can result in long wait times and increase resource utilization.\n      \n    \n      \n        Setting timeouts too low, resulting in artificial failures.\n      \n    \n      \n        Overlooking patterns to deal with timeout errors for remote calls like circuit breakers and retries.\n      \n    \n      \n        Not considering monitoring for service call error rates, service level objectives for latency, and latency outliers. These metrics can provide insight to aggressive or permissive timeouts \n      \n    \n    Benefits of establishing this best practice: Remote call timeouts are configured and systems are designed to handle timeouts gracefully so that resources are conserved when remote calls respond abnormally slow and timeout errors are handled gracefully by service clients. \n  \n    Level of risk exposed if this best practice\n    is not established: High\n  \n\n  Implementation guidance\n    \n      Set both a connection timeout and a request timeout on any service dependency call and generally on any call across processes. Many frameworks offer built-in timeout capabilities, but be careful, as some have default values that are infinite or higher than acceptable for your service goals. A value that is too high reduces the usefulness of the timeout because resources continue to be consumed while the client waits for the timeout to occur. A value that is too low can generate increased traffic on the backend and increased latency because too many requests are retried. In some cases, this can lead to complete outages because all requests are being retried. \n    \n    \n      Consider the following when determining timeout strategies:\n    \n    \n       \n       \n       \n       \n    \n        \n          Requests may take longer than normal to process because of their content, impairments in a target service, or a networking partition failure.\n        \n      \n        \n          Requests with abnormally expensive content could consume unnecessary server and client resources. In this case, timing out these requests and not retrying can preserve resources. Services should also protect themselves from abnormally expensive content with throttles and server-side timeouts.\n        \n      \n        \n          Requests that take abnormally long due to a service impairment can be timed out and retried. Consideration should be given to service costs for the request and retry, but if the cause is a localized impairment, a retry is not likely to be expensive and will reduce client resource consumption. The timeout may also release server resources depending on the nature of the impairment.\n        \n      \n        \n          Requests that take a long time to complete because the request or response has failed to be delivered by the network can be timed out and retried. Because the request or response was not delivered, failure would have been the outcome regardless of the length of timeout. Timing out in this case will not release server resources, but it will release client resources and improve workload performance.\n        \n      \n    \n      Take advantage of well-established design patterns like retries and circuit breakers to handle timeouts gracefully and support fail-fast approaches. AWS SDKs and AWS CLI allow for configuration of both connection and request timeouts and for retries with exponential backoff and jitter. AWS Lambda functions support configuration of timeouts, and with AWS Step Functions, you can build low code circuit breakers that take advantage of pre-built integrations with AWS services and SDKs. AWS App Mesh Envoy provides timeout and circuit breaker capabilities.\n    \n   \n    \n    Implementation steps\n    \n       \n       \n       \n       \n       \n       \n       \n       \n       \n       \n       \n    \n        \n          Configure timeouts on remote service calls and take advantage of built-in language timeout features or open source timeout libraries.\n        \n      \n        \n          When your workload makes calls with an AWS SDK, review the documentation for language specific timeout configuration.\n        \n        \n           \n           \n           \n           \n           \n           \n           \n           \n        \n            \n              Python\n            \n          \n            \n              PHP\n            \n          \n            \n              .NET\n            \n          \n            \n              Ruby\n            \n          \n            \n              Java\n            \n          \n            \n              Go\n            \n          \n            \n              Node.js\n            \n          \n            \n              C++\n            \n          \n      \n        \n          When using AWS SDKs or AWS CLI commands in your workload, configure default timeout values by setting the AWS configuration defaults for connectTimeoutInMillis and  tlsNegotiationTimeoutInMillis.\n        \n      \n        \n          Apply command line options cli-connect-timeout and cli-read-timeout to control one-off AWS CLI commands to AWS services. \n        \n      \n        \n          Monitor remote service calls for timeouts, and set alarms on persistent errors so that you can proactively handle error scenarios. \n        \n      \n        \n          Implement CloudWatch Metrics and CloudWatch anomaly detection on call error rates, service level objectives for latency, and latency outliers to provide insight into managing overly aggressive or permissive timeouts.\n        \n      \n        \n          Configure timeouts on Lambda functions.\n        \n      \n        \n          API Gateway clients must implement their own retries when handling timeouts. API Gateway supports a 50 millisecond to 29 second integration timeout for downstream integrations and does not retry when integration requests timeout. \n        \n      \n        \n          Implement the circuit breaker pattern to avoid making remote calls when they are timing out. Open the circuit to avoid failing calls and close the circuit when calls are responding normally. \n        \n      \n        \n          For container based workloads, review App Mesh Envoy features to leverage built in timeouts and circuit breakers.\n        \n      \n        \n          Use AWS Step Functions to build low code circuit breakers for remote service calls, especially where calling AWS native SDKs and supported Step Functions integrations to simplify your workload. \n        \n      \n   \n\n  Resources\n\n    \n      Related best practices:\n    \n    \n       \n       \n       \n    \n        \n          REL05-BP03 Control and limit retry calls\n        \n      \n        \n          REL05-BP04 Fail fast and limit queues\n        \n      \n        \n          REL06-BP07 Monitor end-to-end tracing of requests through your\n  system\n        \n      \n    \n      Related documents:\n    \n    \n       \n       \n       \n       \n       \n       \n       \n       \n    \n        \n          AWS           SDK: Retries and Timeouts\n        \n      \n        \n          The\n          Amazon Builders' Library: Timeouts, retries, and backoff with\n          jitter\n        \n      \n        \n          Amazon API Gateway quotas and important notes\n        \n      \n        AWS Command Line Interface: Command line options\n        \n      \n        AWS SDK for Java 2.x: Configure API Timeouts\n        \n      \n        AWS Botocore using the config object and Config Reference\n        \n      \n        AWS SDK for .NET: Retries and Timeouts\n        \n      \n        AWS Lambda: Configuring Lambda function options\n        \n      \n    \n      Related examples:\n    \n    \n       \n       \n    \n        \n          Using the circuit breaker pattern with AWS Step Functions and Amazon DynamoDB\n        \n      \n        \n          Martin Fowler: CircuitBreaker\n        \n      \n    \n      Related tools:\n    \n    \n       \n       \n       \n       \n       \n    \n        AWS SDKs\n        \n      \n        AWS Lambda\n      \n        \n          Amazon SQS\n        \n      \n        AWS Step Functions\n      \n        AWS Command Line Interface\n      \n  \u003cdiv\u003e\u003cdiv\u003e\u003cdiv\u003e\u003cdiv id=\"js_error_message\"\u003e\u003cp\u003e\u003cimg src=\"https://d1ge0kk1l5kms0.cloudfront.net/images/G/01/webservices/console/warning.png\" alt=\"Warning\" /\u003e \u003cstrong\u003eJavascript is disabled or is unavailable in your browser.\u003c/strong\u003e\u003c/p\u003e\u003cp\u003eTo use the Amazon Web Services Documentation, Javascript must be enabled. Please refer to your browser's Help pages for instructions.\u003c/p\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003eDocument ConventionsREL05-BP04 Fail fast and limit queuesREL05-BP06 Make systems stateless where possible",
  "REL05-BP06 Make systems stateless where possible\n    Systems should either not require state, or should offload state\n      such that between different client requests, there is no dependence\n      on locally stored data on disk and in memory. This allows servers to\n      be replaced at will without causing an availability impact.\n  \n    When users or services interact with an application, they often\n    perform a series of interactions that form a session. A session is\n    unique data for users that persists between requests while they use\n    the application. A stateless application is an application that does\n    not need knowledge of previous interactions and does not store\n    session information.\n  \n    Once designed to be stateless, you can then use serverless compute\n    services, such as AWS Lambda or AWS Fargate.\n  \n    In addition to server replacement, another benefit of stateless\n    applications is that they can scale horizontally because any of the\n    available compute resources (such as EC2 instances and AWS Lambda\n    functions) can service any request.\n  \n    Benefits of establishing this best\n      practice: Systems that are designed to be stateless are\n    more adaptable to horizontal scaling, making it possible to add or\n    remove capacity based on fluctuating traffic and demand. They are\n    also inherently resilient to failures and provide flexibility and\n    agility in application development.\n  \n    Level of risk exposed if this best practice\n      is not established: Medium\n  \n    \n    Implementation guidance\n    \n    \n    \n      Make your applications stateless. Stateless applications allow\n      horizontal scaling and are tolerant to the failure of an\n      individual node. Analyze and understand the components of your\n      application that maintain state within the architecture. This\n      helps you assess the potential impact of transitioning to a\n      stateless design. A stateless architecture decouples user data and\n      offloads the session data. This provides the flexibility to scale\n      each component independently to meet varying workload demands and\n      optimize resource utilization.\n    \n     \n      \n      Implementation steps\n      \n      \n      \n         \n         \n         \n         \n      \n          \n            Identify and understand the stateful components in your\n            application.\n          \n        \n          \n            Decouple data by separating and managing user data from the\n            core application logic.\n          \n          \n             \n             \n             \n             \n          \n              \n                Amazon Cognito can decouple user data from application\n                code by using features, such as\n                identity\n                  pools,\n                user\n                  pools, and\n                Amazon Cognito Sync.\n              \n            \n              \n                You can use\n                AWS Secrets Manager decouple user data by storing\n                secrets in a secure, centralized location. This means\n                that the application code doesn't need to store secrets,\n                which makes it more secure.\n              \n            \n              \n                Consider using\n                Amazon S3 to store large, unstructured data, such as\n                images and documents. Your application can retrieve this\n                data when required, eliminating the need to store it in\n                memory.\n              \n            \n              \n                Use\n                Amazon DynamoDB to store information such as user\n                profiles. Your application can query this data in\n                near-real time.\n              \n            \n        \n          \n            Offload session data to a database, cache, or external\n            files.\n          \n          \n             \n          \n              \n                Amazon ElastiCache, Amazon DynamoDB,\n                Amazon Elastic File System (Amazon EFS), and\n                Amazon MemoryDB are examples of AWS services\n                that you can use to offload session data.\n              \n            \n        \n          \n            Design a stateless architecture after you identify which\n            state and user data need to be persisted with your storage\n            solution of choice.\n          \n        \n     \n   \n    \n    Resources\n    \n    \n    \n      Related best practices:\n    \n    \n       \n    \n        \n          REL11-BP03\n            Automate healing on all layers\n        \n      \n    \n      Related documents:\n    \n    \n       \n       \n       \n       \n    \n        \n          The\n            Amazon Builders' Library: Avoiding fallback in distributed\n            systems\n        \n      \n        \n          The\n            Amazon Builders' Library: Avoiding insurmountable queue\n            backlogs\n        \n      \n        \n          The\n            Amazon Builders' Library: Caching challenges and\n            strategies\n        \n      \n        \n          Best\n            Practices for Stateless Web Tier on AWS\n        \n      \n  \u003cdiv\u003e\u003cdiv\u003e\u003cdiv\u003e\u003cdiv id=\"js_error_message\"\u003e\u003cp\u003e\u003cimg src=\"https://d1ge0kk1l5kms0.cloudfront.net/images/G/01/webservices/console/warning.png\" alt=\"Warning\" /\u003e \u003cstrong\u003eJavascript is disabled or is unavailable in your browser.\u003c/strong\u003e\u003c/p\u003e\u003cp\u003eTo use the Amazon Web Services Documentation, Javascript must be enabled. Please refer to your browser's Help pages for instructions.\u003c/p\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003eDocument ConventionsREL05-BP05 Set client timeoutsREL05-BP07 Implement emergency levers",
  "REL05-BP07 Implement emergency levers\n    Emergency levers are rapid processes that can mitigate availability\n    impact on your workload.\n  \n    Emergency levers work by disabling, throttling, or changing the behavior of components or dependencies using known and tested mechanisms. This can alleviate workload impairments caused by resource exhaustion due to unexpected increases in demand and reduce the impact of failures in non-critical components within your workload.\n  \n    Desired outcome: By implementing emergency levers, you can establish known-good processes to maintain the availability of critical components in your workload. The workload should degrade gracefully and continue to perform its business-critical functions during the activation of an emergency lever. For more detail on graceful degradation, see REL05-BP01 Implement graceful degradation to transform applicable hard dependencies into soft dependencies.\n  \n    Common anti-patterns:\n  \n     \n     \n     \n  \n      \n        Failure of non-critical dependencies impacts the availability of your core workload.\n      \n    \n      \n        Not testing or verifying critical component behavior during non-critical component impairment.\n      \n    \n      \n        No clear and deterministic criteria defined for activation or deactivation of an emergency lever.\n      \n    \n    Benefits of establishing this best practice: Implementing emergency levers can improve the availability of the critical components in your workload by providing your resolvers with established processes to respond to unexpected spikes in demand or failures of non-critical dependencies. \n  \n    Level of risk exposed if this best practice\n    is not established: Medium\n  \n\n  Implementation guidance\n    \n       \n       \n       \n       \n       \n    \n        \n          Identify critical components in your workload.\n        \n        \n        \n          Design and architect the critical components in your workload to withstand failure of non-critical components.\n        \n      \n        \n          Conduct testing to validate the behavior of your critical components during the failure of non-critical components.\n        \n      \n        \n          Define and monitor relevant metrics or triggers to initiate emergency lever procedures.\n        \n      \n        \n          Define the procedures (manual or automated) that comprise the emergency lever.\n        \n      \n     \n      \n      Implementation steps\n      \n         \n         \n         \n         \n         \n      \n          \n            Identify business-critical components in your workload.\n          \n          \n             \n             \n          \n              \n                Each technical component in your workload should be mapped to its relevant business function and ranked as critical or non-critical. For examples of critical and non-critical functionality at Amazon, see Any Day Can Be Prime Day: How Amazon.com Search Uses Chaos Engineering to Handle Over 84K Requests Per Second.\n              \n            \n              \n                This is both a technical and business decision, and varies by organization and workload.\n              \n            \n        \n          \n            Design and architect the critical components in your workload to withstand failure of non-critical components.\n          \n          \n             \n          \n              \n                During dependency analysis, consider all potential failure modes, and verify that your emergency lever mechanisms deliver the critical functionality to downstream components.\n              \n            \n        \n          \n            Conduct testing to validate the behavior of your critical components during activation of your emergency levers.\n          \n          \n             \n          \n              \n                Avoid bimodal behavior. For more detail, see REL11-BP05 Use static stability to prevent bimodal behavior.\n              \n            \n        \n          \n            Define, monitor, and alert on relevant metrics to initiate the emergency lever procedure.\n          \n          \n             \n          \n              \n                Finding the right metrics to monitor depends on your workload. Some example metrics are latency or the number of failed request to a dependency.\n              \n            \n        \n          \n           Define the procedures, manual or automated, that comprise the emergency lever.\n          \n          \n             \n          \n              \n                This may include mechanisms such as load shedding, throttling requests, or implementing graceful degradation.\n              \n            \n        \n     \n   \n    \n    Resources\n    \n      Related best practices:\n    \n    \n       \n       \n       \n    \n        \n          REL05-BP01 Implement graceful degradation to transform applicable hard dependencies into soft dependencies\n        \n      \n        \n          REL05-BP02 Throttle requests\n        \n      \n        \n          REL11-BP05 Use static stability to prevent bimodal behavior\n        \n      \n    \n      Related documents:\n    \n    \n       \n       \n    \n        \n          Automating safe, hands-off deployments\n        \n      \n        \n          Any Day Can Be Prime Day: How Amazon.com Search Uses Chaos Engineering to Handle Over 84K Requests Per Second\n        \n      \n    \n      Related videos:\n    \n    \n       \n    \n        AWS re:Invent 2020: Reliability, consistency, and confidence through immutability\n      \n  \u003cdiv\u003e\u003cdiv\u003e\u003cdiv\u003e\u003cdiv id=\"js_error_message\"\u003e\u003cp\u003e\u003cimg src=\"https://d1ge0kk1l5kms0.cloudfront.net/images/G/01/webservices/console/warning.png\" alt=\"Warning\" /\u003e \u003cstrong\u003eJavascript is disabled or is unavailable in your browser.\u003c/strong\u003e\u003c/p\u003e\u003cp\u003eTo use the Amazon Web Services Documentation, Javascript must be enabled. Please refer to your browser's Help pages for instructions.\u003c/p\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003eDocument ConventionsREL05-BP06 Make systems stateless where possibleChange management",
  "REL06-BP01 Monitor all components for the workload\n  (Generation)\n    Monitor the components of the workload with Amazon CloudWatch or\n    third-party tools. Monitor AWS services with AWS Health Dashboard.\n  \n    All components of your workload should be monitored, including the\n    front-end, business logic, and storage tiers. Define key metrics,\n    describe how to extract them from logs (if necessary), and set\n    thresholds for invoking corresponding alarm events. Ensure metrics\n    are relevant to the key performance indicators (KPIs) of your\n    workload, and use metrics and logs to identify early warning signs\n    of service degradation. For example, a metric related to business\n    outcomes such as the number of orders successfully processed per\n    minute, can indicate workload issues faster than technical metric,\n    such as CPU Utilization. Use AWS Health Dashboard for a personalized\n    view into the performance and availability of the AWS services\n    underlying your AWS resources.\n  \n    Monitoring in the cloud offers new opportunities. Most cloud\n    providers have developed customizable hooks and can deliver insights\n    to help you monitor multiple layers of your workload. AWS services\n    such as Amazon CloudWatch apply statistical and machine learning\n    algorithms to continually analyze metrics of systems and\n    applications, determine normal baselines, and surface anomalies with\n    minimal user intervention. Anomaly detection algorithms account for\n    the seasonality and trend changes of metrics.\n  \n    AWS makes an abundance of monitoring and log information available\n    for consumption that can be used to define workload-specific\n    metrics, change-in-demand processes, and adopt machine learning\n    techniques regardless of ML expertise.\n  \n    In addition, monitor all of your external endpoints to ensure that\n    they are independent of your base implementation. This active\n    monitoring can be done with synthetic transactions (sometimes\n    referred to as user canaries, but not to be\n    confused with canary deployments) which periodically run a number of\n    common tasks matching actions performed by clients of the workload.\n    Keep these tasks short in duration and be sure not to overload your\n    workload during testing. Amazon CloudWatch Synthetics allows you\n    to create\n    synthetic canaries to monitor your endpoints and APIs. You\n    can also combine the synthetic canary client nodes with AWS X-Ray\n    console to pinpoint which synthetic canaries are experiencing issues\n    with errors, faults, or throttling rates for the selected time\n    frame.\n  \n    Desired Outcome:\n  \n    Collect and use critical metrics from all components of the workload\n    to ensure workload reliability and optimal user experience.\n    Detecting that a workload is not achieving business outcomes allows\n    you to quickly declare a disaster and recover from an incident.\n  \n    Common anti-patterns:\n  \n     \n     \n     \n     \n  \n      \n        Only monitoring external interfaces to your workload.\n      \n    \n      \n        Not generating any workload-specific metrics and only relying on\n        metrics provided to you by the AWS services your workload uses.\n      \n    \n      \n        Only using technical metrics in your workload and not monitoring\n        any metrics related to non-technical KPIs the workload\n        contributes to.\n      \n    \n      \n        Relying on production traffic and simple health checks to\n        monitor and evaluate workload state.\n      \n    \n    Benefits of establishing this best\n    practice: Monitoring at all tiers in your workload\n    allows you to more rapidly anticipate and resolve problems in the\n    components that comprise the workload.\n  \n    Level of risk exposed if this best practice\n    is not established: High\n  \n\n  Implementation guidance\n\n      \n    \n       \n       \n       \n       \n       \n    \n        \n          Turn on logging where\n          available. Monitoring data should be obtained from\n          all components of the workloads. Turn on additional logging,\n          such as S3 Access Logs, and permit your workload to log\n          workload specific data. Collect metrics for CPU, network I/O,\n          and disk I/O averages from services such as Amazon ECS, Amazon EKS, Amazon EC2, Elastic Load Balancing, AWS Auto Scaling, and\n          Amazon EMR. See\n          AWS           Services That Publish CloudWatch Metrics for a list of\n          AWS services that publish metrics to CloudWatch.\n        \n      \n        \n          Review all default metrics and explore\n          any data collection gaps. Every service generates\n          default metrics. Collecting default metrics allows you to\n          better understand the dependencies between workload\n          components, and how component reliability and performance\n          affect the workload. You can also create and\n          publish\n          your own metrics to CloudWatch using the AWS CLI or an\n          API.\n        \n      \n        \n          Evaluate all the metrics to decide\n          which ones to alert on for each AWS service in your\n          workload. You may choose to select a subset of\n          metrics that have a major impact on workload reliability.\n          Focusing on critical metrics and threshold allows you to\n          refine the number of\n          alerts\n          and can help minimize false-positives.\n        \n      \n        \n          Define alerts and the recovery process\n          for your workload after the alert is invoked.\n          Defining alerts allows you to quickly notify, escalate, and\n          follow steps necessary to recover from an incident and meet\n          your prescribed Recovery Time Objective (RTO). You can use\n          Amazon CloudWatch Alarms to invoke automated\n          workflows and initiate recovery procedures based on defined\n          thresholds.\n        \n      \n        \n          Explore use of synthetic transactions\n          to collect relevant data about workloads state.\n          Synthetic monitoring follows the same routes and perform the\n          same actions as a customer, which makes it possible for you to\n          continually verify your customer experience even when you\n          don't have any customer traffic on your workloads. By using\n          synthetic\n          transactions, you can discover issues before your\n          customers do.\n        \n      \n   \n\n  Resources\n\n    \n      Related best practices:\n    \n    \n       \n    REL11-BP03 Automate healing on all layers\n    \n      Related documents:\n    \n    \n       \n       \n       \n       \n       \n       \n       \n       \n       \n       \n       \n       \n       \n       \n       \n       \n       \n       \n       \n       \n    \n        \n          Getting\n          started with your AWS Health Dashboard – Your account\n          health\n        \n      \n        \n          AWS           Services That Publish CloudWatch Metrics\n        \n      \n        \n          Access\n          Logs for Your Network Load Balancer\n        \n      \n        \n          Access\n          logs for your application load balancer\n        \n      \n        \n          Accessing\n          Amazon CloudWatch Logs for AWS Lambda\n        \n      \n        \n          Amazon S3 Server Access Logging\n        \n      \n        \n          Enable\n          Access Logs for Your Classic Load Balancer\n        \n      \n        \n          Exporting\n          log data to Amazon S3\n        \n      \n        \n          Install\n          the CloudWatch agent on an Amazon EC2 instance\n        \n      \n        \n          Publishing\n          Custom Metrics\n        \n      \n        \n          Using\n          Amazon CloudWatch Dashboards\n        \n      \n        \n          Using\n          Amazon CloudWatch Metrics\n        \n      \n        \n          Using\n          Canaries (Amazon CloudWatch Synthetics)\n        \n      \n        \n          What\n          are Amazon CloudWatch Logs?\n        \n        \n          User guides:\n        \n      \n        \n          Creating\n          a trail\n        \n      \n        \n          Monitoring\n          memory and disk metrics for Amazon EC2 Linux instances\n        \n      \n        \n          Using\n          CloudWatch Logs with container instances\n        \n      \n        \n          VPC\n          Flow Logs\n        \n      \n        \n          What\n          is Amazon DevOps Guru?\n        \n      \n        \n          What\n          is AWS X-Ray?\n        \n      \n    \n      Related blogs:\n    \n    \n       \n    \n        \n          Debugging\n          with Amazon CloudWatch Synthetics and AWS X-Ray\n        \n      \n    \n      Related examples:\n    \n    \n       \n       \n    \n        \n          The\n          Amazon Builders' Library: Instrumenting distributed systems\n          for operational visibility\n        \n      \n        \n          Observability\n          workshop\n        \n      \n  \u003cdiv\u003e\u003cdiv\u003e\u003cdiv\u003e\u003cdiv id=\"js_error_message\"\u003e\u003cp\u003e\u003cimg src=\"https://d1ge0kk1l5kms0.cloudfront.net/images/G/01/webservices/console/warning.png\" alt=\"Warning\" /\u003e \u003cstrong\u003eJavascript is disabled or is unavailable in your browser.\u003c/strong\u003e\u003c/p\u003e\u003cp\u003eTo use the Amazon Web Services Documentation, Javascript must be enabled. Please refer to your browser's Help pages for instructions.\u003c/p\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003eDocument Conventions REL 6. How do you monitor workload resources? REL06-BP02 Define and calculate metrics (Aggregation)",
  "REL06-BP02 Define and calculate metrics (Aggregation)\n    Collect metrics and logs from your workload components and calculate\n    relevant aggregate metrics from them. These metrics provide broad\n    and deep observability of your workload and can significantly\n    improve your resilience posture.\n  \n    Observability is more than just collecting metrics from workload\n    components and being able to view and alert on them. It's about\n    having a holistic understanding about your workload's behavior. This\n    behavioral information comes from all components in your workloads,\n    which includes the cloud services on which they depend, well-crafted\n    logs, and metrics. This data gives you oversight on your workload's\n    behavior as a whole, as well as an understanding of every\n    component's interaction with every unit of work at a fine level of\n    detail.\n  \n    Desired outcome:\n  \n     \n     \n     \n     \n     \n  \n      \n        You collect logs from your workload components and AWS service\n        dependencies, and you publish them to a central location where\n        they can be easily accessed and processed.\n      \n    \n      \n        Your logs contain high-fidelity and accurate timestamps.\n      \n    \n      \n        Your logs contain relevant information about the processing\n        context, such as a trace identifier, user or account identifier,\n        and remote IP address.\n      \n    \n      \n        You create aggregate metrics from your logs that represent your\n        workload's behavior from a high-level perspective.\n      \n    \n      \n        You are able to query your aggregated logs to gain deep and\n        relevant insights about your workload and identify actual and\n        potential problems.\n      \n    \n    Common anti-patterns:\n  \n     \n     \n     \n     \n  \n      \n        You don't collect relevant logs or metrics from the compute\n        instances your workloads run on or the cloud services they use.\n      \n    \n      \n        You overlook the collection of logs and metrics related to your\n        business key performance indicators (KPIs).\n      \n    \n      \n        You analyze workload-related telemetry in isolation without\n        aggregation and correlation.\n      \n    \n      \n        You allow metrics and logs to expire too quickly, which hinders\n        trend analysis and recurring issue identification.\n      \n    \n    Benefits of establishing these best\n    practices: You can detect more anomalies and correlate\n    events and metrics between different components of your workload.\n    You can create insights from your workload components based on\n    information contained in logs that frequently aren't available in\n    metrics alone. You can determine causes of failure more quickly by\n    querying your logs at scale.\n  \n    Level of risk exposed if these best\n    practices are not established: High\n  \n\n  Implementation guidance\n\n      \n    \n      Identify the sources of telemetry data that are relevant for your\n      workloads and their components. This data comes not only from\n      components that publish metrics, such as your operating system\n      (OS) and application runtimes such as Java, but also from\n      application and cloud service logs. For example, web servers\n      typically log each request with detailed information such as the\n      timestamp, processing latency, user ID, remote IP address, path,\n      and query string. The level of detail in these logs helps you\n      perform detailed queries and generate metrics that may not have\n      been otherwise available.\n    \n    \n      Collect the metrics and logs using appropriate tools and\n      processes. Logs generated by applications running on Amazon EC2\n      instance can be collected by an agent such as the\n      Amazon CloudWatch Agent and published to a central storage service\n      such as\n      Amazon CloudWatch Logs. AWS-managed compute services such as\n      AWS Lambda and\n      Amazon Elastic Container Service publish logs to CloudWatch Logs for you\n      automatically. Enable log collection for AWS storage and\n      processing services used by your workloads such as\n      Amazon CloudFront,\n      Amazon S3,\n      Elastic Load Balancing, and\n      Amazon API Gateway.\n    \n    \n      Enrich your telemetry data with\n      dimensions\n      that can help you see behavioral patterns more clearly and isolate\n      correlated problems to groups of related components. Once added,\n      you can observe component behavior at a finer level of detail,\n      detect correlated failures, and take appropriate remedial steps.\n      Examples of useful dimensions include Availability Zone, EC2\n      instance ID, and container task or Pod ID.\n    \n    \n      Once you have collected the metrics and logs, you can write\n      queries and generate aggregate metrics from them that provide\n      useful insights into both normal and anomalous behavior. For\n      example, you can use\n      Amazon CloudWatch Logs Insights to derive custom metrics from your\n      application logs,\n      Amazon CloudWatch Metrics Insights to query your metrics at scale,\n      Amazon CloudWatch Container Insights to collect, aggregate and\n      summarize metrics and logs from your containerized applications\n      and microservices, or\n      Amazon CloudWatch Lambda Insights if you're using AWS Lambda\n      functions. To create an aggregate error rate metric, you can\n      increment a counter each time an error response or message is\n      found in your component logs or calculate the aggregate value of\n      an existing error rate metric. You can use this data to generate\n      histograms that show tail behavior, such as\n      the worst-performing requests or processes. You can also scan this\n      data in real time for anomalous patterns using solutions such as\n      CloudWatch Logs\n      anomaly\n      detection. These insights can be placed on dashboards to\n      keep them organized according to your needs and preferences.\n    \n    \n      Querying logs can help you understand how specific requests were\n      handled by your workload components and reveal request patterns or\n      other context that has an impact on your workload's resilience. It\n      can be useful to research and prepare queries in advance, based on\n      your knowledge of how your applications and other components\n      behave, so you can more easily run them as needed. For example,\n      with\n      CloudWatch Logs Insights, you can interactively search and analyze\n      your log data stored in CloudWatch Logs. You can also use\n      Amazon Athena to query logs from multiple sources, including\n      many\n      AWS services, at petabyte scale.\n    \n    \n      When you define a log retention policy, consider the value of\n      historical logs. Historical logs can help identify long-term usage\n      and behavioral patterns, regressions, and improvements in your\n      workload's performance. Permanently deleted logs cannot be\n      analyzed later. However, the value of historical logs tends to\n      diminish over long periods of time. Choose a policy that balances\n      your needs as appropriate and is compliant with any legal or\n      contractual requirements you might be subject to.\n    \n     \n\n  Implementation steps\n\n      \n      \n         \n         \n         \n         \n         \n         \n      \n          \n            Choose collection, storage, analysis, and display mechanisms\n            for your observability data.\n          \n        \n          \n            Install and configure metric and log collectors on the\n            appropriate components of your workload (for example, on\n            Amazon EC2 instances and in\n            sidecar\n            containers). Configure these collectors to restart\n            automatically if they unexpectedly stop. Enable disk or\n            memory buffering for the collectors so that temporary\n            publication failures don't impact your applications or\n            result in lost data.\n          \n        \n          \n            Enable logging on AWS services you use as a part of your\n            workloads, and forward those logs to the storage service you\n            selected if needed. Refer to the respective services' user\n            or developer guides for detailed instructions.\n          \n        \n          \n            Define the operational metrics relevant to your workloads\n            that are based on your telemetry data. These could be based\n            on direct metrics emitted from your workload components,\n            which can include business KPI related metrics, or the\n            results of aggregated calculations such as sums, rates,\n            percentiles, or histograms. Calculate these metrics using\n            your log analyzer, and place them on dashboards as\n            appropriate.\n          \n        \n          \n            Prepare appropriate log queries to analyze workload\n            components, requests, or transaction behavior as needed.\n          \n        \n          \n            Define and enable a log retention policy for your component\n            logs. Periodically delete logs when they become older than\n            the policy permits.\n          \n        \n     \n   \n\n  Resources\n\n      \n    \n      Related best practices:\n    \n    \n       \n       \n       \n       \n       \n       \n    \n        \n          REL06-BP01\n          Monitor all components for the workload (Generation)\n        \n      \n        \n          REL06-BP03\n          Send notifications (Real-time processing and alarming)\n        \n      \n        \n          REL06-BP04\n          Automate responses (Real-time processing and alarming)\n        \n      \n        \n          REL06-BP05\n          Analyze logs\n        \n      \n        \n          REL06-BP06\n          Regularly review monitoring scope and metrics\n        \n      \n        \n          REL06-BP07\n          Monitor end-to-end tracing of requests through your\n          system\n        \n      \n    \n      Related documentation:\n    \n    \n       \n       \n       \n       \n       \n       \n       \n       \n       \n       \n       \n       \n       \n    \n        \n          How\n          Amazon CloudWatch works\n        \n      \n        \n          Amazon\n          Managed Prometheus\n        \n      \n        \n          Amazon Managed Grafana\n        \n      \n        \n          Analyzing\n          log data with CloudWatch Logs Insights\n        \n      \n        \n          Amazon CloudWatch Lambda Insights\n        \n      \n        \n          Amazon CloudWatch Container Insights\n        \n      \n        \n          Query\n          your metrics with CloudWatch Metrics Insights\n        \n      \n        \n          AWS Distro for\n          OpenTelemetry\n        \n      \n        \n          Amazon CloudWatch Logs Insights Sample Queries\n        \n      \n        \n          Debugging\n          with Amazon CloudWatch Synthetics and AWS X-Ray\n        \n      \n        \n          Searching\n          and Filtering Log Data\n        \n      \n        \n          Sending\n          Logs Directly to Amazon S3\n        \n      \n        \n          The\n          Amazon Builders' Library: Instrumenting distributed systems\n          for operational visibility\n        \n      \n    \n      Related workshops:\n    \n    \n       \n    \n        \n          One\n          Observability Workshop\n        \n      \n    \n      Related tools:\n    \n    \n       \n    \n        \n          AWS Distro for\n          OpenTelemetry (GitHub)\n        \n      \n  \u003cdiv\u003e\u003cdiv\u003e\u003cdiv\u003e\u003cdiv id=\"js_error_message\"\u003e\u003cp\u003e\u003cimg src=\"https://d1ge0kk1l5kms0.cloudfront.net/images/G/01/webservices/console/warning.png\" alt=\"Warning\" /\u003e \u003cstrong\u003eJavascript is disabled or is unavailable in your browser.\u003c/strong\u003e\u003c/p\u003e\u003cp\u003eTo use the Amazon Web Services Documentation, Javascript must be enabled. Please refer to your browser's Help pages for instructions.\u003c/p\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003eDocument ConventionsREL06-BP01 Monitor all components for the workload\n  (Generation)REL06-BP03 Send notifications (Real-time processing and\n  alarming)",
  "REL06-BP03 Send notifications (Real-time processing and\n  alarming)When organizations detect potential issues, they send real-time notifications and alerts to the appropriate personnel and systems in order to respond quickly and effectively to these issues.\n    Desired outcome: Rapid responses to operational events are possible through configuration of relevant alarms based on service and application metrics. When alarm thresholds are breached, the appropriate personnel and systems are notified so they can address underlying issues.\n  \n    Common anti-patterns:\n  \n     \n     \n     \n     \n  \n      Configuring alarms with an excessively high threshold, resulting in the failure to send vital notifications.\n    \n      Configuring alarms with a threshold that is too low, resulting in inaction on important alerts due to the noise of excessive notifications.\n    \n      \n        Not updating alarms and their threshold when usage changes.\n      \n    \n      \n        For alarms best addressed through automated actions, sending the notification to personnel instead of generating the automated action results in excessive notifications being sent.\n      \n    \n    Benefits of establishing this best\n      practice: Sending real-time notifications and alerts to the appropriate personnel and systems allows for early detection of issues and rapid responses to operational incidents.\n  \n    Level of risk exposed if this best practice\n    is not established: High\n  \n\n  Implementation guidance\n    \n      Workloads should be equipped with real-time processing and alarming to improve the detectability of issues that could impact the availability of the application and serve as triggers for automated response. Organizations can perform real-time processing and alarming by creating alerts with defined metrics in order to receive notifications whenever significant events occur or a metric exceeds a threshold.\n    \n    \n      Amazon CloudWatch allows you to create metric and composite alarms using CloudWatch alarms based on static threshold, anomaly detection, and other criteria. For more detail on the types of alarms you can configure using CloudWatch, see the alarms section of the CloudWatch documentation.\n    \n    \n      You can construct customized views of metrics and alerts of your AWS resources for your teams using CloudWatch dashboards. The customizable home pages in the CloudWatch console allow you to monitor your resources in a single view across multiple Regions. \n    \n    \n      Alarms can perform one or more actions, like sending a notification to an Amazon SNS topic, performing an Amazon EC2 action or an Amazon EC2 Auto Scaling action, or creating an OpsItem or incident in AWS Systems Manager.\n    \n    \n      Amazon CloudWatch uses Amazon SNS to send notifications when the alarm changes state, providing message delivery from the publishers (producers) to the subscribers (consumers). \n      For more detail on setting up Amazon SNS notifications, see Configuring Amazon SNS.\n    \n    \n      CloudWatch sends EventBridge events whenever a CloudWatch alarm is created, updated, deleted, or its state changes. You can use EventBridge with these events to create rules that perform actions, such as notifying you whenever the state of an alarm changes or \n      automatically triggering events in your account using Systems Manager automation.\n    \n    \n      Stay informed with AWS Health. AWS Health is the authoritative source of information about the health of your AWS Cloud resources. Use AWS Health to get notified of any confirmed service events so you can quickly take steps to mitigate any impact. Create purpose-fit AWS Health event notifications to e-mail and chat channels through AWS User Notifications and integrate programmatically with your monitoring and alerting tools through Amazon EventBridge. If you use AWS Organizations, aggregate AWS Health events across accounts.\n    \n    \n      When should you use EventBridge or Amazon SNS?\n    \n    \n      Both EventBridge and Amazon SNS can be used to develop event-driven applications, and your choice will depend on your specific needs.\n    \n    \n      Amazon EventBridge is recommended when you want to build an application that reacts to events from your own applications, SaaS applications, and AWS services. EventBridge is the only event-based service that integrates directly with third-party SaaS partners. EventBridge also automatically ingests events from over 200 AWS services without requiring developers to create any resources in their account.\n    \n    \n      EventBridge uses a defined JSON-based structure for events, and helps you create rules that are applied across the entire event body to select events to forward to a target. EventBridge currently supports over 20 AWS services as targets, including AWS Lambda, Amazon SQS, Amazon SNS, Amazon Kinesis Data Streams, and Amazon Data Firehose.\n    \n    \n      Amazon SNS is recommended for applications that need high fan out (thousands or millions of endpoints). A common pattern we see is that customers use Amazon SNS as a target for their rule to filter the events that they need, and fan out to multiple endpoints.\n    \n    \n      Messages are unstructured and can be in any format. Amazon SNS supports forwarding messages to six different types of targets, including Lambda, Amazon SQS, HTTP/S endpoints, SMS, mobile push, and email. Amazon SNS typical latency is under 30 milliseconds. A wide range of AWS services send Amazon SNS messages by configuring the service to do so (more than 30, including Amazon EC2, Amazon S3, and Amazon RDS).\n    \n     \n      \n      Implementation steps\n      \n         \n         \n         \n      \n          \n            Create an alarm using Amazon CloudWatch alarms. \n          \n          \n             \n             \n          \n              \n                A metric alarm monitors a single CloudWatch metric or an expression dependent on CloudWatch metrics. The alarm initiates one or more actions based on the value of the metric or expression in comparison to a threshold over a number of time intervals. The action may consist of sending a notification to an Amazon SNS topic, performing an Amazon EC2 action or an Amazon EC2 Auto Scaling action, or creating an OpsItem or incident in AWS Systems Manager.\n              \n            \n              \n                A composite alarm consists of a rule expression that considers the alarm conditions of other alarms you've created. The composite alarm only enters alarm state if all rule conditions are met. The alarms specified in the rule expression of a composite alarm can include metric alarms and additional composite alarms. Composite alarms can send Amazon SNS notifications when their state changes and can create Systems Manager OpsItems or incidents when they enter the alarm state, but they cannot perform Amazon EC2 or Auto Scaling actions. \n              \n            \n        \n          \n            Set up Amazon SNS notifications. When creating a CloudWatch alarm, you can include an Amazon SNS topic to send a notification when the alarm changes state.\n          \n        \n          \n            Create rules in EventBridge that matches specified CloudWatch alarms. Each rule supports multiple targets, including Lambda functions. For example, you can define an alarm that initiates when available disk space is running low, which triggers a Lambda function through an EventBridge rule, to clean up the space. For more detail on EventBridge targets, see EventBridge targets. \n          \n        \n     \n\n   \n\n  Resources\n\n    \n      Related Well-Architected best practices:\n    \n    \n       \n       \n       \n    \n        \n          REL06-BP01 Monitor all components for the workload\n  (Generation)\n        \n      \n        \n          REL06-BP02 Define and calculate metrics (Aggregation)\n        \n      \n        \n          REL12-BP01 Use playbooks to investigate failures\n        \n      \n    \n      Related documents:\n    \n    \n       \n       \n       \n       \n       \n       \n       \n       \n       \n       \n    \n        \n          Amazon CloudWatch \n        \n      \n        \n          CloudWatch Logs insights\n        \n      \n        \n          Using\n          Amazon CloudWatch alarms\n        \n      \n        \n          Using\n          Amazon CloudWatch dashboards\n        \n      \n        \n          Using\n          Amazon CloudWatch metrics\n        \n      \n        \n          Setting up Amazon SNS notifications\n        \n      \n        \n          CloudWatch anomaly detection\n        \n      \n        \n          CloudWatch Logs data protection\n        \n      \n        \n          Amazon EventBridge\n        \n      \n        \n          Amazon Simple Notification Service\n        \n      \n    \n      Related videos:\n    \n    \n       \n       \n    \n        \n          reinvent 2022 observability videos\n        \n      \n        AWS re:Invent 2022 - Observability best practices at Amazon\n        \n      \n    \n      Related examples:\n    \n    \n       \n       \n    \n        \n          One\n            Observability Workshop\n        \n      \n        \n          Amazon EventBridge to AWS Lambda with feedback control by Amazon CloudWatch Alarms\n        \n      \n  \u003cdiv\u003e\u003cdiv\u003e\u003cdiv\u003e\u003cdiv id=\"js_error_message\"\u003e\u003cp\u003e\u003cimg src=\"https://d1ge0kk1l5kms0.cloudfront.net/images/G/01/webservices/console/warning.png\" alt=\"Warning\" /\u003e \u003cstrong\u003eJavascript is disabled or is unavailable in your browser.\u003c/strong\u003e\u003c/p\u003e\u003cp\u003eTo use the Amazon Web Services Documentation, Javascript must be enabled. Please refer to your browser's Help pages for instructions.\u003c/p\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003eDocument ConventionsREL06-BP02 Define and calculate metrics (Aggregation)REL06-BP04 Automate responses (Real-time processing and\n  alarming)",
  "REL06-BP04 Automate responses (Real-time processing and\n  alarming)\n    Use automation to take action when an event is detected, for\n    example, to replace failed components.\n  \n    Automated real-time processing of alarms is implemented so that systems can take quick corrective action and attempt to prevent failures or degraded service when alarms are triggered. Automated responses to alarms could include the replacement of failing components, the adjustment of compute capacity, the redirection of traffic to healthy hosts, availability zones, or other regions, and the notification of operators.\n  \n    Desired outcome: Real-time alarms are identified, and automated processing of alarms is set up to invoke the appropriate actions taken to maintain service level objectives and service-level agreements (SLAs). Automation can range from self-healing activities of single components to full-site failover.\n  \n    Common anti-patterns:\n  \n     \n     \n     \n     \n     \n     \n  \n      \n        Not having a clear inventory or catalog of key real-time alarms.\n      \n    \n      \n        No automated responses on critical alarms (for example, when compute is nearing exhaustion, autoscaling occurs).\n      \n    \n      \n        Contradictory alarm response actions. \n      \n    \n      \n        No standard operating procedures (SOPs) for operators to follow when they receive alert notifications.\n      \n    \n      \n        Not monitoring configuration changes, as undetected configuration changes can cause downtime for workloads.\n      \n    \n      \n        Not having a strategy to undo unintended configuration changes.\n      \n    \n    Benefits of establishing this best practice: Automating alarm processing can improve system resiliency. The system takes corrective actions automatically, reducing manual activities that allow for human, error-prone interventions. Workload operates meet availability goals, and reduces service disruption.\n  \n    Level of risk exposed if this best practice\n    is not established: Medium\n  \n\n  Implementation guidance\n    \n      \n      To effectively manage alerts and automate their response, categorize alerts based on their criticality and impact, document response procedures, and plan responses before ranking tasks.\n      \n    \n    \n      Identify tasks requiring specific actions (often detailed in runbooks), and examine all runbooks and playbooks to determine which tasks can be automated. If actions can be defined, often they can be automated. If actions cannot be automated, document manual steps in an SOP and train operators on them. Continually challenge manual processes for automation opportunities where you can establish and maintain a plan to automate alert responses. \n    \n     \n      \n      Implementation steps\n      \n         \n         \n         \n         \n         \n      \n          \n            Create an inventory of alarms: To obtain a list of all alarms, you can use the AWS CLI using the Amazon CloudWatch command describe-alarms. Depending upon how many alarms you have set up, you might have to use pagination to retrieve a subset of alarms for each call, or alternatively you can use the AWS SDK to obtain the alarms using an API call.\n          \n        \n          \n            Document all alarm actions: Update a runbook with all alarms and their actions, irrespective if they are manual or automated. AWS Systems Manager provides predefined runbooks. For more information about runbooks, see Working with runbooks. For detail on how to view runbook content, see View runbook content.\n          \n        \n          \n            Set up and manage alarm actions: For any of the alarms that require an action, specify the automated action using the CloudWatch SDK. For example, you can change the state of your Amazon EC2 instances automatically based on a CloudWatch alarm by creating and enabling actions on an alarm or disabling actions on an alarm.\n          \n          \n            You can also use Amazon EventBridge to respond automatically to system events, such as application availability issues or resource changes. You can create rules to indicate which events you're interested in, and the actions to take when an event matches a rule. The actions that can be automatically initiated include invoking an AWS Lambda function, invoking Amazon EC2 Run Command, relaying the event to Amazon Kinesis Data Streams, and seeing Automate Amazon EC2 using EventBridge.\n          \n        \n          \n            Standard Operating Procedures (SOPs): Based on your application components, AWS Resilience Hub recommends multiple SOP templates. You can use these SOPs to document all the processes an operator should follow in case an alert is raised. You can also construct a SOP based on Resilience Hub recommendations, where you need an Resilience Hub application with an associated resiliency policy, as well as a historic  resiliency assessment against that application. The recommendations for your SOP are produced by the resiliency assessment.\n          \n          \n            Resilience Hub works with Systems Manager to automate the steps of your SOPs by providing a number of SSM documents you can use as the basis for those SOPs. For example, Resilience Hub may recommend an SOP for adding disk space based on an existing SSM automation document.\n          \n        \n          \n            Perform automated actions using Amazon DevOps Guru:\n            You can use Amazon DevOps Guru to automatically monitor application resources for anomalous behavior and deliver targeted recommendations to speed up problem identification and remediation times. With DevOps Guru, you can monitor streams of operational data in near real time from multiple sources including Amazon CloudWatch metrics, AWS Config, AWS CloudFormation, and AWS X-Ray. You can also use DevOps Guru to automatically create OpsItems in OpsCenter and send events to EventBridge for additional automation. \n          \n        \n     \n    \n   \n\n  Resources\n\n    \n      Related best practices:\n    \n    \n       \n       \n       \n       \n    \n        \n          REL06-BP01 Monitor all components for the workload\n  (Generation)\n        \n      \n        \n          REL06-BP02 Define and calculate metrics (Aggregation)\n        \n      \n        \n          REL06-BP03 Send notifications (Real-time processing and\n  alarming)\n        \n      \n        \n          REL08-BP01 Use runbooks for standard activities such as\n  deployment\n        \n      \n    \n      Related documents:\n    \n    \n       \n       \n       \n       \n       \n       \n    \n        \n          AWS Systems Manager Automation\n        \n      \n        \n          Creating\n          an EventBridge Rule That Triggers on an Event from an AWS\n          Resource\n        \n      \n        \n          One\n          Observability Workshop\n        \n      \n        \n          The\n          Amazon Builders' Library: Instrumenting distributed systems\n          for operational visibility\n        \n      \n        \n          What\n          is Amazon DevOps Guru?\n        \n      \n        \n          Working\n          with Automation Documents (Playbooks)\n        \n      \n    \n      Related videos:\n    \n    \n       \n       \n       \n       \n       \n    \n        AWS re:Invent 2022 - Observability best practices at Amazon\n        \n      \n        AWS re:Invent 2020: Automate anything with AWS Systems Manager\n      \n        \n          Introduction to AWS Resilience Hub\n      \n        \n          Create Custom Ticket Systems for Amazon DevOps Guru Notifications\n        \n      \n        \n          Enable Multi-Account Insight Aggregation with Amazon DevOps Guru\n        \n      \n    \n      Related examples:\n    \n    \n       \n    \n        \n          Amazon CloudWatch and Systems Manager Workshop\n        \n      \n  \u003cdiv\u003e\u003cdiv\u003e\u003cdiv\u003e\u003cdiv id=\"js_error_message\"\u003e\u003cp\u003e\u003cimg src=\"https://d1ge0kk1l5kms0.cloudfront.net/images/G/01/webservices/console/warning.png\" alt=\"Warning\" /\u003e \u003cstrong\u003eJavascript is disabled or is unavailable in your browser.\u003c/strong\u003e\u003c/p\u003e\u003cp\u003eTo use the Amazon Web Services Documentation, Javascript must be enabled. Please refer to your browser's Help pages for instructions.\u003c/p\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003eDocument ConventionsREL06-BP03 Send notifications (Real-time processing and\n  alarming)REL06-BP05 Analyze logs",
  "REL06-BP05 Analyze logs\n    Collect log files and metrics histories and analyze these for\n    broader trends and workload insights.\n  \n    Amazon CloudWatch Logs Insights supports\n    a simple\n    yet powerful query language that you can use to analyze log\n    data. Amazon CloudWatch Logs also supports subscriptions that allow\n    data to flow seamlessly to Amazon S3 where you can use or Amazon Athena to query the data. It also supports queries on a large array\n    of formats.\n    See Supported\n    SerDes and Data Formats in the Amazon Athena User Guide for\n    more information. For analysis of huge log file sets, you can run an\n    Amazon EMR cluster to run petabyte-scale analyses.\n  \n    There are a number of tools provided by AWS Partners and third\n    parties that allow for aggregation, processing, storage, and\n    analytics. These tools include New Relic, Splunk, Loggly, Logstash,\n    CloudHealth, and Nagios. However, outside generation of system and\n    application logs is unique to each cloud provider, and often unique\n    to each service.\n  \n    An often-overlooked part of the monitoring process is data\n    management. You need to determine the retention requirements for\n    monitoring data, and then apply lifecycle policies accordingly.\n    Amazon S3 supports lifecycle management at the S3 bucket level. This\n    lifecycle management can be applied differently to different paths\n    in the bucket. Toward the end of the lifecycle, you can transition\n    data to Amazon S3 Glacier for long-term storage, and then expiration\n    after the end of the retention period is reached. The S3\n    Intelligent-Tiering storage class is designed to optimize costs by\n    automatically moving data to the most cost-effective access tier,\n    without performance impact or operational overhead.\n  \n    Level of risk exposed if this best practice\n    is not established: Medium\n  \n\n  Implementation guidance\n\n   \n   \n \n    CloudWatch Logs Insights allows you to interactively search and\n    analyze your log data in Amazon CloudWatch Logs.\n  \n        \n           \n           \n        \n            \n              Analyzing Log Data\n                with CloudWatch Logs Insights\n            \n          \n            \n              Amazon CloudWatch Logs Insights Sample Queries\n            \n            \n    Use Amazon CloudWatch Logs to send logs to Amazon S3 where you can\n    use or Amazon Athena to query the data.\n  \n        \n           \n        \n            \n              How\n                do I analyze my Amazon S3 server access logs using Athena?\n            \n            \n               \n            \n                 Create an S3 lifecycle policy for your server access logs bucket. Configure\n                  the lifecycle policy to periodically remove log files. Doing so reduces the amount\n                  of data that Athena analyzes for each query. \n                \n                   \n                \n                    \n                      How Do I Create a\n                        Lifecycle Policy for an S3 Bucket?\n                    \n                  \n              \n          \n\n   \n\n  Resources\n\n      \n    \n      Related documents:\n    \n    \n       \n       \n       \n       \n       \n       \n       \n    \n        \n          Amazon CloudWatch Logs Insights Sample Queries\n        \n      \n        \n          Analyzing\n          Log Data with CloudWatch Logs Insights\n        \n      \n        \n          Debugging\n          with Amazon CloudWatch Synthetics and AWS X-Ray\n        \n      \n        \n          How\n          Do I Create a Lifecycle Policy for an S3 Bucket?\n        \n      \n        \n          How\n          do I analyze my Amazon S3 server access logs using\n          Athena?\n        \n      \n        \n          One\n          Observability Workshop\n        \n      \n        \n          The\n          Amazon Builders' Library: Instrumenting distributed systems\n          for operational visibility\n        \n      \n  \u003cdiv\u003e\u003cdiv\u003e\u003cdiv\u003e\u003cdiv id=\"js_error_message\"\u003e\u003cp\u003e\u003cimg src=\"https://d1ge0kk1l5kms0.cloudfront.net/images/G/01/webservices/console/warning.png\" alt=\"Warning\" /\u003e \u003cstrong\u003eJavascript is disabled or is unavailable in your browser.\u003c/strong\u003e\u003c/p\u003e\u003cp\u003eTo use the Amazon Web Services Documentation, Javascript must be enabled. Please refer to your browser's Help pages for instructions.\u003c/p\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003eDocument ConventionsREL06-BP04 Automate responses (Real-time processing and\n  alarming)REL06-BP06 Regularly review monitoring scope and\n      metrics",
  "REL06-BP06 Regularly review monitoring scope and\n    metrics\n    Frequently review how workload monitoring is implemented, and update\n    it as your workload and its architecture evolves. Regular audits of\n    your monitoring helps reduce the risk of missed or overlooked\n    trouble indicators and further helps your workload meet its\n    availability goals.\n  \n    Effective monitoring is anchored in key business metrics, which\n    evolve as your business priorities change. Your monitoring review\n    process should emphasize service-level indicators (SLIs) and\n    incorporate insights from your infrastructure, applications,\n    clients, and users.\n  \n    Desired outcome: You have an\n    effective monitoring strategy that is regularly reviewed and updated\n    periodically, as well as after any significant events or changes.\n    You verify that key application health indicators are still relevant\n    as your workload and business requirements evolve.\n  \n    Common anti-patterns:\n  \n     \n     \n     \n     \n     \n     \n     \n  \n      \n        You collect only default metrics.\n      \n    \n      \n        You set up a monitoring strategy, but you never review it.\n      \n    \n      \n        You don't discuss monitoring when major changes are deployed.\n      \n    \n      \n        You trust outdated metrics to determine workload health.\n      \n    \n      \n        Your operations teams are overwhelmed with false-positive alerts\n        due to outdated metrics and thresholds.\n      \n    \n      \n        You lack observability of application components that are not\n        being monitored.\n      \n    \n      \n        You focus only on low-level technical metrics and excluding\n        business metrics in your monitoring.\n      \n    \n    Benefits of establishing this best\n      practice: When you regularly review your monitoring, you\n    can anticipate potential problems and verify that you are capable of\n    detecting them. It also allows you to uncover blind spots that you\n    might have missed during earlier reviews, which further improves\n    your ability to detect issues.\n  \n    Level of risk exposed if this best practice\n      is not established: Medium\n  \n    \n    Implementation guidance\n    \n    \n    \n      Review monitoring metrics and scope during your\n      operational\n        readiness review (ORR) process. Perform periodic\n      operational readiness reviews on a consistent schedule to evaluate\n      whether there are any gaps between your current workload and the\n      monitoring you have configured. Establish a regular cadence for\n      operational performance reviews and knowledge sharing to enhance\n      your ability to achieve higher performance from your operational\n      teams. Validate whether existing alert thresholds are still\n      adequate, and check for situations where operational teams are\n      receiving false-positive alerts or not monitoring aspects of the\n      application that should be monitored.\n    \n    \n      The\n      Resilience\n        Analysis Framework provides useful guidance that can help\n      you navigate the process. The focus of the framework is to\n      identify potential failure modes and the preventive and corrective\n      controls you can use to mitigate their impact. This knowledge can\n      help you identify the right metrics and events to monitor and\n      alert upon.\n    \n     \n      \n      Implementation steps\n      \n      \n      \n         \n         \n         \n         \n         \n         \n         \n         \n      \n          \n            Schedule and conduct regular reviews of the workload\n            dashboards. You may have different cadences for the depth at\n            which you inspect.\n          \n        \n          \n            Inspect for trends in the metrics. Compare the metric values to historic values to see if there are trends that may indicate that something that needs investigation. Examples of this include increased latency, decreased primary business function, and increased failure responses.\n          \n        \n          \n            Inspect for outliers and anomalies in your metrics, which can be masked by averages or medians. Look at the highest and lowest values during the time frame, and investigate the causes of observations that are far outside of normal bounds. As you continue to remove these causes, you can tighten your expected metric bounds in response to the improved consistency of your workload performance.\n          \n        \n          \n            Look for sharp changes in behavior. An immediate change in quantity or direction of a metric may indicate that there has been a change in the application or external factors that you may need to add additional metrics to track.\n          \n        \n          \n            Review whether the current monitoring strategy remains relevant for the application. Based on an analysis of previous incidents (or the Resilience Analysis Framework), assess if there are additional aspects of the application that should be incorporated into the monitoring scope.\n          \n        \n          \n            Review your Real User Monitoring (RUM) metrics to determine whether there are any gaps in application functionality coverage.\n          \n        \n          \n            Review your change management process. Update your\n            procedures if necessary to include a monitoring analysis\n            step that should be performed before you approve a change.\n          \n        \n          \n            Implement monitoring review as part of your operational\n            readiness review and correction of error processes.\n          \n        \n     \n   \n    \n    Resources\n    \n    \n    \n      Related best practices\n    \n    \n       \n       \n       \n       \n       \n    \n        \n          REL06-BP01\n            Monitor all components for the workload (Generation)\n        \n      \n        \n          REL06-BP02\n            Define and calculate metrics (Aggregation)\n        \n      \n        \n          REL06-BP07\n            Monitor end-to-end tracing of requests through your\n            system\n        \n      \n        \n          REL12-BP02\n            Perform post-incident analysis\n        \n      \n        \n          REL12-BP06\n            Conduct game days regularly\n        \n      \n    \n      Related documents:\n    \n    \n       \n       \n       \n       \n       \n       \n       \n       \n       \n       \n       \n       \n       \n    \n        \n          Why\n            you should develop a correction of error (COE)\n        \n      \n        \n          Using\n            Amazon CloudWatch Dashboards\n        \n      \n        \n          Building\n            dashboards for operational visibility\n        \n      \n        \n          Advanced\n            Multi-AZ Resilience Patterns - Gray failures\n        \n      \n        \n          Amazon CloudWatch Logs Insights Sample Queries\n        \n      \n        \n          Debugging\n            with Amazon CloudWatch Synthetics and AWS X-Ray\n        \n      \n        \n          One\n            Observability Workshop\n        \n      \n        \n          The\n            Amazon Builders' Library: Instrumenting distributed systems\n            for operational visibility\n        \n      \n        \n          Using\n            Amazon CloudWatch Dashboards\n        \n      \n        \n          AWS           Observability Best Practices\n        \n      \n        \n          Resilience\n            Analysis Framework\n        \n      \n        \n          Resilience\n            Analysis Framework - Observability\n        \n      \n        \n          Operational\n            Readiness Review - ORR\n        \n      \n  \u003cdiv\u003e\u003cdiv\u003e\u003cdiv\u003e\u003cdiv id=\"js_error_message\"\u003e\u003cp\u003e\u003cimg src=\"https://d1ge0kk1l5kms0.cloudfront.net/images/G/01/webservices/console/warning.png\" alt=\"Warning\" /\u003e \u003cstrong\u003eJavascript is disabled or is unavailable in your browser.\u003c/strong\u003e\u003c/p\u003e\u003cp\u003eTo use the Amazon Web Services Documentation, Javascript must be enabled. Please refer to your browser's Help pages for instructions.\u003c/p\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003eDocument ConventionsREL06-BP05 Analyze logsREL06-BP07 Monitor end-to-end tracing of requests through your\n  system",
  "REL06-BP07 Monitor end-to-end tracing of requests through your\n  systemTrace requests as they process through service components so product teams can more easily analyze and debug issues and improve performance.\n    Desired outcome: Workloads with comprehensive tracing across all components are easy to debug, improving mean time to resolution (MTTR) of errors and latency by simplifying root cause discovery. End-to-end tracing reduces the time it takes to discover impacted components and drill into the detailed root causes of errors or latency.\n  \n    Common anti-patterns:\n  \n     \n     \n     \n  \n      \n        Tracing is used for some components but not for all. For example, without tracing for AWS Lambda, teams might not clearly understand latency caused by cold starts in a spiky workload.   \n      \n    \n      \n        Synthetic canaries or real-user monitoring (RUM) are not configured with tracing. Without canaries or RUM, client interaction telemetry is omitted from the trace analysis yielding an incomplete performance profile.  \n      \n    \n      \n        Hybrid workloads include both cloud native and third party tracing tools, but steps have not been taken elect and fully integrate a single tracing solution. Based on the elected tracing solution, cloud native tracing SDKs should be used to instrument components that are not cloud native or third party tools should be configured to ingest cloud native trace telemetry. \n    \n    Benefits of establishing this best practice: When development teams are alerted to issues, they can see a full picture of system component interactions, including component by component correlation to logging, performance, and failures. Because tracing makes it easy to visually identify root causes, less time is spent investigating root causes. Teams that understand component interactions in detail make better and faster decisions when resolving issues. Decisions like when to invoke disaster recovery (DR) failover or where to best implement self-healing strategies can be improved by analyzing systems traces, ultimately improving customer satisfaction with your services.\n  \n    Level of risk exposed if this best practice\n    is not established: Medium\n  \n\n  Implementation guidance\n    \n      Teams that operate distributed applications can use tracing tools to establish a correlation identifier, collect traces of requests, and build service maps of connected components. All application components should be included in request traces including service clients, middleware gateways and event buses, compute components, and storage, including key value stores and databases. Include synthetic canaries and real-user monitoring in your end-to-end tracing configuration to measure remote client interactions and latency so that you can accurately evaluate your systems performance against your service level agreements and objectives. \n    \n    \n      You can use AWS X-Ray and Amazon CloudWatch Application Monitoring instrumentation services to provide a complete view of requests as they travel through your application. X-Ray collects application telemetry and allows you to visualize and filter it across payloads, functions, traces, services, APIs, and can be turned on for system components with no-code or low-code. CloudWatch application monitoring includes ServiceLens to integrate your traces with metrics, logs, and alarms. CloudWatch application monitoring also includes synthetics to monitor your endpoints and APIs, as well as real-user monitoring to instrument your web application clients. \n    \n   \n    \n    Implementation steps\n    \n       \n       \n       \n       \n       \n       \n    \n        \n          Use AWS X-Ray on all supported native services like Amazon S3, AWS Lambda, and Amazon API Gateway. These AWS services enable X-Ray with configuration toggles using infrastructure as code, AWS SDKs, or the AWS Management Console. \n        \n      \n        \n          Instrument applications AWS Distro for Open Telemetry and X-Ray or third-party collection agents.\n        \n      \n        Review the AWS X-Ray Developer Guide for programming language specific implementation. These documentation sections detail how to instrument HTTP requests, SQL queries, and other processes specific to your application programming language.\n      \n        \n          Use X-Ray tracing for Amazon CloudWatch Synthetic Canaries and Amazon CloudWatch RUM to analyze the request path from your end user client through your downstream AWS infrastructure. \n        \n      \n        \n          Configure CloudWatch metrics and alarms based on resource health and canary telemetry so that teams are alerted to issues quickly, and can then deep dive into traces and service maps with ServiceLens. \n        \n      \n        \n          Enable X-Ray integration for third party tracing tools like Datadog, New Relic, or Dynatrace if you are using third party tools for your primary tracing solution.\n        \n      \n   \n\n  Resources\n\n    \n      Related best practices:\n    \n    \n       \n       \n    \n        \n          REL06-BP01 Monitor all components for the workload\n  (Generation)\n        \n      \n        \n          REL11-BP01 Monitor all components of the workload to detect\n  failures\n        \n      \n    \n      Related documents:\n    \n    \n       \n       \n       \n       \n       \n       \n       \n       \n       \n       \n    \n        \n          What\n            is AWS X-Ray?\n        \n      \n        \n          Amazon CloudWatch: Application Monitoring\n        \n      \n        \n          Debugging\n          with Amazon CloudWatch Synthetics and AWS X-Ray\n        \n      \n        \n          The\n          Amazon Builders' Library: Instrumenting distributed systems\n          for operational visibility\n        \n      \n        \n          Integrating AWS X-Ray with other AWS services\n        \n      \n        AWS Distro for OpenTelemetry and AWS X-Ray\n      \n        \n         Amazon CloudWatch: Using synthetic monitoring\n        \n      \n        \n          Amazon CloudWatch: Use CloudWatch RUM\n        \n      \n        \n          Set up Amazon CloudWatch synthetics canary and Amazon CloudWatch alarm\n        \n      \n        \n          Availability and Beyond: Understanding and Improving the Resilience of Distributed Systems on AWS\n      \n    \n      Related examples:\n    \n    \n       \n    \n        \n          One Observability Workshop\n        \n      \n    \n      Related videos:\n    \n    \n       \n       \n    \n        AWS re:Invent 2022 - How to monitor applications across multiple accounts\n        \n      \n        \n          How to Monitor your AWS Applications\n        \n      \n    \n      Related tools:\n    \n    \n       \n       \n       \n    \n        AWS X-Ray\n      \n        \n          Amazon CloudWatch\n        \n      \n        \n          Amazon Route 53\n        \n      \n  \u003cdiv\u003e\u003cdiv\u003e\u003cdiv\u003e\u003cdiv id=\"js_error_message\"\u003e\u003cp\u003e\u003cimg src=\"https://d1ge0kk1l5kms0.cloudfront.net/images/G/01/webservices/console/warning.png\" alt=\"Warning\" /\u003e \u003cstrong\u003eJavascript is disabled or is unavailable in your browser.\u003c/strong\u003e\u003c/p\u003e\u003cp\u003eTo use the Amazon Web Services Documentation, Javascript must be enabled. Please refer to your browser's Help pages for instructions.\u003c/p\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003eDocument ConventionsREL06-BP06 Regularly review monitoring scope and\n      metricsREL 7. How do you design your workload to adapt to changes in\n                  demand?",
  "REL07-BP01 Use automation when obtaining or scaling\n  resources\n    A cornerstone of reliability in the cloud is the programmatic\n    definition, provisioning, and management of your infrastructure and\n    resources. Automation helps you streamline resource provisioning,\n    facilitate consistent and secure deployments, and scale resources\n    across your entire infrastructure.\n  \n    Desired outcome: You manage your\n    infrastructure as code (IaC). You define and maintain your\n    infrastructure code in version control systems (VCS). You delegate\n    provisioning AWS resources to automated mechanisms and leverage\n    managed services like Application Load Balancer (ALB), Network Load\n    Balancer (NLB), and Auto Scaling groups. You provision your\n    resources using continuous integration/continuous delivery (CI/CD)\n    pipelines so that code changes automatically initiate resource\n    updates, including updates to your Auto Scaling configurations.\n  \n    Common anti-patterns:\n  \n     \n     \n     \n     \n  \n      \n        You deploy resources manually using the command line or at the\n        AWS Management Console (also known as click-ops).\n      \n    \n      \n        You tightly couple your application components or resources, and\n        create inflexible architectures as a result.\n      \n    \n      \n        You implement inflexible scaling policies that do not adapt to\n        changing business requirements, traffic patterns, or new\n        resource types.\n      \n    \n      \n        You manually estimate capacity to meet anticipated demand.\n      \n    \n    Benefits of establishing this best\n      practice: Infrastructure as code (IaC) allows\n    infrastructure to be defined programmatically. This helps you manage\n    infrastructure changes through the same software development\n    lifecycle as application changes, which promotes consistency and\n    repeatability and reduces the risk of manual, error-prone tasks. You\n    can further streamline the process of provisioning and updating\n    resources through implementing IaC with automated delivery\n    pipelines. You can deploy infrastructure updates reliably and\n    efficiently without the need for manual intervention. This agility\n    is particularly important when scaling resources to meet fluctuating\n    demands.\n  \n    You can achieve dynamic, automated resource scaling in conjunction\n    with IaC and delivery pipelines. By monitoring key metrics and\n    applying predefined scaling policies, Auto Scaling can automatically\n    provision or deprovision resources as needed, which improves\n    performance and cost-efficiency. This reduces the potential for\n    manual errors or delays in response to changes in application or\n    workload requirements.\n  \n    The combination of IaC, automated delivery pipelines, and Auto\n    Scaling helps organizations provision, update, and scale their\n    environments with confidence. This automation is essential to\n    maintain a responsive, resilient, and efficiently-managed cloud\n    infrastructure.\n  \n    Level of risk exposed if this best practice\n      is not established: High\n  \n    \n    Implementation guidance\n    \n    \n    \n      To set up automation with CI/CD pipelines and infrastructure as\n      code (IaC) for your AWS architecture, choose a version control\n      system such as Git to store your IaC templates and configuration.\n      These templates can be written using tools such as\n      AWS CloudFormation. To start, define your infrastructure\n      components (such as AWS VPCs, Amazon EC2 Auto Scaling Groups, and\n      Amazon RDS databases) within these templates.\n    \n    \n      Next, integrate these IaC templates with a CI/CD pipeline to\n      automate the deployment process.\n      AWS CodePipeline provides a seamless AWS-native solution, or\n      you can use other third-party CI/CD solutions. Create a pipeline\n      that activates when changes occur to your version control\n      repository. Configure the pipeline to include stages that lint and\n      validate your IaC templates, deploy the infrastructure to a\n      staging environment, run automated tests, and finally, deploy to\n      production. Incorporate approval steps where necessary to maintain\n      control over changes. This automated pipeline not only speeds up\n      deployment but also facilitates consistency and reliability across\n      environments.\n    \n    \n      Configure Auto Scaling of resources such as Amazon EC2 instances,\n      Amazon ECS tasks, and database replicas in your IaC to provide\n      automatic scale-out and scale-in as needed. This approach enhances\n      application availability and performance and optimizes cost by\n      dynamically adjusting resources based on demand. For a list of\n      supported resources, see\n      Amazon EC2 Auto Scaling and\n      AWS Auto Scaling.\n    \n     \n      \n      Implementation steps\n      \n      \n      \n         \n         \n         \n         \n         \n         \n      \n          \n            Create and use a source code repository to store the code\n            that controls your infrastructure configuration. Commit\n            changes to this repository to reflect any ongoing changes\n            you want to make.\n          \n        \n          \n            Select an infrastructure as code solution such as AWS CloudFormation to keep your infrastructure up to date and\n            detect inconsistency (drift) from your intended state.\n          \n        \n          \n            Integrate your IaC platform with your CI/CD pipeline to\n            automate deployments.\n          \n        \n          \n            Determine and collect the appropriate metrics for automatic\n            scaling of resources.\n          \n        \n          \n            Configure automatic scaling of resources using scale-out and\n            scale-in policies appropriate for your workload components.\n            Consider using scheduled scaling for predictable usage\n            patterns.\n          \n        \n          \n            Monitor deployments to detect failures and regressions.\n            Implement rollback mechanisms within your CI/CD platform to\n            revert changes if necessary.\n          \n        \n     \n   \n\n  Resources\n\n      \n    \n      Related documents:\n    \n    \n       \n       \n       \n       \n       \n       \n       \n       \n       \n       \n       \n       \n       \n       \n    \n        \n          AWS Auto Scaling: How Scaling Plans Work\n        \n      \n        \n          AWS Marketplace: products that can be used with auto\n          scaling\n        \n      \n        \n          Managing\n          Throughput Capacity Automatically with DynamoDB Auto\n          Scaling\n        \n      \n        \n          Using\n          a load balancer with an Auto Scaling group\n        \n      \n        \n          What\n          Is AWS Global Accelerator?\n        \n      \n        \n          What\n          Is Amazon EC2 Auto Scaling?\n        \n      \n        \n          What\n          is AWS Auto Scaling?\n        \n      \n        \n          What\n          is Amazon CloudFront?\n        \n      \n        \n          What\n          is Amazon Route 53?\n        \n      \n        \n          What\n          is Elastic Load Balancing?\n        \n      \n        \n          What\n          is a Network Load Balancer?\n        \n      \n        \n          What\n          is an Application Load Balancer?\n        \n      \n        \n          Integrating\n            Jenkins with AWS CodeBuild and AWS CodeDeploy\n        \n      \n        \n          Creating\n            a four stage pipeline with AWS CodePipeline\n        \n      \n    \n      Related videos:\n    \n    \n       \n       \n       \n       \n       \n    \n        \n          Back\n            to Basics: Deploy Your Code to Amazon EC2\n        \n      \n        \n          AWS Supports You | Starting Your Infrastructure as Code Solution Using\n            AWS CloudFormation Templates\n        \n      \n        \n          Streamline\n            Your Software Release Process Using AWS CodePipeline\n        \n      \n        \n          Monitor\n            AWS Resources Using Amazon CloudWatch Dashboards\n        \n      \n        \n          Create\n            Cross Account \u0026 Cross Region CloudWatch Dashboards | Amazon Web Services\n        \n      \n  \u003cdiv\u003e\u003cdiv\u003e\u003cdiv\u003e\u003cdiv id=\"js_error_message\"\u003e\u003cp\u003e\u003cimg src=\"https://d1ge0kk1l5kms0.cloudfront.net/images/G/01/webservices/console/warning.png\" alt=\"Warning\" /\u003e \u003cstrong\u003eJavascript is disabled or is unavailable in your browser.\u003c/strong\u003e\u003c/p\u003e\u003cp\u003eTo use the Amazon Web Services Documentation, Javascript must be enabled. Please refer to your browser's Help pages for instructions.\u003c/p\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003eDocument ConventionsREL 7. How do you design your workload to adapt to changes in\n                  demand?REL07-BP02 Obtain resources upon detection of impairment to a\n  workload",
  "REL07-BP02 Obtain resources upon detection of impairment to a\n  workload\n    Scale resources reactively when necessary if availability is\n    impacted, to restore workload availability.\n  \n    You first must configure health checks and the criteria on these\n    checks to indicate when availability is impacted by lack of\n    resources. Then, either notify the appropriate personnel to manually\n    scale the resource, or start automation to automatically scale it.\n  \n    Scale can be manually adjusted for your workload (for example,\n    changing the number of EC2 instances in an Auto Scaling group, or\n    modifying throughput of a DynamoDB table through the AWS Management Console or AWS CLI). However, automation should be used\n    whenever possible (refer to Use automation\n    when obtaining or scaling resources).\n  \n    Desired outcome: Scaling activities (either automatically or manually) are initiated to restore availability upon detection of a failure or degraded customer experience.\n  \n    Level of risk exposed if this best practice\n    is not established: Medium\n  \n\n  Implementation guidance\n    \n      Implement observability and monitoring across all components in your workload, to monitor customer experience and detect failure. Define the procedures, manual or automated, that scale the required resources. o\tFor more information, see REL11-BP01 Monitor all components of the workload to detect failures.\n    \n     \n      \n      Implementation steps\n      \n         \n        \n      \n          \n            Define the procedures, manual or automated, that scale the required resources.\n          \n          \n             \n             \n          \n              \n                Scaling procedures depend on how the different components within your workload are designed.\n              \n            \n              \n                Scaling procedures also vary depending on the underlying technology utilized.\n              \n              \n                 \n                 \n                 \n              \n                  \n                    Components using AWS Auto Scaling can use scaling plans to configure a set of instructions for scaling your resources. If you work with AWS CloudFormation or add tags to AWS resources, you can set up scaling plans for different sets of resources per application. Auto Scaling provides recommendations for scaling strategies customized to each resource. After you create your scaling plan, Auto Scaling combines dynamic scaling and predictive scaling methods together to support your scaling strategy. For more detail, see How scaling plans work.\n                  \n                \n                  \n                    Amazon EC2 Auto Scaling verifies that you have the correct number of Amazon EC2 instances available to handle the load for your application. You create collections of EC2 instances, called Auto Scaling groups. You can specify the minimum and maximum number of instances in each Auto Scaling group, and Amazon EC2 Auto Scaling ensures that your group never goes below or above these limits. For more detail, see What is Amazon EC2 Auto Scaling?\n                  \n                \n                  \n                    Amazon DynamoDB auto scaling uses the Application Auto Scaling service to dynamically adjust provisioned throughput capacity on your behalf, in response to actual traffic patterns. This allows a table or a global secondary index to increase its provisioned read and write capacity to handle sudden increases in traffic, without throttling. For more detail, see Managing throughput capacity automatically with DynamoDB auto scaling.\n                  \n                \n            \n        \n     \n    \n   \n\n  Resources\n\n    \n      Related best practices:\n    \n    \n       \n       \n    \n        \n          REL07-BP01 Use automation when obtaining or scaling resources\n        \n      \n        \n          REL11-BP01 Monitor all components of the workload to detect failures\n        \n      \n    \n      Related documents:\n    \n    \n       \n       \n       \n    \n        \n          AWS Auto Scaling: How Scaling Plans Work\n        \n      \n        \n          Managing\n          Throughput Capacity Automatically with DynamoDB Auto\n          Scaling\n        \n      \n        \n          What\n          Is Amazon EC2 Auto Scaling?\n        \n      \n  \u003cdiv\u003e\u003cdiv\u003e\u003cdiv\u003e\u003cdiv id=\"js_error_message\"\u003e\u003cp\u003e\u003cimg src=\"https://d1ge0kk1l5kms0.cloudfront.net/images/G/01/webservices/console/warning.png\" alt=\"Warning\" /\u003e \u003cstrong\u003eJavascript is disabled or is unavailable in your browser.\u003c/strong\u003e\u003c/p\u003e\u003cp\u003eTo use the Amazon Web Services Documentation, Javascript must be enabled. Please refer to your browser's Help pages for instructions.\u003c/p\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003eDocument ConventionsREL07-BP01 Use automation when obtaining or scaling\n  resourcesREL07-BP03 Obtain resources upon detection that more resources\n  are needed for a workload",
  "REL07-BP03 Obtain resources upon detection that more resources\n  are needed for a workload\n    One of the most valuable features of cloud computing is the ability\n    to provision resources dynamically.\n  \n    In traditional on-premises compute environments, you must identify\n    and provision enough capacity in advance to serve peak demand. This\n    is a problem because it is expensive and because it poses risks to\n    availability if you underestimate the workload's peak capacity\n    needs.\n  \n    In the cloud, you don't have to do this. Instead, you can provision\n    compute, database, and other resource capacity as needed to meet\n    current and forecasted demand. Automated solutions such as Amazon EC2 Auto Scaling and Application Auto Scaling can bring resources\n    online for you based on metrics you specify. This can make the\n    scaling process easier and predictable, and it can make your\n    workload significantly more reliable by ensuring you have enough\n    resources available at all times.\n  \n    Desired outcome: You configure\n    automatic scaling of compute and other resources to meet demand. You\n    provide sufficient headroom in your scaling policies to allow bursts\n    of traffic to be served while additional resources are brought\n    online.\n  \n    Common anti-patterns:\n  \n     \n     \n     \n     \n     \n  \n      \n        You provision a fixed number of scalable resources.\n      \n    \n      \n        You choose a scaling metric that does not correlate to actual\n        demand.\n      \n    \n      \n        You fail to provide enough headroom in your scaling plans to\n        accommodate demand bursts.\n      \n    \n      \n        Your scaling policies add capacity too late, which leads to\n        capacity exhaustion and degraded service while additional\n        resources are brought online.\n      \n    \n      \n        You fail to correctly configure minimum and maximum resource\n        counts, which leads to scaling failures.\n      \n    \n    Benefits of establishing this best\n    practice: Having enough resources to meet current demand\n    is critical to provide high availability of your workload and adhere\n    to your defined service-level objectives (SLOs). Automatic scaling\n    allows you to provide the right amount of compute, database, and\n    other resources your workload needs in order to serve current and\n    forecasted demand. You don't need to determine peak capacity needs\n    and statically allocate resources to serve it. Instead, as demand\n    grows, you can allocate more resources to accommodate it, and after\n    demand falls, you can deactivate resources to reduce cost.\n  \n    Level of risk exposed if this best practice\n    is not established: Medium\n  \n\n  Implementation guidance\n\n      \n    \n      First, determine whether the workload component is suitable for\n      automatic scaling. These components are called\n      horizontally scalable because they provide\n      the same resources and behave identically. Examples of\n      horizontally-scalable components include EC2 instances that are\n      configured alike,\n      Amazon Elastic Container Service (ECS) tasks, and pods running on\n      Amazon Elastic Kubernetes Service (EKS). These compute resources are\n      typically located behind a load balancer and are referred to as\n      replicas.\n    \n    \n      Other replicated resources may include database read replicas,\n      Amazon DynamoDB tables, and\n      Amazon ElastiCache (Redis OSS) clusters. For a complete list of\n      supported resources, see\n      AWS       services that you can use with Application Auto Scaling.\n    \n    \n      For container-based architectures, you may need to scale two\n      different ways. First, you may need to scale the containers that\n      provide horizontally-scalable services. Second, you may need to\n      scale the compute resources to make space for new containers.\n      Different automatic scaling mechanisms exist for each layer. To\n      scale ECS tasks, you can use\n      Application\n      Auto Scaling. To scale Kubernetes pods, you can use\n      Horizontal\n      Pod Autoscaler (HPA) or\n      Kubernetes Event-driven\n      Autoscaling (KEDA). To scale the compute resources, you can\n      use\n      Capacity\n      Providers for ECS, or for Kubernetes, you can use\n      Karpenter or\n      Cluster\n      Autoscaler.\n    \n    \n      Next, select how you will perform automatic scaling. There are\n      three major options: metric-based scaling, scheduled scaling, and\n      predictive scaling.\n    \n    \n      Metric-based scaling\n    \n    \n      Metric-based scaling provisions resources based on the value of\n      one or more scaling metrics. A scaling metric\n      is one that corresponds to your workload's demand. A good way to\n      determine appropriate scaling metrics is to perform load testing\n      in a non-production environment. During your load tests, keep the\n      number of scalable resources fixed, and slowly increase demand\n      (for example, throughput, concurrency, or simulated users). Then\n      look for metrics that increase (or decrease) as demand grows, and\n      conversely decrease (or increase) as demand falls. Typical scaling\n      metrics include CPU utilization, work queue depth (such as an\n      Amazon SQS\n      queue), number of active users, and network throughput.\n    \n    Note\n      AWS has observed that\n      with most applications, memory utilization increases as the\n      application warms up and then reaches a steady value. When demand\n      decreases, memory utilization typically remains elevated rather\n      than decreasing in parallel. Because memory utilization does not\n      correspond to demand in both directions–that is, growing and\n      falling with demand–consider carefully before you select this\n      metric for automatic scaling.\n    \n    \n      Metric-based scaling is a latent operation.\n      It can take several minutes for utilization metrics to propagate\n      to auto scaling mechanisms, and these mechanisms typically wait\n      for a clear signal of increased demand before reacting. Then, as\n      the auto scaler creates new resources, it can take additional time\n      for them to come to full service. Because of this, it is important\n      to not set your scaling metric targets too close to full\n      utilization (for example, 90% CPU utilization). Doing so risks\n      exhausting existing resource capacity before additional capacity\n      can come online. Typical resource utilization targets can range\n      between 50-70% for optimum availability, depending on demand\n      patterns and time required to provision additional resources.\n    \n    \n      Scheduled scaling\n    \n    \n      Scheduled scaling provisions or removes resources based on the\n      calendar or time of day. It is frequently used for workloads that\n      have predictable demand, such as peak utilization during weekday\n      business hours or sales events. Both\n      Amazon EC2 Auto Scaling and\n      Application\n      Auto Scaling support scheduled scaling. KEDA's\n      cron\n      scaler supports scheduled scaling of Kubernetes pods.\n    \n    \n      Predictive scaling\n    \n    \n      Predictive scaling uses machine learning to automatically scale\n      resources based on anticipated demand. Predictive scaling analyzes\n      the historical value of a utilization metric you provide and\n      continuously predicts its future value. The predicted value is\n      then used to scale the resource up or down.\n      Amazon EC2 Auto Scaling can perform predictive scaling.\n    \n     \n\n  Implementation steps\n\n      \n      \n         \n         \n         \n         \n         \n         \n      \n          \n            Determine whether the workload component is suitable for\n            automatic scaling.\n          \n        \n          \n            Determine what kind of scaling mechanism is most appropriate\n            for the workload: metric-based scaling, scheduled scaling,\n            or predictive scaling.\n          \n        \n          \n            Select the appropriate automatic scaling mechanism for the\n            component. For Amazon EC2 instances, use Amazon EC2 Auto Scaling. For other AWS services, use Application Auto\n            Scaling. For Kubernetes pods (such as those running in an\n            Amazon EKS cluster), consider Horizontal Pod Autoscaler\n            (HPA) or Kubernetes Event-driven Autoscaling (KEDA). For\n            Kubernetes or EKS nodes, consider Karpenter and Cluster Auto\n            Scaler (CAS).\n          \n        \n          \n            For metric or scheduled scaling, conduct load testing to\n            determine the appropriate scaling metrics and target values\n            for your workload. For scheduled scaling, determine the\n            number of resources needed at the dates and times you\n            select. Determine the maximum number of resources needed to\n            serve expected peak traffic.\n          \n        \n          \n            Configure the auto scaler based on the information collected\n            above. Consult the auto scaling service's documentation for\n            details. Verify that the maximum and minimum scaling limits\n            are configured correctly.\n          \n        \n          \n            Verify the scaling configuration is working as expected.\n            Perform load testing in a non-production environment and\n            observe how the system reacts, and adjust as needed. When\n            enabling auto scaling in production, configure appropriate\n            alarms to notify you of any unexpected behavior.\n          \n        \n     \n   \n\n  Resources\n\n      \n    \n      Related documents:\n    \n    \n       \n       \n       \n       \n       \n       \n       \n    \n        \n          What\n          Is Amazon EC2 Auto Scaling?\n        \n      \n        \n          AWS           Prescriptive Guidance: Load testing applications\n        \n      \n        \n          AWS Marketplace: products that can be used with auto\n          scaling\n        \n      \n        \n          Managing\n          Throughput Capacity Automatically with DynamoDB Auto\n          Scaling\n        \n      \n        \n          Predictive\n          Scaling for EC2, Powered by Machine Learning\n        \n      \n        \n          Scheduled\n          Scaling for Amazon EC2 Auto Scaling\n        \n      \n        \n          Telling\n          Stories About Little's Law\n        \n      \n  \u003cdiv\u003e\u003cdiv\u003e\u003cdiv\u003e\u003cdiv id=\"js_error_message\"\u003e\u003cp\u003e\u003cimg src=\"https://d1ge0kk1l5kms0.cloudfront.net/images/G/01/webservices/console/warning.png\" alt=\"Warning\" /\u003e \u003cstrong\u003eJavascript is disabled or is unavailable in your browser.\u003c/strong\u003e\u003c/p\u003e\u003cp\u003eTo use the Amazon Web Services Documentation, Javascript must be enabled. Please refer to your browser's Help pages for instructions.\u003c/p\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003eDocument ConventionsREL07-BP02 Obtain resources upon detection of impairment to a\n  workloadREL07-BP04 Load test your workload",
  "REL07-BP04 Load test your workload\n    Adopt a load testing methodology to measure if scaling activity\n    meets workload requirements.\n  \n    It’s important to perform sustained load testing. Load tests should\n    discover the breaking point and test the performance of your\n    workload. AWS makes it easy to set up temporary testing environments\n    that model the scale of your production workload. In the cloud, you\n    can create a production-scale test environment on demand, complete\n    your testing, and then decommission the resources. Because you only\n    pay for the test environment when it's running, you can simulate\n    your live environment for a fraction of the cost of testing on\n    premises.\n  \n    Load testing in production should also be considered as part of game\n    days where the production system is stressed, during hours of lower\n    customer usage, with all personnel on hand to interpret results and\n    address any problems that arise.\n  \n    Common anti-patterns:\n  \n     \n     \n     \n     \n  \n      \n        Performing load testing on deployments that are not the same\n        configuration as your production.\n      \n    \n      \n        Performing load testing only on individual pieces of your\n        workload, and not on the entire workload.\n      \n    \n      \n        Performing load testing with a subset of requests and not a\n        representative set of actual requests.\n      \n    \n      \n        Performing load testing to a small safety factor above expected\n        load.\n      \n    \n    Benefits of establishing this best\n    practice: You know what components in your architecture\n    fail under load and be able to identify what metrics to watch to\n    indicate that you are approaching that load in time to address the\n    problem, preventing the impact of that failure.\n  \n    Level of risk exposed if this best practice\n    is not established: Medium\n  \n\n  Implementation guidance\n\n      \n    \n    \n       \n    \n        Perform load testing to identify which aspect of your workload\n        indicates that you must add or remove capacity. Load testing\n        should have representative traffic similar to what you receive in\n        production. Increase the load while watching the metrics you have\n        instrumented to determine which metric indicates when you must add\n        or remove resources.\n      \n        \n           \n        \n            \n              Distributed\n                Load Testing on AWS: simulate thousands of connected users\n            \n            \n               \n               \n               \n               \n            \n                 Identify the mix of requests. You may have varied mixes of requests, so you\n                  should look at various time frames when identifying the mix of traffic. \n              \n                 Implement a load driver. You can use custom code, open source, or commercial\n                  software to implement a load driver. \n              \n                 Load test initially using small capacity. You see some immediate effects by\n                  driving load onto a lesser capacity, possibly as small as one instance or\n                  container. \n              \n                 Load test against larger capacity. The effects will be different on a\n                  distributed load, so you must test against as close to a product environment as\n                  possible. \n              \n          \n    \n   \n\n  Resources\n    \n      Related documents:\n    \n    \n       \n       \n    \n        \n          Distributed\n          Load Testing on AWS: simulate thousands of connected\n          users\n        \n      \n        \n          Load testing applications\n        \n      \n    \n      Related videos:\n    \n    \n       \n    \n        \n          AWS Summit ANZ 2023: Accelerate with confidence through AWS Distributed Load Testing\n        \n      \n  \u003cdiv\u003e\u003cdiv\u003e\u003cdiv\u003e\u003cdiv id=\"js_error_message\"\u003e\u003cp\u003e\u003cimg src=\"https://d1ge0kk1l5kms0.cloudfront.net/images/G/01/webservices/console/warning.png\" alt=\"Warning\" /\u003e \u003cstrong\u003eJavascript is disabled or is unavailable in your browser.\u003c/strong\u003e\u003c/p\u003e\u003cp\u003eTo use the Amazon Web Services Documentation, Javascript must be enabled. Please refer to your browser's Help pages for instructions.\u003c/p\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003eDocument ConventionsREL07-BP03 Obtain resources upon detection that more resources\n  are needed for a workloadREL 8. How do you implement change?",
  "REL08-BP01 Use runbooks for standard activities such as\n  deployment\n    Runbooks are the predefined procedures to achieve specific outcomes.\n    Use runbooks to perform standard activities, whether done manually\n    or automatically. Examples include deploying a workload, patching a\n    workload, or making DNS modifications.\n  \n    For example, put processes in place\n    to ensure\n    rollback safety during deployments. Ensuring that you can\n    roll back a deployment without any disruption for your customers is\n    critical in making a service reliable.\n  \n    For runbook procedures, start with a valid effective manual process,\n    implement it in code, and invoke it to automatically run where\n    appropriate.\n  \n    Even for sophisticated workloads that are highly automated, runbooks\n    are still useful\n    for running\n    game days or meeting rigorous reporting and auditing\n    requirements.\n  \n    Note that playbooks are used in response to specific incidents, and\n    runbooks are used to achieve specific outcomes. Often, runbooks are\n    for routine activities, while playbooks are used for responding to\n    non-routine events.\n  \n    Common anti-patterns:\n  \n     \n     \n     \n  \n      \n        Performing unplanned changes to configuration in production.\n      \n    \n      \n        Skipping steps in your plan to deploy faster, resulting in a\n        failed deployment.\n      \n    \n      \n        Making changes without testing the reversal of the change.\n      \n    \n    Benefits of establishing this best\n    practice: Effective change planning increases your\n    ability to successfully run the change because you are aware of\n    all the systems impacted. Validating your change in test\n    environments increases your confidence.\n  \n    Level of risk exposed if this best practice\n    is not established: High\n  \n\n  Implementation guidance\n\n   \n   \n \n    Provide consistent and prompt responses to well-understood events\n    by documenting procedures in runbooks.\n  \n        \n           \n        \n            \n              AWS\n                Well-Architected Framework: Concepts: Runbook\n            \n            \n    Use the principle of infrastructure as code to define your\n    infrastructure. By using AWS CloudFormation (or a trusted third\n    party) to define your infrastructure, you can use version control\n    software to version and track changes.\n  \n        \n           \n           \n        \n             Use AWS CloudFormation (or a trusted third-party provider) to define your infrastructure. \n            \n               \n            \n                \n                  What is AWS CloudFormation?\n                \n              \n          \n             Create templates that are singular and decoupled, using good software design\n              principles. \n            \n               \n               \n            \n                 Determine the permissions, templates, and responsible parties for\n                  implementation. \n                \n                   \n                \n                     Controlling access with AWS Identity and Access Management\n                  \n              \n                Use a hosted source code management system based on a popular technology such as Git to store your source code and infrastructure as code (IaC) configuration.\n              \n          \n\n   \n\n  Resources\n\n      \n    \n      Related documents:\n    \n    \n       \n       \n       \n       \n    \n        \n          APN\n          Partner: partners that can help you create automated\n          deployment solutions\n        \n      \n        \n          AWS Marketplace: products that can be used to automate your\n          deployments\n        \n      \n        \n          AWS Well-Architected Framework: Concepts: Runbook\n        \n      \n        \n          What\n          is AWS CloudFormation?\n        \n      \n        \n          Related examples:\n        \n    \n         \n    \n          Automating\n          operations with Playbooks and Runbooks\n        \n  \u003cdiv\u003e\u003cdiv\u003e\u003cdiv\u003e\u003cdiv id=\"js_error_message\"\u003e\u003cp\u003e\u003cimg src=\"https://d1ge0kk1l5kms0.cloudfront.net/images/G/01/webservices/console/warning.png\" alt=\"Warning\" /\u003e \u003cstrong\u003eJavascript is disabled or is unavailable in your browser.\u003c/strong\u003e\u003c/p\u003e\u003cp\u003eTo use the Amazon Web Services Documentation, Javascript must be enabled. Please refer to your browser's Help pages for instructions.\u003c/p\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003eDocument ConventionsREL 8. How do you implement change?REL08-BP02 Integrate functional testing as part of your\n  deployment",
  "REL08-BP02 Integrate functional testing as part of your\n  deployment\n    Use techniques such as unit tests and integration tests that\n    validate required functionality.\n  \n    Unit testing is the process where you test the smallest functional\n    unit of code to validate its behavior. Integration testing seeks to\n    validate that each application feature works according to the\n    software requirements. While unit tests focus on testing part of an\n    application in isolation, integration tests consider side effects\n    (for example, the effect of data being changed through a mutation\n    operation). In either case, tests should be integrated into a\n    deployment pipeline, and if success criteria are not met, the\n    pipeline is halted or rolled back. These tests are run in a\n    pre-production environment, which is staged prior to production in\n    the pipeline.\n  \n    You achieve the best outcomes when these tests are run automatically\n    as part of build and deployment actions. For instance, with AWS CodePipeline, developers commit changes to a source repository where\n    CodePipeline automatically detects the changes. The application is\n    built, and unit tests are run. After the unit tests have passed, the\n    built code is deployed to staging servers for testing. From the\n    staging server, CodePipeline runs more tests, such as integration or\n    load tests. Upon the successful completion of those tests,\n    CodePipeline deploys the tested and approved code to production\n    instances.\n  \n    Desired outcome: You use\n    automation to perform unit and integration tests to validate that\n    your code behaves as expected. These tests are integrated into the\n    deployment process, and a test failure aborts the deployment.\n  \n    Common anti-patterns:\n  \n     \n     \n     \n     \n     \n  \n      \n        You ignore or bypass test failures and plans during the\n        deployment process in order to accelerate the deployment\n        timeline.\n      \n    \n      \n        You manually perform tests outside the deployment pipeline.\n      \n    \n      \n        You skip testing steps in the automation through manual\n        emergency workflows.\n      \n    \n      \n        You run automated tests in an environment that does not closely\n        resemble the production environment.\n      \n    \n      \n        You build a test suite that is insufficiently flexible and is\n        difficult to maintain, update, or scale as the application\n        evolves.\n      \n    \n    Benefits of establishing this best\n      practice: Automated testing during the deployment process\n    catches issues early, which reduces the risk of a release to\n    production with bugs or unexpected behavior. Unit tests validate the\n    code behaves as desired and API contracts are honored. Integration\n    tests validate that the system operates according to specified\n    requirements. These types of tests verify the intended working order\n    of components such as user interfaces, APIs, databases, and source\n    code.\n  \n    Level of risk exposed if this best practice\n      is not established: High\n  \n    \n    Implementation guidance\n    \n    \n    \n      Adopt a test-driven development (TDD) approach to writing\n      software, where you develop test cases to specify and validate\n      your code. To start, create test cases for each function. If the\n      test fails, you write new code to pass the test. This approach\n      helps you validate the expected result of each function. Run unit\n      tests and validate that they pass before you commit code to a\n      source code repository.\n    \n    \n      Implement both unit and integration tests as part of the build,\n      test, and deployment stages of the CI/CD pipeline. Automate\n      testing, and automatically initiate tests whenever a new version\n      of the application is ready to be deployed. If success criteria\n      are not met, the pipeline is halted or rolled back.\n    \n    \n      If the application is a web or mobile app, perform automated\n      integration testing on multiple desktop browsers or real devices.\n      This approach is particularly useful to validate the compatibility\n      and functionality of mobile apps across a diverse range of\n      devices.\n    \n     \n      \n      Implementation steps\n      \n      \n      \n         \n         \n         \n         \n         \n         \n         \n      \n          \n            Write unit tests before you write functional code\n            (test-driven development, or TDD).\n            Establish code guidelines so that writing and running unit\n            tests are a non-functional coding requirement.\n          \n        \n          \n            Create a suite of automated integration tests that cover the\n            identified testable functionalities. These tests should\n            simulate user interactions and validate the expected\n            outcomes.\n          \n        \n          \n            Create the necessary test environment to run the integration\n            tests. This may include staging or pre-production\n            environments that closely mimic the production environment.\n          \n        \n          \n            Set up your source, build, test, and deploy stages using the\n            AWS CodePipeline console or AWS Command Line Interface\n            (CLI).\n          \n        \n          \n            Deploy the application once the code has been built and\n            tested. AWS CodeDeploy can deploy it to your staging\n            (testing) and production environments. These environments\n            may include Amazon EC2 instances, AWS Lambda functions, or\n            on-premises servers. The same deployment mechanism should be\n            used to deploy the application to all environments.\n          \n        \n          \n            Monitor the progress of your pipeline and the status of each\n            stage. Use quality checks to block the pipeline based on the\n            status of your tests. You can also receive notifications for\n            any pipeline stage failure or pipeline completion.\n          \n        \n          \n            Continually monitor the results of the tests, and look for\n            patterns, regressions or areas that require more attention.\n            Use this information to improve the test suite, identify\n            areas of the application that need more robust testing, and\n            optimize the deployment process.\n          \n        \n     \n   \n    \n    Resources\n    \n    \n    \n      Related best practices:\n    \n    \n       \n       \n       \n    \n        \n          REL07-BP04\n            Load test your workload\n        \n      \n        \n          REL08-BP03\n            Integrate resiliency testing as part of your deployment\n        \n      \n        \n          REL12-BP04\n            Test resiliency using chaos engineering\n        \n      \n    \n      Related documents:\n    \n    \n       \n       \n       \n       \n       \n       \n    \n        \n          AWS           Prescriptive Guidance: Test automation\n        \n      \n        \n          Continuous\n            Delivery and Continuous Integration\n        \n      \n        \n          Indicators\n            for functional testing\n        \n      \n        \n          Monitoring\n            pipelines\n        \n      \n        \n          Use AWS CodePipeline with AWS CodeBuild to test code and run builds\n        \n      \n        \n          AWS Device Farm\n        \n      \n  \u003cdiv\u003e\u003cdiv\u003e\u003cdiv\u003e\u003cdiv id=\"js_error_message\"\u003e\u003cp\u003e\u003cimg src=\"https://d1ge0kk1l5kms0.cloudfront.net/images/G/01/webservices/console/warning.png\" alt=\"Warning\" /\u003e \u003cstrong\u003eJavascript is disabled or is unavailable in your browser.\u003c/strong\u003e\u003c/p\u003e\u003cp\u003eTo use the Amazon Web Services Documentation, Javascript must be enabled. Please refer to your browser's Help pages for instructions.\u003c/p\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003eDocument ConventionsREL08-BP01 Use runbooks for standard activities such as\n  deploymentREL08-BP03 Integrate resiliency testing as part of your\n  deployment",
  "REL08-BP03 Integrate resiliency testing as part of your\n  deployment\n    Integrate resiliency testing by consciously introducing failures in\n    your system to measure its capability in case of disruptive\n    scenarios. Resilience tests are different from unit and function\n    tests that are usually integrated in deployment cycles, as they\n    focus on the identification of unanticipated failures in your\n    system. While it is safe to start with resiliency testing\n    integration in pre-production, set a goal to implement these tests\n    in production as a part of your\n    game\n      days.\n  \n    Desired outcome: Resiliency\n    testing helps build confidence in the system's ability to withstand\n    degradation in production. Experiments identify weak points that\n    could lead to failure, which helps you improve your system to\n    automatically and efficiently mitigate failure and degradation.\n  \n    Common anti-patterns:\n  \n     \n     \n     \n     \n     \n     \n  \n      \n        Lack of observability and monitoring in deployment processes\n      \n    \n      \n        Reliance on humans to resolve system failures\n      \n    \n      \n        Poor quality analysis mechanisms\n      \n    \n      \n        Focus on known issues in a system and a lack of experimentation\n        to identify any unknowns\n      \n    \n      \n        Identification of failures, but no resolution\n      \n    \n      \n        No documentation of findings and runbooks\n      \n    \n    Benefits of establishing best\n      practices: Resilience testing integrated in your\n    deployments helps to identify unknown issues in the system that\n    otherwise go unnoticed, which can lead to downtime in production.\n    Identification of these unknowns in a system helps you document\n    findings, integrate testing into your CI/CD process, and build\n    runbooks, which simplify mitigation through efficient, repeatable\n    mechanisms.\n  \n    Level of risk exposed if this best practice\n      is not established: Medium\n  \n    \n    Implementation guidance\n    \n    \n    \n      The most common resiliency testing forms that can be integrated in\n      your system's deployments are disaster recovery and chaos\n      engineering.\n    \n    \n       \n       \n       \n       \n       \n    \n        \n          Include updates to your disaster recovery plans and standard\n          operating procedures (SOPs) with any significant deployment.\n        \n      \n        \n          Integrate reliability testing into your automated deployment\n          pipelines. Services such\n          asAWS Resilience Hubcan be\n          integrated\n            into your CI/CD pipeline to establish continuous\n          resilience assessments that are automatically evaluated as\n          part of every deployment.\n        \n      \n        \n          Define your applications in AWS Resilience Hub. Resilience\n          assessments generate code snippets that help you create\n          recovery procedures as AWS Systems Manager documents for your\n          applications and provide a list of recommended Amazon CloudWatch monitors and alarms.\n        \n      \n        \n          Once your DR plans and SOPs are updated, complete disaster\n          recovery testing to verify that they are effective. Disaster\n          recovery testing helps you determine if you can restore your\n          system after an event and return to normal operations. You can\n          simulate various disaster recovery strategies and identify\n          whether your planning is sufficient to meet your uptime\n          requirements. Common disaster recovery strategies include\n          backup and restore, pilot light, cold standby, warm standby,\n          hot standby, and active-active, and they all differ in cost\n          and complexity. Before disaster recovery testing, we recommend\n          that you define your recovery time objective (RTO) and\n          recovery point objective (RPO) to simplify the choice of\n          strategy to simulate. AWS offers disaster recovery tools like\n          AWS Elastic Disaster Recovery to help you get started with\n          your planning and testing.\n        \n      \n        \n          Chaos engineering experiments introduce disruptions to the\n          system, such as network outages and service failures. By\n          simulating with controlled failures, you can discover your\n          system's vulnerabilities while containing the impacts of the\n          injected failures. Just like the other strategies, run\n          controlled failure simulations in non-production environments\n          using services like\n          AWS Fault Injection Service to gain confidence before deploying\n          in production.\n        \n      \n   \n    \n    Resources\n    \n    \n    \n      Related documents:\n    \n    \n       \n       \n       \n       \n       \n       \n    \n        \n          Experiment\n            with failure using resilience testing to build recovery\n            preparedness\n        \n      \n        \n          Continually\n            assessing application resilience with AWS Resilience Hub and\n            AWS CodePipeline\n        \n      \n        \n          Disaster\n            recovery (DR) architecture on AWS, part 1: Strategies for\n            recovery in the cloud\n        \n      \n        \n          Verify\n            the resilience of your workloads using Chaos\n            Engineering\n        \n      \n        \n          Principles\n            of Chaos Engineering\n        \n      \n        \n          Chaos\n            Engineering Workshop\n        \n      \n    \n      Related videos:\n    \n    \n       \n       \n       \n    \n        \n          AWS           re:Invent 2020: Testing Resilience using Chaos\n            Engineering\n        \n      \n        \n          Improve\n            Application Resilience with AWS Fault Injection Service\n        \n      \n        \n          Prepare\n            \u0026 Protect Your Applications From Disruption With AWS Resilience Hub\n        \n      \n  \u003cdiv\u003e\u003cdiv\u003e\u003cdiv\u003e\u003cdiv id=\"js_error_message\"\u003e\u003cp\u003e\u003cimg src=\"https://d1ge0kk1l5kms0.cloudfront.net/images/G/01/webservices/console/warning.png\" alt=\"Warning\" /\u003e \u003cstrong\u003eJavascript is disabled or is unavailable in your browser.\u003c/strong\u003e\u003c/p\u003e\u003cp\u003eTo use the Amazon Web Services Documentation, Javascript must be enabled. Please refer to your browser's Help pages for instructions.\u003c/p\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003eDocument ConventionsREL08-BP02 Integrate functional testing as part of your\n  deploymentREL08-BP04 Deploy using immutable infrastructure",
  "REL08-BP04 Deploy using immutable infrastructure\n    Immutable infrastructure is a model that mandates that no updates,\n    security patches, or configuration changes happen in-place on\n    production workloads. When a change is needed, the architecture is\n    built onto new infrastructure and deployed into production.\n  \n    Follow an immutable infrastructure deployment strategy to increase the reliability, consistency, and reproducibility in your workload deployments.\n  \n    Desired outcome: With immutable infrastructure, no in-place modifications are allowed to run infrastructure resources within a workload. Instead, when a change is needed, a new set of updated infrastructure resources containing all the necessary changes are deployed in parallel to your existing resources. This deployment is validated automatically, and if successful, traffic is gradually shifted to the new set of resources.\n  \n    This deployment strategy applies to software updates, security patches, infrastructure changes, configuration updates, and application updates, among others.\n  \n    Common anti-patterns:\n  \n     \n  \n      \n        Implementing in-place changes to running infrastructure resources.\n      \n    \n    Benefits of establishing this best practice:\n  \n     \n     \n     \n     \n     \n     \n  \n      \n        Increased consistency across environments: Since there are no differences in infrastructure resources across environments, consistency is increased and testing is simplified.\n      \n    \n      \n        Reduction in configuration drifts: By replacing infrastructure resources with a known and version-controlled configuration, the infrastructure is set to a known, tested, and trusted state, avoiding configuration drifts.\n      \n    \n      \n        Reliable atomic deployments: Deployments either complete successfully or nothing changes, increasing consistency and reliability in the deployment process. \n      \n    \n      \n        Simplified deployments: Deployments are simplified because they don't need to support upgrades. Upgrades are just new deployments. \n      \n    \n      \n        Safer deployments with fast rollback and recovery processes: Deployments are safer because the previous working version is not changed. You can roll back to it if errors are detected. \n      \n    \n      \n        Enhanced security posture: By not allowing changes to infrastructure, remote access mechanisms (such as SSH) can be disabled. This reduces the attack vector, improving your organization's security posture.\n      \n    \n    Level of risk exposed if this best practice\n      is not established: Medium\n  \n\n  Implementation guidance\n    \n      Automation\n    \n    \n      When defining an immutable infrastructure deployment strategy, it is recommended to use automation as much as possible to increase reproducibility and minimize the potential of human error. For more detail, see REL08-BP05 Deploy changes with automation and Automating safe, hands-off deployments.\n    \n    \n      With infrastructure as code (IaC), infrastructure provisioning, orchestration, and deployment steps are defined in a programmatic, descriptive, and declarative way and stored in a source control system. Leveraging infrastructure as code makes it simpler to automate infrastructure deployment and helps achieve infrastructure immutability. \n    \n    \n      Deployment patterns\n    \n    \n      When a change in the workload is required, the immutable infrastructure deployment strategy mandates that a new set of infrastructure resources is deployed, including all necessary changes. It is important for this new set of resources to follow a rollout pattern that minimizes user impact. There are two main strategies for this deployment:\n    \n    \n      Canary\n        deployment: The practice of directing a small\n      number of your customers to the new version, usually running on a\n      single service instance (the canary). You then deeply scrutinize any\n      behavior changes or errors that are generated. You can remove\n      traffic from the canary if you encounter critical problems and send\n      the users back to the previous version. If the deployment is\n      successful, you can continue to deploy at your desired velocity,\n      while monitoring the changes for errors, until you are fully\n      deployed. AWS CodeDeploy can be configured with a deployment\n      configuration that allows a canary deployment.\n    \n    \n      Blue/green\n        deployment: Similar to the canary deployment,\n      except that a full fleet of the application is deployed in parallel.\n      You alternate your deployments across the two stacks (blue and\n      green). Once again, you can send traffic to the new version, and\n      fall back to the old version if you see problems with the\n      deployment. Commonly all traffic is switched at once, however you\n      can also use fractions of your traffic to each version to dial up\n      the adoption of the new version using the weighted DNS\n      routing capabilities of Amazon Route 53. AWS CodeDeploy and AWS Elastic Beanstalk can be configured with a deployment configuration\n      that allows a blue/green deployment.\n     \n    \n       \n        \n       \n       \n      Figure 8: Blue/green deployment with AWS Elastic Beanstalk\n        and Amazon Route 53\n      \n    \n    \n      Drift detection\n    \n    \n      Drift is defined as any change that causes an infrastructure resource to have a different state or configuration to what is expected. Any type of unmanaged configuration change goes against the notion of immutable infrastructure, and should be detected and remediated in order to have a successful implementation of immutable infrastructure.\n    \n     \n      \n      Implementation steps\n      \n         \n         \n         \n         \n      \n          \n            Disallow the in-place modification of running infrastructure resources.\n          \n          \n             \n          \n              \n                You can use AWS Identity and Access Management (IAM) to specify who or what can access services and resources in AWS, centrally manage fine-grained permissions, and analyze access to refine permissions across AWS.\n              \n            \n        \n          \n            Automate the deployment of infrastructure resources to increase reproducibility and minimize the potential of human error.\n          \n          \n             \n             \n             \n             \n          \n              \n                As described in the Introduction to DevOps on AWS whitepaper, automation is a cornerstone with AWS services and is internally supported in all services, features, and offerings.\n              \n            \n              \n                Prebaking your Amazon Machine Image (AMI) can speed up the time to launch them. EC2 Image Builder is a fully managed AWS service that helps you automate the creation, maintenance, validation, sharing, and deployment of customized, secure, and up-to-date Linux or Windows custom AMI.\n              \n            \n              \n                Some of the services that support automation are:\n              \n              \n                 \n                 \n              \n                  \n                    AWS Elastic Beanstalk is a service to rapidly deploy and scale web applications developed with Java, .NET, PHP, Node.js, Python, Ruby, Go, and Docker on familiar servers such as Apache, NGINX, Passenger, and IIS.\n                  \n                \n                  \n                    AWS Proton helps platform teams connect and coordinate all the different tools your development teams need for infrastructure provisioning, code deployments, monitoring, and updates. AWS Proton enables automated infrastructure as code provisioning and deployment of serverless and container-based applications.\n                  \n                \n            \n              \n                Leveraging infrastructure as code makes it easy to automate infrastructure deployment, and helps achieve infrastructure immutability. AWS provides services that enable the creation, deployment, and maintenance of infrastructure in a programmatic, descriptive, and declarative way. \n              \n              \n                 \n                 \n                 \n                 \n              \n                  \n                    AWS CloudFormation helps developers create AWS resources in an orderly and predictable fashion. Resources are written in text files using JSON or YAML format. The templates require a specific syntax and structure that depends on the types of resources being created and managed. You author your resources in JSON or YAML with any code editor, check it into a version control system, and then AWS CloudFormation builds the specified services in safe, repeatable manner.\n                  \n                \n                  \n                    AWS Serverless Application Model (AWS SAM) is an open-source framework that you can use to build serverless applications on AWS. AWS SAM integrates with other AWS services, and is an extension of AWS CloudFormation.\n                  \n                \n                  \n                    AWS Cloud Development Kit (AWS CDK) is an open-source software development framework to model and provision your cloud application resources using familiar programming languages. You can use AWS CDK to model application infrastructure using TypeScript, Python, Java, and .NET. AWS CDK uses AWS CloudFormation in the background to provision resources in a safe, repeatable manner.\n                  \n                \n                  \n                    AWS Cloud Control API introduces a common set of Create, Read, Update, Delete, and List (CRUDL) APIs to help developers manage their cloud infrastructure in an easy and consistent way. The Cloud Control API common APIs allow developers to uniformly manage the lifecycle of AWS and third-party services.\n                  \n                \n            \n        \n          \n            Implement deployment patterns that minimize user impact.\n          \n          \n             \n             \n          \n              \n                Canary deployments:\n              \n              \n                 \n                 \n              \n                  \n                    Set up an API Gateway canary release deployment\n                  \n                \n                  \n                    Create a pipeline with canary deployments for Amazon ECS using AWS App Mesh\n                \n            \n              \n                Blue/green deployments: the Blue/Green Deployments on AWS whitepaper describes example techniques to implement blue/green deployment strategies.\n              \n            \n        \n          \n            Detect configuration or state drifts. For more detail, see Detecting unmanaged configuration changes to stacks and resources.\n          \n        \n     \n   \n\n  Resources\n\n    \n      Related best practices:\n    \n    \n       \n    \n        \n          REL08-BP05 Deploy changes with automation\n        \n      \n    \n      Related documents:\n    \n    \n       \n       \n       \n       \n    \n        \n          Automating safe, hands-off deployments\n        \n      \n        \n          Leveraging AWS CloudFormation to create an immutable infrastructure at Nubank\n        \n      \n        \n          Infrastructure as code\n        \n      \n        \n          Implementing an alarm to automatically detect drift in AWS CloudFormation stacks\n        \n      \n    \n      Related videos:\n    \n    \n       \n    \n        AWS re:Invent 2020: Reliability, consistency, and confidence through immutability \n        \n      \n  \u003cdiv\u003e\u003cdiv\u003e\u003cdiv\u003e\u003cdiv id=\"js_error_message\"\u003e\u003cp\u003e\u003cimg src=\"https://d1ge0kk1l5kms0.cloudfront.net/images/G/01/webservices/console/warning.png\" alt=\"Warning\" /\u003e \u003cstrong\u003eJavascript is disabled or is unavailable in your browser.\u003c/strong\u003e\u003c/p\u003e\u003cp\u003eTo use the Amazon Web Services Documentation, Javascript must be enabled. Please refer to your browser's Help pages for instructions.\u003c/p\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003eDocument ConventionsREL08-BP03 Integrate resiliency testing as part of your\n  deploymentREL08-BP05 Deploy changes with automation",
  "REL08-BP05 Deploy changes with automation\n    Deployments and patching are automated to eliminate negative impact.\n  \n    Making changes to production systems is one of the largest risk\n    areas for many organizations. We consider deployments a first-class\n    problem to be solved alongside the business problems that the\n    software addresses. Today, this means the use of automation wherever\n    practical in operations, including testing and deploying changes,\n    adding or removing capacity, and migrating data.\n  \n    Desired outcome: You build\n    automated deployment safety into the release process with extensive\n    pre-production testing, automatic rollbacks, and staggered\n    production deployments. This automation minimizes the potential\n    impact on production caused by failed deployments, and developers no\n    longer need to actively watch deployments to production.\n  \n    Common anti-patterns:\n  \n     \n     \n     \n     \n  \n      \n        You perform manual changes.\n      \n    \n      \n        You skip steps in your automation through manual emergency\n        workflows.\n      \n    \n      \n        You don't follow your established plans and processes in favor\n        of accelerated timelines.\n      \n    \n      \n        You perform rapid follow-on deployments without allowing for\n        bake time.\n      \n    \n    Benefits of establishing this best\n      practice: When you use automation to deploy all changes,\n    you remove the potential for introduction of human error and provide\n    the ability to test before you change production. Performing this\n    process prior to production push verifies that your plans are\n    complete. Additionally, automatic rollback into your release process\n    can identify production issues and return your workload to its\n    previously-working operational state.\n  \n    Level of risk exposed if this best practice\n      is not established: Medium\n  \n    \n    Implementation guidance\n    \n    \n    \n      Automate your deployment pipeline. Deployment pipelines allow you\n      to invoke automated testing and detection of anomalies, and either\n      halt the pipeline at a certain step before production deployment,\n      or automatically roll back a change. An integral part of this is\n      the adoption of the culture\n      of continuous\n        integration and continuous delivery/deployment (CI/CD),\n      where a commit or code change passes through various automated\n      stage gates from build and test stages to deployment on production\n      environments.\n    \n    \n      Although conventional wisdom suggests that you keep people in the\n      loop for the most difficult operational procedures, we suggest\n      that you automate the most difficult procedures for that very\n      reason.\n    \n     \n      \n       Implementation steps\n      \n      \n      \n        You can automate deployments to remove manual operations by\n        following these steps:\n      \n      \n         \n         \n         \n         \n      \n          \n            Set up a code repository to store\n              your code securely: Use a hosted source code management system based on a popular technology such as Git to store your source code and infrastructure as code (IaC) configuration.\n          \n        \n          \n            Configure a continuous integration\n              service to compile your source code, run tests, and create\n              deployment artifacts: To set up a build project\n            for this purpose, see\n            Getting\n              started with AWS CodeBuild using the console.\n          \n        \n          \n            Set up a deployment service that\n              automates application deployments and handles the complexity\n              of application updates without reliance on error-prone\n              manual deployments:\n            AWS CodeDeploy automates software deployments to a\n            variety of compute services, such as Amazon EC2,\n            AWS Fargate,\n            AWS Lambda, and your on-premise servers. To configure\n            these steps, see\n            Getting\n              started with CodeDeploy.\n          \n        \n          \n            Set up a continuous delivery service\n              that automates your release pipelines for quicker and more\n              reliable application and infrastructure updates:\n            Consider using\n            AWS CodePipeline to help you automate your release\n            pipelines. For more detail, see\n            CodePipeline\n              tutorials.\n          \n        \n     \n   \n    \n    Resources\n    \n    \n    \n      Related best practices:\n    \n    \n       \n       \n       \n       \n    \n        \n          OPS05-BP04\n            Use build and deployment management systems\n        \n      \n        \n          OPS05-BP10\n            Fully automate integration and deployment\n        \n      \n        \n          OPS06-BP02\n            Test deployments\n        \n      \n        \n          OPS06-BP04\n            Automate testing and rollback\n        \n      \n    \n      Related documents:\n    \n    \n       \n       \n       \n       \n       \n       \n       \n       \n       \n       \n       \n    \n        \n          Continuous\n            Delivery of Nested AWS CloudFormation Stacks Using AWS CodePipeline\n        \n      \n        \n          APN\n            Partner: partners that can help you create automated\n            deployment solutions\n        \n      \n        \n          AWS Marketplace: products that can be used to automate your\n            deployments\n        \n      \n        \n          Automate\n            chat messages with webhooks.\n        \n      \n        \n          The\n            Amazon Builders' Library: Ensuring rollback safety during\n            deployments\n        \n      \n        \n          The\n            Amazon Builders' Library: Going faster with continuous\n            delivery\n        \n      \n        \n          What\n            Is AWS CodePipeline?\n        \n      \n        \n          What\n            Is CodeDeploy?\n        \n      \n        \n          AWS Systems Manager Patch Manager\n        \n      \n        \n          What\n            is Amazon SES?\n        \n      \n        \n          What\n            is Amazon Simple Notification Service?\n        \n      \n    \n      Related videos:\n    \n    \n       \n    \n        \n          AWS Summit\n            2019: CI/CD on AWS\n        \n      \n  \u003cdiv\u003e\u003cdiv\u003e\u003cdiv\u003e\u003cdiv id=\"js_error_message\"\u003e\u003cp\u003e\u003cimg src=\"https://d1ge0kk1l5kms0.cloudfront.net/images/G/01/webservices/console/warning.png\" alt=\"Warning\" /\u003e \u003cstrong\u003eJavascript is disabled or is unavailable in your browser.\u003c/strong\u003e\u003c/p\u003e\u003cp\u003eTo use the Amazon Web Services Documentation, Javascript must be enabled. Please refer to your browser's Help pages for instructions.\u003c/p\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003eDocument ConventionsREL08-BP04 Deploy using immutable infrastructureFailure management",
  "REL09-BP01 Identify and back up all data that needs to be\n  backed up, or reproduce the data from sourcesUnderstand and use the backup capabilities of the data services and resources used by the workload. Most services provide capabilities to back up workload data. \n    Desired outcome: Data sources have been identified and classified based on\n    criticality. Then, establish a strategy for data recovery based on\n    the RPO. This strategy involves either backing up these data\n    sources, or having the ability to reproduce data from other sources.\n    In the case of data loss, the strategy implemented allows recovery\n    or the reproduction of data within the defined RPO and RTO.\n  \n    Cloud maturity phase:\n    Foundational\n  \n    Common anti-patterns:\n  \n     \n     \n     \n     \n     \n  \n      \n        Not aware of all data sources for the workload and their\n        criticality.\n      \n    \n      \n        Not taking backups of critical data sources.\n      \n    \n      \n        Taking backups of only some data sources without using\n        criticality as a criterion.\n      \n    \n      \n        No defined RPO, or backup frequency cannot meet RPO.\n      \n    \n      \n        Not evaluating if a backup is necessary or if data can be\n        reproduced from other sources.\n      \n    \n    Benefits of establishing this best\n    practice: Identifying the places where backups are\n    necessary and implementing a mechanism to create backups, or being\n    able to reproduce the data from an external source improves the\n    ability to restore and recover data during an outage.\n  \n    Level of risk exposed if this best practice\n    is not established: High\n  \n\n  Implementation guidance\n    \n      All AWS data stores offer backup capabilities. Services such as Amazon RDS and Amazon DynamoDB additionally support automated backup that allows point-in-time recovery (PITR), which allows you to restore a backup to any time up to five minutes or less before the current time. Many AWS services offer the ability to copy backups to another AWS Region. AWS Backup is a tool that gives you the ability to centralize and automate data protection across AWS services. AWS Elastic Disaster Recovery allows you to copy full server workloads and maintain continuous data protection from on-premise, cross-AZ or cross-Region, with a Recovery Point Objective (RPO) measured in seconds.\n    \n    \n      Amazon S3 can be used as a backup destination for self-managed and AWS-managed data sources. AWS services such as Amazon EBS, Amazon RDS, and Amazon DynamoDB have built in capabilities to create backups. Third-party backup software can also be used.\n    \n    \n      On-premises data can be backed up to the AWS Cloud using AWS Storage Gateway or AWS DataSync. Amazon S3 buckets can be used to store this data on AWS. Amazon S3 offers multiple storage tiers such as Amazon S3 Glacier or S3 Glacier Deep Archive to reduce cost of data storage.\n    \n    \n      You might be able to meet data recovery needs by reproducing the data from other sources. For example, Amazon ElastiCache replica nodes or Amazon RDS read replicas could be used to reproduce data if the primary is lost. In cases where sources like this can be used to meet your Recovery Point Objective (RPO) and Recovery Time Objective (RTO), you might not require a backup. Another example, if working with Amazon EMR, it might not be necessary to backup your HDFS data store, as long as you can reproduce the data into Amazon EMR from Amazon S3.\n    \n    \n      When selecting a backup strategy, consider the time it takes to recover data. The time needed to recover data depends on the type of backup (in the case of a backup strategy), or the complexity of the data reproduction mechanism. This time should fall within the RTO for the workload.\n    \n    \n      Implementation steps\n    \n    \n       \n       \n       \n       \n       \n    \n        \n          Identify all data sources for the\n          workload. Data can be stored on a number of\n          resources such as\n          databases,\n          volumes,\n          filesystems,\n          logging\n          systems, and\n          object\n          storage. Refer to the\n          Resources section to find\n          Related documents on\n          different AWS services where data is stored, and the backup\n          capability these services provide.\n        \n      \n        \n          Classify data sources based on\n          criticality. Different data sets will have\n          different levels of criticality for a workload, and therefore\n          different requirements for resiliency. For example, some data\n          might be critical and require a RPO near zero, while other\n          data might be less critical and can tolerate a higher RPO and\n          some data loss. Similarly, different data sets might have\n          different RTO requirements as well.\n        \n      \n        \n          Use AWS or third-party services to\n          create backups of the data.\n          AWS Backup is a managed service that allows creating\n          backups of various data sources on AWS. AWS Elastic Disaster Recovery handles automated sub-second data\n          replication to an AWS Region. Most AWS services\n          also have native capabilities to create backups. The AWS Marketplace has many solutions that provide these capabilites\n          as well. Refer to the\n          Resources listed below for\n          information on how to create backups of data from various AWS\n          services.\n        \n      \n        \n          For data that is not backed up,\n          establish a data reproduction mechanism. You might\n          choose not to backup data that can be reproduced from other\n          sources for various reasons. There might be a situation where\n          it is cheaper to reproduce data from sources when needed\n          rather than creating a backup as there may be a cost\n          associated with storing backups. Another example is where\n          restoring from a backup takes longer than reproducing the data\n          from sources, resulting in a breach in RTO. In such\n          situations, consider tradeoffs and establish a well-defined\n          process for how data can be reproduced from these sources when\n          data recovery is necessary. For example, if you have loaded\n          data from Amazon S3 to a data warehouse (like Amazon Redshift), or MapReduce cluster (like Amazon EMR) to do\n          analysis on that data, this may be an example of data that can\n          be reproduced from other sources. As long as the results of\n          these analyses are either stored somewhere or reproducible,\n          you would not suffer a data loss from a failure in the data\n          warehouse or MapReduce cluster. Other examples that can be\n          reproduced from sources include caches (like Amazon ElastiCache) or RDS read replicas.\n        \n      \n        \n          Establish a cadence for backing up\n          data. Creating backups of data sources is a\n          periodic process and the frequency should depend on the RPO.\n        \n      \n    \n      Level of effort for the Implementation\n      Plan: Moderate\n    \n   \n\n  Resources\n\n      \n    \n      Related Best Practices:\n    \n    \n    REL13-BP01 Define recovery objectives for downtime and data\n  loss\n    \n    REL13-BP02 Use defined recovery strategies to meet the recovery\n  objectives\n    \n    \n      Related documents:\n    \n    \n       \n       \n       \n       \n       \n       \n       \n       \n       \n       \n       \n       \n       \n       \n       \n       \n       \n       \n       \n       \n    \n        \n          What\n          Is AWS Backup?\n        \n      \n        \n          What\n          is AWS DataSync?\n        \n      \n        \n          What\n          is Volume Gateway?\n        \n      \n        \n          APN\n          Partner: partners that can help with backup\n        \n      \n        \n          AWS Marketplace: products that can be used for backup\n        \n      \n        \n          Amazon EBS Snapshots\n        \n      \n        \n          Backing\n          Up Amazon EFS\n        \n      \n        \n          Backing\n          up Amazon FSx for Windows File Server\n        \n      \n        \n          Backup\n          and Restore for ElastiCache for Redis\n        \n      \n        \n          Creating\n          a DB Cluster Snapshot in Neptune\n        \n      \n        \n          Creating\n          a DB Snapshot\n        \n      \n        \n          Creating\n          an EventBridge Rule That Triggers on a Schedule\n        \n      \n        \n          Cross-Region\n          Replication with Amazon S3\n        \n      \n        \n          EFS-to-EFS\n          AWS Backup\n        \n      \n        \n          Exporting\n          Log Data to Amazon S3\n        \n      \n        \n          Object\n          lifecycle management\n        \n      \n        \n          On-Demand\n          Backup and Restore for DynamoDB\n        \n      \n        \n          Point-in-time\n          recovery for DynamoDB\n        \n      \n        \n          Working\n          with Amazon OpenSearch Service Index Snapshots\n        \n      \n        \n          What is AWS Elastic Disaster Recovery?\n        \n      \n    \n      Related videos:\n    \n    \n       \n       \n       \n    \n        \n          AWS           re:Invent 2021 - Backup, disaster recovery, and ransomware\n          protection with AWS\n        \n      \n        \n          AWS Backup Demo: Cross-Account and Cross-Region Backup\n        \n      \n        \n          AWS re:Invent\n          2019: Deep dive on AWS Backup, ft. Rackspace (STG341)\n        \n      \n  \u003cdiv\u003e\u003cdiv\u003e\u003cdiv\u003e\u003cdiv id=\"js_error_message\"\u003e\u003cp\u003e\u003cimg src=\"https://d1ge0kk1l5kms0.cloudfront.net/images/G/01/webservices/console/warning.png\" alt=\"Warning\" /\u003e \u003cstrong\u003eJavascript is disabled or is unavailable in your browser.\u003c/strong\u003e\u003c/p\u003e\u003cp\u003eTo use the Amazon Web Services Documentation, Javascript must be enabled. Please refer to your browser's Help pages for instructions.\u003c/p\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003eDocument ConventionsREL 9. How do you back up data?REL09-BP02 Secure and encrypt backups",
  "REL09-BP02 Secure and encrypt backupsControl and detect access to backups using authentication and authorization. Prevent and detect if data integrity of backups is compromised using encryption.\n    Common anti-patterns:\n  \n     \n     \n  \n      \n        Having the same access to the backups and restoration automation\n        as you do to the data.\n      \n    \n      \n        Not encrypting your backups.\n      \n    \n    Benefits of establishing this best\n    practice: Securing your backups prevents tampering with\n    the data, and encryption of the data prevents access to that data if\n    it is accidentally exposed.\n  \n    Level of risk exposed if this best practice\n    is not established: High\n  \n\n  Implementation guidance\n    \n      Control and detect access to backups using authentication and authorization, such as AWS Identity and Access Management (IAM). Prevent and detect if data integrity of backups is compromised using encryption.\n    \n    \n      Amazon S3 supports several methods of encryption of your data at\n      rest. Using server-side encryption, Amazon S3 accepts your objects\n      as unencrypted data, and then encrypts them as they are stored.\n      Using client-side encryption, your workload application is\n      responsible for encrypting the data before it is sent to Amazon S3.\n      Both methods allow you to use AWS Key Management Service (AWS KMS)\n      to create and store the data key, or you can provide your own key,\n      which you are then responsible for. Using AWS KMS, you can set\n      policies using IAM on who can and cannot access your data keys and\n      decrypted data.\n    \n    \n      For Amazon RDS, if you have chosen to encrypt your databases, then\n      your backups are encrypted also. DynamoDB backups are always\n      encrypted. When using AWS Elastic Disaster Recovery, all data in transit and at rest is encrypted. With Elastic Disaster Recovery, data at rest can be encrypted using either the default Amazon EBS encryption Volume Encryption Key or a custom customer-managed key.\n    \n    \n      Implementation steps\n    \n   \n      \n      \n   \n       Use encryption on each of your data stores. If your source data is\n       encrypted, then the backup will also be encrypted.\n     \n        \n           \n           \n           \n           \n           \n           \n        \n            Use encryption in Amazon RDS.. You can configure encryption at rest using AWS Key Management Service\n              when you create an RDS instance. \n          \n            Use encryption on Amazon EBS volumes.. You can configure default encryption or specify\n              a unique key upon volume creation. \n          \n             Use the required Amazon DynamoDB encryption. DynamoDB encrypts all data at rest. You can\n              either use an AWS owned AWS KMS key or an AWS managed KMS key, specifying a key that\n              is stored in your account. \n          \n            Encrypt your data stored in Amazon EFS. Configure the encryption when you create your\n              file system. \n          \n             Configure the encryption in the source and destination Regions. You can configure\n              encryption at rest in Amazon S3 using keys stored in KMS, but the keys are Region-specific.\n              You can specify the destination keys when you configure the replication. \n          \n            \n              Choose whether to use the default or custom Amazon EBS encryption for Elastic Disaster Recovery. This option will encrypt your replicated data at rest on the Staging Area Subnet disks and the replicated disks.\n            \n          \n       Implement least privilege permissions to access your backups.\n       Follow best practices to limit the access to the backups,\n       snapshots, and replicas in accordance with security best\n       practices.\n     \n           \n\n   \n\n  Resources\n\n      \n    \n      Related documents:\n    \n    \n       \n       \n       \n       \n       \n       \n       \n       \n       \n       \n       \n    \n        \n          AWS Marketplace: products that can be used for backup\n        \n      \n        \n          Amazon EBS Encryption\n        \n      \n        \n          Amazon S3: Protecting Data Using Encryption\n        \n      \n        \n          CRR\n          Additional Configuration: Replicating Objects Created with Server-Side Encryption (SSE) Using Encryption Keys stored in AWS KMS\n        \n      \n        \n          DynamoDB\n          Encryption at Rest\n        \n      \n        \n          Encrypting\n          Amazon RDS Resources\n        \n      \n        \n          Encrypting\n          Data and Metadata in Amazon EFS\n        \n      \n        \n          Encryption\n          for Backups in AWS\n        \n      \n        \n          Managing\n          Encrypted Tables\n        \n      \n        \n          Security\n          Pillar - AWS Well-Architected Framework\n        \n      \n        \n          What is AWS Elastic Disaster Recovery?\n        \n      \n  \u003cdiv\u003e\u003cdiv\u003e\u003cdiv\u003e\u003cdiv id=\"js_error_message\"\u003e\u003cp\u003e\u003cimg src=\"https://d1ge0kk1l5kms0.cloudfront.net/images/G/01/webservices/console/warning.png\" alt=\"Warning\" /\u003e \u003cstrong\u003eJavascript is disabled or is unavailable in your browser.\u003c/strong\u003e\u003c/p\u003e\u003cp\u003eTo use the Amazon Web Services Documentation, Javascript must be enabled. Please refer to your browser's Help pages for instructions.\u003c/p\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003eDocument ConventionsREL09-BP01 Identify and back up all data that needs to be\n  backed up, or reproduce the data from sourcesREL09-BP03 Perform data backup automatically",
  "REL09-BP03 Perform data backup automaticallyConfigure backups to be taken automatically based on a periodic schedule informed by the\nRecovery Point Objective (RPO), or by changes in the dataset. Critical datasets with low data \nloss requirements need to be backed up automatically on a frequent basis, whereas less \ncritical data where some loss is acceptable can be backed up less frequently.\n    Desired outcome: An automated process that creates backups of data sources at an\n    established cadence.\n  \n    Common anti-patterns:\n  \n     \n     \n  \n      \n        Performing backups manually.\n      \n    \n      \n        Using resources that have backup capability, but not including\n        the backup in your automation.\n      \n    \n    Benefits of establishing this best\n    practice: Automating backups verifies that they are taken\n    regularly based on your RPO, and alerts you if they are not taken.\n  \n    Level of risk exposed if this best practice\n    is not established: Medium\n  \n\n  Implementation guidance\n    \n      AWS Backup can be used to create automated data backups of various\n      AWS data sources. Amazon RDS instances can be backed up almost\n      continuously every five minutes and Amazon S3 objects can be backed\n      up almost continuously every fifteen minutes, providing for\n      point-in-time recovery (PITR) to a specific point in time within the\n      backup history. For other AWS data sources, such as Amazon EBS\n      volumes, Amazon DynamoDB tables, or Amazon FSx file systems, AWS Backup can run automated backup as frequently as every hour. These\n      services also offer native backup capabilities. AWS services that\n      offer automated backup with point-in-time recovery\n      include Amazon DynamoDB, Amazon RDS,\n      and Amazon\n        Keyspaces (for Apache Cassandra) – these can be restored to a\n      specific point in time within the backup history. Most other AWS\n      data storage services offer the ability to schedule periodic\n      backups, as frequently as every hour.\n    \n     Amazon RDS and Amazon DynamoDB offer continuous backup with point-in-time recovery. Amazon S3 versioning,\n      once turned on, is automatic. Amazon Data Lifecycle Manager can be used to automate the creation, copy and deletion of Amazon EBS snapshots.\n      It can also automate the creation, copy, deprecation and deregistration of Amazon EBS-backed Amazon\n      Machine Images (AMIs) and their underlying Amazon EBS snapshots. \n    \n      AWS Elastic Disaster Recovery provides continuous block-level replication from the source\n      environment (on-premises or AWS) to the target recovery region. Point-in-time Amazon EBS snapshots are\n      automatically created and managed by the service.\n    \n    \n      For a centralized view of your backup automation and history, AWS Backup provides a fully managed, policy-based backup solution. It\n      centralizes and automates the back up of data across multiple AWS\n      services in the cloud as well as on premises using the AWS Storage Gateway.\n    \n    \n      In additional to versioning, Amazon S3 features replication. The\n      entire S3 bucket can be automatically replicated to another bucket\n      in the same, or a different AWS Region.\n    \n    \n      Implementation steps\n    \n    \n       \n       \n       \n       \n    \n        \n          Identify data sources that\n          are currently being backed up manually. For more detail, see REL09-BP01 Identify and back up all data that needs to be\n  backed up, or reproduce the data from sources.\n        \n      \n        \n          Determine the RPO for the\n          workload. For more detail, see REL13-BP01 Define recovery objectives for downtime and data\n  loss.\n        \n      \n        \n          Use an automated backup solution or\n          managed service. AWS Backup is a fully-managed\n          service that makes it easy to\n          centralize\n          and automate data protection across AWS services, in the\n          cloud, and on-premises. Using backup plans in\n          AWS Backup, create rules which define the\n          resources to backup, and the frequency at which these backups\n          should be created. This frequency should be informed by the\n          RPO established in Step 2. For hands-on guidance on how to create\n          automated backups using AWS Backup, see\n          Testing Backup and Restore of Data. \n          Native backup capabilities\n          are offered by most AWS services that store data. For example,\n          RDS can be leveraged for automated backups with point-in-time\n          recovery (PITR).\n        \n      \n        \n          For data sources not\n          supported by an automated backup solution or\n          managed service such as on-premises data sources or message\n          queues, consider using a trusted third-party solution to\n          create automated backups. Alternatively, you can create\n          automation to do this using the AWS CLI or SDKs. You can use\n          AWS Lambda Functions or AWS Step Functions to define the logic\n          involved in creating a data backup, and use Amazon EventBridge\n          to invoke it at a frequency based on your RPO.\n        \n      \n    \n      Level of effort for the Implementation\n      Plan: Low\n    \n   \n\n  Resources\n\n      \n    \n      Related documents:\n    \n    \n       \n       \n       \n       \n       \n       \n    \n        \n          APN\n          Partner: partners that can help with backup\n        \n      \n        \n          AWS Marketplace: products that can be used for backup\n        \n      \n        \n          Creating\n          an EventBridge Rule That Triggers on a Schedule\n        \n      \n        \n          What\n          Is AWS Backup?\n        \n      \n        \n          What\n          Is AWS Step Functions?\n        \n      \n        \n          What is AWS Elastic Disaster Recovery?\n        \n      \n    \n      Related videos:\n    \n    \n       \n    \n        \n          AWS re:Invent\n          2019: Deep dive on AWS Backup, ft. Rackspace (STG341)\n        \n      \n  \u003cdiv\u003e\u003cdiv\u003e\u003cdiv\u003e\u003cdiv id=\"js_error_message\"\u003e\u003cp\u003e\u003cimg src=\"https://d1ge0kk1l5kms0.cloudfront.net/images/G/01/webservices/console/warning.png\" alt=\"Warning\" /\u003e \u003cstrong\u003eJavascript is disabled or is unavailable in your browser.\u003c/strong\u003e\u003c/p\u003e\u003cp\u003eTo use the Amazon Web Services Documentation, Javascript must be enabled. Please refer to your browser's Help pages for instructions.\u003c/p\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003eDocument ConventionsREL09-BP02 Secure and encrypt backupsREL09-BP04 Perform periodic recovery of the data to verify\n  backup integrity and processes",
  "REL09-BP04 Perform periodic recovery of the data to verify\n  backup integrity and processesValidate that your backup process implementation meets your Recovery Time Objectives (RTO) and Recovery Point Objectives (RPO) by performing a recovery test.\n    Desired outcome: Data from\n    backups is periodically recovered using well-defined mechanisms to\n    verify that recovery is possible within the established recovery\n    time objective (RTO) for the workload. Verify that restoration from\n    a backup results in a resource that contains the original data\n    without any of it being corrupted or inaccessible, and with data\n    loss within the recovery point objective (RPO).\n  \n    Common anti-patterns:\n  \n     \n     \n     \n     \n     \n     \n  \n      \n        Restoring a backup, but not querying or retrieving any data to\n        check that the restoration is usable.\n      \n    \n      \n        Assuming that a backup exists.\n      \n    \n      \n        Assuming that the backup of a system is fully operational and\n        that data can be recovered from it.\n      \n    \n      \n        Assuming that the time to restore or recover data from a backup\n        falls within the RTO for the workload.\n      \n    \n      \n        Assuming that the data contained on the backup falls within the\n        RPO for the workload\n      \n    \n      \n        Restoring when necessary, without using a runbook or outside of an\n        established automated procedure.\n      \n    \n    Benefits of establishing this best\n    practice: Testing the recovery of the backups verifies that\n    data can be restored when needed without having any worry that data\n    might be missing or corrupted, that the restoration and recovery is\n    possible within the RTO for the workload, and any data loss falls\n    within the RPO for the workload.\n  \n    Level of risk exposed if this best practice\n    is not established: Medium\n  \n\n  Implementation guidance\n      \n    \n      Testing backup and restore capability increases confidence in the\n      ability to perform these actions during an outage. Periodically\n      restore backups to a new location and run tests to verify the\n      integrity of the data. Some common tests that should be performed\n      are checking if all data is available, is not corrupted, is accessible, and that any data loss falls within the RPO for the workload. Such tests can also help ascertain if recovery mechanisms are fast enough to accommodate the workload's RTO.\n    \n    \n      Using AWS, you can stand up a testing environment and restore your\n      backups to assess RTO and RPO capabilities, and run tests on data\n      content and integrity.\n    \n    \n      Additionally, Amazon RDS and Amazon DynamoDB allow point-in-time\n      recovery (PITR). Using continuous backup, you can restore your\n      dataset to the state it was in at a specified date and time.\n    \n    \n      If all the data is available, is not corrupted, is accessible, and\n      any data loss falls within the RPO for the workload. Such tests\n      can also help ascertain if recovery mechanisms are fast enough to\n      accommodate the workload's RTO.\n    \n    \n      AWS Elastic Disaster Recovery offers continual point-in-time recovery snapshots of Amazon EBS volumes. As source servers are replicated, point-in-time states are chronicled over time based on the configured policy. Elastic Disaster Recovery helps you verify the integrity of these snapshots by launching instances for test and drill purposes without redirecting the traffic.\n    \n    \n      Implementation steps\n    \n    \n       \n       \n       \n       \n       \n       \n       \n       \n       \n    \n        \n          Identify data sources that\n          are currently being backed up and where these backups are\n          being stored. For implementation guidance, see REL09-BP01 Identify and back up all data that needs to be\n  backed up, or reproduce the data from sources.\n        \n      \n        \n          Establish criteria for data\n          validation for each data source. Different types of\n          data will have different properties which might require\n          different validation mechanisms. Consider how this data might\n          be validated before you are confident to use it in production.\n          Some common ways to validate data are using data and backup\n          properties such as data type, format, checksum, size, or a\n          combination of these with custom validation logic. For\n          example, this might be a comparison of the checksum values\n          between the restored resource and the data source at the time\n          the backup was created.\n        \n      \n        \n          Establish RTO and RPO for\n          restoring the data based on data criticality. For implementation guidance, see\n          REL13-BP01 Define recovery objectives for downtime and data\n  loss.\n        \n      \n        \n          Assess your recovery\n          capability. Review your backup and restore strategy\n          to understand if it can meet your RTO and RPO, and adjust the\n          strategy as necessary. Using\n          AWS           Resilience Hub, you can run an assessment of your\n          workload. The assessment evaluates your application\n          configuration against the resiliency policy and reports if\n          your RTO and RPO targets can be met.\n        \n      \n        \n          Do a test restore using\n          currently established processes used in production for data\n          restoration. These processes depend on how the original data\n          source was backed up, the format and storage location of the\n          backup itself, or if the data is reproduced from other\n          sources. For example, if you are using a managed service such\n          as\n          AWS Backup, this might be as simple as restoring the backup into a\n          new resource. If you used AWS Elastic Disaster Recovery\n          you can\n          launch\n          a recovery drill.\n        \n      \n        \n          Validate data recovery from\n          the restored resource based on\n          criteria you previously established for data validation. Does the restored and recovered data contain the most\n          recent record or item at the time of backup? Does this data fall\n          within the RPO for the workload?\n        \n      \n        \n          Measure time required for\n          restore and recovery and compare it to your established RTO. Does this process fall within the RTO for the\n          workload? For example, compare the timestamps from when the\n          restoration process started and when the recovery validation\n          completed to calculate how long this process takes. All AWS\n          API calls are timestamped and this information is available in\n          AWS CloudTrail. While this information can provide details\n          on when the restore process started, the end timestamp for\n          when the validation was completed should be recorded by your\n          validation logic. If using an automated process, then services\n          like\n          Amazon DynamoDB can be used to store this information.\n          Additionally, many AWS services provide an event history which\n          provides timestamped information when certain actions\n          occurred. Within AWS Backup, backup and restore actions are\n          referred to as jobs, and these jobs\n          contain timestamp information as part of its metadata which\n          can be used to measure time required for restoration and\n          recovery.\n        \n      \n        \n          Notify stakeholders if data\n          validation fails, or if the time required for restoration and\n          recovery exceeds the established RTO for the workload. When\n          implementing automation to do this,\n          such\n          as in this lab, services like Amazon Simple Notification Service (Amazon SNS) can be used to send push\n          notifications such as email or SMS to stakeholders.\n          These\n          messages can also be published to messaging applications such\n          as Amazon Chime, Slack, or Microsoft Teams or used to\n          create\n          tasks as OpsItems using AWS Systems Manager OpsCenter.\n        \n      \n        \n          Automate this process to run\n          periodically. For example, services like AWS Lambda\n          or a State Machine in AWS Step Functions can be used to\n          automate the restore and recovery processes, and Amazon EventBridge can be used to invoke this automation workflow\n          periodically as shown in the architecture diagram below. Learn\n          how to\n          Automate\n          data recovery validation with AWS Backup. Additionally,\n          this\n          Well-Architected lab provides a hands-on experience on\n          one way to do automation for several of the steps here.\n        \n      \n    \n      \n        \n           \n\n           \n           \n          Figure 9. An automated backup and restore process\n   \n    \n    \n      Level of effort for the Implementation\n      Plan: Moderate to high depending on the complexity of\n      the validation criteria.\n    \n   \n\n  Resources\n\n      \n    \n      Related documents:\n    \n    \n       \n       \n       \n       \n       \n       \n       \n       \n       \n    \n        \n          Automate\n          data recovery validation with AWS Backup\n        \n      \n        \n          APN\n          Partner: partners that can help with backup\n        \n      \n        \n          AWS Marketplace: products that can be used for backup\n        \n      \n        \n          Creating\n          an EventBridge Rule That Triggers on a Schedule\n        \n      \n        \n          On-demand\n          backup and restore for DynamoDB\n        \n      \n        \n          What\n          Is AWS Backup?\n        \n      \n        \n          What\n          Is AWS Step Functions?\n        \n      \n        \n          What\n          is AWS Elastic Disaster Recovery\n        \n      \n        \n          AWS Elastic Disaster Recovery\n        \n      \n  \u003cdiv\u003e\u003cdiv\u003e\u003cdiv\u003e\u003cdiv id=\"js_error_message\"\u003e\u003cp\u003e\u003cimg src=\"https://d1ge0kk1l5kms0.cloudfront.net/images/G/01/webservices/console/warning.png\" alt=\"Warning\" /\u003e \u003cstrong\u003eJavascript is disabled or is unavailable in your browser.\u003c/strong\u003e\u003c/p\u003e\u003cp\u003eTo use the Amazon Web Services Documentation, Javascript must be enabled. Please refer to your browser's Help pages for instructions.\u003c/p\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003eDocument ConventionsREL09-BP03 Perform data backup automaticallyREL 10. How do you use fault isolation to protect your workload?",
  "REL10-BP01 Deploy the workload to multiple locations\n    Distribute workload data and resources across multiple Availability\n    Zones or, where necessary, across AWS Regions.\n  \n    A fundamental principle for service design in AWS is to avoid single\n    points of failure, including the underlying physical infrastructure.\n    AWS provides cloud computing resources and services globally across\n    multiple geographic locations called\n    Regions.\n    Each Region is physically and logically independent and consists of\n    three or more\n    Availability\n    Zones (AZs). Availability Zones are geographically close to\n    each other but are physically separated and isolated. When you\n    distribute your workloads among Availability Zones and Regions, you\n    mitigate the risk of threats such as fires, floods, weather-related\n    disasters, earthquakes, and human error.\n  \n    Create a location strategy to provide high availability that is\n    appropriate for your workloads.\n  \n    Desired outcome: Production\n    workloads are distributed among multiple Availability Zones (AZs) or\n    Regions to achieve fault tolerance and high availability.\n  \n    Common anti-patterns:\n  \n     \n     \n     \n     \n  \n      \n        Your production workload exists only in a single Availability\n        Zone.\n      \n    \n      \n        You implement a multi-Region architecture when a multi-AZ\n        architecture would satisfy business requirements.\n      \n    \n      \n        Your deployments or data become desynchronized, which results in\n        configuration drift or under-replicated data.\n      \n    \n      \n        You don't account for dependencies between application\n        components if resilience and multi-location requirements differ\n        between those components.\n      \n    \n    Benefits of establishing this best\n    practice:\n  \n     \n     \n  \n      \n        Your workload is more resilient to incidents, such as power or\n        environmental control failures, natural disasters, upstream\n        service failures, or network issues that impact an AZ or an\n        entire Region.\n      \n    \n      \n        You can access a wider inventory of Amazon EC2 instances and\n        reduce the likelihood of\n        InsufficientCapacityExceptions (ICE) when\n        launching specific EC2 instance types.\n      \n    \n    Level of risk exposed if this best practice\n    is not established: High\n  \n\n  Implementation guidance\n\n      \n    \n      Deploy and operate all production workloads in at least two\n      Availability Zones (AZs) in a Region.\n    \n    \n      Using multiple Availability\n      Zones\n    \n    \n      Availability Zones are resource hosting locations that are\n      physically separated from each other to avoid correlated failures\n      due to risks such as fires, floods, and tornadoes. Each\n      Availability Zone has independent physical infrastructure,\n      including utility power connections, backup power sources,\n      mechanical services, and network connectivity. This arrangement \n      limits faults in any of these components to just the impacted\n      Availability Zone. For example, if an AZ-wide incident makes EC2\n      instances unavailable in the affected Availability Zone, your\n      instances in other Availability Zone remains available.\n    \n    \n      Despite being physically separated, Availability Zones in the same\n      AWS Region are close enough to provide high-throughput,\n      low-latency (single-digit millisecond) networking. You can\n      replicate data synchronously between Availability Zones for most\n      workloads without significantly impacting user experience. This\n      means you can use Availability Zones in a Region in an\n      active/active or active/standby configuration.\n    \n    \n      All compute associated with your workload should be distributed\n      among multiple Availability Zones. This includes\n      Amazon EC2\n      instances, AWS Fargate tasks, and VPC-attached\n      AWS Lambda functions. AWS compute services, including\n      EC2\n      Auto Scaling,\n      Amazon Elastic Container Service (ECS), and\n      Amazon Elastic Kubernetes Service (EKS), provide ways for you to launch\n      and manage compute across Availability Zones. Configure them to\n      automatically replace compute as needed in a different\n      Availability Zone to maintain availability. To direct traffic to\n      available Availability Zones, place a load balancer in front of\n      your compute, such as an Application Load Balancer or Network Load\n      Balancer. AWS load balancers can reroute traffic to available\n      instances in the event of an Availability Zone impairment.\n    \n    \n      You should also replicate data for your workload and make it\n      available in multiple Availability Zones. Some AWS managed data\n      services, such as\n      Amazon S3,\n      Amazon Elastic File\n      Service (EFS),\n      Amazon Aurora,\n      Amazon DynamoDB,\n      Amazon Simple Queue Service (SQS), and\n      Amazon Kinesis Data Streams replicate data in multiple\n      Availability Zones by default and are robust against Availability\n      Zone impairment. With other AWS managed data services, such as\n      Amazon Relational Database Service (RDS),\n      Amazon Redshift, and\n      Amazon ElastiCache, you must enable multi-AZ replication. Once\n      enabled, these services automatically detect an Availability Zone\n      impairment, redirect requests to an available Availability Zone,\n      and re-replicate data as needed after recovery without customer\n      intervention. Familiarize yourself with the user guide for each\n      AWS managed data service you use to understand its multi-AZ\n      capabilities, behaviors, and operations.\n    \n    \n      If you are using self-managed storage, such as\n      Amazon Elastic Block Store (EBS) volumes or Amazon EC2 instance storage,\n      you must manage multi-AZ replication yourself.\n    \n    \n    \n       \n        \n       \n       \n      Figure 9: Multi-tier architecture deployed across three\n        Availability Zones. Note that Amazon S3 and Amazon DynamoDB are\n        always Multi-AZ automatically. The ELB also is deployed to all three\n        zones.\n    \n    \n    \n      Using multiple AWS Regions\n    \n    \n      If you have workloads that require extreme resilience (such as\n      critical infrastructure, health-related applications, or services\n      with stringent customer or mandated availability requirements),\n      you may require additional availability beyond what a single AWS Region can provide. In this case, you should deploy and operate\n      your workload across at least two AWS Regions (assuming that your\n      data residency requirements allow it).\n    \n    \n      AWS Regions are located in different geographical regions around\n      the world and in multiple continents. AWS Regions have even\n      greater physical separation and isolation than Availability\n      Zones alone. AWS services, with few exceptions, take advantage of\n      this design to operate fully independently between different\n      Regions (also known as Regional services). A\n      failure of an AWS Regional service is designed not to impact the\n      service in a different Region.\n    \n    \n      When you operate your workload in multiple Regions, you should\n      consider additional requirements. Because resources in different\n      Regions are separate from and independent of one another, you must\n      duplicate your workload's components in each Region. This includes\n      foundational infrastructure, such as VPCs, in addition to compute\n      and data services.\n    \n    \n      NOTE: When you consider a\n      multi-Regional design, verify that your workload is capable of\n      running in a single Region. If you create dependencies between\n      Regions where a component in one Region relies on services or\n      components in a different Region, you can increase the risk of\n      failure and significantly weaken your reliability posture.\n    \n    \n      To ease multi-Regional deployments and maintain consistency,\n      AWS CloudFormation StackSets can replicate your entire AWS\n      infrastructure across multiple Regions.\n      AWS CloudFormation can also detect configuration drift and\n      inform you when your AWS resources in a Region are out of sync.\n      Many AWS services offer multi-region replication for important\n      workload assets. For example,\n      EC2 Image Builder can publish your EC2 machine images (AMIs) after\n      every build to each Region you use.\n      Amazon Elastic Container Registry (ECR) can replicate your container\n      images to your selected Regions.\n    \n    \n      You must also replicate your data across each of your chosen\n      Regions. Many AWS managed data services provide cross-Regional\n      replication capability, including Amazon S3, Amazon DynamoDB,\n      Amazon RDS, Amazon Aurora, Amazon Redshift, Amazon Elasticache,\n      and Amazon EFS.\n      Amazon DynamoDB global tables accept writes in any supported\n      Region and will replicate data among all your other configured\n      Regions. With other services, you must designate a primary Region\n      for writes, as other Regions contain read-only replicas. For each\n      AWS-managed data service your workload uses, refer to its user\n      guide and developer guide to understand its multi-Region\n      capabilities and limitations. Pay special attention to where\n      writes must be directed, transactional capabilities and\n      limitations, how replication is performed, and how to monitor\n      synchronization between Regions.\n    \n    \n      AWS also provides the ability to route request traffic to your\n      Regional deployments with great flexibility. For example, you can\n      configure your DNS records using\n      Amazon Route 53 to direct traffic to the closest available Region to the\n      user. Alternatively, you can configure your DNS records in an\n      active/standby configuration, where you designate one Region as\n      primary and fall back to a Regional replica only if the primary\n      Region becomes unhealthy. You can configure\n      Route 53 health checks to detect unhealthy endpoints and perform\n      automatic failover and additionally use\n      Amazon Application Recovery Controller (ARC) to provide a\n      highly-available routing control for manually re-routing traffic\n      as needed.\n    \n    \n      Even if you choose not to operate in multiple Regions for high\n      availability, consider multiple Regions as part of your disaster\n      recovery (DR) strategy. If possible, replicate your workload's\n      infrastructure components and data in a warm\n      standby or pilot light\n      configuration in a secondary Region. In this design, you replicate\n      baseline infrastructure from the primary Region such as VPCs, Auto\n      Scaling groups, container orchestrators, and other components, but\n      you configure the variable-sized components in the standby Region\n      (such as the number of EC2 instances and database replicas) to be\n      a minimally-operable size. You also arrange for continuous data\n      replication from the primary Region to the standby Region. If an\n      incident occurs, you can then scale out, or grow, the resources in\n      the standby Region, and then promote it to become the primary\n      Region.\n    \n     \n\n  Implementation steps\n\n      \n      \n         \n         \n         \n         \n         \n      \n          \n            Work with business stakeholders and data residency experts\n            to determine which AWS Regions can be used to host your\n            resources and data.\n          \n        \n          \n            Work with business and technical stakeholders to evaluate\n            your workload, and determine whether its resilience needs\n            can be met by a multi-AZ approach (single AWS Region) or if\n            they require a multi-Region approach (if multiple Regions\n            are permitted). The use of multiple Regions can achieve\n            greater availability but can involve additional complexity\n            and cost. Consider the following factors in your evaluation:\n          \n          \n             \n             \n          \n              \n                Business objectives and customer\n                requirements: How much downtime is permitted\n                should a workload-impacting incident occur in an\n                Availability Zone or a Region? Evaluate your recovery\n                point objectives as discussed in\n                REL13-BP01\n                Define recovery objectives for downtime and data\n                loss.\n              \n            \n              \n                Disaster recovery (DR)\n                requirements: What kind of potential disaster\n                do you want to insure yourself against? Consider the\n                possibility of data loss or long-term unavailability at\n                different scopes of impact from a single Availability\n                Zone to an entire Region. If you replicate data and\n                resources across Availability Zones, and a single\n                Availability Zone experiences a sustained failure, you\n                can recover service in another Availability Zone. If you\n                replicate data and resources across Regions, you can\n                recover service in another Region.\n              \n            \n        \n          \n            Deploy your compute resources into multiple Availability\n            Zones.\n          \n          \n             \n             \n             \n             \n             \n          \n              \n                In your VPC, create multiple subnets in different\n                Availability Zones. Configure each to be large enough to\n                accommodate the resources needed to serve the workload,\n                even during an incident. For more detail, see\n                REL02-BP03\n                Ensure IP subnet allocation accounts for expansion and\n                availability.\n              \n            \n              \n                If you are using Amazon EC2 instances, use\n                EC2\n                Auto Scaling to manage your instances. Specify\n                the subnets you chose in the previous step when you\n                create your Auto Scaling groups.\n              \n            \n              \n                If you are using AWS Fargate compute for\n                Amazon ECS or\n                Amazon EKS, select the subnets you chose in the first\n                step when you create an ECS Service, launch an ECS task,\n                or create a\n                Fargate\n                profile for EKS.\n              \n            \n              \n                If you are using AWS Lambda functions that need to run\n                in your VPC, select the subnets you chose in the first\n                step when you create the Lambda function. For any\n                functions that do not have a VPC configuration, AWS Lambda manages availability for you automatically.\n              \n            \n              \n                Place traffic directors such as load balancers in front\n                of your compute resources. If cross-zone load balancing\n                is enabled,\n                AWS                 Application Load Balancers and\n                Network\n                Load Balancers detect when targets such as EC2\n                instances and containers are unreachable due to\n                Availability Zone impairment and reroute traffic towards\n                targets in healthy Availability Zones. If you disable\n                cross-zone load balancing, use Amazon Application Recovery Controller (ARC) to provide zonal\n                shift capability. If you are using a third-party load\n                balancer or have implemented your own load balancers,\n                configure them with multiple front ends across different\n                Availability Zones.\n              \n            \n        \n          \n            Replicate your workload's data across multiple Availability\n            Zones.\n          \n          \n             \n             \n             \n          \n              \n                If you use an AWS-managed data service such as Amazon RDS, Amazon ElastiCache, or Amazon FSx, study its user\n                guide to understand its data replication and resilience\n                capabilities. Enable cross-AZ replication and failover\n                if necessary.\n              \n            \n              \n                If you use AWS-managed storage services such as Amazon S3, Amazon EFS, and Amazon FSx, avoid using single-AZ or\n                One Zone configurations for data that requires high\n                durability. Use a multi-AZ configuration for these\n                services. Check the respective service's user guide to\n                determine whether multi-AZ replication is enabled by\n                default or whether you must enable it.\n              \n            \n              \n                If you run a self-managed database, queue, or other\n                storage service, arrange for multi-AZ replication\n                according to the application's instructions or best\n                practices. Familiarize yourself with the failover\n                procedures for your application.\n              \n            \n        \n          \n            Configure your DNS service to detect AZ impairment and\n            reroute traffic to a healthy Availability Zone. Amazon Route 53, when used in combination with Elastic Load Balancers,\n            can do this automatically. Route 53 can also be configured\n            with failover records that use health checks to respond to\n            queries with only healthy IP addresses. For any DNS records\n            used for failover, specify a short time to live (TTL) value\n            (for example, 60 seconds or less) to help prevent record\n            caching from impeding recovery (Route 53 alias records\n            supply appropriate TTLs for you).\n          \n        \n      \n        Additional steps when using multiple AWS Regions\n      \n      \n         \n         \n         \n         \n         \n      \n          \n            Replicate all operating system (OS) and application code\n            used by your workload across your selected Regions.\n            Replicate Amazon Machine Images (AMIs) used by your EC2\n            instances if necessary using solutions such as Amazon EC2\n            Image Builder. Replicate container images stored in\n            registries using solutions such as Amazon ECR cross-Region\n            replication. Enable Regional replication for any Amazon S3\n            buckets used for storing application resources.\n          \n        \n          \n            Deploy your compute resources and configuration metadata\n            (such as parameters stored in AWS Systems Manager Parameter\n            Store) into multiple Regions. Use the same procedures\n            described in previous steps, but replicate the configuration\n            for each Region you are using for your workload. Use\n            infrastructure as code solutions such as AWS CloudFormation\n            to uniformly reproduce the configurations among Regions. If\n            you are using a secondary Region in a pilot light\n            configuration for disaster recovery, you may reduce the\n            number of your compute resources to a minimum value to save\n            cost, with a corresponding increase in time to recovery.\n          \n        \n          \n            Replicate your data from your primary Region into your\n            secondary Regions.\n          \n          \n             \n             \n             \n          \n              \n                Amazon DynamoDB global tables provide global replicas of\n                your data that can be written to from any supported\n                Region. With other AWS-managed data services, such as\n                Amazon RDS, Amazon Aurora, and Amazon Elasticache, you\n                designate a primary (read/write) Region and replica\n                (read-only) Regions. Consult the respective services'\n                user and developer guides for details on Regional\n                replication.\n              \n            \n              \n                If you are running a self-managed database, arrange for\n                multi-Region replication according to the application's\n                instructions or best practices. Familiarize yourself\n                with the failover procedures for your application.\n              \n            \n              \n                If your workload uses AWS EventBridge, you may need to\n                forward selected events from your primary Region to your\n                secondary Regions. To do so, specify event buses in your\n                secondary Regions as targets for matched events in your\n                primary Region.\n              \n            \n        \n          \n            Consider whether and to what extent you want to use\n            identical encryption keys across Regions. A typical approach\n            that balances security and ease of use is to use\n            Region-scoped keys for Region-local data and authentication,\n            and use globally-scoped keys for encryption of data that is\n            replicated among different Regions.\n            AWS Key Management Service (KMS) supports\n            multi-region\n            keys to securely distribute and protect keys shared\n            across Regions.\n          \n        \n          \n            Consider AWS Global Accelerator to improve the availability\n            of your application by directing traffic to Regions that\n            contain healthy endpoints.\n          \n        \n     \n   \n\n  Resources\n\n      \n    \n      Related best practices:\n    \n    \n       \n       \n       \n    \n        \n          REL02-BP03\n          Ensure IP subnet allocation accounts for expansion and\n          availability\n        \n      \n        \n          REL11-BP05\n          Use static stability to prevent bimodal behavior\n        \n      \n        \n          REL13-BP01\n          Define recovery objectives for downtime and data loss\n        \n      \n    \n      Related documents:\n    \n    \n       \n       \n       \n       \n       \n       \n       \n       \n       \n       \n       \n       \n       \n       \n       \n       \n       \n       \n       \n       \n       \n    \n        \n          AWS           Global Infrastructure\n        \n      \n        \n          White\n          paper: AWS Fault Isolation Boundaries\n        \n      \n        \n          Resilience\n          in Amazon EC2 Auto Scaling\n        \n      \n        \n          Amazon EC2 Auto Scaling: Example: Distribute instances across\n          Availability Zones\n        \n      \n        \n          How\n          EC2 Image Builder works\n        \n      \n        \n          How\n          Amazon ECS places tasks on container instances (includes\n          Fargate)\n        \n      \n        \n          Resilience\n          in AWS Lambda\n        \n      \n        \n          Amazon S3: Replicating objects overview\n        \n      \n        \n          Private\n          image replication in Amazon ECR\n        \n      \n        \n          Global\n          Tables: Multi-Region Replication with DynamoDB\n        \n      \n        \n          Amazon\n          Elasticache for Redis OSS: Replication across AWS Regions\n          using global datastores\n        \n      \n        \n          Resilience\n          in Amazon RDS\n        \n      \n        \n          Using\n          Amazon Aurora global databases\n        \n      \n        \n          AWS           Global Accelerator Developer Guide\n        \n      \n        \n          Multi-Region\n          keys in AWS KMS\n        \n      \n        \n          Amazon Route 53: Configuring DNS failover\n        \n      \n        \n          Amazon Application Recovery Controller (ARC) Developer\n          Guide\n        \n      \n        \n          Sending\n          and receiving Amazon EventBridge events between AWS Regions\n        \n      \n        \n          Creating\n          a Multi-Region Application with AWS Services blog\n          series\n        \n      \n        \n          Disaster\n          Recovery (DR) Architecture on AWS, Part I: Strategies for\n          Recovery in the Cloud\n        \n      \n        \n          Disaster\n          Recovery (DR) Architecture on AWS, Part III: Pilot Light and\n          Warm Standby\n        \n      \n    \n      Related videos:\n    \n    \n       \n       \n    \n        \n          AWS re:Invent\n          2018: Architecture Patterns for Multi-Region Active-Active\n          Applications\n        \n      \n        \n          AWS re:Invent\n          2019: Innovation and operation of the AWS global network\n          infrastructure\n        \n      \n  \u003cdiv\u003e\u003cdiv\u003e\u003cdiv\u003e\u003cdiv id=\"js_error_message\"\u003e\u003cp\u003e\u003cimg src=\"https://d1ge0kk1l5kms0.cloudfront.net/images/G/01/webservices/console/warning.png\" alt=\"Warning\" /\u003e \u003cstrong\u003eJavascript is disabled or is unavailable in your browser.\u003c/strong\u003e\u003c/p\u003e\u003cp\u003eTo use the Amazon Web Services Documentation, Javascript must be enabled. Please refer to your browser's Help pages for instructions.\u003c/p\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003eDocument ConventionsREL 10. How do you use fault isolation to protect your workload?REL10-BP02 Automate recovery for components constrained to a\n  single location",
  "REL10-BP02 Automate recovery for components constrained to a\n  single locationIf components of the workload can only run in a single Availability Zone or in an on-premises data center, implement the capability to do a complete rebuild of the workload within your defined recovery objectives.\n    Level of risk exposed if this best practice\n    is not established: Medium\n  \n\n  Implementation guidance\n    \n      If the best practice to deploy the workload to multiple locations is\n      not possible due to technological constraints, you must implement an\n      alternate path to resiliency. You must automate the ability to\n      recreate necessary infrastructure, redeploy applications, and\n      recreate necessary data for these cases.\n    \n    \n      For example, Amazon EMR launches all nodes for a given cluster in\n      the same Availability Zone because running a cluster in the same\n      zone improves performance of the jobs flows as it provides a higher\n      data access rate. If this component is required for workload\n      resilience, then you must have a way to redeploy the cluster and its\n      data. Also for Amazon EMR, you should provision redundancy in ways\n      other than using Multi-AZ. You can\n      provision multiple\n        nodes.\n      Using EMR\n        File System (EMRFS), data in EMR can be stored in Amazon S3,\n      which in turn can be replicated across multiple Availability Zones\n      or AWS Regions.\n    \n    \n      Similarly, for Amazon Redshift, by default it provisions your\n      cluster in a randomly selected Availability Zone within the AWS Region that you select. All the cluster nodes are provisioned in the\n      same zone.\n    \n    \n      For stateful server-based workloads deployed to an on-premise data center, you can use AWS Elastic Disaster Recovery to protect your workloads in AWS. If you are already hosted in AWS, you can use Elastic Disaster Recovery to protect your workload to an alternative Availability Zone or Region. Elastic Disaster Recovery uses continual block-level replication to a lightweight staging area to provide fast, reliable recovery of on-premises and cloud-based applications.\n    \n    \n      Implementation steps\n    \n    \n       \n     \n        Implement self-healing. Deploy your instances or containers using\n        automatic scaling when possible. If you cannot use automatic\n        scaling, use automatic recovery for EC2 instances or implement\n        self-healing automation based on Amazon EC2 or ECS container\n        lifecycle events.\n      \n        \n           \n           \n           \n           \n        \n             Use Amazon EC2 Auto Scaling groups for instances and container workloads that have no\n              requirements for a single instance IP address, private IP address, Elastic IP address,\n              and instance metadata. \n            \n               \n            \n                \n                  The launch template user data can be used to implement automation that can self-heal most workloads.\n                \n              \n          \n             Use automatic recovery of Amazon EC2 instances for workloads that require a single\n              instance ID address, private IP address, elastic IP address, and instance metadata. \n            \n               \n            \n                \n                  Automatic Recovery will send recovery status alerts to a SNS topic as the\n                  instance failure is detected. \n                \n              \n          \n             Use Amazon EC2 instance lifecycle events or Amazon ECS events to automate self-healing where\n              automatic scaling or EC2 recovery cannot be used. \n            \n               \n            \n                \n                  Use the events to invoke automation that will heal your component\n                  according to the process logic you require.\n                \n              \n          \n            \n              Protect stateful workloads that are limited to a single location using AWS Elastic Disaster Recovery.\n            \n          \n   \n    \n   \n\n  Resources\n\n      \n    \n      Related documents:\n    \n    \n       \n       \n       \n       \n       \n       \n    \n        \n          Amazon ECS events\n        \n      \n        \n          Amazon EC2 Auto Scaling lifecycle hooks\n        \n      \n        \n          Recover\n          your instance.\n        \n      \n        \n          Service\n          automatic scaling\n        \n      \n        \n          What\n          Is Amazon EC2 Auto Scaling?\n        \n      \n        AWS Elastic Disaster Recovery\n      \n  \u003cdiv\u003e\u003cdiv\u003e\u003cdiv\u003e\u003cdiv id=\"js_error_message\"\u003e\u003cp\u003e\u003cimg src=\"https://d1ge0kk1l5kms0.cloudfront.net/images/G/01/webservices/console/warning.png\" alt=\"Warning\" /\u003e \u003cstrong\u003eJavascript is disabled or is unavailable in your browser.\u003c/strong\u003e\u003c/p\u003e\u003cp\u003eTo use the Amazon Web Services Documentation, Javascript must be enabled. Please refer to your browser's Help pages for instructions.\u003c/p\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003eDocument ConventionsREL10-BP01 Deploy the workload to multiple locationsREL10-BP03 Use bulkhead architectures to limit scope of\n  impact",
  "REL10-BP03 Use bulkhead architectures to limit scope of\n  impactImplement bulkhead architectures (also known as cell-based architectures) to restrict the effect of failure within a workload to a limited number of components.\n    Desired outcome: A cell-based architecture uses multiple isolated instances of a workload, where each instance is known as a cell. Each cell is independent, does not share state with other cells, and handles a subset of the overall workload requests. This reduces the potential impact of a failure, such as a bad software update, to an individual cell and the requests it is processing. If a workload uses 10 cells to service 100 requests, when a failure occurs, 90% of the overall requests would be unaffected by the failure.\n  \n    Common anti-patterns:\n  \n     \n     \n     \n     \n     \n  \n      \n        Allowing cells to grow without bounds.\n      \n    \n      \n        Applying code updates or deployments to all cells at the same time.\n      \n    \n      \n        Sharing state or components between cells (with the exception of the router layer).\n      \n    \n      \n        Adding complex business or routing logic to the router layer.\n      \n    \n      \n        Not minimizing cross-cell interactions.\n      \n    \n    Benefits of establishing this best practice: With cell-based architectures, many common types of failure are contained within the cell itself, providing additional fault isolation. These fault boundaries can provide resilience against failure types that otherwise are hard to contain, such as unsuccessful code deployments or requests that are corrupted or invoke a specific failure mode (also known as poison pill requests).\n  \n    Level of risk exposed if this best practice\n      is not established: High\n  \n    \n    Implementation guidance\n    \n      On a ship, bulkheads ensure that a hull breach is contained within one section of the hull. In complex systems, this pattern is often replicated to allow fault isolation. Fault isolated boundaries restrict the effect of a failure within a workload to a limited number of components. Components outside of the boundary are unaffected by the failure. Using multiple fault isolated boundaries, you can limit the impact on your workload. On AWS, customers can use multiple Availability Zones and Regions to provide fault isolation, but the concept of fault isolation can be extended to your workload’s architecture as well.\n    \n    \n      The overall workload is partitioned cells by a partition key. This key needs to align with the grain of the service, or the natural way that a service's workload can be subdivided with minimal cross-cell interactions. Examples of partition keys are customer ID, resource ID, or any other parameter easily accessible in most API calls. A cell routing layer distributes requests to individual cells based on the partition key and presents a single endpoint to clients.\n    \n    \n       \n\n       \n       \n      Figure 11: Cell-based architecture\n   \n\n    \n      Implementation steps\n    \n    \n      When designing a cell-based architecture, there are several design considerations to consider:\n    \n    \n       \n       \n       \n       \n       \n       \n    \n        \n          Partition key: Special consideration should be taken\n          while choosing the partition key. \n        \n           \n           \n        \n            \n              It should align with the grain of the service, or the natural way that a service's workload can be subdivided with minimal cross-cell interactions. Examples are customer ID or resource ID.\n            \n          \n            \n              The partition key must be available in all requests, either directly or in a way that could be easily inferred deterministically by other parameters.\n            \n          \n      \n        \n          Persistent cell mapping: Upstream services should only\n          interact with a single cell for the lifecycle of their resources. \n        \n           \n           \n           \n        \n            \n              Depending on the workload, a cell migration strategy may be needed to migrate data from one cell to another. A possible scenario when a cell migration may be needed is if a particular user or resource in your workload becomes too big and requires it to have a dedicated cell.\n            \n          \n            \n              Cells should not share state or components between cells.\n            \n          \n            \n              Consequently, cross-cell interactions should be avoided or kept to a minimum, as those interactions create dependencies between cells and therefore diminish the fault isolation improvements.\n            \n          \n      \n        \n          Router layer: The router layer is a shared component\n          between cells, and therefore cannot follow the same compartmentalization strategy as with\n          cells. \n        \n           \n           \n        \n            \n              It is recommended for the router layer to distribute requests to individual cells using a partition mapping algorithm in a computationally efficient manner, such as combining cryptographic hash functions and modular arithmetic to map partition keys to cells.\n            \n          \n            \n              To avoid multi-cell impacts, the routing layer must remain as simple and horizontally scalable as possible, which necessitates avoiding complex business logic within this layer. This has the added benefit of making it easy to understand its expected behavior at all times, allowing for thorough testability. As explained by Colm MacCárthaigh in Reliability, constant work, and a good cup of coffee, simple designs and constant work patterns produce reliable systems and reduce anti-fragility.\n            \n          \n      \n        \n          Cell size: Cells should have a maximum size and should\n          not be allowed to grow beyond it. \n        \n           \n           \n        \n            \n              The maximum size should be identified by performing thorough testing, until breaking points are reached and safe operating margins are established. For more detail on how to implement testing practices, see REL07-BP04 Load test your workload\n            \n          \n            \n              The overall workload should grow by adding additional cells, allowing the workload to scale with increases in demand.\n            \n          \n      \n        \n          Multi-AZ or Multi-Region strategies: Multiple layers of\n          resilience should be leveraged to protect against different failure domains. \n        \n           \n        \n            \n              For resilience, you should use an approach that builds layers of defense. One layer protects against smaller, more common disruptions by building a highly available architecture using multiple AZs. Another layer of defense is meant to protect against rare events like widespread natural disasters and Region-level disruptions. This second layer involves architecting your application to span multiple AWS Regions. Implementing a multi-Region strategy for your workload helps protect it against widespread natural disasters that affect a large geographic region of a country, or technical failures of Region-wide scope. Be aware that implementing a multi-Region architecture can be significantly complex, and is usually not required for most workloads. For more detail, see REL10-BP01 Deploy the workload to multiple locations.\n            \n          \n      \n        \n          Code deployment: A staggered code deployment strategy\n          should be preferred over deploying code changes to all cells at the same time. \n        \n           \n        \n            \n              This helps minimize potential failure to multiple cells due to a bad deployment or human error. For more detail, see Automating safe, hands-off deployment.\n            \n          \n      \n   \n\n  Resources\n    \n      Related best practices:\n    \n    \n       \n       \n    \n        \n          REL07-BP04 Load test your workload\n        \n      \n        \n          REL10-BP01 Deploy the workload to multiple locations\n        \n      \n\n      \n    \n      Related documents:\n    \n    \n       \n       \n       \n       \n    \n        \n          Reliability, constant work, and a good cup of coffee\n        \n      \n        AWS and Compartmentalization\n        \n      \n        \n          Workload isolation using shuffle-sharding\n        \n      \n        \n          Automating safe, hands-off deployment\n        \n      \n    \n      Related videos:\n    \n    \n       \n       \n       \n       \n    \n        AWS re:Invent 2018: Close Loops and Opening Minds: How to Take Control of Systems, Big and Small\n        \n      \n        \n          AWS re:Invent\n          2018: How AWS Minimizes the Blast Radius of Failures\n          (ARC338)\n        \n      \n        \n          Shuffle-sharding:\n          AWS re:Invent 2019: Introducing The Amazon Builders’ Library\n          (DOP328)\n        \n      \n        AWS Summit ANZ 2021 - Everything fails, all the time: Designing for resilience \n        \n      \n  \u003cdiv\u003e\u003cdiv\u003e\u003cdiv\u003e\u003cdiv id=\"js_error_message\"\u003e\u003cp\u003e\u003cimg src=\"https://d1ge0kk1l5kms0.cloudfront.net/images/G/01/webservices/console/warning.png\" alt=\"Warning\" /\u003e \u003cstrong\u003eJavascript is disabled or is unavailable in your browser.\u003c/strong\u003e\u003c/p\u003e\u003cp\u003eTo use the Amazon Web Services Documentation, Javascript must be enabled. Please refer to your browser's Help pages for instructions.\u003c/p\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003eDocument ConventionsREL10-BP02 Automate recovery for components constrained to a\n  single locationREL 11. How do you design your workload to withstand component\n                  failures?",
  "REL11-BP01 Monitor all components of the workload to detect\n  failures\n    Continually monitor the health of your workload so that you and your\n    automated systems are aware of failures or degradations as soon as\n    they occur. Monitor for key performance indicators (KPIs) based on\n    business value.\n  \n    All recovery and healing mechanisms must start with the ability to\n    detect problems quickly. Technical failures should be detected first\n    so that they can be resolved. However, availability is based on the\n    ability of your workload to deliver business value, so key\n    performance indicators (KPIs) that measure this need to be a part of\n    your detection and remediation strategy.\n  \n    Desired outcome: Essential components of a workload are monitored independently to\n    detect and alert on failures when and where they happen.\n  \n    Common anti-patterns:\n  \n     \n     \n     \n     \n     \n     \n     \n  \n      \n        No alarms have been configured, so outages occur without\n        notification.\n      \n    \n      \n        Alarms exist, but at thresholds that don't provide adequate time\n        to react.\n      \n    \n      \n        Metrics are not collected often enough to meet the recovery time\n        objective (RTO).\n      \n    \n      \n        Only the customer facing interfaces of the workload are actively\n        monitored.\n      \n    \n      \n        Only collecting technical metrics, no business function metrics.\n      \n    \n      \n        No metrics measuring the user experience of the workload.\n      \n    \n      \n        Too many monitors are created.\n      \n    \n    Benefits of establishing this best\n      practice: Having appropriate monitoring at all layers allows you to reduce\n    recovery time by reducing time to detection.\n  \n    Level of risk exposed if this best practice\n      is not established: High\n  \n    \n    Implementation guidance \n    \n    \n    \n      Identify all workloads that will be reviewed for monitoring. Once\n      you have identified all components of the workload that will need\n      to monitored, you will now need to determine the monitoring\n      interval. The monitoring interval will have a direct impact on how\n      fast recovery can be initiated based on the time it takes to\n      detect a failure. The mean time to detection (MTTD) is the amount\n      of time between a failure occurring and when repair operations\n      begin. The list of services should be extensive and complete.\n    \n    \n      Monitoring must cover all layers of the application stack\n      including application, platform, infrastructure, and network.\n    \n    \n      Your monitoring strategy should consider the impact of\n      gray failures. For more detail on gray\n      failures, see\n      \n        Gray failures in the Advanced Multi-AZ Resilience Patterns whitepaper.\n    \n     \n      \n      Implementation steps\n      \n      \n      \n         \n         \n         \n         \n         \n         \n         \n         \n         \n         \n      \n          \n            Your monitoring interval is dependent on how quickly you\n            must recover. Your recovery time is driven by the time it\n            takes to recover, so you must determine the frequency of\n            collection by accounting for this time and your recovery\n            time objective (RTO).\n          \n        \n          \n            Configure detailed monitoring for components and managed\n            services.\n          \n          \n             \n             \n             \n             \n          \n              \n                Determine if\n                detailed\n                  monitoring for EC2 instances and\n                Auto Scaling is necessary. Detailed monitoring\n                provides one minute interval metrics, and default\n                monitoring provides five minute interval metrics.\n              \n            \n              \n                Determine if\n                enhanced\n                  monitoring for RDS is necessary. Enhanced\n                monitoring uses an agent on RDS instances to get useful\n                information about different process or threads.\n              \n            \n              \n                Determine the monitoring requirements of critical\n                serverless components for\n                Lambda,\n                API Gateway,\n                Amazon EKS,\n                Amazon ECS,\n                and all types of\n                load\n                  balancers.\n              \n            \n              \n                Determine the monitoring requirements of storage\n                components for\n                Amazon S3,\n                Amazon FSx,\n                Amazon EFS,\n                and\n                Amazon EBS.\n              \n            \n        \n          \n            Create\n            custom\n              metrics to measure business key performance\n            indicators (KPIs). Workloads implement key business\n            functions, which should be used as KPIs that help identify\n            when an indirect problem happens.\n          \n        \n          \n            Monitor the user experience for failures using user\n            canaries.\n            Synthetic\n              transaction testing (also known as canary testing,\n            but not to be confused with canary deployments) that can run\n            and simulate customer behavior is among the most important\n            testing processes. Run these tests constantly against your\n            workload endpoints from diverse remote locations.\n          \n        \n          \n            Create\n            custom\n              metrics that track the user's experience. If you can\n            instrument the experience of the customer, you can determine\n            when the consumer experience degrades.\n          \n        \n          \n            Set\n              alarms to detect when any part of your workload is\n            not working properly and to indicate when to automatically\n            scale resources. Alarms can be visually displayed on\n            dashboards, send alerts through Amazon SNS or email, and\n            work with Auto Scaling to scale workload resources up or\n            down.\n          \n        \n          \n            Create\n            dashboards\n            to visualize your metrics. Dashboards can be used to\n            visually see trends, outliers, and other indicators of\n            potential problems or to provide an indication of problems\n            you may want to investigate.\n          \n        \n          \n            Create\n            distributed\n              tracing monitoring for your services. With\n            distributed monitoring, you can understand how your\n            application and its underlying services are performing to\n            identify and troubleshoot the root cause of performance\n            issues and errors.\n          \n        \n          \n            Create monitoring systems (using\n            CloudWatch\n            or\n            X-Ray)\n            dashboards and data collection in a separate Region and\n            account.\n          \n        \n          \n            Stay informed about service degradations with AWS Health. Create purpose-fit AWS Health event notifications to e-mail and chat channels through AWS User Notifications and integrate programmatically with your monitoring and alerting tools through Amazon EventBridge.\n          \n        \n     \n   \n    \n    Resources  \n    \n      Related best practices:\n    \n    \n       \n       \n    \n        \n          Availability\n            Definition\n        \n      \n        \n          REL11-BP06\n            Send Notifications when events impact availability\n        \n      \n    \n    \n      Related documents:\n    \n    \n    \n       \n       \n       \n       \n       \n       \n       \n       \n       \n       \n    \n        \n          Amazon CloudWatch Synthetics enables you to create user\n            canaries\n        \n      \n        \n          Enable\n            or Disable Detailed Monitoring for Your Instance\n        \n      \n        \n          Enhanced\n            Monitoring\n        \n      \n        \n          Monitoring\n            Your Auto Scaling Groups and Instances Using Amazon CloudWatch\n        \n      \n        \n          Publishing\n            Custom Metrics\n        \n      \n        \n          Using\n            Amazon CloudWatch Alarms\n        \n      \n        \n          Using\n            CloudWatch Dashboards\n        \n      \n        \n          Using\n            Cross Region Cross Account CloudWatch Dashboards\n        \n      \n        \n          Using\n            Cross Region Cross Account X-Ray Tracing\n        \n      \n        \n          Understanding\n            availability\n        \n         \n    \n      Related videos:\n        \n    \n       \n    \n        \n          Mitigating\n            gray failures\n        \n         \n    \n      Related examples:\n       \n    \n       \n    \n        \n          One\n            Observability Workshop: Explore X-Ray\n        \n      \n    \n      Related tools:\n    \n    \n       \n       \n    \n        \n          CloudWatch\n        \n      \n        \n          CloudWatch\n            X-Ray\n        \n      \n  \u003cdiv\u003e\u003cdiv\u003e\u003cdiv\u003e\u003cdiv id=\"js_error_message\"\u003e\u003cp\u003e\u003cimg src=\"https://d1ge0kk1l5kms0.cloudfront.net/images/G/01/webservices/console/warning.png\" alt=\"Warning\" /\u003e \u003cstrong\u003eJavascript is disabled or is unavailable in your browser.\u003c/strong\u003e\u003c/p\u003e\u003cp\u003eTo use the Amazon Web Services Documentation, Javascript must be enabled. Please refer to your browser's Help pages for instructions.\u003c/p\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003eDocument ConventionsREL 11. How do you design your workload to withstand component\n                  failures?REL11-BP02 Fail over to healthy resources",
  "REL11-BP02 Fail over to healthy resources\n    If a resource failure occurs, healthy resources should continue to\n    serve requests. For location impairments (such as Availability Zone\n    or AWS Region), ensure that you have systems in place to fail over\n    to healthy resources in unimpaired locations.\n  \n    When designing a service, distribute load across resources,\n    Availability Zones, or Regions. Therefore, failure of an individual\n    resource or impairment can be mitigated by shifting traffic to\n    remaining healthy resources. Consider how services are discovered\n    and routed to in the event of a failure.\n  \n    Design your services with fault recovery in mind. At AWS, we design\n    services to minimize the time to recover from failures and impact on\n    data. Our services primarily use data stores that acknowledge\n    requests only after they are durably stored across multiple replicas\n    within a Region. They are constructed to use cell-based isolation\n    and use the fault isolation provided by Availability Zones. We use\n    automation extensively in our operational procedures. We also\n    optimize our replace-and-restart functionality to recover quickly\n    from interruptions.\n  \n    The patterns and designs that allow for the failover vary for each\n    AWS platform service. Many AWS native managed services are natively\n    multiple Availability Zone (like Lambda or API Gateway). Other AWS\n    services (like EC2 and EKS) require specific best practice designs to\n    support failover of resources or data storage across AZs.\n  \n    Monitoring should be set up to check that the failover resource is\n    healthy, track the progress of the resources failing over, and\n    monitor business process recovery.\n  \n    Desired outcome: Systems are\n    capable of automatically or manually using new resources to recover\n    from degradation.\n  \n    Common anti-patterns:\n  \n     \n     \n     \n     \n     \n     \n     \n     \n     \n  \n      \n        Planning for failure is not part of the planning and design\n        phase.\n      \n    \n      \n        RTO and RPO are not established.\n      \n    \n      \n        Insufficient monitoring to detect failing resources.\n      \n    \n      \n        Proper isolation of failure domains.\n      \n    \n      \n        Multi-Region fail over is not considered.\n      \n    \n      \n        Detection for failure is too sensitive or aggressive when\n        deciding to failover.\n      \n    \n      \n        Not testing or validating failover design.\n      \n    \n      \n        Performing auto healing automation, but not notifying that\n        healing was needed.\n      \n    \n      \n        Lack of dampening period to avoid failing back too soon.\n      \n    \n    Benefits of establishing this best\n      practice: You can build more resilient systems that maintain reliability when\n    experiencing failures by degrading gracefully and recovering\n    quickly.\n  \n    Level of risk exposed if this best practice\n      is not established: High\n  \n    \n    Implementation guidance \n    \n    \n    \n      AWS services, such as Elastic Load Balancing and Amazon EC2 Auto Scaling, help distribute load across resources and Availability\n      Zones. Therefore, failure of an individual resource (such as an\n      EC2 instance) or impairment of an Availability Zone can be\n      mitigated by shifting traffic to remaining healthy resources.\n    \n    \n      For multi-Region workloads, designs are more complicated. For\n      example, cross-Region read replicas allow you to deploy your data\n      to multiple AWS Regions. However, failover is still required to\n      promote the read replica to primary and then point your traffic to\n      the new endpoint. Amazon Route 53, Amazon Application Recovery Controller (ARC), Amazon CloudFront, and\n      AWS Global Accelerator can help route traffic across AWS Regions.\n    \n    \n      AWS services, such as Amazon S3, Lambda, API Gateway, Amazon SQS, Amazon SNS,\n      Amazon SES, Amazon Pinpoint, Amazon ECR, AWS Certificate Manager, EventBridge, or Amazon DynamoDB, are\n      automatically deployed to multiple Availability Zones by AWS. In\n      case of failure, these AWS services automatically route traffic to\n      healthy locations. Data is redundantly stored in multiple\n      Availability Zones and remains available.\n    \n    \n      For Amazon RDS, Amazon Aurora, Amazon Redshift, Amazon EKS, or Amazon ECS, Multi-AZ is a\n      configuration option. AWS can direct traffic to the healthy\n      instance if failover is initiated. This failover action may be\n      taken by AWS or as required by the customer\n    \n    \n      For Amazon EC2 instances, Amazon Redshift, Amazon ECS tasks, or Amazon EKS pods, you choose which Availability Zones to deploy to. For\n      some designs, Elastic Load Balancing provides the solution to\n      detect instances in unhealthy zones and route traffic to the\n      healthy ones. Elastic Load Balancing can also route traffic to\n      components in your on-premises data center.\n    \n    \n      For Multi-Region traffic failover, rerouting can leverage Amazon\n      Route 53, Amazon Application Recovery Controller, AWS Global Accelerator, Route 53 Private DNS for VPCs, or\n      CloudFront to provide a way to define internet domains and assign\n      routing policies, including health checks, to route traffic to\n      healthy Regions. AWS Global Accelerator provides static IP\n      addresses that act as a fixed entry point to your application,\n      then route to endpoints in AWS Regions of your choosing, using the\n      AWS global network instead of the internet for better performance\n      and reliability.\n    \n     \n      \n      Implementation steps\n      \n      \n      \n         \n         \n         \n         \n         \n         \n         \n         \n      \n          \n            Create failover designs for all appropriate applications and\n            services. Isolate each architecture component and create\n            failover designs meeting RTO and RPO for each component.\n          \n        \n          \n            Configure lower environments (like development or test) with all services that are\n            required to have a failover plan. Deploy the solutions using\n            infrastructure as code (IaC) to ensure repeatability.\n          \n        \n          \n            Configure a recovery site such as a second Region to implement and test the failover\n            designs. If necessary, resources for testing can be\n            configured temporarily to limit additional costs.\n          \n        \n          \n            Determine which failover plans are automated by AWS, which\n            can be automated by a DevOps process, and which might be\n            manual. Document and measure each service's RTO and RPO.\n          \n        \n          \n            Create a failover playbook and include all steps to failover\n            each resource, application, and service.\n          \n        \n          \n            Create a failback playbook and include all steps to failback\n            (with timing) each resource, application, and service\n          \n        \n          \n            Create a plan to initiate and rehearse the playbook. Use\n            simulations and chaos testing to test the playbook steps and\n            automation.\n          \n        \n          \n            For location impairment (such as Availability Zone or AWS Region), ensure you have systems in place to fail over to\n            healthy resources in unimpaired locations. Check quota,\n            autoscaling levels, and resources running before failover\n            testing.\n          \n        \n     \n   \n    \n    Resources \n    \n    \n    \n    \n      Related Well-Architected best\n        practices:\n    \n    \n    \n       \n       \n    \n        \n          REL13-\n            Plan for DR\n        \n      \n        \n          REL10\n            - Use fault isolation to protect your workload\n        \n      \n    \n    \n      Related documents:\n    \n    \n    \n       \n       \n       \n       \n       \n       \n       \n       \n       \n       \n       \n       \n       \n       \n       \n       \n       \n       \n       \n       \n    \n        \n          Setting\n            RTO and RPO Targets\n        \n      \n        \n          Failover\n            using Route 53 Weighted routing\n        \n      \n        \n          Disaster Recovery\n            with Amazon Application Recovery Controller\n        \n      \n        \n          EC2\n            with autoscaling\n        \n      \n        \n          EC2\n            Deployments - Multi-AZ\n        \n      \n        \n          ECS\n            Deployments - Multi-AZ\n        \n      \n        \n          Switch\n            traffic using Amazon Application Recovery Controller\n        \n      \n        \n          Lambda\n            with an Application Load Balancer and Failover\n        \n      \n        \n          ACM\n            Replication and Failover\n        \n      \n        \n          Parameter\n            Store Replication and Failover\n        \n      \n        \n          ECR\n            cross region replication and Failover\n        \n      \n        \n          Secrets\n            manager cross region replication configuration\n        \n      \n        \n          Enable\n            cross region replication for EFS and Failover\n        \n      \n        \n          EFS\n            Cross Region Replication and Failover\n        \n      \n        \n          Networking\n            Failover\n        \n      \n        \n          S3\n            Endpoint failover using MRAP\n        \n      \n        \n          Create\n            cross region replication for S3\n        \n      \n        \n          Guidance for Cross Region Failover and Graceful Failback on AWS\n        \n      \n        \n          Failover\n            using multi-region global accelerator\n        \n      \n        \n          Failover\n            with DRS\n        \n      \n    \n    \n      Related examples:\n    \n    \n    \n       \n       \n    \n        \n          Disaster\n            Recovery on AWS\n        \n      \n        \n          Elastic\n            Disaster Recovery on AWS\n        \n      \n  \u003cdiv\u003e\u003cdiv\u003e\u003cdiv\u003e\u003cdiv id=\"js_error_message\"\u003e\u003cp\u003e\u003cimg src=\"https://d1ge0kk1l5kms0.cloudfront.net/images/G/01/webservices/console/warning.png\" alt=\"Warning\" /\u003e \u003cstrong\u003eJavascript is disabled or is unavailable in your browser.\u003c/strong\u003e\u003c/p\u003e\u003cp\u003eTo use the Amazon Web Services Documentation, Javascript must be enabled. Please refer to your browser's Help pages for instructions.\u003c/p\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003eDocument ConventionsREL11-BP01 Monitor all components of the workload to detect\n  failuresREL11-BP03 Automate healing on all layers",
  "REL11-BP03 Automate healing on all layers\n    Upon detection of a failure, use automated capabilities to perform\n    actions to remediate. Degradations may be automatically healed\n    through internal service mechanisms or require resources to be\n    restarted or removed through remediation actions.\n  \n    For self-managed applications and cross-Region healing, recovery\n    designs and automated healing processes can be pulled from\n    existing\n      best practices.\n  \n    The ability to restart or remove a resource is an important tool to\n    remediate failures. A best practice is to make services stateless\n    where possible. This prevents loss of data or availability on\n    resource restart. In the cloud, you can (and generally should)\n    replace the entire resource (for example, a compute instance or\n    serverless function) as part of the restart. The restart itself is a\n    simple and reliable way to recover from failure. Many different\n    types of failures occur in workloads. Failures can occur in\n    hardware, software, communications, and operations.\n  \n    Restarting or retrying also applies to network requests. Apply the\n    same recovery approach to both a network timeout and a dependency\n    failure where the dependency returns an error. Both events have a\n    similar effect on the system, so rather than attempting to make\n    either event a special case, apply a similar strategy of limited\n    retry with exponential backoff and jitter. Ability to restart is a\n    recovery mechanism featured in recovery-oriented computing and high\n    availability cluster architectures.\n  \n    Desired outcome: Automated actions are performed to remediate detection of a failure.\n  \n    Common anti-patterns:\n  \n     \n     \n     \n     \n     \n     \n     \n  \n      \n        Provisioning resources without autoscaling.\n      \n    \n      \n        Deploying applications in instances or containers individually.\n      \n    \n      \n        Deploying applications that cannot be deployed into multiple\n        locations without using automatic recovery.\n      \n    \n      \n        Manually healing applications that automatic scaling and\n        automatic recovery fail to heal.\n      \n    \n      \n        No automation to failover databases.\n      \n    \n      \n        Lack automated methods to reroute traffic to new endpoints.\n      \n    \n      \n        No storage replication.\n      \n    \n    Benefits of establishing this best\n      practice: Automated healing can reduce your mean time to recovery and improve\n    your availability.\n  \n    Level of risk exposed if this best practice\n      is not established: High\n  \n    \n    Implementation guidance \n    \n      Designs for Amazon EKS or other Kubernetes services should include both\n      minimum and maximum replica or stateful sets and the minimum\n      cluster and node group sizing. These mechanisms provide a minimum\n      amount of continually-available processing resources while\n      automatically remediating any failures using the Kubernetes\n      control plane.\n    \n    \n      Design patterns that are accessed through a load balancer using\n      compute clusters should leverage Auto Scaling groups. Elastic Load Balancing (ELB) automatically distributes incoming\n      application traffic across multiple targets and virtual\n      appliances in one or more Availability Zones (AZs).\n    \n    \n      Clustered compute-based designs that do not use load balancing\n      should have their size designed for loss of at least one node.\n      This will allow for the service to maintain itself running in\n      potentially reduced capacity while it's recovering a new node.\n      Example services are Mongo, DynamoDB Accelerator, Amazon Redshift, Amazon EMR,\n      Cassandra, Kafka, MSK-EC2, Couchbase, ELK, and Amazon OpenSearch Service.\n      Many of these services can be designed with additional auto\n      healing features. Some cluster technologies must generate an\n      alert upon the loss a node triggering an automated or manual\n      workflow to recreate a new node. This workflow can be automated\n      using AWS Systems Manager to remediate issues quickly.\n    \n    \n      Amazon EventBridge can be used to monitor and filter for events\n      such as CloudWatch alarms or changes in state in other AWS\n      services. Based on event information, it can then invoke AWS Lambda, Systems Manager Automation, or other targets to run\n      custom remediation logic on your workload. Amazon EC2 Auto Scaling can be configured to check for EC2 instance health. If\n      the instance is in any state other than running, or if the\n      system status is impaired, Amazon EC2 Auto Scaling considers the\n      instance to be unhealthy and launches a replacement instance.\n      For large-scale replacements (such as the loss of an entire\n      Availability Zone), static stability is preferred for high\n      availability.\n    \n    \n     \n      \n      Implementation steps\n      \n         \n         \n         \n         \n         \n         \n      \n          \n            Use Auto Scaling groups to deploy tiers in a workload.\n            Auto Scaling can perform self-healing on stateless\n            applications and add or remove capacity.\n          \n        \n          \n            For compute instances noted previously, use\n            load\n              balancing and choose the appropriate type of load\n            balancer.\n          \n        \n          \n            Consider healing for Amazon RDS. With standby instances, configure\n            for\n            auto\n              failover to the standby instance. For Amazon RDS Read\n            Replica, automated workflow is required to make a read\n            replica primary.\n          \n        \n          \n            Implement\n            automatic\n              recovery on EC2 instances that have applications\n            deployed that cannot be deployed in multiple locations, and\n            can tolerate rebooting upon failures. Automatic recovery can\n            be used to replace failed hardware and restart the instance\n            when the application is not capable of being deployed in\n            multiple locations. The instance metadata and associated IP\n            addresses are kept, as well as the\n            EBS volumes and mount points to\n            Amazon Elastic File System or\n            File\n              Systems for Lustre and\n            Windows.\n            Using\n            AWS OpsWorks, you can configure automatic healing of EC2\n            instances at the layer level.\n          \n        \n          \n            Implement automated recovery using\n            AWS Step Functions and\n            AWS Lambda when you cannot use automatic scaling or\n            automatic recovery, or when automatic recovery fails. When\n            you cannot use automatic scaling, and either cannot use\n            automatic recovery or automatic recovery fails, you can\n            automate the healing using AWS Step Functions and AWS Lambda.\n          \n        \n          \n            Amazon EventBridge can be used to monitor and filter for\n            events such as\n            CloudWatch\n              alarms or changes in state in other AWS services.\n            Based on event information, it can then invoke AWS Lambda\n            (or other targets) to run custom remediation logic on your\n            workload.\n          \n        \n     \n   \n    \n    Resources \n    \n    \n    \n    \n      Related best practices:\n    \n    \n    \n       \n       \n    \n        \n          Availability\n            Definition\n        \n      \n        \n          REL11-BP01\n            Monitor all components of the workload to detect\n            failures\n        \n      \n    \n    \n      Related documents:\n    \n    \n    \n       \n       \n       \n       \n       \n       \n       \n       \n       \n       \n       \n       \n       \n       \n    \n        \n          How\n            AWS Auto Scaling Works\n        \n      \n        \n          Amazon EC2 Automatic Recovery\n        \n      \n        \n          Amazon Elastic Block Store (Amazon EBS)\n        \n      \n        \n          Amazon Elastic File System (Amazon EFS)\n        \n      \n        \n          What\n            is Amazon FSx for Lustre?\n        \n      \n        \n          What\n            is Amazon FSx for Windows File Server?\n        \n      \n        \n          AWS OpsWorks: Using Auto Healing to Replace Failed\n            Instances\n        \n      \n        \n          What\n            is AWS Step Functions?\n        \n      \n        \n          What\n            is AWS Lambda?\n        \n      \n        \n          What\n            Is Amazon EventBridge?\n        \n      \n        \n          Using\n            Amazon CloudWatch Alarms\n        \n      \n        \n          Amazon RDS\n            Failover\n        \n      \n        \n          SSM\n            - Systems Manager Automation\n        \n      \n        \n          Resilient\n            Architecture Best Practices\n        \n      \n    \n    \n      Related videos:\n    \n    \n    \n       \n       \n    \n        \n          Automatically\n            Provision and Scale OpenSearch Service\n        \n      \n        \n          Amazon RDS\n            Failover Automatically\n        \n      \n    \n    \n      Related examples:\n    \n    \n    \n       \n    \n        \n          Amazon RDS\n            Failover Workshop\n        \n      \n    \n    \n      Related tools:\n    \n    \n    \n       \n       \n    \n        \n          CloudWatch\n        \n      \n        \n          CloudWatch\n            X-Ray\n        \n      \n  \u003cdiv\u003e\u003cdiv\u003e\u003cdiv\u003e\u003cdiv id=\"js_error_message\"\u003e\u003cp\u003e\u003cimg src=\"https://d1ge0kk1l5kms0.cloudfront.net/images/G/01/webservices/console/warning.png\" alt=\"Warning\" /\u003e \u003cstrong\u003eJavascript is disabled or is unavailable in your browser.\u003c/strong\u003e\u003c/p\u003e\u003cp\u003eTo use the Amazon Web Services Documentation, Javascript must be enabled. Please refer to your browser's Help pages for instructions.\u003c/p\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003eDocument ConventionsREL11-BP02 Fail over to healthy resourcesREL11-BP04 Rely on the data plane and not the control plane\n  during recovery",
  "REL11-BP04 Rely on the data plane and not the control plane\n  during recovery\n    Control planes provide the administrative APIs used to create, read and describe, update, delete, and list (CRUDL) resources, while data planes handle day-to-day service traffic. When implementing recovery or mitigation responses to potentially resiliency-impacting events, focus on using a minimal number of control plane operations to recover, rescale, restore, heal, or failover the service. Data plane action should supersede any activity during these degradation events.\n  \n    For example, the following are all control plane actions: launching a new compute instance, creating block storage, and describing queue services. When you launch compute instances, the control plane has to perform multiple tasks like finding a physical host with capacity, allocating network interfaces, preparing local block storage volumes, generating credentials, and adding security rules. Control planes tend to be complicated orchestration.\n  \n    Desired outcome: When a resource enters an impaired state, the system is capable of automatically or manually recovering by shifting traffic from impaired to healthy resources.\n  \n    Common anti-patterns:\n  \n     \n     \n     \n  \n      \n        Dependence on changing DNS records to re-route traffic.\n      \n    \n      \n        Dependence on control-plane scaling operations to replace impaired components due to insufficiently provisioned resources.\n      \n    \n      \n        Relying on extensive, multi service, multi-API control plane actions to remediate any category of impairment.\n      \n    \n    Benefits of establishing this best practice: Increased success rate for automated remediation can reduce your mean time to recovery and improve availability of the workload.\n  \n    Level of risk exposed if this best practice\n      is not established: Medium:  For certain types of service degradations, control planes are affected. Dependencies on extensive use of the control plane for remediation may increase recovery time (RTO) and mean time to recovery (MTTR).\n  \n\n  Implementation guidance\n    \n      To limit data plane actions, assess each service for what actions are required to restore service. \n    \n    \n      Leverage Amazon Application Recovery Controller to shift the DNS traffic. These features continually monitor your application’s ability to recover from failures and allow you to control your application recovery across multiple AWS Regions, Availability Zones, and on premises. \n    \n    \n      Route 53 routing policies use the control plane, so do not rely on it for recovery. The Route 53 data planes answer DNS queries and perform and evaluate health checks. They are globally distributed and designed for a 100% availability service level agreement (SLA).\n    \n    \n      The Route 53 management APIs and consoles where you create, update, and delete Route 53 resources run on control planes that are designed to prioritize the strong consistency and durability that you need when managing DNS. To achieve this, the control planes are located in a single Region: US East (N. Virginia). While both systems are built to be very reliable, the control planes are not included in the SLA. There could be rare events in which the data plane’s resilient design allows it to maintain availability while the control planes do not. For disaster recovery and failover mechanisms, use data plane functions to provide the best possible reliability. \n    \n    \n      Design your compute infrastructure to be statically stable to avoid using the control plane during an incident. For example, if you are using Amazon EC2 instances, avoid provisioning new instances manually or instructing Auto Scaling Groups to add instances in response. For the highest levels of resilience, provision sufficient capacity in the cluster used for failover. If this capacity threshold must be limited, set throttles on the overall end-to-end system to safely limit the total traffic reaching the limited set of resources.\n    \n    \n      For services like Amazon DynamoDB, Amazon API Gateway, load balancers, and AWS Lambda serverless, using those services leverages the data plane. However, creating new functions, load balancers, API gateways, or DynamoDB tables is a control plane action and should be completed before the degradation as preparation for an event and rehearsal of failover actions. For Amazon RDS, data plane actions allow for access to data.\n    \n    \n      For more information about data planes, control planes, and how AWS builds services to meet high availability targets, see Static stability using Availability Zones.\n    \n    \n      Understand which operations are on the data plane and which are on the control plane. \n    \n     \n      \n      Implementation steps\n      \n        For each workload that needs to be restored after a degradation event, evaluate the failover runbook, high availability design, auto healing design, or HA resource restoration plan. Identity each action that might be considered a control plane action.\n      \n     \n    \n      Consider changing the control action to a data plane action:\n    \n    \n       \n       \n       \n    \n        Auto Scaling (control plane) to pre-scaled Amazon EC2 resources (data plane)\n      \n        Amazon EC2 instance scaling (control plane) to AWS Lambda scaling (data plane)\n      \n        \n          Assess any designs using Kubernetes and the nature of the control plane actions. Adding pods is a data plane action in Kubernetes. Actions should be limited to adding pods and not adding nodes. Using over-provisioned nodes is the preferred method to limit control plane actions\n        \n      \n    \n      Consider alternate approaches that allow for data plane actions to affect the same remediation.\n    \n    \n       \n       \n    \n        \n          Route 53 Record change (control plane) or Amazon Application Recovery Controller (data plane)\n        \n      \n        \n          Route 53 Health checks for more automated updates\n        \n      \n    \n      Consider some services in a secondary Region, if the service is mission critical, to allow for more control plane and data plane actions in an unaffected Region.\n    \n    \n       \n       \n    \n        \n          Amazon EC2 Auto Scaling or Amazon EKS in a primary Region compared to Amazon EC2 Auto Scaling or Amazon EKS in a secondary Region and routing traffic to secondary Region (control plane action)\n        \n      \n        \n          Make read replica in secondary primary or attempting same action in primary Region (control plane action)\n        \n      \n   \n\n  Resources\n\n    \n      Related best practices:\n    \n    \n       \n       \n    \n        \n          Availability\n            Definition\n        \n      \n        \n          REL11-BP01\n            Monitor all components of the workload to detect\n            failures\n        \n      \n    \n      Related documents:\n    \n    \n       \n       \n       \n       \n       \n       \n       \n       \n       \n       \n       \n    \n        \n          APN\n          Partner: partners that can help with automation of your fault\n          tolerance\n        \n      \n        \n          AWS Marketplace: products that can be used for fault\n          tolerance\n        \n      \n        \n          Amazon\n          Builders' Library: Avoiding overload in distributed systems by\n          putting the smaller service in control\n        \n      \n        \n          Amazon DynamoDB API (control plane and data plane)\n        \n      \n        \n          AWS Lambda Executions (split into the control plane and the data plane)\n        \n      \n        \n          AWS Elemental MediaStore Data Plane\n        \n      \n        \n          Building\n          highly resilient applications using Amazon Application Recovery Controller, Part 1: Single-Region stack\n        \n      \n        \n          Building\n          highly resilient applications using Amazon Application Recovery Controller, Part 2: Multi-Region\n          stack\n        \n      \n        \n          Creating\n          Disaster Recovery Mechanisms Using Amazon Route 53\n        \n      \n        \n          What\n          is Amazon Application Recovery Controller\n        \n      \n        \n          Kubernetes Control Plane and data plane\n        \n      \n    \n      Related videos:\n    \n    \n       \n       \n    \n        \n          Back to Basics - Using Static Stability\n        \n      \n        \n          Building resilient multi-site workloads using AWS global services\n        \n      \n    \n      Related examples:\n    \n    \n       \n       \n       \n       \n       \n    \n        \n          Introducing\n          Amazon Application Recovery Controller\n        \n      \n        \n          Amazon Builders' Library: Avoiding overload in distributed systems by putting the smaller service in control\n        \n      \n        \n          Building highly resilient applications using Amazon Application Recovery Controller, Part 1: Single-Region stack\n        \n      \n        \n          Building highly resilient applications using Amazon Application Recovery Controller, Part 2: Multi-Region stack\n        \n      \n        \n          Static stability using Availability Zones\n        \n      \n    \n      Related tools:\n    \n    \n       \n       \n    \n        \n          Amazon CloudWatch\n        \n      \n        AWS X-Ray\n      \n  \u003cdiv\u003e\u003cdiv\u003e\u003cdiv\u003e\u003cdiv id=\"js_error_message\"\u003e\u003cp\u003e\u003cimg src=\"https://d1ge0kk1l5kms0.cloudfront.net/images/G/01/webservices/console/warning.png\" alt=\"Warning\" /\u003e \u003cstrong\u003eJavascript is disabled or is unavailable in your browser.\u003c/strong\u003e\u003c/p\u003e\u003cp\u003eTo use the Amazon Web Services Documentation, Javascript must be enabled. Please refer to your browser's Help pages for instructions.\u003c/p\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003eDocument ConventionsREL11-BP03 Automate healing on all layersREL11-BP05 Use static stability to prevent bimodal\n  behavior",
  "REL11-BP05 Use static stability to prevent bimodal\n  behavior\n    Workloads should be statically stable and only operate in a single\n    normal mode. Bimodal behavior is when your workload exhibits\n    different behavior under normal and failure modes.\n  \n    For example, you might try and recover from an Availability Zone\n    failure by launching new instances in a different Availability Zone.\n    This can result in a bimodal response during a failure mode. You\n    should instead build workloads that are statically stable and\n    operate within only one mode. In this example, those instances\n    should have been provisioned in the second Availability Zone before\n    the failure. This static stability design verifies that the workload\n    only operates in a single mode.\n  \n    Desired outcome: Workloads do not exhibit bimodal behavior during normal and failure\n    modes.\n  \n    Common anti-patterns:\n  \n     \n     \n     \n     \n  \n      \n        Assuming resources can always be provisioned regardless of the\n        failure scope.\n      \n    \n      \n        Trying to dynamically acquire resources during a failure.\n      \n    \n      \n        Not provisioning adequate resources across zones or Regions until a\n        failure occurs.\n      \n    \n      \n        Considering static stable designs for compute resources only.\n      \n    \n    Benefits of establishing this best\n      practice: Workloads running with statically stable designs are capable of\n    having predictable outcomes during normal and failure events.\n  \n    Level of risk exposed if this best practice\n      is not established: Medium\n  \n    \n    Implementation guidance \n    \n    \n    \n      Bimodal behavior occurs when your workload exhibits different\n      behavior under normal and failure modes (for example, relying on\n      launching new instances if an Availability Zone fails). An example\n      of bimodal behavior is when stable Amazon EC2 designs provision enough\n      instances in each Availability Zone to handle the workload load if\n      one AZ were removed. Elastic Load Balancing or Amazon Route 53\n      health would check to shift a load away from the impaired\n      instances. After traffic has shifted, use AWS Auto Scaling to\n      asynchronously replace instances from the failed zone and launch\n      them in the healthy zones. Static stability for compute deployment\n      (such as EC2 instances or containers) results in the highest\n      reliability.\n    \n    \n       \n        \n       \n       \n      Static stability of EC2 instances across\n        Availability Zones\n      \n    \n    \n      This must be weighed against the cost for this model and the\n      business value of maintaining the workload under all resilience\n      cases. It's less expensive to provision less compute capacity and\n      rely on launching new instances in the case of a failure, but for\n      large-scale failures (such as an Availability Zone or Regional\n      impairment), this approach is less effective because it relies on\n      both an operational plane, and sufficient resources being\n      available in the unaffected zones or Regions.\n    \n    \n      Your solution should also weigh reliability against the costs\n      needs for your workload. Static stability architectures apply to a\n      variety of architectures including compute instances spread across\n      Availability Zones, database read replica designs, Kubernetes\n      (Amazon EKS) cluster designs, and multi-Region failover architectures.\n    \n    \n      It is also possible to implement a more statically stable design\n      by using more resources in each zone. By adding more zones, you\n      reduce the amount of additional compute you need for static\n      stability.\n    \n    \n      An example of bimodal behavior would be a network timeout that\n      could cause a system to attempt to refresh the configuration state\n      of the entire system. This would add an unexpected load to another\n      component and might cause it to fail, resulting in other\n      unexpected consequences. This negative feedback loop impacts the\n      availability of your workload. Instead, you can build systems that\n      are statically stable and operate in only one mode. A statically\n      stable design would do constant work and always refresh the\n      configuration state on a fixed cadence. When a call fails, the\n      workload would use the previously cached value and initiate an\n      alarm.\n    \n    \n      Another example of bimodal behavior is allowing clients to bypass\n      your workload cache when failures occur. This might seem to be a\n      solution that accommodates client needs but it can significantly\n      change the demands on your workload and is likely to result in\n      failures.\n    \n    \n      Assess critical workloads to determine what workloads require\n      this type of resilience design. For those that are deemed\n      critical, each application component must be reviewed. Example\n      types of services that require static stability evaluations are:\n    \n    \n       \n       \n       \n       \n    \n        \n          Compute: Amazon EC2, EKS-EC2,\n          ECS-EC2, EMR-EC2\n        \n      \n        \n          Databases: Amazon Redshift, Amazon RDS,\n          Amazon Aurora\n        \n      \n        \n          Storage: Amazon S3 (Single Zone),\n          Amazon EFS (mounts), Amazon FSx (mounts)\n        \n      \n        \n          Load balancers: Under certain\n          designs\n        \n      \n     \n      \n      Implementation steps \n      \n         \n         \n         \n         \n      \n          \n            Build systems that are statically stable and operate in\n            only one mode. In this case, provision enough instances in\n            each Availability Zone or Region to handle the workload\n            capacity if one Availability Zone or Region were removed.\n            A variety of services can be used for routing to healthy\n            resources, such as:\n          \n          \n             \n             \n             \n             \n          \n              \n                Cross\n                  Region DNS Routing\n              \n            \n              \n                MRAP\n                  Amazon S3 MultiRegion Routing\n              \n            \n              \n                AWS Global Accelerator\n              \n            \n              \n                Amazon Application Recovery Controller\n              \n            \n        \n          \n          \n            Configure\n            database\n              read replicas to account for the loss of a single\n            primary instance or a read replica. If traffic is being\n            served by read replicas, the quantity in each Availability\n            Zone and each Region should equate to the overall need in\n            case of the zone or Region failure.\n          \n          \n        \n          \n          \n            Configure critical data in Amazon S3 storage that is designed to\n            be statically stable for data stored in case of an\n            Availability Zone failure. If\n            Amazon S3\n              One Zone-IA storage class is used, this should not\n            be considered statically stable, as the loss of that zone\n            minimizes access to this stored data.\n          \n          \n        \n          \n          \n            Load\n              balancers are sometimes configured incorrectly or\n            by design to service a specific Availability Zone. In this\n            case, the statically stable design might be to spread a\n            workload across multiple AZs in a more complex design. The\n            original design may be used to reduce interzone traffic\n            for security, latency, or cost reasons.\n          \n          \n        \n     \n   \n    \n    Resources \n    \n      Related Well-Architected best\n        practices:\n       \n    \n       \n       \n       \n    \n        \n          Availability\n            Definition\n        \n      \n        \n          REL11-BP01\n            Monitor all components of the workload to detect\n            failures\n        \n      \n        \n          REL11-BP04\n            Rely on the data plane and not the control plane during\n            recovery\n        \n        \n    \n      Related documents:\n    \n    \n       \n       \n       \n       \n       \n       \n       \n       \n       \n       \n       \n       \n    \n        \n          Minimizing\n            Dependencies in a Disaster Recovery Plan\n        \n      \n        \n          The\n            Amazon Builders' Library: Static stability using Availability\n            Zones\n        \n      \n        \n          Fault\n            Isolation Boundaries\n        \n      \n        \n          Static\n            stability using Availability Zones\n        \n      \n        \n          Multi-Zone RDS\n        \n      \n        \n          Minimizing\n            Dependencies in a Disaster Recovery Plan\n        \n      \n        \n          Cross\n            Region DNS Routing\n        \n      \n        \n          MRAP\n            Amazon S3 MultiRegion Routing\n        \n      \n        \n          AWS Global Accelerator\n        \n      \n        \n          Amazon Application Recovery Controller\n        \n      \n        \n          Single\n            Zone Amazon S3\n        \n      \n        \n          Cross\n            Zone Load Balancing\n        \n      \n    \n    \n      Related videos:\n    \n    \n    \n       \n    \n        \n          Static\n            stability in AWS: AWS re:Invent 2019: Introducing The Amazon\n            Builders' Library (DOP328)\n        \n      \n  \u003cdiv\u003e\u003cdiv\u003e\u003cdiv\u003e\u003cdiv id=\"js_error_message\"\u003e\u003cp\u003e\u003cimg src=\"https://d1ge0kk1l5kms0.cloudfront.net/images/G/01/webservices/console/warning.png\" alt=\"Warning\" /\u003e \u003cstrong\u003eJavascript is disabled or is unavailable in your browser.\u003c/strong\u003e\u003c/p\u003e\u003cp\u003eTo use the Amazon Web Services Documentation, Javascript must be enabled. Please refer to your browser's Help pages for instructions.\u003c/p\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003eDocument ConventionsREL11-BP04 Rely on the data plane and not the control plane\n  during recoveryREL11-BP06 Send notifications when events impact\n  availability",
  "REL11-BP06 Send notifications when events impact\n  availability\n    Notifications are sent upon the detection of thresholds breached,\n    even if the event causing the issue was automatically resolved.\n  \n    Automated healing allows your workload to be reliable. However, it\n    can also obscure underlying problems that need to be addressed.\n    Implement appropriate monitoring and events so that you can detect\n    patterns of problems, including those addressed by auto healing, so\n    that you can resolve root cause issues.\n  \n    Resilient systems are designed so that degradation events are\n    immediately communicated to the appropriate teams. These\n    notifications should be sent through one or many communication\n    channels.\n  \n    Desired outcome: Alerts are\n    immediately sent to operations teams when thresholds are breached,\n    such as error rates, latency, or other critical key performance\n    indicator (KPI) metrics, so that these issues are resolved as soon as\n    possible and user impact is avoided or minimized.\n  \n    Common anti-patterns:\n  \n     \n     \n     \n     \n     \n     \n  \n      \n        Sending too many alarms.\n      \n    \n      \n        Sending alarms that are not actionable.\n      \n    \n      \n        Setting alarm thresholds too high (over sensitive) or too low\n        (under sensitive).\n      \n    \n      \n        Not sending alarms for external dependencies.\n      \n    \n      \n        Not considering\n        gray\n          failures when designing monitoring and alarms.\n      \n    \n      \n        Performing healing automation, but not notifying the appropriate\n        team that healing was needed.\n      \n    \n    Benefits of establishing this best\n      practice: Notifications of recovery make operational and\n    business teams aware of service degradations so that they can react\n    immediately to minimize both mean time to detect (MTTD) and mean\n    time to repair (MTTR). Notifications of recovery events also assure\n    that you don't ignore problems that occur infrequently.\n  \n    Level of risk exposed if this best practice\n      is not established: Medium. Failure to implement\n    appropriate monitoring and events notification mechanisms can result\n    in failure to detect patterns of problems, including those addressed\n    by auto healing. A team will only be made aware of system\n    degradation when users contact customer service or by chance.\n  \n    \n    Implementation guidance \n    \n    \n    \n      When defining a monitoring strategy, a triggered alarm is a common\n      event. This event would likely contain an identifier for the\n      alarm, the alarm state (such as IN ALARM or OK), and details of\n      what triggered it. In many cases, an alarm event should be\n      detected and an email notification sent. This is an example of an\n      action on an alarm. Alarm notification is critical in\n      observability, as it informs the right people that there is an\n      issue. However, when action on events mature in your observability\n      solution, it can automatically remediate the issue without the\n      need for human intervention.\n    \n    \n      Once KPI-monitoring alarms have been established, alerts should be\n      sent to appropriate teams when thresholds are exceeded. Those\n      alerts may also be used to trigger automated processes that will\n      attempt to remediate the degradation.\n    \n    \n      For more complex threshold monitoring, composite alarms should be\n      considered. Composite alarms use a number of KPI-monitoring alarms\n      to create an alert based on operational business logic. CloudWatch\n      Alarms can be configured to send emails, or to log incidents in\n      third-party incident tracking systems using Amazon SNS integration\n      or Amazon EventBridge.\n    \n     \n      \n      Implementation steps \n      \n      \n      \n        Create various types of alarms based on how the workloads are\n        monitored, such as:\n      \n      \n         \n         \n         \n         \n         \n         \n               \n          \n            Application alarms are used to detect when\n            any part of your workload is not working properly.\n                   \n                \n          \n            Infrastructure\n              alarms indicate when to scale resources. Alarms\n            can be visually displayed on dashboards, send alerts\n            through Amazon SNS or email, and work with Auto Scaling to\n            scale workload resources in or out.\n                   \n                 \n          \n            Simple\n            static\n              alarms can be created to monitor when a\n            metric breaches a static threshold for a specified number\n            of evaluation periods.\n                   \n                  \n          \n            Composite\n              alarms can account for complex alarms from multiple sources.\n                   \n                \n          \n            Once the alarm has been created,\n            create appropriate notification events. You can directly\n            invoke an\n            Amazon SNS\n              API to send notifications and link any automation\n            for remediation or communication.\n                   \n        \n          \n            Stay informed about service degradations with AWS Health. Create purpose-fit AWS Health event notifications to e-mail and chat channels through AWS User Notifications and integrate programmatically with your monitoring and alerting tools through Amazon EventBridge.\n          \n        \n     \n   \n    \n    Resources \n    \n    \n    \n    \n      Related Well-Architected best\n        practices:\n    \n    \n    \n       \n    \n        \n          Availability\n            Definition\n        \n      \n    \n    \n      Related documents:\n    \n    \n    \n       \n       \n       \n       \n       \n       \n       \n    \n        \n          Creating\n            a CloudWatch Alarm Based on a Static Threshold\n        \n      \n        \n          What\n            Is Amazon EventBridge?\n        \n      \n        \n          What\n            is Amazon Simple Notification Service?\n        \n      \n        \n          Publishing\n            Custom Metrics\n        \n      \n        \n          Using\n            Amazon CloudWatch Alarms\n        \n      \n        \n          Setup\n            CloudWatch Composite alarms\n        \n      \n        \n          What's\n            new in AWS Observability at re:Invent 2022\n        \n      \n    \n      Related tools:\n    \n    \n       \n       \n    \n        \n          CloudWatch\n        \n      \n        \n          CloudWatch\n            X-Ray\n        \n      \n  \u003cdiv\u003e\u003cdiv\u003e\u003cdiv\u003e\u003cdiv id=\"js_error_message\"\u003e\u003cp\u003e\u003cimg src=\"https://d1ge0kk1l5kms0.cloudfront.net/images/G/01/webservices/console/warning.png\" alt=\"Warning\" /\u003e \u003cstrong\u003eJavascript is disabled or is unavailable in your browser.\u003c/strong\u003e\u003c/p\u003e\u003cp\u003eTo use the Amazon Web Services Documentation, Javascript must be enabled. Please refer to your browser's Help pages for instructions.\u003c/p\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003eDocument ConventionsREL11-BP05 Use static stability to prevent bimodal\n  behaviorREL11-BP07 Architect your product to meet availability targets and uptime service level agreements (SLAs)",
  "REL11-BP07 Architect your product to meet availability targets and uptime service level agreements (SLAs)Architect your product to meet availability targets and uptime service level agreements (SLAs). If you publish or privately agree to availability targets or uptime SLAs, verify that your architecture and operational processes are designed to support them. \n    Desired outcome: Each application has a defined target for availability and SLA for performance metrics, which can be monitored and maintained in order to meet business outcomes.\n  \n    Common anti-patterns:\n  \n     \n     \n     \n     \n  \n      \n        Designing and deploying workload’s without setting any SLAs.\n      \n    \n      \n        SLA metrics are set too high without rationale or business requirements.\n      \n    \n      \n        Setting SLAs without taking into account for dependencies and their underlying SLA.\n      \n    \n      \n        Application designs are created without considering the Shared Responsibility Model for Resilience.\n      \n    \n    Benefits of establishing this best practice: Designing applications based on key resiliency targets helps you meet business objectives and customer expectations. These objectives help drive the application design process that evaluates different technologies and considers various tradeoffs. \n  \n    Level of risk exposed if this best practice\n      is not established: Medium\n  \n    Implementation guidance\n    \n      Application designs have to account for a diverse set of requirements that are derived from business, operational, and financial objectives. Within the operational requirements, workloads need to have specific resilience metric targets so they can be properly monitored and supported. Resilience metrics should not be set or derived after deploying the workload. They should be defined during the design phase and help guide various decisions and tradeoffs. \n    \n    \n       \n       \n       \n       \n       \n       \n       \n    \n        \n          Every workload should have its own set of resilience metrics. Those metrics may be different from other business applications. \n        \n      \n        \n          Reducing dependencies can have a positive impact on availability. Each workload should consider its dependencies and their SLAs. In general, select dependencies with availability goals equal to or greater than the goals of your workload. \n        \n      \n        \n          Consider loosely coupled designs so your workload can operate correctly despite dependency impairment, where possible. \n        \n      \n        \n          Reduce control plane dependencies, especially during recovery or a degradation. Evaluate designs that are statically stable for mission critical workloads. Use resource sparing to increase the availability of those dependencies in a workload.\n        \n      \n        \n          Observability and instrumentation are critical for achieving SLAs by reducing Mean Time to Detection (MTTD) and Mean Time to Repair (MTTR). \n        \n      \n        \n          Less frequent failure (longer MTBF), shorter failure detection times (shorter MTTD), and shorter repair times (shorter MTTR) are the three factors that are used to improve availability in distributed systems. \n        \n      \n        \n          Establishing and meeting resilience metrics for a workload is foundational to any effective design. Those designs must factor in tradeoffs of design complexity, service dependencies, performance, scaling, and costs.\n        \n      \n    \n      Implementation steps\n    \n    \n       \n       \n       \n       \n       \n    \n        \n          Review and document the workload design considering the following questions:\n        \n        \n           \n           \n           \n           \n           \n           \n        \n            \n              Where are control planes used in the workload?\n            \n          \n            \n              How does the workload implement fault tolerance?\n            \n          \n            \n              What are the design patterns for scaling, automatic scaling, redundancy, and highly available components?\n            \n          \n            \n              What are the requirements for data consistency and availability?\n            \n          \n            \n              Are there considerations for resource sparing or resource static stability?\n            \n          \n            \n              What are the service dependencies?\n            \n          \n      \n        \n          Define SLA metrics based on the workload architecture while working with stakeholders.  Consider the SLAs of all dependencies used by the workload.\n        \n      \n        \n          Once the SLA target has been set, optimize the architecture to meet the SLA.\n        \n      \n        \n          Once the design is set that will meet the SLA, implement operational changes, process automation, and runbooks that also will have focus on reducing MTTD and MTTR.\n        \n      \n        \n          Once deployed, monitor and report on the SLA.\n        \n      \n   \n    \n    Resources\n    \n      Related best practices:\n    \n    \n       \n       \n       \n       \n       \n       \n       \n    \n        \n          REL03-BP01 Choose how to segment your workload\n        \n      \n        \n          REL10-BP01 Deploy the workload to multiple locations\n        \n      \n        \n          REL11-BP01 Monitor all components of the workload to detect\n  failures\n        \n      \n        \n          REL11-BP03 Automate healing on all layers\n        \n      \n        \n          REL12-BP04 Test resiliency using chaos engineering\n        \n      \n        \n          REL13-BP01 Define recovery objectives for downtime and data\n  loss\n        \n      \n        \n          Understanding workload health\n        \n      \n    \n      Related documents:\n    \n    \n       \n       \n       \n       \n       \n       \n       \n       \n       \n       \n    \n        \n          Availability with redundancy\n        \n      \n        \n          Reliability pillar - Availability\n        \n      \n        \n          Measuring availability\n        \n      \n        AWS Fault Isolation Boundaries\n        \n      \n        \n          Shared Responsibility Model for Resiliency\n        \n      \n        \n          Static stability using Availability Zones\n        \n      \n        AWS Service Level Agreements (SLAs)\n        \n      \n        \n          Guidance for Cell-based Architecture on AWS\n      \n        AWS infrastructure\n        \n      \n        \n          Advanced Multi-AZ Resilience Patterns whitepaper\n        \n      \n    \n      Related services:\n    \n    \n       \n       \n       \n    \n        \n          Amazon CloudWatch\n        \n      \n        AWS Config\n      \n        AWS Trusted Advisor\n      \n  \u003cdiv\u003e\u003cdiv\u003e\u003cdiv\u003e\u003cdiv id=\"js_error_message\"\u003e\u003cp\u003e\u003cimg src=\"https://d1ge0kk1l5kms0.cloudfront.net/images/G/01/webservices/console/warning.png\" alt=\"Warning\" /\u003e \u003cstrong\u003eJavascript is disabled or is unavailable in your browser.\u003c/strong\u003e\u003c/p\u003e\u003cp\u003eTo use the Amazon Web Services Documentation, Javascript must be enabled. Please refer to your browser's Help pages for instructions.\u003c/p\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003eDocument ConventionsREL11-BP06 Send notifications when events impact\n  availabilityREL 12. How do you test reliability?",
  "REL12-BP01 Use playbooks to investigate failures\n    Permit consistent and prompt responses to failure scenarios that are\n    not well understood, by documenting the investigation process in\n    playbooks. Playbooks are the predefined steps performed to identify\n    the factors contributing to a failure scenario. The results from any\n    process step are used to determine the next steps to take until the\n    issue is identified or escalated.\n  \n    The playbook is proactive planning that you must do, to be able to\n    take reactive actions effectively. When failure scenarios not\n    covered by the playbook are encountered in production, first address\n    the issue (put out the fire). Then go back and look at the steps you\n    took to address the issue and use these to add a new entry in the\n    playbook.\n  \n    Note that playbooks are used in response to specific incidents,\n    while runbooks are used to achieve specific outcomes. Often,\n    runbooks are used for routine activities and playbooks are used to\n    respond to non-routine events.\n  \n    Common anti-patterns:\n  \n     \n     \n     \n  \n      \n        Planning to deploy a workload without knowing the processes to\n        diagnose issues or respond to incidents.\n      \n    \n      \n        Unplanned decisions about which systems to gather logs and\n        metrics from when investigating an event.\n      \n    \n      \n        Not retaining metrics and events long enough to be able to\n        retrieve the data.\n      \n    \n    Benefits of establishing this best\n    practice: Capturing playbooks ensures that processes can\n    be consistently followed. Codifying your playbooks limits the\n    introduction of errors from manual activity. Automating playbooks\n    shortens the time to respond to an event by eliminating the\n    requirement for team member intervention or providing them\n    additional information when their intervention begins.\n  \n    Level of risk exposed if this best practice\n    is not established: High\n  \n\n  Implementation guidance\n\n   \n\n    Use playbooks to identify issues. Playbooks are documented\n    processes to investigate issues. Allow consistent and prompt\n    responses to failure scenarios by documenting processes in\n    playbooks. Playbooks must contain the information and guidance\n    necessary for an adequately skilled person to gather applicable\n    information, identify potential sources of failure, isolate\n    faults, and determine contributing factors (perform post-incident\n    analysis).\n  \n        \n           \n        \n             Implement playbooks as code. Perform your operations as code by scripting your\n              playbooks to ensure consistency and limit reduce errors caused by manual processes.\n              Playbooks can be composed of multiple scripts representing the different steps that\n              might be necessary to identify the contributing factors to an issue. Runbook\n              activities can be invoked or performed as part of playbook activities, or might prompt\n              to run a playbook in response to identified events. \n            \n               \n               \n               \n               \n               \n               \n            \n                \n                  Automate your operational playbooks with AWS Systems Manager\n                \n              \n                \n                  AWS Systems Manager\n                    Run Command\n                \n              \n                \n                  AWS\n                    Systems Manager Automation\n                \n              \n                \n                  What is\n                    AWS Lambda?\n                \n              \n                \n                  What Is\n                    Amazon EventBridge?\n                \n              \n                \n                  Using Amazon CloudWatch\n                    Alarms\n                \n              \n          \n    \n   \n\n  Resources\n\n      \n    \n      Related documents:\n    \n    \n       \n       \n       \n       \n       \n       \n       \n    \n        \n          AWS Systems Manager Automation\n        \n      \n        \n          AWS Systems Manager Run Command\n        \n      \n        \n          Automate\n          your operational playbooks with AWS Systems Manager\n        \n      \n        \n          Using\n          Amazon CloudWatch Alarms\n        \n      \n        \n          Using\n          Canaries (Amazon CloudWatch Synthetics)\n        \n      \n        \n          What\n          Is Amazon EventBridge?\n        \n      \n        \n          What\n          is AWS Lambda?\n        \n      \n    \n      Related examples:\n    \n    \n       \n    \n        \n          Automating\n          operations with Playbooks and Runbooks\n        \n      \n  \u003cdiv\u003e\u003cdiv\u003e\u003cdiv\u003e\u003cdiv id=\"js_error_message\"\u003e\u003cp\u003e\u003cimg src=\"https://d1ge0kk1l5kms0.cloudfront.net/images/G/01/webservices/console/warning.png\" alt=\"Warning\" /\u003e \u003cstrong\u003eJavascript is disabled or is unavailable in your browser.\u003c/strong\u003e\u003c/p\u003e\u003cp\u003eTo use the Amazon Web Services Documentation, Javascript must be enabled. Please refer to your browser's Help pages for instructions.\u003c/p\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003eDocument ConventionsREL 12. How do you test reliability?REL12-BP02 Perform post-incident analysis",
  "REL12-BP02 Perform post-incident analysis\n    Review customer-impacting events, and identify the contributing\n    factors and preventative action items. Use this information to\n    develop mitigations to limit or prevent recurrence. Develop\n    procedures for prompt and effective responses. Communicate\n    contributing factors and corrective actions as appropriate, tailored\n    to target audiences. Have a method to communicate these causes to\n    others as needed.\n  \n    Assess why existing testing did not find the issue. Add tests for\n    this case if tests do not already exist.\n  \n    Desired outcome: Your teams have a consistent and agreed upon approach to handling post-incident analysis. One mechanism is the correction of error (COE) process. The COE process helps your teams identify, understand, and address the root causes for incidents, while also building mechanisms and guardrails to limit the probability of the same incident happening again.\n  \n    Common anti-patterns:\n  \n     \n     \n     \n     \n     \n  \n      \n        Finding contributing factors, but not continuing to look deeper\n        for other potential problems and approaches to mitigate.\n      \n    \n      \n        Only identifying human error causes, and not providing any\n        training or automation that could prevent human errors.\n      \n    \n      \n        Focus on assigning blame rather than understanding the root cause, creating a culture of fear and hindering open communication\n      \n    \n      \n        Failure to share insights, which keeps incident analysis findings within a small group and prevents others from benefiting from the lessons learned\n      \n    \n      \n        No mechanism to capture institutional knowledge, thereby losing valuable insights by not preserving the lessons-learned in the form of updated best practices and resulting in repeat incidents with the same or similar root cause\n      \n    \n    Benefits of establishing this best\n    practice: Conducting post-incident analysis and sharing\n    the results permits other workloads to mitigate the risk if they\n    have implemented the same contributing factors, and allows them to\n    implement the mitigation or automated recovery before an incident\n    occurs.\n  \n    Level of risk exposed if this best practice\n    is not established: High\n  \n\n  Implementation guidance\n    \n      Good post-incident analysis provides opportunities to propose common solutions for problems with architecture patterns that are used in other places in your systems.\n    \n    \n      A cornerstone of the COE process is documenting and addressing issues. It is recommended to define a standardized way to document critical root causes, and ensure they are reviewed and addressed. Assign clear ownership for the post-incident analysis process. Designate a responsible team or individual who will oversee incident investigations and follow-ups.\n    \n    \n      Encourage a culture that focuses on learning and improvement rather than assigning blame. Emphasize that the goal is to prevent future incidents, not to penalize individuals.\n    \n    \n      Develop well-defined procedures for conducting post-incident analyses. These procedures should outline the steps to be taken, the information to be collected, and the key questions to be addressed during the analysis. Investigate incidents thoroughly, going beyond immediate causes to identify root causes and contributing factors. Use techniques like the five whys to delve deep into the underlying issues.\n    \n    \n      Maintain a repository of lessons learned from incident analyses. This institutional knowledge can serve as a reference for future incidents and prevention efforts. Share findings and insights from post-incident analyses, and consider holding open-invite post-incident review meetings to discuss lessons learned.\n    \n     \n      \n      Implementation steps\n      \n         \n         \n         \n         \n         \n         \n         \n      \n          \n            While conducting post-incident analysis, ensure the process is blame-free. This allows people involved in the incident to be dispassionate about the proposed corrective actions and promote honest self-assessment and collaboration across teams.\n          \n        \n          \n            Define a standardized way to document critical issues. An example structure for such document is as follows:\n          \n          \n             \n             \n             \n             \n             \n             \n             \n          \n              \n                What happened?\n              \n            \n              \n                What was the impact on customers and your business?\n              \n            \n              \n                What was the root cause?\n              \n            \n              \n                What data do you have to support this?\n              \n              \n                 \n              \n                  \n                    For example, metrics and graphs\n                  \n                \n            \n              \n                What were the critical pillar implications, especially security?\n              \n              \n                 \n              \n                  \n                    When architecting workloads, you make trade-offs between pillars based upon your business context. These business decisions can drive your engineering priorities. You might optimize to reduce cost at the expense of reliability in development environments, or, for mission-critical solutions, you might optimize reliability with increased costs. Security is always job zero, as you have to protect your customers.\n                  \n                \n            \n              \n                What lessons did you learn?\n              \n            \n              \n                What corrective actions are you taking?\n              \n              \n                 \n                 \n              \n                  \n                    Action items\n                  \n                \n                  \n                    Related items\n                  \n                \n            \n        \n          \n            Create well-defined standard operating procedures for conducting post-incident analyses.\n          \n        \n          \n            Set up a standardized incident reporting process. Document all incidents comprehensively, including the initial incident report, logs, communications, and actions taken during the incident.\n          \n        \n          \n            Remember that an incident does not require an outage. It could be a near-miss, or a system performing in an unexpected way while still fulfilling its business function.\n          \n        \n          \n            Continually improve your post-incident analysis process based on feedback and lessons learned.\n          \n        \n          \n            Capture key findings in a knowledge management system, and consider any patterns that should be added to developer guides or pre-deployment checklists.\n          \n        \n     \n   \n\n  Resources\n\n      \n    \n      Related documents:\n    \n    \n       \n    \n        \n          Why\n          you should develop a correction of error (COE)\n        \n      \n    \n      Related videos:\n    \n    \n       \n       \n    \n        \n          Amazon’s approach to failing successfully \n        \n      \n        AWS re:Invent 2021 - Amazon Builders’ Library: Operational Excellence at Amazon \n        \n      \n  \u003cdiv\u003e\u003cdiv\u003e\u003cdiv\u003e\u003cdiv id=\"js_error_message\"\u003e\u003cp\u003e\u003cimg src=\"https://d1ge0kk1l5kms0.cloudfront.net/images/G/01/webservices/console/warning.png\" alt=\"Warning\" /\u003e \u003cstrong\u003eJavascript is disabled or is unavailable in your browser.\u003c/strong\u003e\u003c/p\u003e\u003cp\u003eTo use the Amazon Web Services Documentation, Javascript must be enabled. Please refer to your browser's Help pages for instructions.\u003c/p\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003eDocument ConventionsREL12-BP01 Use playbooks to investigate failuresREL12-BP03 Test scalability and performance requirements",
  "REL12-BP03 Test scalability and performance requirements\n    Use techniques such as load testing to validate that the workload\n    meets scaling and performance requirements.\n  \n    In the cloud, you can create a production-scale test environment for\n    your workload on demand. Instead of reliance on a scaled-down test\n    environment, which could lead to inaccurate predictions of\n    production behaviors, you can use the cloud to provision a test\n    environment that closely mirrors your expected production\n    environment. This environment helps you test in a more accurate\n    simulation of the real-world conditions your application faces.\n  \n    Alongside your performance testing efforts, it's essential to\n    validate that your base resources, scaling settings, service quotas,\n    and resiliency design operate as expected under load. This holistic\n    approach verifies that your application can reliably scale and\n    perform as required, even under the most demanding conditions.\n  \n    Desired outcome: Your workload\n    maintains its expected behavior even while subject to peak load. You\n    proactively address any performance-related issues that may arise as\n    the application grows and evolves.\n  \n    Common anti-patterns:\n  \n     \n     \n     \n     \n     \n     \n     \n  \n      \n        You use test environments that do not closely match the\n        production environment.\n      \n    \n      \n        You treat load testing as a separate, one-time activity rather\n        than an integrated part of the deployment continuous integration\n        (CI) pipeline.\n      \n    \n      \n        You don't define clear and measurable performance requirements,\n        such as response time, throughput, and scalability targets.\n      \n    \n      \n        You perform tests with unrealistic or insufficient load\n        scenarios, and you fail to test for peak loads, sudden spikes,\n        and sustained high load.\n      \n    \n      \n        You don't stress test the workload by exceeding expected load\n        limits.\n      \n    \n      \n        You use inadequate or inappropriate load testing and performance\n        profiling tools.\n      \n    \n      \n        You lack comprehensive monitoring and alerting systems to track\n        performance metrics and detect anomalies.\n      \n    \n    Benefits of establishing this best\n    practice:\n  \n     \n     \n     \n     \n     \n     \n  \n      \n        Load testing helps you identify potential performance\n        bottlenecks in your system before it goes into production. When\n        you simulate production-level traffic and workloads, you can\n        identify areas where your system may struggle to handle the\n        load, such as slow response times, resource constraints, or\n        system failures.\n      \n    \n      \n        As you test your system under various load conditions, you can\n        better understand the resource requirements needed to support\n        your workload. This information can help you make informed\n        decisions about resource allocation and prevent\n        over-provisioning or under-provisioning of resources.\n      \n    \n      \n        To identify potential failure points, you can observe how your\n        workload performs under high load conditions. This information\n        helps you improve your workload's reliability and resiliency by\n        implementing fault-tolerance mechanisms, failover strategies,\n        and redundancy measures, as appropriate.\n      \n    \n      \n        You identify and address performance issues early, which helps\n        you avoid the costly consequences of system outages, slow\n        response times, and dissatisfied users.\n      \n    \n      \n        Detailed performance data and profiling information collected\n        during testing can help you troubleshoot performance-related\n        issues that may arise in production. This can lead to faster\n        incident response and resolution, which reduces the impact on\n        users and your organization's operations.\n      \n    \n      \n        In certain industries, proactive performance testing can help\n        your workload meet compliance standards, which reduces the risk\n        of penalties or legal issues.\n      \n    \n    Level of risk exposed if this best practice\n    is not established: High\n  \n\n  Implementation guidance\n\n      \n    \n      The first step is to define a comprehensive testing strategy that\n      covers all aspects of scaling and performance requirements. To\n      start, clearly define your workload's service-level objectives\n      (SLOs) based on your business needs, such as throughput, latency\n      histogram, and error rate. Next, design a suite of tests that can\n      simulate various load scenarios that range from average usage to\n      sudden spikes and sustained peak loads, and verify that the\n      workload's behavior meets your SLOs. These tests should be\n      automated and integrated into your continuous integration and\n      deployment pipeline to catch performance regressions early in the\n      development process.\n    \n    \n      To effectively test scaling and performance, invest in the right\n      tools and infrastructure. This includes load testing tools that\n      can generate realistic user traffic, performance profiling tools\n      to identify bottlenecks, and monitoring solutions to track key\n      metrics. Importantly, you should verify that your test\n      environments closely match the production environment in terms of\n      infrastructure and environment conditions to make your test\n      results as accurate as possible. To make it easier to reliably\n      replicate and scale production-like setups, use infrastructure as\n      code and container-based applications.\n    \n    \n      Scaling and performance tests are an ongoing process, not a\n      one-time activity. Implement comprehensive monitoring and alerting\n      to track the application's performance in production, and use this\n      data to continually refine your test strategies and optimization\n      efforts. Regularly analyze performance data to identify emerging\n      issues, test new scaling strategies, and implement optimizations\n      to improve the application's efficiency and reliability. When you\n      adopt an iterative approach and constantly learn from production\n      data, you can verify that your application can adapt to variable\n      user demands and maintain resiliency and optimal performance over\n      time.\n    \n     \n\n  Implementation steps\n\n      \n      \n         \n         \n         \n         \n         \n         \n         \n      \n          \n            Establish clear and measurable performance requirements,\n            such as response time, throughput, and scalability targets.\n            These requirements should be based on your workload's usage\n            patterns, user expectations, and business needs.\n          \n        \n          \n            Select and configure a load testing tool that can accurately\n            mimic the load patterns and user behavior in your production\n            environment.\n          \n        \n          \n            Set up a test environment that closely matches the\n            production environment, including infrastructure and\n            environment conditions, to improve the accuracy of your test\n            results.\n          \n        \n          \n            Create a test suite that covers a wide range of scenarios,\n            from average usage patterns to peak loads, rapid spikes, and\n            sustained high loads. Integrate the tests into your\n            continuous integration and deployment pipelines to catch\n            performance regressions early in the development process.\n          \n        \n          \n            Conduct load testing to simulate real-world user traffic and\n            understand how your application behaves under different load\n            conditions. To stress test your application, exceed the\n            expected load and observe its behavior, such as response\n            time degradation, resource exhaustion, or system failures,\n            which helps identify the breaking point of your application\n            and inform scaling strategies. Evaluate the scalability of\n            your workload by incrementally increasing the load, and\n            measure the performance impact to identify scaling limits\n            and plan for future capacity needs.\n          \n        \n          \n            Implement comprehensive monitoring and alerting to track\n            performance metrics, detect anomalies, and initiate scaling\n            actions or notifications when thresholds are exceeded.\n          \n        \n          \n            Continually monitor and analyze performance data to identify\n            areas for improvement. Iterate on your testing strategies\n            and optimization efforts.\n          \n        \n     \n   \n\n  Resources\n\n      \n    \n      Related best practices:\n    \n    \n       \n       \n       \n    \n        \n          REL01-BP04\n          Monitor and manage quotas\n        \n      \n        \n          REL06-BP01\n          Monitor all components for the workload (Generation)\n        \n      \n        \n          REL06-BP03\n          Send notifications (Real-time processing and alarming))\n        \n      \n    \n      Related documents:\n    \n    \n       \n       \n       \n       \n    \n        \n          Load\n          testing applications\n        \n      \n        \n          Distributed\n          Load Testing on AWS\n        \n      \n        \n          Application\n          Performance Monitoring\n        \n      \n        \n          Amazon EC2 Testing Policy\n        \n      \n    \n      Related examples:\n    \n    \n       \n    \n        \n          Distributed\n          Load Testing on AWS (GitHub)\n        \n      \n    \n      Related tools:\n    \n    \n       \n       \n       \n       \n       \n       \n       \n       \n       \n    \n        \n          Amazon CodeGuru Profiler\n        \n      \n        \n          Amazon CloudWatch RUM\n        \n      \n        \n          Apache\n          JMeter\n        \n      \n        \n          K6\n        \n      \n        \n          Vegeta\n        \n      \n        \n          Hey\n        \n      \n        \n          ab\n        \n      \n        \n          wrk\n        \n      \n        \n          Distributed Load Testing on AWS\n      \n  \u003cdiv\u003e\u003cdiv\u003e\u003cdiv\u003e\u003cdiv id=\"js_error_message\"\u003e\u003cp\u003e\u003cimg src=\"https://d1ge0kk1l5kms0.cloudfront.net/images/G/01/webservices/console/warning.png\" alt=\"Warning\" /\u003e \u003cstrong\u003eJavascript is disabled or is unavailable in your browser.\u003c/strong\u003e\u003c/p\u003e\u003cp\u003eTo use the Amazon Web Services Documentation, Javascript must be enabled. Please refer to your browser's Help pages for instructions.\u003c/p\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003eDocument ConventionsREL12-BP02 Perform post-incident analysisREL12-BP04 Test resiliency using chaos engineering",
  "REL12-BP04 Test resiliency using chaos engineering\n    Run chaos experiments regularly in environments that are in or as close to production as possible to understand how your system responds to adverse conditions. \n  \n    \n      Desired outcome:\n    \n  \n    The resilience of the workload is regularly verified by applying chaos engineering in the form of fault injection experiments or injection of unexpected load, \n    in addition to resilience testing that validates known expected behavior of your workload during an event. Combine both chaos engineering and resilience testing \n    to gain confidence that your workload can survive component failure and can recover from unexpected disruptions with minimal to no impact.\n  \n    \n      Common anti-patterns:\n    \n  \n     \n    \n    \n    \n    \n  \n     Designing for resiliency, but not verifying how the workload functions as a whole when\n        faults occur. \n    \n     \n       Never experimenting under real-world conditions and expected load.\n     \n   \n     \n       Not treating your experiments as code or maintaining them through the development cycle.\n     \n   \n     \n       Not running chaos experiments both as part of your CI/CD pipeline, as well as outside of deployments.\n     \n   \n     \n       Neglecting to use past post-incident analyses when determining which faults to experiment with.\n     \n   \n     Benefits of establishing this best practice: Injecting faults to \n    verify the resilience of your workload allows you to gain confidence that the recovery procedures of \n    your resilient design will work in the case of a real fault.\n  \n    Level of risk exposed if this best practice is not established: Medium\n  \n    \n    Implementation guidance\n    \n      Chaos engineering provides your teams with capabilities to continually inject real world \n      disruptions (simulations) in a controlled way at the service provider, infrastructure, workload, \n      and component level, with minimal to no impact to your customers. It allows your teams to learn \n      from faults and observe, measure, and improve the resilience of your workloads, as well as validate \n      that alerts fire and teams get notified in the case of an event. \n    \n    \n      When performed continually, chaos engineering can highlight deficiencies in your workloads that, \n      if left unaddressed, could negatively affect availability and operation.\n    \n  NoteChaos engineering is the discipline of experimenting on a system in order to build\n      confidence in the system’s capability to withstand turbulent conditions in production. –\n        Principles of Chaos Engineering\n    \n  \n    If a system is able to withstand these disruptions, the chaos experiment should be maintained \n    as an automated regression test. In this way, chaos experiments should be performed as part of \n    your systems development lifecycle (SDLC) and as part of your CI/CD pipeline.\n  \n  \n    To ensure that your workload can survive component failure, inject real world events as part of \n    your experiments. For example, experiment with the loss of Amazon EC2 instances or failover of the primary \n    Amazon RDS database instance, and verify that your workload is not impacted (or only minimally impacted). \n    Use a combination of component faults to simulate events that may be caused by a disruption in an Availability Zone.\n  \n  \n    For application-level faults (such as crashes), you can start with stressors such as memory and CPU exhaustion.\n  \n     \n      To validate fallback or failover mechanisms \n      for external dependencies due to intermittent network disruptions, your components should simulate such an event by blocking access to the\n      third-party providers for a specified duration that can last from seconds to hours. \n  \n    Other modes of degradation might cause reduced functionality and slow responses, often resulting in a disruption of your services. \n    Common sources of this degradation are increased latency on critical services and unreliable network communication (dropped packets). \n    Experiments with these faults, including networking effects such as latency, dropped messages, and DNS failures, could include the \n    inability to resolve a name, reach the DNS service, or establish connections to dependent services.\n  \n    \n      Chaos engineering tools:\n    \n  \n    AWS Fault Injection Service (AWS FIS) is a fully managed service for running fault injection \n    experiments that can be used as part of your CD pipeline, or outside of the pipeline. AWS FIS is a \n    good choice to use during chaos engineering game days. It supports simultaneously introducing \n    faults across different types of resources including Amazon EC2, Amazon Elastic Container Service (Amazon ECS), Amazon Elastic Kubernetes Service (Amazon EKS), and Amazon RDS. \n    These faults include termination of resources, forcing failovers, stressing CPU or memory, throttling, \n    latency, and packet loss. Since it is integrated with Amazon CloudWatch Alarms, you can set up stop \n    conditions as guardrails to rollback an experiment if it causes unexpected impact.  \n  \n  \n    \n       \n\n       \n       \n      AWS Fault Injection Service integrates with AWS\n          resources to allow you to run fault injection experiments for your\n        workloads.   \n\n    \n  There are also several third-party options for fault injection experiments. These include\n      open-source tools such as Chaos Toolkit, Chaos Mesh, and Litmus Chaos, as well as commercial options like Gremlin. To expand the scope of\n      faults that can be injected on AWS, AWS FIS integrates with Chaos Mesh and Litmus Chaos, allowing you to coordinate fault\n      injection workflows among multiple tools. For example, you can run a stress test on a pod’s\n      CPU using Chaos Mesh or Litmus faults while terminating a randomly selected percentage of\n      cluster nodes using AWS FIS fault actions.\n  \n   \n\n  Implementation steps\n\n    \n   \n       \n       \n       \n       \n \n    \n    Determine which faults to use for experiments. \n    \n     Assess the design of your workload for resiliency. Such designs (created using the best\n          practices of the Well-Architected Framework) account for risks based on critical dependencies,\n          past events, known issues, and compliance requirements. List each element of the design\n          intended to maintain resilience and the faults it is designed to mitigate. For more\n          information about creating such lists, see the Operational Readiness Review whitepaper which guides you on how to\n          create a process to prevent reoccurrence of previous incidents. The Failure Modes and\n          Effects Analysis (FMEA) process provides you with a framework for performing a component-level\n          analysis of failures and how they impact your workload. FMEA is outlined in more detail by\n          Adrian Cockcroft in Failure Modes and Continuous Resilience. \n  \n        \n          Assign a priority to each fault.\n        \n        \n          Start with a coarse categorization such as high, medium, or low. \n          To assess priority, consider frequency of the fault and impact of failure to the overall workload.\n        \n        \n          When considering frequency of a given fault, analyze past data for this workload when available. \n          If not available, use data from other workloads running in a similar environment. \n        \n        \n          When considering impact of a given fault, the larger the scope of the fault, generally the larger the impact. \n          Also consider the workload design and purpose. For example, the ability to access the source data stores is \n          critical for a workload doing data transformation and analysis. In this case, you would prioritize experiments \n          for access faults, as well as throttled access and latency insertion.\n        \n        \n          Post-incident analyses are a good source of data to understand both frequency and impact of failure modes. \n        \n        \n          Use the assigned priority to determine which faults to experiment with first \n          and the order with which to develop new fault injection experiments. \n        \n      \n        \n          For each experiment that you perform, follow the chaos engineering and continuous resilience flywheel in the following figure.\n        \n        \n        \n           \n            \n           \n           \n          Chaos engineering and continuous resilience flywheel, using the scientific method by Adrian Hornsby.   \n        \n          \n        \n        \n           \n           \n           \n           \n           \n        \n            \n              Define steady state as some measurable output of a workload that indicates normal behavior.\n            \n            \n              Your workload exhibits steady state if it is operating reliably and as expected. Therefore, validate that your \n              workload is healthy before defining steady state. Steady state does not necessarily mean no impact to the workload \n              when a fault occurs, as a certain percentage in faults could be within acceptable limits. The steady state is \n              your baseline that you will observe during the experiment, which will highlight anomalies if your hypothesis \n              defined in the next step does not turn out as expected.\n            \n            \n              For example, a steady state of a payments system can be defined as the processing of 300 TPS with a success rate \n              of 99% and round-trip time of 500 ms. \n            \n          \n            \n              Form a hypothesis about how the workload will react to the fault.\n            \n            \n              A good hypothesis is based on how the workload is expected to mitigate the fault to maintain the steady state. \n              The hypothesis states that given the fault of a specific type, the system or workload will continue steady state, \n              because the workload was designed with specific mitigations. The specific type of fault and mitigations should be \n              specified in the hypothesis. \n            \n            \n              The following template can be used for the hypothesis (but other wording is also acceptable):\n            \n            Note\n                If specific fault occurs, the workload name workload will describe mitigating controls to maintain\n                business or technical metric impact.\n              \n            \n              For example:\n            \n            \n               \n               \n               \n            \n                \n                  If 20% of the nodes in the Amazon EKS node-group are taken down, the Transaction Create API continues to serve the 99th percentile of \n                  requests in under 100 ms (steady state). The Amazon EKS nodes will recover within five minutes, and pods will get scheduled and process \n                  traffic within eight minutes after the initiation of the experiment. Alerts will fire within three minutes.\n                \n              \n                \n                  If a single Amazon EC2 instance failure occurs, the order system’s Elastic Load Balancing health check will cause the Elastic Load Balancing \n                  to only send requests to the remaining healthy instances while the Amazon EC2 Auto Scaling replaces the failed instance, maintaining a \n                  less than 0.01% increase in server-side (5xx) errors (steady state).\n                \n              \n                \n                  If the primary Amazon RDS database instance fails, the Supply Chain data collection workload will failover and connect to the standby \n                  Amazon RDS database instance to maintain less than 1 minute of database read or write errors (steady state).\n                \n              \n          \n            \n              Run the experiment by injecting the fault.\n            \n             An experiment should by default be fail-safe and tolerated by the workload. If\n              you know that the workload will fail, do not run the experiment. Chaos engineering\n              should be used to find known-unknowns or unknown-unknowns. Known-unknowns are things you are aware of but don’t fully understand,\n              and unknown-unknowns are things you are neither\n              aware of nor fully understand. Experimenting against a workload that you know is\n              broken won’t provide you with new insights. Your experiment should be carefully\n              planned, have a clear scope of impact, and provide a rollback mechanism that can be\n              applied in case of unexpected turbulence. If your due-diligence shows that your\n              workload should survive the experiment, move forward with the\n              experiment. There are several options for injecting the faults. For workloads on\n              AWS, AWS FIS provides many predefined fault simulations called actions. You\n              can also define custom actions that run in AWS FIS using AWS Systems Manager\n                documents. \n            \n              We discourage the use of custom scripts for chaos experiments, unless the scripts \n              have the capabilities to understand the current state of the workload, are able to emit logs, \n              and provide mechanisms for rollbacks and stop conditions where possible.\n            \n             An effective framework or toolset which supports chaos engineering should track\n              the current state of an experiment, emit logs, and provide rollback mechanisms to\n              support the controlled running of an experiment. Start with an established service\n              like AWS FIS that allows you to perform experiments with a clearly defined scope and\n              safety mechanisms that rollback the experiment if the experiment introduces unexpected\n              turbulence. To learn about a wider variety of experiments using AWS FIS, also see the\n                Resilient and Well-Architected Apps with Chaos Engineering lab. Also,\n                AWS Resilience Hub will\n              analyze your workload and create experiments that you can choose to implement and run\n              in AWS FIS. \n            \n            Note\n                For every experiment, clearly understand the scope and its impact. We recommend that \n                faults should be simulated first on a non-production environment before being run in production.\n              \n             \n              Experiments should run in production under real-world load using \n              canary deployments \n              that spin up both a control and experimental system deployment, where feasible. Running experiments during off-peak times is a good\n              practice to mitigate potential impact when first experimenting in production. Also, if\n              using actual customer traffic poses too much risk, you can run experiments using\n              synthetic traffic on production infrastructure against the control and experimental\n              deployments. When using production is not possible, run experiments in pre-production\n              environments that are as close to production as possible. \n            \n             You must establish and monitor guardrails to ensure the experiment does not\n              impact production traffic or other systems beyond acceptable limits. Establish stop\n              conditions to stop an experiment if it reaches a threshold on a guardrail metric that\n              you define. This should include the metrics for steady state for the workload, as well\n              as the metric against the components into which you’re injecting the fault. A synthetic monitor \n              (also known as a user canary) is one metric you should\n              usually include as a user proxy. Stop conditions for AWS FIS \n              are supported as part of the experiment template, allowing up to five stop-conditions per template. \n            \n            \n              One of the principles of chaos is minimize the scope of the experiment and its impact:\n            \n            \n              While there must be an allowance for some short-term negative impact, it is the responsibility and \n              obligation of the Chaos Engineer to ensure the fallout from experiments are minimized and contained.\n            \n            \n              A method to verify the scope and potential impact is to perform the experiment in a non-production environment \n              first, verifying that thresholds for stop conditions activate as expected during an experiment and observability \n              is in place to catch an exception, instead of directly experimenting in production.\n            \n            \n              When running fault injection experiments, verify that all responsible parties are well-informed. Communicate with \n              appropriate teams such as the operations teams, service reliability teams, and customer support to let them know \n              when experiments will be run and what to expect. Give these teams communication tools to inform those running the \n              experiment if they see any adverse effects.\n            \n            \n              You must restore the workload and its underlying systems back to the original known-good state. Often, the resilient \n              design of the workload will self-heal. But some fault designs or failed experiments can leave your workload in an \n              unexpected failed state. By the end of the experiment, you must be aware of this and restore the workload and systems. \n              With AWS FIS you can set a rollback configuration (also called a post action) within the action parameters. A post action \n              returns the target to the state that it was in before the action was run. Whether automated (such as using AWS FIS) or \n              manual, these post actions should be part of a playbook that describes how to detect and handle failures.\n            \n          \n            \n              Verify the hypothesis.\n            \n            Principles of Chaos\n                Engineering gives this guidance on how to verify steady state of your\n              workload: \n            Focus on the measurable output of a system, rather than internal attributes of the system. \n              Measurements of that output over a short period of time constitute a proxy for the system’s steady state. \n              The overall system’s throughput, error rates, and latency percentiles could all be metrics of interest \n              representing steady state behavior. By focusing on systemic behavior patterns during experiments, chaos engineering verifies \n              that the system does work, rather than trying to validate how it works.\n            \n              In our two previous examples, we include the steady state metrics of less than 0.01% increase in server-side (5xx) errors and \n              less than one minute of database read and write errors.\n            \n            \n              The 5xx errors are a good metric because they are a consequence of the failure mode that a client of the workload will experience directly. \n              The database errors measurement is good as a direct consequence of the fault, but should also be supplemented with a client impact measurement \n              such as failed customer requests or errors surfaced to the client. Additionally, include a synthetic monitor (also known as a user canary) on \n              any APIs or URIs directly accessed by the client of your workload.\n            \n          \n            \n              Improve the workload design for resilience.\n            \n             If steady state was not maintained, then investigate how the workload design can\n              be improved to mitigate the fault, applying the best practices of the AWS Well-Architected\n                Reliability pillar. Additional guidance and resources can be found in the\n                AWS Builder’s Library,\n              which hosts articles about how to improve your health\n                checks or employ retries with\n                backoff in your application code, among others. \n            \n              After these changes have been implemented, run the experiment again (shown by the dotted line in the chaos engineering flywheel) \n              to determine their effectiveness. If the verify step indicates the hypothesis holds true, then the workload \n              will be in steady state, and the cycle continues.\n            \n                   \n      \n        \n          Run experiments regularly.\n        \n         A chaos experiment is a cycle, and experiments should be run regularly as part of\n          chaos engineering. After a workload meets the experiment’s hypothesis, the experiment\n          should be automated to run continually as a regression part of your CI/CD pipeline. To\n          learn how to do this, see this blog on how to run AWS FIS experiments using AWS CodePipeline. This lab on recurrent AWS FIS\n            experiments in a CI/CD pipeline allows you to work hands-on. \n         Fault injection experiments are also a part of game days (see REL12-BP05 Conduct game days regularly). \n          Game days simulate a failure or event to verify systems, processes, and team responses. \n          The purpose is to actually perform the actions the team would perform as if an exceptional event happened.\n        \n      \n        \n          Capture and store experiment results.\n        \n        Results for fault injection experiments must be captured and persisted. Include all\n          necessary data (such as time, workload, and conditions) to be able to later\n          analyze experiment results and trends. Examples of results might include screenshots of\n          dashboards, CSV dumps from your metric’s database, or a hand-typed record of events and\n          observations from the experiment. Experiment logging with AWS FIS\n          can be part of this data capture.\n      \n   \n\n  Resources\n\n\n  Related best practices:\n\n    \n       \n       \n    \n        \n          REL08-BP03 Integrate resiliency testing as part of your\n  deployment\n        \n      \n        \n          REL13-BP03 Test disaster recovery implementation to validate\n  the implementation\n        \n      \n      \n    \n      Related documents:\n    \n    \n       \n       \n       \n       \n       \n       \n       \n       \n    \n        \n          What\n            is AWS Fault Injection Service?\n        \n      \n        \n          What is AWS Resilience Hub?\n        \n      \n        \n          Principles\n          of Chaos Engineering\n        \n      \n        \n          Chaos Engineering: Planning your first experiment\n        \n      \n        \n          Resilience Engineering: Learning\n            to Embrace Failure\n        \n      \n        \n          Chaos Engineering\n            stories\n        \n      \n        \n          Avoiding fallback\n            in distributed systems\n        \n      \n        \n          Canary Deployment for Chaos Experiments\n        \n      \n    \n      Related videos:\n    \n    \n       \n       \n       \n    \n        AWS re:Invent 2020:\n            Testing resiliency using chaos engineering (ARC316)\n        \n      \n        \n          AWS re:Invent\n          2019: Improving resiliency with chaos engineering\n          (DOP309-R1)\n        \n      \n        \n          AWS re:Invent 2019: Performing\n            chaos engineering in a serverless world (CMY301)\n        \n         \n    \n      \n        Related tools:\n      \n    \n    \n       \n       \n       \n       \n       \n    \n        \n          AWS Fault Injection Service\n        \n      \n        AWS Marketplace: Gremlin Chaos\n            Engineering Platform\n        \n      \n        \n          Chaos Toolkit\n        \n      \n        \n          Chaos Mesh\n        \n      \n        \n          Litmus\n        \n      \n  \u003cdiv\u003e\u003cdiv\u003e\u003cdiv\u003e\u003cdiv id=\"js_error_message\"\u003e\u003cp\u003e\u003cimg src=\"https://d1ge0kk1l5kms0.cloudfront.net/images/G/01/webservices/console/warning.png\" alt=\"Warning\" /\u003e \u003cstrong\u003eJavascript is disabled or is unavailable in your browser.\u003c/strong\u003e\u003c/p\u003e\u003cp\u003eTo use the Amazon Web Services Documentation, Javascript must be enabled. Please refer to your browser's Help pages for instructions.\u003c/p\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003eDocument ConventionsREL12-BP03 Test scalability and performance requirementsREL12-BP05 Conduct game days regularly",
  "REL12-BP05 Conduct game days regularly\n    Conduct game days to regularly exercise your procedures for\n    responding to workload-impacting events and impairments. Involve the\n    same teams who would be responsible for handling production\n    scenarios. These exercises help enforce measures to prevent user\n    impact caused by production events. When you practice your response\n    procedures in realistic conditions, you can identify and address any\n    gaps or weaknesses before a real event occurs.\n  \n    Game days simulate events in production-like environments to test\n    systems, processes, and team responses. The purpose is to perform\n    the same actions the team would perform as if the event actually\n    occurred. These exercises help you understand where improvements can\n    be made and can help develop organizational experience in dealing\n    with events and impairments. These should be conducted regularly so\n    that your team knows builds ingrained habits for how to respond.\n  \n    Game days prepare teams to handle production events with greater\n    confidence. Teams that are well-practiced are more able to quickly\n    detect and respond to various scenarios. This results in a\n    significantly improved readiness and resilience posture.\n  \n    Desired outcome: You run\n    resilience game days on a consistent, scheduled basis. These game\n    days are seen as a normal and expected part of doing business. Your\n    organization has built a culture of preparedness, and when\n    production issues occur, your teams are well-prepared to respond\n    effectively, resolve the issues efficiently, and mitigate the impact\n    on customers.\n  \n    Common anti-patterns:\n  \n     \n     \n     \n     \n     \n     \n  \n      \n        You document your procedures, but your never exercise them.\n      \n    \n      \n        You exclude business decision makers in the test exercises.\n      \n    \n      \n        You run a game day, but you don't inform all relevant\n        stakeholders.\n      \n    \n      \n        You focus solely on technical failures, but you don't involve\n        business stakeholders.\n      \n    \n      \n        You don't incorporate lessons learned from game days into your\n        recovery processes.\n      \n    \n      \n        You blame teams for failures or bugs.\n      \n    \n    Benefits of establishing this best\n    practice:\n  \n     \n     \n     \n     \n     \n  \n      \n        Enhance response skills: On game days, teams practice their\n        duties and test their communication mechanisms during simulated\n        events, which creates a more coordinated and efficient response\n        in production situations.\n      \n    \n      \n        Identify and address dependencies: Complex environments often\n        involve intricate dependencies between various systems,\n        services, and components. Game days can help you identify and\n        address these dependencies, and verify that your critical\n        systems and services are properly covered by your runbook\n        procedures and can be scaled up or recovered in a timely manner.\n      \n    \n      \n        Foster a culture of resilience: Game days can help cultivate a\n        mindset of resilience within an organization. When you involve\n        cross-functional teams and stakeholders, these exercises promote\n        awareness, collaboration, and a shared understanding of the\n        importance of resilience across the entire organization.\n      \n    \n      \n        Continuous improvement and adaptation: Regular game days help\n        you to continually assess and adapt your resilience strategies,\n        which keeps them relevant and effective in the face of changing\n        circumstances.\n      \n    \n      \n        Increase confidence in the system: Successful game days can help\n        you build confidence in the system's ability to withstand and\n        recover from disruptions.\n      \n    \n    Level of risk exposed if this best practice\n    is not established: Medium\n  \n\n  Implementation guidance\n\n      \n    \n      Once you have designed and implemented the necessary resilience\n      measures, conduct a game day to validate that everything works as\n      planned in production. A game day, especially the first one,\n      should involve all team members, and all stakeholders and\n      participants should be informed in advance about the date, time,\n      and simulated scenarios.\n    \n    \n      During the game day, the involved teams simulate various events\n      and potential scenarios according to the prescribed procedures.\n      The participants closely monitor and assess the impact of these\n      simulated events. If the system operates as designed, the\n      automated detection, scaling, and self-healing mechanisms should\n      activate and result in little to no impact on users. If the team\n      observes any negative impact, they roll back the test and remedy\n      the identified issues, either through automated means or manual\n      intervention documented in the applicable runbooks.\n    \n    \n      To continuously improve resilience, it's critical to document and\n      incorporate lessons learned. This process is a feedback\n      loop that systematically captures insights from game\n      days and uses them to enhance systems, processes, and team\n      capabilities.\n    \n    \n      To help you reproduce real-world scenarios where system components\n      or services may fail unexpectedly, inject simulated faults as a\n      game day exercise. Teams can test the resilience and fault\n      tolerance of their systems and simulate their incident response\n      and recovery processes in a controlled environment.\n    \n    \n      In AWS, your game days can be carried out with replicas of your\n      production environment using infrastructure as code. Through this\n      process, you can test in a safe environment that closely resembles\n      your production environment. Consider\n      AWS Fault Injection Service to create different failure scenarios. Use\n      services like\n      Amazon CloudWatch and\n      AWS X-Ray\n      to monitor system behavior during game days. Use\n      AWS Systems Manager to manage and run playbooks, and use\n      AWS Step Functions to orchestrate recurring game day workflows.\n    \n     \n\n  Implementation steps\n\n      \n      \n         \n         \n         \n         \n      \n          \n            Establish a game day\n            program: Develop a structured program that\n            defines the frequency, scope and objectives of game days.\n            Involve key stakeholders and subject matter experts in\n            planning and running these exercises.\n          \n        \n          \n            Prepare the game day:\n          \n          \n             \n             \n          \n              \n                Identify the key business-critical services that are the\n                focus of the game day. Catalog and map the people,\n                processes, and technologies that support those services.\n              \n            \n              \n                Set the agenda for the game day, and prepare the\n                involved teams to participate in the event. Prepare your\n                automation services to simulate the planned scenarios\n                and run the appropriate recovery processes. AWS services\n                such as\n                AWS Fault Injection Service,\n                AWS Step Functions, and\n                AWS Systems Manager can help you automate various\n                aspects of game days, such as injection of faults and\n                initiation of recovery actions.\n              \n            \n        \n          \n            Run your simulation: On\n            the game day, run the planned scenario. Observe and document\n            how the people, processes, and technologies react to the\n            simulated event.\n          \n        \n          \n            Conduct post-exercise\n            reviews: After the game day, conduct a\n            retrospective session to review the lessons learned.\n            Identify areas for improvement and any actions needed to\n            improve operational resilience. Document your findings, and\n            track any necessary changes to enhance your resilience\n            strategies and preparedness to completion.\n          \n        \n     \n   \n\n  Resources\n\n      \n    \n      Related best practices:\n    \n    \n       \n       \n       \n       \n       \n    \n        \n          REL12-BP01\n          Use playbooks to investigate failures\n        \n      \n        \n          REL12-BP04\n          Test resiliency using chaos engineering\n        \n      \n        \n          OPS04-BP01\n          Identify key performance indicators\n        \n      \n        \n          OPS07-BP03\n          Use runbooks to perform procedures\n        \n      \n        \n          OPS10-BP01\n          Use a process for event, incident, and problem\n          management\n        \n      \n    \n      Related documents:\n    \n    \n       \n       \n    \n        \n          What is AWS\n          GameDay?\n        \n      \n        \n          AWS           Well-Architected Concepts - Game Day\n        \n      \n    \n      Related videos:\n    \n    \n       \n    \n        \n          AWS           re:Invent 2023 - Practice like you play: How Amazon scales\n          resilience to new heights\n        \n      \n    \n      Related examples:\n    \n    \n       \n       \n    \n        \n          AWS           Workshop - Navigate the storm: Unleashing controlled chaos for\n          resilient systems\n        \n      \n        \n          Build\n          Your Own Game Day to Support Operational Resilience\n        \n      \n  \u003cdiv\u003e\u003cdiv\u003e\u003cdiv\u003e\u003cdiv id=\"js_error_message\"\u003e\u003cp\u003e\u003cimg src=\"https://d1ge0kk1l5kms0.cloudfront.net/images/G/01/webservices/console/warning.png\" alt=\"Warning\" /\u003e \u003cstrong\u003eJavascript is disabled or is unavailable in your browser.\u003c/strong\u003e\u003c/p\u003e\u003cp\u003eTo use the Amazon Web Services Documentation, Javascript must be enabled. Please refer to your browser's Help pages for instructions.\u003c/p\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003eDocument ConventionsREL12-BP04 Test resiliency using chaos engineering REL 13. How do you plan for disaster recovery (DR)? ",
  "REL13-BP01 Define recovery objectives for downtime and data\n  loss\n    Failures can impact your business in several ways. First, failures\n    can cause service interruption (downtime). Second, failures can\n    cause data to become lost, inconsistent, or stale. In order to guide\n    how you respond and recover from failures, define a Recovery Time\n    Objective (RTO) and Recovery Point Objective (RPO) for each\n    workload. Recovery Time Objective (RTO) is the\n    maximum acceptable delay between the interruption of service and\n    restoration of service. Recovery Point Objective\n    (RPO)  is the maximum acceptable time after the last data\n    recovery point.\n  \n    Desired outcome: Every workload has a designated RTO and RPO based on technical\n    considerations and business impact.\n  \n    Common anti-patterns:\n  \n     \n     \n     \n     \n     \n     \n     \n     \n  \n      \n        You haven't designated recovery objectives.\n      \n    \n      \n        You select arbitrary recovery objectives.\n      \n    \n      \n        You select recovery objectives that are too lenient and do not\n        meet business objectives.\n      \n    \n      \n        You have not evaluated the impact of downtime and data loss.\n      \n    \n      \n        You select unrealistic recovery objectives, such as zero time to\n        recover or zero data loss, which may not be achievable for your\n        workload configuration.\n      \n    \n      \n        You select recovery objectives that are more stringent than\n        actual business objectives. This forces recovery implementations\n        that are costlier and more complicated than what the workload\n        needs.\n      \n    \n      \n        You select recovery objectives that are incompatible with those\n        of a dependent workload.\n      \n    \n      \n        You fail to consider regulatory and compliance requirements.\n      \n    \n    Benefits of establishing this best\n    practice: When you set RTOs and RPOs for your workloads,\n    you establish clear and measurable goals for recovery based on your\n    business needs. Once you've set those goals, you can create disaster\n    recovery (DR) plans that are tailored to meet them.\n  \n    Level of risk exposed if this best practice\n    is not established: High\n  \n\n  Implementation guidance\n\n      \n    \n      Construct a matrix or worksheet to help guide your disaster\n      recovery planning. In your matrix, create different workload\n      categories or tiers based on their business impact (such as\n      critical, high, medium, and low) and the associated RTOs and RPOs\n      to target for each one. The following matrix provides an example\n      (note that your RTO and RPO values may differ) you can follow:\n    \n    \n       \n        \n       \n       \n      Example disaster recovery matrix\n    \n    \n      For each workload, investigate and understand the impact of\n      downtime and lost data on your business. The impact typically\n      grows with downtime and data loss, but the shape of the impact can\n      differ based on the workload type. For example, downtime for up to\n      an hour might have low impact, but after that, the impact could\n      quickly intensify. Impact can take many forms, including financial\n      impact (such as lost revenue), reputational impact (including loss\n      of customer trust), operational impact (such as a missed payroll\n      or decreased productivity), and regulatory risk. Once completed,\n      assign the workload to the appropriate tier.\n    \n    \n      Consider the following questions when you analyze the impact of\n      failure:\n    \n    \n       \n       \n       \n       \n       \n       \n       \n       \n       \n       \n       \n    \n        \n          What is the maximum time the workload can be unavailable\n          before unacceptable impact to the business is incurred?\n        \n      \n        \n          How much impact, and what kind, will be incurred by the\n          business by a workload disruption? Consider all kinds of\n          impact, including financial, reputational, operational, and\n          regulatory.\n        \n      \n        \n          What is the maximum amount of data that can be lost or\n          unrecoverable before unacceptable impact to the business is\n          incurred?\n        \n      \n        \n          Can lost data be recreated from other sources (also known as\n          derived data)? If so, also consider the\n          RPOs of all source data used to recreate the workload data.\n        \n      \n        \n          What are the recovery objectives and availability expectations\n          of workloads that this one depends on (downstream)? Your\n          workload's objectives must be achievable given the recovery\n          capabilities of its downstream dependencies. Consider possible\n          downstream dependency workarounds or mitigations that can\n          improve this workload's recovery capability.\n        \n      \n        \n          What are the recovery objectives and availability expectations\n          of workloads that depend on this one (upstream)? Upstream\n          workload objectives may require this workload to have more\n          stringent recovery capabilities than it first appears.\n        \n      \n        \n          Are there different recovery objectives based on the type of\n          incident? For example, you might have different RTOs and RPOs\n          depending on whether the incident impacts an Availability Zone\n          or an entire Region.\n        \n      \n        \n          Do your recovery objectives change during certain events or\n          times of the year? For example, you might have different RTOs\n          and RPOs around holiday shopping seasons, sporting events,\n          special sales, and new product launches.\n        \n      \n        \n          How do the recovery objectives align with any line of business\n          and organizational disaster recovery strategy you might have?\n        \n      \n        \n          Are there legal or contractual ramifications to consider? For\n          example, are you contractually obligated to provide a service\n          with a given RTO or RPO? What penalties might you incur for\n          not meeting them?\n        \n      \n        \n          Are you required to maintain data integrity to meet regulatory\n          or compliance requirements?\n        \n      \n    \n      The following worksheet can aid your evaluation of each workload.\n      You may modify this worksheet to suit your specific needs, such as\n      adding additional questions.\n    \n    \n       \n        \n       \n       \n      Worksheet\n    \n     \n\n  Implementation steps\n\n      \n      \n         \n         \n         \n         \n      \n          \n            Identify the business stakeholders and technical teams\n            responsible for each workload, and engage with them.\n          \n        \n          \n            Create categories or tiers of criticality for workload\n            impact in your organization. Example categories include\n            critical, high, medium, and low. For each category, choose\n            an RTO and RPO that reflects your business objectives and\n            requirements.\n          \n        \n          \n            Assign one of the impact categories you created in the\n            previous step to each workload. To decide how a workload\n            maps to a category, consider the workload's importance to\n            the business and the impact of interruption or data loss,\n            and use the questions above to guide you. This results in an\n            RTO and RPO for each workload.\n          \n        \n          \n            Consider the RTO and RPO for each workload determined in the\n            previous step. Involve the workload's business and technical\n            teams to determine whether the objectives should be\n            adjusted. For example, business stakeholders could determine\n            that more stringent targets are required. Alternatively,\n            technical teams could determine that targets should be\n            modified to make them achievable with available resources\n            and technological constraints.\n          \n        \n     \n   \n\n  Resources\n\n      \n    \n      Related best practices:\n    \n    \n       \n       \n       \n       \n    \n        \n          REL09-BP04\n          Perform periodic recovery of the data to verify backup\n          integrity and processes\n        \n      \n        \n          REL12-BP01\n          Use playbooks to investigate failures\n        \n      \n        \n          REL13-BP02\n          Use defined recovery strategies to meet the recovery\n          objectives\n        \n      \n        \n          REL13-BP03\n          Test disaster recovery implementation to validate the\n          implementation\n        \n      \n    \n      Related documents:\n    \n    \n       \n       \n       \n       \n       \n    \n        \n          AWS           Architecture Blog: Disaster Recovery Series\n        \n      \n        \n          Disaster\n          Recovery of Workloads on AWS: Recovery in the Cloud (AWS           Whitepaper)\n        \n      \n        \n          Managing\n          resiliency policies with AWS Resilience Hub\n        \n      \n        \n          APN\n          Partner: partners that can help with disaster recovery\n        \n      \n        \n          AWS Marketplace: products that can be used for disaster\n          recovery\n        \n      \n    \n      Related videos:\n    \n    \n       \n       \n    \n        \n          AWS re:Invent\n          2018: Architecture Patterns for Multi-Region Active-Active\n          Applications\n        \n      \n        \n          Disaster\n          Recovery of Workloads on AWS\n        \n      \n  \u003cdiv\u003e\u003cdiv\u003e\u003cdiv\u003e\u003cdiv id=\"js_error_message\"\u003e\u003cp\u003e\u003cimg src=\"https://d1ge0kk1l5kms0.cloudfront.net/images/G/01/webservices/console/warning.png\" alt=\"Warning\" /\u003e \u003cstrong\u003eJavascript is disabled or is unavailable in your browser.\u003c/strong\u003e\u003c/p\u003e\u003cp\u003eTo use the Amazon Web Services Documentation, Javascript must be enabled. Please refer to your browser's Help pages for instructions.\u003c/p\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003eDocument Conventions REL 13. How do you plan for disaster recovery (DR)? REL13-BP02 Use defined recovery strategies to meet the recovery\n  objectives",
  "REL13-BP02 Use defined recovery strategies to meet the recovery\n  objectivesDefine a disaster recovery (DR) strategy that meets your workload's recovery objectives. Choose a strategy such as backup and restore, standby (active/passive), or active/active.\n    Desired outcome: For each workload, there is a defined and implemented DR strategy\n    that allows the workload to achieve DR objectives. DR strategies\n    between workloads make use of reusable patterns (such as the\n    strategies previously described),\n  \n    Common anti-patterns:\n  \n     \n     \n     \n     \n  \n      \n        Implementing inconsistent recovery procedures for workloads with\n        similar DR objectives.\n      \n    \n      \n        Leaving the DR strategy to be implemented ad-hoc when a disaster\n        occurs.\n      \n    \n      \n        Having no plan for disaster recovery.\n      \n    \n      \n        Dependency on control plane operations during recovery.\n      \n    \n    Benefits of establishing this best\n    practice:\n  \n     \n     \n  \n      \n        Using defined recovery strategies allows you to use common\n        tooling and test procedures.\n      \n    \n      \n        Using defined recovery strategies improves knowledge sharing between teams and implementation of DR on\n        the workloads they own.\n      \n    \n    Level of risk exposed if this best practice\n      is not established: High. Without a planned, implemented, and tested DR strategy, you are\n    unlikely to achieve recovery objectives in the event of a\n    disaster.\n  \n\n  Implementation guidance\n    \n      A DR strategy relies on the ability to stand up your workload in a\n      recovery site if your primary location becomes unable to run the\n      workload. The most common recovery objectives are RTO and RPO, as\n      discussed in REL13-BP01 Define recovery objectives for downtime and data\n  loss.\n    \n    \n      A DR strategy across multiple Availability Zones (AZs) within a\n      single AWS Region, can provide mitigation against disaster events\n      like fires, floods, and major power outages. If it is a requirement\n      to implement protection against an unlikely event that prevents your\n      workload from being able to run in a given AWS Region, you can use a\n      DR strategy that uses multiple Regions.\n    \n    \n      When architecting a DR strategy across multiple Regions, you should\n      choose one of the following strategies. They are listed in\n      increasing order of cost and complexity, and decreasing order of RTO\n      and RPO. Recovery Region refers to an AWS Region other than the primary one used for your workload.\n    \n    \n    \n       \n        \n       \n       \n      Figure 17: Disaster recovery (DR) strategies\n    \n    \n     \n    \n    \n       \n       \n       \n       \n    \n        \n          Backup and restore (RPO in\n          hours, RTO in 24 hours or less): Back up your data and\n          applications into the recovery Region. Using automated or\n          continuous backups will permit point in time recovery (PITR), which can\n          lower RPO to as low as 5 minutes in some cases. In the event of\n          a disaster, you will deploy your infrastructure (using\n          infrastructure as code to reduce RTO), deploy your code, and\n          restore the backed-up data to recover from a disaster in the\n          recovery Region.\n        \n      \n        \n          Pilot light (RPO in minutes,\n          RTO in tens of minutes): Provision a copy of your core workload\n          infrastructure in the recovery Region. Replicate your data into\n          the recovery Region and create backups of it there. Resources\n          required to support data replication and backup, such as\n          databases and object storage, are always on. Other elements such\n          as application servers or serverless compute are not deployed,\n          but can be created when needed with the necessary configuration\n          and application code.\n        \n      \n        \n          Warm standby (RPO in seconds,\n          RTO in minutes): Maintain a scaled-down but fully functional\n          version of your workload always running in the recovery Region.\n          Business-critical systems are fully duplicated and are always\n          on, but with a scaled down fleet. Data is replicated and live in\n          the recovery Region. When the time comes for recovery, the\n          system is scaled up quickly to handle the production load. The\n          more scaled-up the warm standby is, the lower RTO and control\n          plane reliance will be. When fully scales this is known as\n          hot standby.\n        \n      \n        \n          Multi-Region (multi-site)\n            active-active (RPO near zero, RTO potentially zero):\n          Your workload is deployed to, and actively serving traffic from,\n          multiple AWS Regions. This strategy requires you to synchronize\n          data across Regions. Possible conflicts caused by writes to the\n          same record in two different regional replicas must be avoided\n          or handled, which can be complex. Data replication is useful for\n          data synchronization and will protect you against some types of\n          disaster, but it will not protect you against data corruption or\n          destruction unless your solution also includes options for\n          point-in-time recovery.\n        \n      \n    Note The difference between pilot light and warm standby can sometimes be difficult to\n        understand. Both include an environment in your recovery Region with copies of your primary\n        region assets. The distinction is that pilot light cannot process requests without additional\n        action taken first, while warm standby can handle traffic (at reduced capacity levels)\n        immediately. Pilot light will require you to turn on servers, possibly deploy additional\n        (non-core) infrastructure, and scale up, while warm standby only requires you to scale up\n        (everything is already deployed and running). Choose between these based on your RTO and RPO\n        needs. \n        When cost is a concern, and you wish to achieve a similar RPO and RTO objectives as defined in the warm standby strategy, you could consider cloud native solutions, like AWS Elastic Disaster Recovery, that take the pilot light approach and offer improved RPO and RTO targets. \n      \n    \n      Implementation steps\n    \n    \n       \n       \n       \n       \n       \n       \n    \n        \n          Determine a DR strategy that will\n          satisfy recovery requirements for this workload.\n        \n         Choosing a DR strategy is a trade-off between reducing downtime and data loss (RTO\n          and RPO) and the cost and complexity of implementing the strategy. You should avoid\n          implementing a strategy that is more stringent than it needs to be, as this incurs\n          unnecessary costs. \n         For example, in the following diagram, the business has determined their maximum\n          permissible RTO as well as the limit of what they can spend on their service restoration\n          strategy. Given the business’ objectives, the DR strategies pilot light or warm standby\n          will satisfy both the RTO and the cost criteria. \n        \n           \n            \n           \n           \n          \n            Figure 18: Choosing a DR strategy based on RTO and\n              cost\n          \n        \n         To learn more, see Business Continuity Plan (BCP). \n      \n        \n          Review the patterns for how the selected DR strategy can be\n            implemented.\n        \n         This step is to understand how you will implement the selected strategy. The\n          strategies are explained using AWS Regions as the primary and recovery sites. However,\n          you can also choose to use Availability Zones within a single Region as your DR strategy,\n          which makes use of elements of multiple of these strategies. \n         In the following steps, you can apply the strategy to your specific workload. \n        \n          Backup and restore  \n        \n          Backup and restore is the least complex strategy to implement, but\n          will require more time and effort to restore the workload, leading to higher RTO and RPO.\n          It is a good practice to always make backups of your data, and copy these to another site\n          (such as another AWS Region). \n        \n           \n            \n           \n           \n          \n            Figure 19: Backup and restore architecture\n          \n        \n         For more details on this strategy see Disaster Recovery (DR) Architecture on AWS, Part II: Backup and Restore with Rapid\n            Recovery. \n        \n          Pilot light\n        \n         With the pilot light approach, you replicate your data from your\n          primary Region to your recovery Region. Core resources used for the workload\n          infrastructure are deployed in the recovery Region, however additional resources and any\n          dependencies are still needed to make this a functional stack. For example, in Figure 20,\n          no compute instances are deployed. \n        \n           \n            \n           \n           \n          \n            Figure 20: Pilot light architecture\n          \n        \n         For more details on this strategy, see Disaster Recovery (DR) Architecture on AWS, Part III: Pilot Light and Warm\n            Standby. \n        \n          Warm standby\n        \n         The warm standby approach involves ensuring that there is a\n          scaled down, but fully functional, copy of your production environment in another Region.\n          This approach extends the pilot light concept and decreases the time to recovery because\n          your workload is always-on in another Region. If the recovery Region is deployed at full\n          capacity, then this is known as hot standby. \n        \n           \n            \n           \n           \n          \n            Figure 21: Warm standby architecture\n          \n        \n         Using warm standby or pilot light requires scaling up resources in the recovery\n          Region. To verify capacity is available when needed, consider the use for capacity reservations for EC2 instances. If using AWS Lambda, then provisioned\n            concurrency can provide runtime environments so that they are prepared to\n          respond immediately to your function's invocations. \n         For more details on this strategy, see Disaster Recovery (DR) Architecture on AWS, Part III: Pilot Light and Warm\n            Standby. \n        \n          Multi-site active/active\n        \n         You can run your workload simultaneously in multiple Regions as part of\n            a multi-site active/active strategy. Multi-site active/active\n          serves traffic from all regions to which it is deployed. Customers may select this\n          strategy for reasons other than DR. It can be used to increase availability, or when\n          deploying a workload to a global audience (to put the endpoint closer to users and/or to\n          deploy stacks localized to the audience in that region). As a DR strategy, if the workload\n          cannot be supported in one of the AWS Regions to which it is deployed, then that Region\n          is evacuated, and the remaining Regions are used to maintain availability. Multi-site\n          active/active is the most operationally complex of the DR strategies, and should only be\n          selected when business requirements necessitate it. \n        \n           \n            \n           \n           \n          \n            Figure 22: Multi-site active/active architecture\n          \n        \n         \n         For more details on this strategy, see Disaster Recovery (DR) Architecture on AWS, Part IV: Multi-site\n          Active/Active. \n        \n          AWS Elastic Disaster Recovery\n        \n         If you are considering the pilot light or warm standby strategy for disaster\n          recovery, AWS Elastic Disaster Recovery could provide an alternative approach with improved benefits. Elastic Disaster Recovery\n          can offer an RPO and RTO target similar to warm standby, but maintain the low-cost\n          approach of pilot light. Elastic Disaster Recovery replicates your data from your primary region to your\n          recovery Region, using continual data protection to achieve an RPO measured in seconds and\n          an RTO that can be measured in minutes. Only the resources required to replicate the data\n          are deployed in the recovery region, which keeps costs down, similar to the pilot light\n          strategy. When using Elastic Disaster Recovery, the service coordinates and orchestrates the recovery of\n          compute resources when initiated as part of failover or drill. \n        \n           \n            \n           \n           \n          \n            Figure 23: AWS Elastic Disaster Recovery architecture\n          \n        \n        \n          Additional practices for protecting data\n        \n         With all strategies, you must also mitigate against a data disaster. Continuous data\n          replication protects you against some types of disaster, but it may not protect you\n          against data corruption or destruction unless your strategy also includes versioning of\n          stored data or options for point-in-time recovery. You must also back up the replicated\n          data in the recovery site to create point-in-time backups in addition to the replicas. \n        \n          Using multiple Availability Zones (AZs) within a single\n            AWS Region\n        \n         When using multiple AZs within a single Region, your DR implementation uses multiple\n          elements of the above strategies. First you must create a high-availability (HA)\n          architecture, using multiple AZs as shown in Figure 23. This architecture makes use of a\n          multi-site active/active approach, as the Amazon EC2 instances and the Elastic Load Balancer have resources deployed in multiple AZs, actively handing\n          requests. The architecture also demonstrates hot standby, where if the primary Amazon RDS instance fails (or the AZ itself fails), then the standby instance is\n          promoted to primary. \n        \n           \n            \n           \n           \n          \n            Figure 24: Multi-AZ architecture\n          \n        \n         In addition to this HA architecture, you need to add backups of all data required to\n          run your workload. This is especially important for data that is constrained to a single\n          zone such as Amazon EBS volumes or Amazon Redshift clusters. If an\n          AZ fails, you will need to restore this data to another AZ. Where possible, you should\n          also copy data backups to another AWS Region as an additional layer of protection. \n         An less common alternative approach to single Region, multi-AZ DR is illustrated in\n          the blog post, Building highly resilient applications using Amazon Application Recovery Controller,\n            Part 1: Single-Region stack. Here, the strategy is to maintain as much isolation\n          between the AZs as possible, like how Regions operate. Using this alternative strategy,\n          you can choose an active/active or active/passive approach. \n        NoteSome workloads have regulatory data residency requirements. If this applies to your\n            workload in a locality that currently has only one AWS Region, then multi-Region will\n            not suit your business needs. Multi-AZ strategies provide good protection against most\n            disasters. \n      \n        \n          Assess the resources of your workload, and what their configuration\n            will be in the recovery Region prior to failover (during normal operation).\n        \n         For infrastructure and AWS resources use infrastructure as code such as AWS CloudFormation or third-party tools like\n          Hashicorp Terraform. To deploy across multiple accounts and Regions with a single\n          operation you can use AWS CloudFormation\n            StackSets. For Multi-site active/active and Hot Standby strategies, the deployed\n          infrastructure in your recovery Region has the same resources as your primary Region. For\n          Pilot Light and Warm Standby strategies, the deployed infrastructure will require\n          additional actions to become production ready. Using CloudFormation parameters and conditional logic, you can control whether a deployed stack is active or\n          standby with a single template. When using Elastic Disaster Recovery, the service will replicate and orchestrate\n          the restoration of application configurations and compute resources. \n         All DR strategies require that data sources are backed up within the AWS Region,\n          and then those backups are copied to the recovery Region. AWS Backup provides a centralized view where you can configure,\n          schedule, and monitor backups for these resources. For Pilot Light, Warm Standby, and\n          Multi-site active/active, you should also replicate data from the primary Region to data\n          resources in the recovery Region, such as Amazon Relational Database Service\n            (Amazon RDS) DB instances or Amazon DynamoDB tables. These data resources are therefore live and ready to serve\n          requests in the recovery Region. \n         To learn more about how AWS services operate across Regions, see this blog series\n          on Creating a Multi-Region Application with AWS Services. \n      \n        \n          Determine and implement how you will make your recovery Region ready\n            for failover when needed (during a disaster event).\n        \n         For multi-site active/active, failover means evacuating a Region, and relying on the\n          remaining active Regions. In general, those Regions are ready to accept traffic. For Pilot\n          Light and Warm Standby strategies, your recovery actions will need to deploy the missing\n          resources, such as the EC2 instances in Figure 20, plus any other missing resources. \n         For all of the above strategies you may need to promote read-only instances of\n          databases to become the primary read/write instance. \n         For backup and restore, restoring data from backup creates resources for that data\n          such as EBS volumes, RDS DB instances, and DynamoDB tables. You also need to restore the\n          infrastructure and deploy code. You can use AWS Backup to restore data in the recovery\n          Region. See REL09-BP01 Identify and back up all data that needs to be\n  backed up, or reproduce the data from sources for more details.\n          Rebuilding the infrastructure includes creating resources like EC2 instances in addition\n          to the Amazon Virtual Private Cloud (Amazon VPC), subnets, and security\n          groups needed. You can automate much of the restoration process. To learn how, see this blog post. \n      \n        \n          Determine and implement how you will reroute traffic to failover\n            when needed (during a disaster event).\n        \n         This failover operation can be initiated either automatically or manually.\n          Automatically initiated failover based on health checks or alarms should be used with\n          caution since an unnecessary failover (false alarm) incurs costs such as non-availability\n          and data loss. Manually initiated failover is therefore often used. In this case, you\n          should still automate the steps for failover, so that the manual initiation is like the\n          push of a button. \n         There are several traffic management options to consider when using AWS services.\n          One option is to use Amazon Route 53. Using\n          Amazon Route 53, you can associate multiple IP endpoints in one or more AWS Regions with\n          a Route 53 domain name. To implement manually initiated failover you can use Amazon\n            Application Recovery Controller, which provides a highly available data plane\n          API to reroute traffic to the recovery Region. When implementing failover, use data plane\n          operations and avoid control plane ones as described in REL11-BP04 Rely on the data plane and not the control plane\n  during recovery. \n         To learn more about this and other options see this section of the Disaster Recovery Whitepaper. \n      \n        \n          Design a plan for how your workload will fail back.\n        \n         Failback is when you return workload operation to the primary Region, after a\n          disaster event has abated. Provisioning infrastructure and code to the primary Region\n          generally follows the same steps as were initially used, relying on infrastructure as code\n          and code deployment pipelines. The challenge with failback is restoring data stores, and\n          ensuring their consistency with the recovery Region in operation. \n         In the failed over state, the databases in the recovery Region are live and have the\n          up-to-date data. The goal then is to re-synchronize from the recovery Region to the\n          primary Region, ensuring it is up-to-date. \n         Some AWS services will do this automatically. If using Amazon DynamoDB global tables, even if the table in the\n          primary Region had become not available, when it comes back online, DynamoDB resumes\n          propagating any pending writes. If using Amazon Aurora Global Database and using managed planned failover, then Aurora global database's existing replication\n          topology is maintained. Therefore, the former read/write instance in the primary Region\n          will become a replica and receive updates from the recovery Region. \n         In cases where this is not automatic, you will need to re-establish the database in\n          the primary Region as a replica of the database in the recovery Region. In many cases this\n          will involve deleting the old primary database, and creating new replicas. \n         After a failover, if you can continue running in your recovery Region, consider\n          making this the new primary Region. You would still do all the above steps to make the\n          former primary Region into a recovery Region. Some organizations do a scheduled rotation,\n          swapping their primary and recovery Regions periodically (for example every three months). \n         All of the steps required to fail over and fail back should be maintained in a\n          playbook that is available to all members of the team, and is periodically reviewed. \n         When using Elastic Disaster Recovery, the service will assist in orchestrating and automating the\n          failback process. For more details, see Performing a failback.\n        \n      \n    \n      Level of effort for the Implementation Plan: High \n   \n\n  Resources\n\n      \n    \n      Related best practices:\n    \n    \n       \n       \n       \n    \n        REL09-BP01 Identify and back up all data that needs to be\n  backed up, or reproduce the data from sources\n      \n        REL11-BP04 Rely on the data plane and not the control plane\n  during recovery\n      \n        \n          REL13-BP01 Define recovery objectives for downtime and data\n  loss\n        \n      \n    \n      Related documents:\n    \n    \n       \n       \n       \n       \n       \n       \n       \n       \n       \n       \n       \n       \n       \n       \n    \n        \n          AWS           Architecture Blog: Disaster Recovery Series\n        \n      \n        \n          Disaster\n          Recovery of Workloads on AWS: Recovery in the Cloud (AWS           Whitepaper)\n        \n      \n        \n          Disaster\n          recovery options in the cloud\n        \n      \n        \n          Build\n          a serverless multi-region, active-active backend solution in\n          an hour\n        \n      \n        \n          Multi-region\n          serverless backend — reloaded\n        \n      \n        \n          RDS:\n          Replicating a Read Replica Across Regions\n        \n      \n        \n          Route 53: Configuring DNS Failover\n        \n      \n        \n          S3:\n          Cross-Region Replication\n        \n      \n        \n          What\n          Is AWS Backup?\n        \n      \n        \n          What\n          is Amazon Application Recovery Controller?\n        \n      \n        \n          AWS           Elastic Disaster Recovery\n        \n      \n        \n          HashiCorp\n          Terraform: Get Started - AWS\n        \n      \n        \n          APN\n          Partner: partners that can help with disaster recovery\n        \n      \n        \n          AWS Marketplace: products that can be used for disaster\n          recovery\n        \n      \n    \n      Related videos:\n    \n    \n       \n       \n       \n    \n        \n          Disaster\n          Recovery of Workloads on AWS\n        \n      \n        \n          AWS re:Invent\n          2018: Architecture Patterns for Multi-Region Active-Active\n          Applications (ARC209-R2)\n        \n      \n        \n          Get\n          Started with AWS Elastic Disaster Recovery | Amazon Web Services\n        \n      \n  \u003cdiv\u003e\u003cdiv\u003e\u003cdiv\u003e\u003cdiv id=\"js_error_message\"\u003e\u003cp\u003e\u003cimg src=\"https://d1ge0kk1l5kms0.cloudfront.net/images/G/01/webservices/console/warning.png\" alt=\"Warning\" /\u003e \u003cstrong\u003eJavascript is disabled or is unavailable in your browser.\u003c/strong\u003e\u003c/p\u003e\u003cp\u003eTo use the Amazon Web Services Documentation, Javascript must be enabled. Please refer to your browser's Help pages for instructions.\u003c/p\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003eDocument ConventionsREL13-BP01 Define recovery objectives for downtime and data\n  lossREL13-BP03 Test disaster recovery implementation to validate\n  the implementation",
  "REL13-BP03 Test disaster recovery implementation to validate\n  the implementationRegularly test failover to your recovery site to verify that it operates properly and that RTO and RPO are met.\n    Common anti-patterns:\n  \n     \n  \n      \n        Never exercise failovers in production.\n      \n    \n    Benefits of establishing this best\n    practice: Regularly testing you disaster recovery plan\n    verifies that it will work when it needs to, and that your team knows\n    how to perform the strategy.\n  \n    Level of risk exposed if this best practice\n    is not established: High\n  \n\n  Implementation guidance\n    \n      A pattern to avoid is developing recovery paths that are rarely\n      exercised. For example, you might have a secondary data store that\n      is used for read-only queries. When you write to a data store and\n      the primary fails, you might want to fail over to the secondary data\n      store. If you don’t frequently test this failover, you might find\n      that your assumptions about the capabilities of the secondary data\n      store are incorrect. The capacity of the secondary, which might have\n      been sufficient when you last tested, might be no longer be able to\n      tolerate the load under this scenario. Our experience has shown that\n      the only error recovery that works is the path you test frequently.\n      This is why having a small number of recovery paths is best. You can\n      establish recovery patterns and regularly test them. If you have a\n      complex or critical recovery path, you still need to regularly\n      exercise that failure in production to convince yourself that the\n      recovery path works. In the example we just discussed, you should\n      fail over to the standby regularly, regardless of need.\n    \n    \n      Implementation steps\n    \n    \n       \n       \n        \n        Engineer your workloads for recovery. Regularly test your recovery\n        paths. Recovery-oriented computing identifies the\n        characteristics in systems that enhance recovery: isolation and redundancy, system-wide ability\n        to roll back changes, ability to monitor and determine health,\n        ability to provide diagnostics, automated recovery, modular\n        design, and ability to restart. Exercise the recovery path to\n        verify that you can accomplish the recovery in the specified time\n        to the specified state. Use your runbooks during this recovery to\n        document problems and find solutions for them before the next\n        test.\n      \n      For Amazon EC2-based workloads, use AWS Elastic Disaster Recovery to implement and launch drill instances for your DR strategy. AWS Elastic Disaster Recovery provides the ability to efficiently run drills, which helps you prepare for a failover event. You can also frequently launch of your instances using Elastic Disaster Recovery for test and drill purposes without redirecting the traffic.\n      \n    \n   \n   \n\n  Resources\n\n      \n    \n      Related documents:\n    \n    \n       \n       \n       \n       \n       \n       \n       \n       \n    \n        \n          APN\n          Partner: partners that can help with disaster recovery\n        \n      \n        \n          AWS           Architecture Blog: Disaster Recovery Series\n        \n      \n        \n          AWS Marketplace: products that can be used for disaster\n          recovery\n        \n      \n        \n          AWS Elastic Disaster Recovery\n        \n      \n        \n          Disaster\n          Recovery of Workloads on AWS: Recovery in the Cloud (AWS           Whitepaper)\n        \n      \n        \n          AWS Elastic Disaster Recovery Preparing for Failover\n        \n      \n        \n          The\n          Berkeley/Stanford recovery-oriented computing project\n        \n      \n        \n          What\n          is AWS Fault Injection Simulator?\n        \n      \n    \n      Related videos:\n    \n    \n       \n       \n    \n        \n          AWS re:Invent\n          2018: Architecture Patterns for Multi-Region Active-Active\n          Applications\n        \n      \n        \n          AWS re:Invent\n          2019: Backup-and-restore and disaster-recovery solutions with\n          AWS\n        \n      \n  \u003cdiv\u003e\u003cdiv\u003e\u003cdiv\u003e\u003cdiv id=\"js_error_message\"\u003e\u003cp\u003e\u003cimg src=\"https://d1ge0kk1l5kms0.cloudfront.net/images/G/01/webservices/console/warning.png\" alt=\"Warning\" /\u003e \u003cstrong\u003eJavascript is disabled or is unavailable in your browser.\u003c/strong\u003e\u003c/p\u003e\u003cp\u003eTo use the Amazon Web Services Documentation, Javascript must be enabled. Please refer to your browser's Help pages for instructions.\u003c/p\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003eDocument ConventionsREL13-BP02 Use defined recovery strategies to meet the recovery\n  objectivesREL13-BP04 Manage configuration drift at the DR site or\n  Region",
  "REL13-BP04 Manage configuration drift at the DR site or\n  Region\n    To perform a successful disaster recovery (DR) procedure, your\n    workload must be able to resume normal operations in a timely manner\n    with no relevant loss of functionality or data once the DR\n    environment has been brought online. To achieve this goal, it's\n    essential to maintain consistent infrastructure, data, and\n    configurations between your DR environment and the primary\n    environment.\n  \n    Desired outcome: Your disaster\n    recovery site's configuration and data are in parity with the\n    primary site, which facilitates rapid and complete recovery when\n    needed.\n  \n    Common anti-patterns:\n  \n     \n     \n     \n     \n  \n      \n        You fail to update recovery locations when changes are made to\n        the primary locations, which results in outdated configurations\n        that could hinder recovery efforts.\n      \n    \n      \n        You do not consider potential limitations such as service\n        differences between primary and recovery locations, which can\n        lead to unexpected failures during failover.\n      \n    \n      \n        You rely on manual processes to update and synchronize the DR\n        environment, which increases the risk of human error and\n        inconsistency.\n      \n    \n      \n        You fail to detect configuration drift, which leads to a false\n        sense of DR site readiness prior to an incident.\n      \n    \n    Benefits of establishing this best\n    practice: Consistency between the DR environment and the\n    primary environment significantly improves the likelihood of a\n    successful recovery after an incident and reduces the risk of a\n    failed recovery procedure.\n  \n    Level of risk exposed if this best practice\n    is not established: High\n  \n\n  Implementation guidance\n\n      \n    \n      A comprehensive approach to configuration management and failover\n      readiness can help you verify that the DR site is consistently\n      updated and prepared to take over in the event of a primary site\n      failure.\n    \n    \n      To achieve consistency between your primary and disaster recovery\n      (DR) environments, validate that your delivery pipelines\n      distribute applications to both your primary and DR sites. Roll\n      out changes to the DR sites after an appropriate evaluation period\n      (also known as staggered deployments) to\n      detect problems at the primary site and halt the deployment before\n      they spread. Implement monitoring to detect configuration drift,\n      and track changes and compliance across your environments. Perform\n      automated remediation in the DR site to keep it fully consistent\n      and ready to take over in the event of an incident.\n    \n     \n\n  Implementation steps\n\n      \n      \n         \n         \n         \n         \n         \n         \n         \n         \n         \n      \n          \n            Validate that the DR region contains the AWS services and\n            features required for a successful execution of your DR\n            plan.\n          \n        \n          \n            Use infrastructure as code (IaC). Keep your production\n            infrastructure and application configuration templates\n            accurate, and regularly apply them to your disaster recovery\n            environment.\n            AWS CloudFormation can detect drift between what your\n            CloudFormation templates specify and what is actually\n            deployed.\n          \n        \n          \n            Configure CI/CD pipelines to deploy applications and\n            infrastructure updates to all environments, including\n            primary and DR sites. CI/CD solutions such as\n            AWS CodePipeline can automate the deployment process,\n            which reduces the risk of configuration drift.\n          \n        \n          \n            Stagger deployments between the primary and DR environments.\n            This approach allows updates to be initially deployed and\n            tested in the primary environment, which isolates issues in\n            the primary site before they are propagated to the DR site.\n            This approach prevents defects from being simultaneously\n            pushed to production and the DR site at the same time and\n            maintains the integrity of the DR environment.\n          \n        \n          \n            Continually monitor resource configurations in both primary\n            and DR environments. Solutions such as\n            AWS Config can help to enforce configuration compliance\n            and detect drift, which helps maintain the consistent\n            configurations across environments.\n          \n        \n          \n            Implement alerting mechanisms to track and notify upon any\n            configuration drift or data replication interruption or lag.\n          \n        \n          \n            Automate the remediation of detected configuration drift.\n          \n        \n          \n            Schedule regular audits and compliance checks to verify\n            ongoing alignment between primary and DR configurations.\n            Periodic reviews help you maintain compliance with defined\n            rules and identify any discrepancies that need to be\n            addressed.\n          \n        \n          \n            Check for mismatches in AWS provisioned capacity, service\n            quotas, throttle limits, and configuration and version\n            discrepancies.\n          \n        \n     \n   \n\n  Resources\n\n      \n    \n      Related best practices:\n    \n    \n       \n       \n       \n       \n    \n        \n          REL01-BP01\n          Aware of service quotas and constraints\n        \n      \n        \n          REL01-BP02\n          Manage service quotas across accounts and regions\n        \n      \n        \n          REL01-BP04\n          Monitor and manage quotas\n        \n      \n        \n          REL13-BP03\n          Test disaster recovery implementation to validate the\n          implementation\n        \n      \n    \n      Related documents:\n    \n    \n       \n       \n       \n       \n       \n       \n       \n       \n    \n        \n          Remediating\n          Noncompliant AWS Resources by AWS Config Rules\n        \n      \n        \n          AWS Systems Manager Automation\n        \n      \n        \n          AWS CloudFormation: Detecting unmanaged configuration changes to\n          stacks and resources\n        \n      \n        \n          AWS CloudFormation: Detect Drift on an Entire CloudFormation\n          Stack\n        \n      \n        \n          AWS Systems Manager Automation\n        \n      \n        \n          Disaster\n          Recovery of Workloads on AWS: Recovery in the Cloud (AWS           Whitepaper)\n        \n      \n        \n          How\n          do I implement an Infrastructure Configuration Management\n          solution on AWS?\n        \n      \n        \n          Remediating\n          Noncompliant AWS Resources by AWS Config Rules\n        \n      \n    \n      Related videos:\n    \n    \n       \n    \n        \n          AWS re:Invent\n          2018: Architecture Patterns for Multi-Region Active-Active\n          Applications (ARC209-R2)\n        \n      \n    \n      Related examples:\n      \n    \n    \n       \n       \n       \n       \n       \n       \n    \n        \n          AWS CloudFormation\n            Registry\n        \n      \n        \n          Quota\n          Monitor for AWS\n        \n      \n        \n          Implement\n          automatic drift remediation for AWS CloudFormation using\n          Amazon CloudWatch and AWS Lambda\n        \n      \n        \n          AWS           Architecture Blog: Disaster Recovery Series\n        \n      \n        \n          AWS Marketplace: products that can be used for disaster\n          recovery\n        \n      \n        \n          Automating\n          safe, hands-off deployments\n        \n      \n  \u003cdiv\u003e\u003cdiv\u003e\u003cdiv\u003e\u003cdiv id=\"js_error_message\"\u003e\u003cp\u003e\u003cimg src=\"https://d1ge0kk1l5kms0.cloudfront.net/images/G/01/webservices/console/warning.png\" alt=\"Warning\" /\u003e \u003cstrong\u003eJavascript is disabled or is unavailable in your browser.\u003c/strong\u003e\u003c/p\u003e\u003cp\u003eTo use the Amazon Web Services Documentation, Javascript must be enabled. Please refer to your browser's Help pages for instructions.\u003c/p\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003eDocument ConventionsREL13-BP03 Test disaster recovery implementation to validate\n  the implementationREL13-BP05 Automate recovery",
  "REL13-BP05 Automate recovery\n    Implement tested and automated recovery mechanisms that are\n    reliable, observable, and reproducible to reduce the risk and\n    business impact of failure.\n  \n    Desired outcome: You have\n    implemented a well-documented, standardized, and thoroughly-tested\n    automation workflow for recovery processes. Your recovery automation\n    automatically corrects minor issues that pose low risk of data loss\n    or unavailability. You are able to quickly invoke recovery processes\n    for serious incidents, observe the remediation behavior while they\n    operate, and end the processes if you observe dangerous situations\n    or failures.\n  \n    Common anti-patterns:\n  \n     \n     \n     \n     \n  \n      \n        You depend on components or mechanisms that are in a failed or\n        degraded state as part of your recovery plan.\n      \n    \n      \n        Your recovery processes require manual intervention, such as\n        console access (also known as click ops).\n      \n    \n      \n        You automatically initiate recovery procedures in situations\n        that present a high risk of data loss or unavailability.\n      \n    \n      \n        You fail to include a mechanism to abort a recovery procedure\n        (like an Andon cord or big red\n        stop button) that is not working or that poses\n        additional risks.\n      \n    \n    Benefits of establishing this best\n    practice:\n  \n     \n     \n     \n     \n  \n      \n        Increased reliability, predictability, and consistency of\n        recovery operations.\n      \n    \n      \n        Ability to meet more stringent recovery objectives, including\n        Recovery Time Objective (RTO) and Recovery Point Objective\n        (RPO).\n      \n    \n      \n        Reduced likelihood of recovery failing during an incident.\n      \n    \n      \n        Reduced risk of failures associated with manual recovery\n        processes that are prone to human error.\n      \n    \n    Level of risk exposed if this best practice\n    is not established: Medium\n  \n\n  Implementation guidance\n\n      \n    \n      To implement automated recovery, you need a comprehensive approach\n      that uses AWS services and best practices. To start, identify\n      critical components and potential failure points in your workload.\n      Develop automated processes that can recover your workloads and\n      data from failures without human intervention.\n    \n    \n      Develop your recovery automation using infrastructure as code\n      (IaC) principles. This makes your recovery environment consistent\n      with the source environment and allows for version control of your\n      recovery processes. To orchestrate complex recovery workflows,\n      consider solutions such as\n      AWS Systems Manager Automations or\n      AWS Step Functions.\n    \n    \n      Automation of recovery processes provides significant benefits and\n      can help you more easily achieve your Recovery Time Objective\n      (RTO) and Recovery Point Objective (RPO). However, they can\n      encounter unexpected situations that may cause them to fail or\n      create new risks of their own such as additional downtime and data\n      loss. To mitigate this risk, provide the ability to quickly halt a\n      recovery automation in progress. Once halted, you can investigate\n      and take corrective steps.\n    \n    \n      For supported workloads, consider solutions such as AWS Elastic\n      Disaster Recovery (AWS DRS) to provide automated failover. AWS DRS\n      continually replicates your machines (including operating system,\n      system state configuration, databases, applications, and files)\n      into a staging area in your target AWS account and preferred\n      Region. If an incident occurs, AWS DRS automates the conversion of\n      your replicated servers into fully-provisioned workloads in your\n      recovery Region on AWS.\n    \n    \n      Maintenance and improvement of automated recovery is an ongoing\n      process. Continually test and refine your recovery procedures\n      based on lessons learned, and stay updated on new AWS services and\n      features that can enhance your recovery capabilities.\n    \n     \n\n  Implementation steps\n\n      \n      \n         \n         \n         \n      \n          \n            Plan for automated\n            recovery\n          \n          \n             \n             \n             \n             \n             \n             \n          \n              \n                Conduct a thorough review of your workload architecture,\n                components, and dependencies to identify and plan\n                automated recovery mechanisms. Categorize your\n                workload's dependencies into hard\n                and soft dependencies. Hard\n                dependencies are those that the workload cannot operate\n                without and for which no substitute can be provided.\n                Soft dependencies are those that the workload ordinarily\n                uses but are replaceable with temporary substitute\n                systems or processes or can be handled by\n                graceful\n                degradation.\n              \n            \n              \n                Establish processes to identify and recover missing or\n                corrupted data.\n              \n            \n              \n                Define steps to confirm a recovered steady state after\n                recovery actions have been completed.\n              \n            \n              \n                Consider any actions required to make the recovered\n                system ready for full service, such as pre-warming and\n                populating caches.\n              \n            \n              \n                Consider problems that could be encountered during the\n                recovery process and how to detect and remediate them.\n              \n            \n              \n                Consider scenarios where the primary site and its\n                control plane are inaccessible. Verify that recovery\n                actions can be performed independently without reliance\n                on the primary site. Consider solutions such as\n                Amazon Application Recovery Controller (ARC) to\n                redirect traffic without the need to manually mutate DNS\n                records.\n              \n            \n        \n          \n            Develop automated recovery\n            process\n          \n          \n             \n             \n             \n          \n              \n                Implement automated fault detection and failover\n                mechanisms for hands-free recovery. Build dashboards\n                such as with\n                Amazon CloudWatch to report the progress and health of\n                automated recovery procedures. Include procedures to\n                validate successful recovery. Provide a mechanism to\n                abort a recovery in process.\n              \n            \n              \n                Build\n                playbooks\n                as a fallback process for faults that cannot be\n                automatically recovered from, and take into\n                consideration your\n                disaster\n                recovery plan.\n              \n            \n              \n                Test recovery processes as discussed in\n                REL13-BP03.\n              \n            \n        \n          \n            Prepare for recovery\n          \n          \n             \n             \n             \n             \n          \n              \n                Evaluate the state of your recovery site and deploy\n                critical components to it in advance. For more detail,\n                see\n                REL13-BP04.\n              \n            \n              \n                Define clear roles, responsibilities, and\n                decision-making processes for recovery operations,\n                involving relevant stakeholders and teams across the\n                organization.\n              \n            \n              \n                Define the conditions to initiate your recovery\n                processes.\n              \n            \n              \n                Create a plan to revert the recovery process and fall\n                back to your primary site if required or after it's\n                considered safe.\n              \n            \n        \n     \n   \n\n  Resources\n\n      \n    \n      Related best practices:\n    \n    \n       \n       \n       \n       \n       \n    \n        \n          REL07-BP01\n          Use automation when obtaining or scaling resources\n        \n      \n        \n          REL11-BP01\n          Monitor all components of the workload to detect\n          failures\n        \n      \n        \n          REL13-BP02\n          Use defined recovery strategies to meet the recovery\n          objectives\n        \n      \n        \n          REL13-BP03\n          Test disaster recovery implementation to validate the\n          implementation\n        \n      \n        \n          REL13-BP04\n          Manage configuration drift at the DR site or Region\n        \n      \n    \n      Related documents:\n    \n    \n       \n       \n       \n       \n       \n       \n       \n       \n       \n       \n    \n        \n          AWS           Architecture Blog: Disaster Recovery Series\n        \n      \n        \n          Disaster\n          Recovery of Workloads on AWS: Recovery in the Cloud (AWS           Whitepaper)\n        \n      \n        \n          Orchestrate\n          Disaster Recovery Automation using Amazon Route 53 ARC and AWS Step Functions\n        \n      \n        \n          Build\n          AWS Systems Manager Automation runbooks using AWS CDK\n        \n      \n        \n          AWS Marketplace: Products That Can Be Used for Disaster\n          Recovery\n        \n      \n        \n          AWS Systems Manager Automation\n        \n      \n        \n          AWS           Elastic Disaster Recovery\n        \n      \n        \n          Using\n          Elastic Disaster Recovery for Failover and Failback\n        \n      \n        \n          AWS           Elastic Disaster Recovery Resources\n        \n      \n        \n          APN\n          Partner: Partners That Can Help with Disaster Recovery\n        \n      \n    \n      Related videos:\n    \n    \n       \n       \n    \n        \n          AWS re:Invent\n          2018: Architecture Patterns for Multi-Region Active-Active\n          Applications (ARC209-R2)\n        \n      \n        \n          AWS re:Invent\n          2022: AWS On Air ft. AWS Failback for AWS Elastic Disaster\n          Recovery\n        \n      \n  \u003cdiv\u003e\u003cdiv\u003e\u003cdiv\u003e\u003cdiv id=\"js_error_message\"\u003e\u003cp\u003e\u003cimg src=\"https://d1ge0kk1l5kms0.cloudfront.net/images/G/01/webservices/console/warning.png\" alt=\"Warning\" /\u003e \u003cstrong\u003eJavascript is disabled or is unavailable in your browser.\u003c/strong\u003e\u003c/p\u003e\u003cp\u003eTo use the Amazon Web Services Documentation, Javascript must be enabled. Please refer to your browser's Help pages for instructions.\u003c/p\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003eDocument ConventionsREL13-BP04 Manage configuration drift at the DR site or\n  RegionPerformance efficiency",
  "PERF01-BP01 Learn\n      about and understand available cloud services and features Continually learn about and discover available services and configurations that help you\n    make better architectural decisions and improve performance efficiency in your workload\n    architecture. \n    Common anti-patterns:\n  \n     \n     \n     \n     \n     \n  \n       You use the cloud as a collocated data center. \n    \n       You do not modernize your application after migration to the cloud. \n    \n       You only use one storage type for all things that need to be persisted. \n    \n       You use instance types that are closest matched to your current standards, but are\n        larger where needed. \n    \n       You deploy and manage technologies that are available as managed services. \n    \n    Benefits of establishing this best practice: By considering new\n    services and configurations, you may be able to greatly improve performance, reduce cost, and\n    optimize the effort required to maintain your workload. It can also help you accelerate the\n    time-to-value for cloud-enabled products. \n    Level of risk exposed if this best practice is not established:\n    High \n\n    Implementation guidance\n     AWS continually releases new services and features that can improve performance and\n      reduce the cost of cloud workloads. Staying up-to-date with these new services and features is\n      crucial for maintaining performance efficacy in the cloud. Modernizing your workload\n      architecture also helps you accelerate productivity, drive innovation, and unlock more growth\n      opportunities. \n     \n\n      Implementation steps\n      \n         \n         \n         \n         \n         \n      \n           Inventory your workload software and architecture for related services. Decide\n            which category of products to learn more about. \n        \n           Explore AWS offerings to identify and learn about the relevant services and\n            configuration options that can help you improve performance and reduce cost and\n            operational complexity. \n          \n             \n             \n             \n             \n             \n             \n             \n             \n             \n             \n          \n              \n                  Amazon Web Services Cloud \n            \n              AWS Academy\n                \n            \n              \n                What’s New with AWS?\n              \n            \n              \n                AWS Blog\n              \n            \n              \n                AWS Skill Builder\n              \n            \n              \n                AWS Events and Webinars\n              \n            \n              \n                AWS Training and Certifications\n              \n            \n              \n                AWS Youtube\n                  Channel\n              \n            \n              \n                AWS Workshops\n              \n            \n              \n                AWS\n                  Communities\n              \n            \n        \n          Use Amazon Q to get relevant information and advice about services.\n        \n           Use sandbox (non-production) environments to learn and experiment with new services\n            without incurring extra cost. \n        \n           Continually learn about new cloud services and features. \n        \n     \n   \n\n    Resources\n\n\n    \n      Related documents:\n    \n    \n       \n       \n       \n       \n       \n       \n       \n       \n       \n       \n    \n        \n            Overview of Amazon Web Services \n      \n         Amazon EC2 features \n      \n         Learn\n            step-by-step with an AWS Partner Learning Plan \n      \n        AWS Training and Certification\n          \n      \n         My learning path to become an AWS solutions architect \n      \n        \n          AWS Architecture Center\n        \n      \n        \n          AWS Partner Network\n        \n      \n        \n          AWS Solutions Library\n        \n      \n        \n          AWS Knowledge\n            Center\n        \n      \n        \n          Build modern applications on AWS\n        \n      \n    \n      Related videos:\n    \n    \n       \n       \n       \n       \n       \n    \n        AWS re:Invent 2023 -\n            What’s new with Amazon EC2 \n      \n        AWS re:Invent 2022 -\n            Reduce your operational and infrastructure costs with Amazon ECS \n      \n        AWS re:Invent 2023 - Build\n            with the efficiency, agility \u0026 innovation of the cloud with AWS\n      \n        AWS re:Invent 2022 -\n            Deploy ML models for inference at high performance and low cost \n      \n        \n          This is my\n            Architecture\n        \n      \n    \n      Related examples:\n    \n    \n       \n       \n    \n        \n          AWS Samples\n        \n      \n        \n          AWS SDK Examples\n        \n      \n  \u003cdiv\u003e\u003cdiv\u003e\u003cdiv\u003e\u003cdiv id=\"js_error_message\"\u003e\u003cp\u003e\u003cimg src=\"https://d1ge0kk1l5kms0.cloudfront.net/images/G/01/webservices/console/warning.png\" alt=\"Warning\" /\u003e \u003cstrong\u003eJavascript is disabled or is unavailable in your browser.\u003c/strong\u003e\u003c/p\u003e\u003cp\u003eTo use the Amazon Web Services Documentation, Javascript must be enabled. Please refer to your browser's Help pages for instructions.\u003c/p\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003eDocument Conventions PERF 1. How do you select appropriate cloud resources and architecture for your workload? PERF01-BP02 Use guidance from your cloud provider or an\n  appropriate partner to learn about architecture patterns and best\n  practices",
  "PERF01-BP02 Use guidance from your cloud provider or an\n  appropriate partner to learn about architecture patterns and best\n  practices\n    Use cloud company resources such as documentation, solutions\n    architects, professional services, or appropriate partners to guide\n    your architectural decisions. These resources help you review and\n    improve your architecture for optimal performance.\n  \n    Common anti-patterns:\n  \n     \n     \n     \n  \n      \n        You use AWS as a common cloud provider.\n      \n    \n      \n        You use AWS services in a manner that they were not designed\n        for.\n      \n    \n      \n        You follow all guidance without considering your business\n        context.\n      \n    \n    Benefits of establishing this best\n    practice: Using guidance from a cloud provider or an\n    appropriate partner can help you to make the right architectural\n    choices for your workload and give you confidence in your decisions.\n  \n    Level of risk exposed if this best practice\n    is not established: Medium\n  \n\n  Implementation guidance\n\n      \n    \n      AWS offers a wide range of guidance, documentation, and resources\n      that can help you build and manage efficient cloud workloads. AWS\n      documentation provides code samples, tutorials, and detailed\n      service explanations. In addition to documentation, AWS provides\n      training and certification programs, solutions architects, and\n      professional services that can help customers explore different\n      aspects of cloud services and implement efficient cloud\n      architecture on AWS.\n    \n    \n      Leverage these resources to gain insights into valuable knowledge\n      and best practices, save time, and achieve better outcomes in the\n      AWS Cloud.\n    \n     \n\n  Implementation steps\n\n      \n      \n         \n         \n         \n         \n      \n          \n            Review AWS documentation and guidance and follow the best\n            practices. These resources can help you effectively choose\n            and configure services and achieve better performance.\n          \n          \n             \n             \n             \n             \n          \n              \n                AWS documentation (like user guides and whitepapers)\n              \n            \n              \n                AWS Blog\n              \n            \n              \n                AWS Training and Certifications\n              \n            \n              \n                AWS Youtube Channel\n              \n            \n        \n          \n            Join AWS partner events (like AWS Global Summits, AWS\n            re:Invent, user groups, and workshops) to learn from AWS\n            experts about best practices for using AWS services.\n          \n          \n             \n             \n             \n             \n          \n              \n                Learn step-by-step with an AWS Partner Learning Plan\n              \n            \n              \n                AWS Events and Webinars\n              \n            \n              \n                AWS Workshops\n              \n            \n              \n                AWS Communities\n              \n            \n        \n          \n            Reach out to AWS for assistance when you need additional\n            guidance or product information. AWS Solutions Architects\n            and AWS Professional Services provide guidance for solution\n            implementation.\n            AWS Partners provide AWS expertise to help you unlock\n            agility and innovation for your business.\n          \n        \n          \n            Use\n            Support if you need technical support to use a\n            service effectively.\n            Our\n            Support plans are designed to give you the right mix\n            of tools and access to expertise so that you can be\n            successful with AWS while optimizing performance, managing\n            risk, and keeping costs under control.\n          \n        \n     \n   \n\n  Resources\n    \n      Related documents:\n    \n    \n       \n       \n       \n       \n       \n    \n        \n          AWS           Architecture Center\n        \n      \n        AWS Partner Network\n      \n        \n          AWS           Solutions Library\n        \n      \n        \n          AWS           Knowledge Center\n        \n      \n        \n          AWS           Enterprise Support\n        \n      \n    \n      Related videos:\n    \n    \n       \n       \n       \n       \n    \n        \n          This\n          is my Architecture\n        \n      \n        AWS re:Invent 2023 - Advanced event-driven patterns with Amazon EventBridge\n        \n      \n        AWS re:Invent 2023 - Implementing distributed design patterns on AWS\n      \n        AWS re:Invent 2023 - Application architecture as code\n        \n      \n    \n      Related examples:\n    \n    \n       \n       \n       \n    \n        \n          AWS           Samples\n        \n      \n        \n          AWS           SDK Examples\n        \n      \n        AWS Analytics Reference Architecture\n        \n      \n  \u003cdiv\u003e\u003cdiv\u003e\u003cdiv\u003e\u003cdiv id=\"js_error_message\"\u003e\u003cp\u003e\u003cimg src=\"https://d1ge0kk1l5kms0.cloudfront.net/images/G/01/webservices/console/warning.png\" alt=\"Warning\" /\u003e \u003cstrong\u003eJavascript is disabled or is unavailable in your browser.\u003c/strong\u003e\u003c/p\u003e\u003cp\u003eTo use the Amazon Web Services Documentation, Javascript must be enabled. Please refer to your browser's Help pages for instructions.\u003c/p\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003eDocument ConventionsPERF01-BP01 Learn about and understand available cloud services and\n      featuresPERF01-BP03 Factor cost into architectural decisions",
  "PERF01-BP03 Factor cost into architectural decisions\n    Factor cost into your architectural decisions to improve resource\n    utilization and performance efficiency of your cloud workload. When\n    you are aware of the cost implications of your cloud workload, you\n    are more likely to leverage efficient resources and reduce wasteful\n    practices.\n  \n    Common anti-patterns:\n  \n     \n     \n     \n     \n     \n  \n      \n        You only use one family of instances.\n      \n    \n      \n        You do not evaluate licensed solutions against open-source\n        solutions.\n      \n    \n      \n        You do not define storage lifecycle policies.\n      \n    \n      \n        You do not review new services and features of the AWS Cloud.\n      \n    \n      \n        You only use block storage.\n      \n    \n    Benefits of establishing this best\n    practice: Factoring cost into your decision making allows\n    you to use more efficient resources and explore other investments.\n  \n    Level of risk exposed if this best practice\n    is not established: Medium\n  \n\n  Implementation guidance\n\n      \n    \n      Optimizing workloads for cost can improve resource utilization\n      and avoid waste in a cloud workload. Factoring cost into\n      architectural decisions usually includes right-sizing workload\n      components and enabling elasticity, which results in improved\n      cloud workload performance efficiency.\n    \n     \n\n  Implementation steps\n\n      \n      \n         \n         \n         \n         \n         \n      \n          \n            Establish cost objectives like budget limits for your cloud\n            workload.\n          \n        \n          \n            Identify the key components (like instances and storage)\n            that drive cost of your workload. You can use\n            AWS Pricing Calculator and\n            AWS Cost Explorer to identify key cost drivers in your\n            workload.\n          \n        \n          \n            Understand pricing models in the cloud, such as On-Demand, Reserved Instances, Savings Plans, and Spot Instances.\n          \n        \n          \n            Use\n            Well-Architected\n            cost optimization best practices to optimize these\n            key components for cost.\n          \n        \n          \n            Continually monitor and analyze cost to identify cost\n            optimization opportunities in your workload.\n          \n          \n             \n             \n             \n          \n              \n                Use\n                AWS                 Budgets to get alerts for unacceptable costs.\n              \n            \n              \n                Use\n                AWS Compute Optimizer or\n                AWS Trusted Advisor to get cost optimization\n                recommendations.\n              \n            \n              \n                Use\n                AWS                 Cost Anomaly Detection to get automated cost\n                anomaly detection and root cause analysis.\n              \n            \n        \n     \n   \n\n  Resources\n\n      \n    \n      Related documents:\n    \n    \n       \n       \n       \n       \n       \n       \n       \n       \n    \n        \n          What is AWS Billing and Cost Management?\n        \n      \n        \n          Cost Optimization with AWS\n      \n        \n          Choosing an AWS cost management strategy\n        \n      \n        \n          A Beginner’s Guide to AWS Cost Management\n        \n      \n        \n          A\n          Detailed Overview of the Cost Intelligence Dashboard\n        \n      \n        \n          AWS           Architecture Center\n        \n      \n        \n          AWS           Solutions Library\n        \n      \n        \n          AWS           Knowledge Center\n        \n      \n    \n      Related videos:\n    \n    \n       \n       \n       \n       \n       \n    \n        \n          This\n          is my Architecture\n        \n      \n        AWS re:Invent 2023 -  What’s new with AWS cost optimization\n        \n      \n        AWS re:Invent 2023 -  Optimize cost and performance and track progress toward mitigation\n        \n      \n        AWS re:Invent 2023 -  AWS storage cost-optimization best practices\n        \n      \n        AWS re:Invent 2023 -  Optimize costs in your multi-account environments\n        \n      \n    \n      Related examples:\n    \n    \n       \n       \n       \n       \n       \n       \n    \n        AWS Compute Optimizer Demo code\n        \n      \n        \n          Cost Optimization Workshop\n        \n      \n        \n          Cloud Financial Management Technical Implementation Playbooks\n        \n      \n        \n          Startup optimization: Tuning application performance for maximum efficiency\n        \n      \n        \n          Serverless Optimization Workshop (Performance and Cost)\n        \n      \n        \n          Scaling cost effective architectures\n        \n      \n  \u003cdiv\u003e\u003cdiv\u003e\u003cdiv\u003e\u003cdiv id=\"js_error_message\"\u003e\u003cp\u003e\u003cimg src=\"https://d1ge0kk1l5kms0.cloudfront.net/images/G/01/webservices/console/warning.png\" alt=\"Warning\" /\u003e \u003cstrong\u003eJavascript is disabled or is unavailable in your browser.\u003c/strong\u003e\u003c/p\u003e\u003cp\u003eTo use the Amazon Web Services Documentation, Javascript must be enabled. Please refer to your browser's Help pages for instructions.\u003c/p\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003eDocument ConventionsPERF01-BP02 Use guidance from your cloud provider or an\n  appropriate partner to learn about architecture patterns and best\n  practicesPERF01-BP04 Evaluate how trade-offs impact customers and\n  architecture efficiency",
  "PERF01-BP04 Evaluate how trade-offs impact customers and\n  architecture efficiency\n    When evaluating performance-related improvements, determine which\n    choices impact your customers and workload efficiency. For example,\n    if using a key-value data store increases system performance, it is\n    important to evaluate how the eventually consistent nature of this\n    change will impact customers.\n  \n    Common anti-patterns:\n  \n     \n     \n  \n      \n        You assume that all performance gains should be implemented,\n        even if there are tradeoffs for implementation.\n      \n    \n      \n        You only evaluate changes to workloads when a performance issue\n        has reached a critical point.\n      \n    \n    Benefits of establishing this best\n    practice: When you are evaluating potential\n    performance-related improvements, you must decide if the tradeoffs\n    for the changes are acceptable with the workload requirements. In\n    some cases, you may have to implement additional controls to\n    compensate for the tradeoffs.\n  \n    Level of risk exposed if this best practice\n    is not established: High\n  \n\n  Implementation guidance\n\n      \n    \n      Identify critical areas in your architecture in terms of\n      performance and customer impact. Determine how you can make\n      improvements, what trade-offs those improvements bring, and how\n      they impact the system and the user experience. For example,\n      implementing caching data can help dramatically improve\n      performance but requires a clear strategy for how and when to\n      update or invalidate cached data to prevent incorrect system\n      behavior.\n    \n     \n\n  Implementation steps\n\n      \n      \n         \n         \n         \n         \n      \n          \n            Understand your workload requirements and SLAs.\n          \n        \n          \n            Clearly define evaluation factors. Factors may relate to\n            cost, reliability, security, and performance of your workload.\n          \n        \n          \n            Select architecture and services that can address your\n            requirements.\n          \n        \n          \n            Conduct experimentation and proof of concepts (POCs) to\n            evaluate trade-off factors and impact on customers and\n            architecture efficiency. Usually, highly-available,\n            performant, and secure workloads consume more cloud\n            resources while providing better customer experience. Understand the trade-offs across your workload’s complexity, performance, and cost. \n            Typically, prioritizing two of the factors comes at the expense of the third. \n          \n        \n     \n   \n\n  Resources\n\n      \n    \n      Related documents:\n    \n    \n       \n       \n       \n       \n       \n    \n        \n          Amazon\n          Builders’ Library\n        \n      \n        \n          QuickSight KPIs\n        \n      \n        \n          Amazon CloudWatch RUM\n        \n      \n        \n          X-Ray\n          Documentation\n        \n      \n        \n          Understand resiliency patterns and trade-offs to architect efficiently in the cloud\n        \n      \n    \n      Related videos:\n    \n    \n      \n       \n       \n       \n    \n        \n          Optimize\n          applications through Amazon CloudWatch RUM\n        \n      \n        AWS re:Invent 2023 - Capacity, availability, cost efficiency: Pick three\n        \n      \n        AWS re:Invent 2023 - Advanced integration patterns \u0026 trade-offs for loosely coupled systems\n        \n      \n    \n      Related examples:\n    \n    \n       \n       \n    \n        \n          Measure\n          page load time with Amazon CloudWatch Synthetics\n        \n      \n        \n          Amazon CloudWatch RUM Web Client\n        \n      \n  \u003cdiv\u003e\u003cdiv\u003e\u003cdiv\u003e\u003cdiv id=\"js_error_message\"\u003e\u003cp\u003e\u003cimg src=\"https://d1ge0kk1l5kms0.cloudfront.net/images/G/01/webservices/console/warning.png\" alt=\"Warning\" /\u003e \u003cstrong\u003eJavascript is disabled or is unavailable in your browser.\u003c/strong\u003e\u003c/p\u003e\u003cp\u003eTo use the Amazon Web Services Documentation, Javascript must be enabled. Please refer to your browser's Help pages for instructions.\u003c/p\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003eDocument ConventionsPERF01-BP03 Factor cost into architectural decisionsPERF01-BP05 Use policies and reference architectures",
  "PERF01-BP05 Use policies and reference architectures\n    Use internal policies and existing reference architectures when\n    selecting services and configurations to be more efficient when\n    designing and implementing your workload.\n  \n    Common anti-patterns:\n  \n     \n  \n      \n        You allow a wide variety of technology that may impact the\n        management overhead of your company.\n      \n    \n    Benefits of establishing this best\n    practice: Establishing a policy for architecture,\n    technology, and vendor choices allows decisions to be made quickly.\n  \n    Level of risk exposed if this best practice\n    is not established: Medium\n  \n\n  Implementation guidance\n\n      \n    \n      Having internal policies in selecting resources and architecture\n      provides standards and guidelines to follow when making\n      architectural choices. Those guidelines streamline the decision-making\n      process when choosing the right cloud service and can help improve\n      performance efficiency. Deploy your workload using policies or\n      reference architectures. Integrate the services into your cloud\n      deployment, then use your performance tests to verify that you can\n      continue to meet your performance requirements.\n    \n     \n\n  Implementation steps\n\n      \n      \n         \n         \n         \n         \n         \n         \n      \n          \n            Clearly understand the requirements of your cloud workload.\n          \n        \n          \n            Review internal and external policies to identify the\n            most relevant ones.\n          \n        \n          \n            Use the appropriate reference architectures provided by AWS\n            or your industry best practices.\n          \n        \n          \n            Create a continuum consisting of policies, standards,\n            reference architectures, and prescriptive guidelines for\n            common situations. Doing so allows your teams to move\n            faster. Tailor the assets for your vertical if applicable.\n          \n        \n          \n            Validate these policies and reference architectures for your\n            workload in sandbox environments.\n          \n        \n          \n            Stay up-to-date with industry standards and AWS updates to\n            make sure your policies and reference architectures help\n            optimize your cloud workload.\n          \n        \n     \n   \n\n  Resources\n\n      \n    \n      Related documents:\n    \n    \n       \n       \n       \n       \n       \n    \n        \n          AWS           Architecture Center\n        \n      \n        \n          AWS Partner Network\n        \n      \n        \n          AWS           Solutions Library\n        \n      \n        \n          AWS           Knowledge Center\n        \n      \n        AWS Architecture Blog\n        \n      \n    \n      Related videos:\n    \n    \n       \n       \n    \n        \n          This\n          is my Architecture\n        \n      \n        AWS re:Invent 2022 - Accelerate value for your business with SAP \u0026 AWS reference architecture\n        \n      \n    \n      Related examples:\n    \n    \n       \n       \n    \n        \n          AWS           Samples\n        \n      \n        \n          AWS           SDK Examples\n        \n      \n  \u003cdiv\u003e\u003cdiv\u003e\u003cdiv\u003e\u003cdiv id=\"js_error_message\"\u003e\u003cp\u003e\u003cimg src=\"https://d1ge0kk1l5kms0.cloudfront.net/images/G/01/webservices/console/warning.png\" alt=\"Warning\" /\u003e \u003cstrong\u003eJavascript is disabled or is unavailable in your browser.\u003c/strong\u003e\u003c/p\u003e\u003cp\u003eTo use the Amazon Web Services Documentation, Javascript must be enabled. Please refer to your browser's Help pages for instructions.\u003c/p\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003eDocument ConventionsPERF01-BP04 Evaluate how trade-offs impact customers and\n  architecture efficiencyPERF01-BP06 Use benchmarking to drive architectural decisions",
  "PERF01-BP06 Use benchmarking to drive\n      architectural decisions Benchmark the performance of an existing workload to understand how it performs on the\n    cloud and drive architectural decisions based on that data. \n    Common anti-patterns:\n  \n     \n     \n  \n       You rely on common benchmarks that are not indicative of your workload’s\n        characteristics. \n    \n       You rely on customer feedback and perceptions as your only benchmark. \n    \n    Benefits of establishing this best practice: Benchmarking your\n    current implementation allows you to measure performance improvements. \n    Level of risk exposed if this best practice is not established:\n    Medium \n\n    Implementation guidance\n\n\n     Use benchmarking with synthetic tests to assess how your workload’s components perform.\n      Benchmarking is generally quicker to set up than load testing and is used to evaluate the\n      technology for a particular component. Benchmarking is often used at the start of a new\n      project, when you lack a full solution to load test. \n     You can either build your own custom benchmark tests or use an industry standard test,\n      such as TPC-DS, to benchmark your workloads.\n      Industry benchmarks are helpful when comparing environments. Custom benchmarks are useful for\n      targeting specific types of operations that you expect to make in your architecture. \n     When benchmarking, it is important to pre-warm your test environment to get valid\n      results. Run the same benchmark multiple times to verify that you’ve captured any variance\n      over time. \n     Because benchmarks are generally faster to run than load tests, they can be used earlier\n      in the deployment pipeline and provide faster feedback on performance deviations. When you\n      evaluate a significant change in a component or service, a benchmark can be a quick way to see\n      if you can justify the effort to make the change. Using benchmarking in conjunction with load\n      testing is important because load testing informs you about how your workload performs in\n      production. \n     \n\n      Implementation steps\n\n\n      \n         \n         \n         \n         \n         \n      \n           Plan and define: \n          \n             \n             \n             \n          \n               Define the objectives, baseline, testing scenarios, metrics (like CPU\n                utilization, latency, or throughput), and KPIs for your benchmark. \n            \n               Focus on user requirements in terms of user experience and factors such as\n                response time and accessibility. \n            \n               Identify a benchmarking tool that is suitable for your workload. You can use\n                AWS services like Amazon CloudWatch or a third-party tool that is compatible with your workload.\n              \n            \n        \n           Configure and instrument: \n          \n             \n             \n          \n               Set up your environment and configure your resources. \n            \n               Implement monitoring and logging to capture testing results. \n            \n        \n           Benchmark and monitor: \n          \n             \n          \n               Perform your benchmark tests and monitor the metrics during the test. \n            \n        \n           Analyze and document: \n          \n             \n             \n             \n          \n               Document your benchmarking process and findings. \n            \n               Analyze the results to identify bottlenecks, trends, and areas of improvement.\n              \n            \n               Use test results to make architectural decisions and adjust your workload. This\n                may include changing services or adopting new features. \n            \n        \n           Optimize and repeat: \n          \n             \n             \n             \n          \n               Adjust resource configurations and allocations based on your benchmarks.\n              \n            \n               Retest your workload after the adjustment to validate your improvements.\n              \n            \n               Document your learnings, and repeat the process to identify other areas of\n                improvement. \n            \n        \n     \n   \n\n    Resources\n\n\n    \n      Related documents:\n    \n    \n       \n       \n       \n       \n       \n       \n       \n       \n    \n        \n          AWS Architecture Center\n        \n      \n        \n          AWS Partner Network\n        \n      \n        \n          AWS Solutions Library\n        \n      \n        \n          AWS Knowledge\n            Center\n        \n      \n        \n          Amazon CloudWatch\n            RUM\n        \n      \n        \n          Amazon CloudWatch\n            Synthetics\n        \n      \n        \n          Genomics workflows, Part 5: automated benchmarking\n        \n      \n        \n          Benchmark and optimize endpoint deployment in Amazon SageMaker AI JumpStart\n        \n      \n    \n      Related videos:\n    \n    \n       \n       \n       \n       \n       \n    \n        \n          AWS re:Invent 2023 - Benchmarking AWS Lambda cold starts\n        \n      \n        \n          Benchmarking stateful services in the cloud\n        \n      \n        \n          This is my\n            Architecture\n        \n      \n        \n          Optimize applications through\n            Amazon CloudWatch RUM\n        \n      \n        \n          Demo of Amazon CloudWatch\n            Synthetics\n        \n      \n    \n      Related examples:\n    \n    \n       \n       \n       \n       \n       \n    \n        \n          AWS Samples\n        \n      \n        \n          AWS SDK Examples\n        \n      \n        \n          Distributed Load Tests\n        \n      \n        \n          Measure page load time with Amazon CloudWatch Synthetics\n        \n      \n        \n          Amazon CloudWatch RUM Web\n            Client\n        \n      \n  \u003cdiv\u003e\u003cdiv\u003e\u003cdiv\u003e\u003cdiv id=\"js_error_message\"\u003e\u003cp\u003e\u003cimg src=\"https://d1ge0kk1l5kms0.cloudfront.net/images/G/01/webservices/console/warning.png\" alt=\"Warning\" /\u003e \u003cstrong\u003eJavascript is disabled or is unavailable in your browser.\u003c/strong\u003e\u003c/p\u003e\u003cp\u003eTo use the Amazon Web Services Documentation, Javascript must be enabled. Please refer to your browser's Help pages for instructions.\u003c/p\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003eDocument ConventionsPERF01-BP05 Use policies and reference architecturesPERF01-BP07 Use a data-driven approach for architectural\n  choices",
  "PERF01-BP07 Use a data-driven approach for architectural\n  choices\n    Define a clear, data-driven approach for architectural choices to\n    verify that the right cloud services and configurations are used to\n    meet your specific business needs.\n  \n    Common anti-patterns:\n  \n     \n     \n     \n  \n      \n        You assume your current architecture is static and should not be\n        updated over time.\n      \n    \n      \n        Your architectural choices are based upon guesses and\n        assumptions.\n      \n    \n      \n        You introduce architecture changes over time without\n        justification.\n      \n    \n    Benefits of establishing this best\n    practice: By having a well-defined approach for making\n    architectural choices, you use data to influence your workload\n    design and make informed decisions over time.\n  \n    Level of risk exposed if this best practice\n    is not established: Medium\n  \n\n  Implementation guidance\n\n      \n    \n      Use internal experience and knowledge of the cloud or external\n      resources such as published use cases or whitepapers to choose\n      resources and services in your architecture. You should have a\n      well-defined process that encourages experimentation and\n      benchmarking with the services that could be used in your\n      workload.\n    \n    \n      Backlogs for critical workloads should consist of not just user\n      stories which deliver functionality relevant to business and\n      users, but also technical stories which form an architecture\n      runway for the workload. This runway is informed by new\n      advancements in technology and new services and adopts them based\n      on data and proper justification. This verifies that the\n      architecture remains future-proof and does not stagnate.\n    \n     \n\n  Implementation steps\n\n      \n      \n         \n         \n         \n         \n      \n          \n            Engage with key stakeholders to define workload\n            requirements, including performance, availability, and cost\n            considerations. Consider factors such as the number of\n            users and usage pattern for your workload.\n          \n        \n          \n            Create an architecture runway or a technology backlog which\n            is prioritized along with the functional backlog.\n          \n        \n          \n            Evaluate and assess different cloud services (for more\n            detail, see PERF01-BP01 Learn\n      about and understand available cloud services and features).\n          \n        \n          \n            Explore different architectural patterns, like microservices\n            or serverless, that meet your performance requirements (for\n            more detail, see PERF01-BP02 Use guidance from your cloud provider or an\n  appropriate partner to learn about architecture patterns and best\n  practices).\n          \n        \n      \n         \n      \n          \n            Consult other teams, architecture diagrams, and resources,\n            such as AWS Solution Architects,\n            AWS             Architecture Center, and\n            AWS Partner Network, to help you choose the right\n            architecture for your workload.\n          \n        \n      \n         \n         \n         \n         \n         \n      \n          \n            Define performance metrics like throughput and response time\n            that can help you evaluate the performance of your workload.\n          \n        \n          \n            Experiment and use defined metrics to validate the\n            performance of the selected architecture.\n          \n        \n          \n            Continually monitor and make adjustments as needed to\n            maintain the optimal performance of your architecture.\n          \n        \n          \n            Document your selected architecture and decisions as a\n            reference for future updates and learnings.\n          \n        \n          \n            Continually review and update the architecture selection\n            approach based on learnings, new technologies, and metrics\n            that indicate a needed change or problem in the current\n            approach.\n          \n        \n     \n   \n\n  Resources\n\n      \n    \n      Related documents:\n    \n    \n       \n       \n       \n    \n        \n          AWS           Solutions Library\n        \n      \n        \n          AWS           Knowledge Center\n        \n      \n        \n          Architectural Patterns to Build End-to-End Data Driven Applications on AWS\n      \n    \n      Related videos:\n    \n    \n       \n       \n       \n       \n       \n    \n        \n          This\n          is my Architecture\n        \n      \n        AWS re:Invent 2021 -  Data-driven enterprise: Going from vision to value\n        \n      \n        AWS re:Invent 2022 - Delivering sustainable, high-performing architectures \n        \n      \n        AWS re:Invent 2023 - Optimize cost and performance and track progress toward mitigation\n        \n      \n        AWS re:Invent 2022 - AWS optimization: Actionable steps for immediate results\n        \n      \n    \n      Related examples:\n    \n    \n       \n       \n    \n        \n          AWS           Samples\n        \n      \n        \n          AWS           SDK Examples\n        \n      \n  \u003cdiv\u003e\u003cdiv\u003e\u003cdiv\u003e\u003cdiv id=\"js_error_message\"\u003e\u003cp\u003e\u003cimg src=\"https://d1ge0kk1l5kms0.cloudfront.net/images/G/01/webservices/console/warning.png\" alt=\"Warning\" /\u003e \u003cstrong\u003eJavascript is disabled or is unavailable in your browser.\u003c/strong\u003e\u003c/p\u003e\u003cp\u003eTo use the Amazon Web Services Documentation, Javascript must be enabled. Please refer to your browser's Help pages for instructions.\u003c/p\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003eDocument ConventionsPERF01-BP06 Use benchmarking to drive architectural decisionsCompute and hardware",
  "PERF02-BP01 Select the best compute options for your\n  workload\n    Selecting the most appropriate compute option for your workload\n    allows you to improve performance, reduce unnecessary infrastructure\n    costs, and lower the operational efforts required to maintain your\n    workload.\n  \n    Common anti-patterns:\n  \n     \n     \n     \n  \n      \n        You use the same compute option that was used on\n        premises.\n      \n    \n      \n        You lack awareness of the cloud compute options, features, and\n        solutions, and how those solutions might improve your compute\n        performance.\n      \n    \n      \n        You over-provision an existing compute option to meet scaling or\n        performance requirements when an alternative compute option\n        would align to your workload characteristics more precisely.\n      \n    \n    Benefits of establishing this best\n    practice: By identifying the compute requirements and\n    evaluating against the options available, you can make your workload\n    more resource efficient.\n  \n    Level of risk exposed if this best practice\n    is not established: High\n  \n\n  Implementation guidance\n\n      \n    \n      To optimize your cloud workloads for performance efficiency, it is\n      important to select the most appropriate compute options for your\n      use case and performance requirements. AWS provides a variety of\n      compute options that cater to different workloads in the cloud.\n      For instance, you can use Amazon EC2 to launch and manage virtual\n      servers, AWS Lambda to run code without having to provision or\n      manage servers, Amazon ECS or Amazon EKS to run and manage containers, or\n      AWS Batch to process large volumes of data in parallel. Based on\n      your scale and compute needs, you should choose and configure the\n      optimal compute solution for your situation. You can also consider\n      using multiple types of compute solutions in a single workload, as\n      each one has its own advantages and drawbacks.\n    \n    \n      The following steps guide you through selecting the right compute\n      options to match your workload characteristics and performance\n      requirements.\n    \n   \n    \n    Implementation steps\n  \n    \n       \n       \n       \n       \n       \n       \n    \n        \n          Understand your workload compute requirements. Key\n          requirements to consider include processing needs, traffic\n          patterns, data access patterns, scaling needs, and latency\n          requirements.\n        \n      \n        \n          Learn about different AWS compute services for your\n          workload. For more information, see PERF01-BP01 Learn\n      about and understand available cloud services and features. Here are some key\n          AWS compute options, their characteristics, and common\n          use cases:\n        \n        \n              \n                \n                  AWS service\n                \n                \n                  Key characteristics\n                \n                \n                  Common use cases\n                \n              \n            \n              \n                \n                  Amazon Elastic Compute Cloud (Amazon EC2)\n                \n                \n                  Has dedicated option for hardware, license requirements,\n                  large selection of different instance families, processor\n                  types and compute accelerators\n                \n                \n                  Lift and shift migrations, monolithic application, hybrid\n                  environments, enterprise applications\n                \n              \n              \n                \n                  Amazon Elastic Container Service (Amazon ECS), Amazon Elastic Kubernetes Service (Amazon EKS) \n                \n                \n                  Easy deployment, consistent environments, scalable\n                \n                \n                  Microservices, hybrid environments\n                \n              \n              \n                \n                  AWS Lambda\n                \n                \n                   Serverless\n                    compute service that runs code in response to\n                  events and automatically manages the underlying compute\n                  resources.\n                \n                \n                  Microservices, event-driven applications\n                \n              \n              \n                \n                  AWS Batch\n                \n                \n                  Efficiently and dynamically provisions and\n                  scales Amazon Elastic Container Service (Amazon ECS), Amazon Elastic Kubernetes Service (Amazon EKS),\n                  and AWS Fargate compute resources, with an option to use\n                  On-Demand or Spot Instances based on your job requirements\n                \n                \n                  HPC, train ML models\n                \n              \n              \n                \n                  Amazon Lightsail\n                \n                \n                  Preconfigured Linux and Windows application for running\n                  small workloads\n                \n                \n                  Simple web applications, custom website\n                \n              \n            \n      \n        \n          Evaluate cost (like hourly charge or data transfer) and\n          management overhead (like patching and scaling) associated to\n          each compute option.\n        \n      \n        \n          Perform experiments and benchmarking in a non-production\n          environment to identify which compute option can best address\n          your workload requirements.\n        \n      \n        \n          Once you have experimented and identified your new compute\n          solution, plan your migration and validate your performance\n          metrics.\n        \n      \n        \n          Use AWS monitoring tools like Amazon CloudWatch and\n          optimization services like AWS Compute Optimizer to\n          continually optimize your compute resources based on real-world usage\n          patterns.\n        \n      \n    \n    \n   \n\n  Resources\n\n      \n    \n      Related documents:\n    \n    \n       \n       \n       \n       \n       \n       \n       \n    \n        \n          Cloud\n          Compute with AWS \n        \n      \n        \n          Amazon EC2\n          Instance Types \n        \n      \n        \n          Amazon EKS\n          Containers: Amazon EKS Worker Nodes \n        \n      \n        \n          Amazon ECS Containers: Amazon ECS Container\n          Instances \n        \n      \n        \n          Functions:\n          Lambda Function Configuration\n        \n      \n        Prescriptive Guidance for Containers\n        \n      \n        \n          Prescriptive\n          Guidance for Serverless\n        \n      \n    \n      Related videos:\n    \n    \n       \n       \n       \n       \n       \n       \n       \n       \n       \n       \n       \n    \n        \n          AWS\n            re:Invent 2023 - AWS Graviton: The best price performance for your AWS\n            workloads\n        \n      \n        \n          AWS\n            re:Invent 2023 - New Amazon Elastic Compute Cloud generative AI capabilities in AMS\n        \n      \n        \n          AWS\n            re:Invent 2023 - What’s new with Amazon Elastic Compute Cloud\n        \n      \n        \n          AWS\n            re:Invent 2023 - Smart savings: Amazon Elastic Compute Cloud cost-optimization strategies\n        \n      \n        \n          AWS\n            re:Invent 2021 - Powering next-gen Amazon Elastic Compute Cloud: Deep dive on the Nitro System\n        \n      \n        \n          AWS re:Invent 2019 - Optimize\n            performance and cost for your AWS compute\n        \n      \n        \n          AWS\n            re:Invent 2019 - Amazon Elastic Compute Cloud foundations\n        \n      \n        \n          AWS re:Invent 2022 - Deploy ML\n            models for inference at high performance and low cost\n        \n      \n        \n          AWS re:Invent 2019 - Optimize\n            performance and cost for your AWS compute \n        \n      \n        \n          Amazon EC2\n            foundations\n        \n      \n        \n          Deploy ML models for inference at\n            high performance and low cost\n        \n      \n    \n      Related examples:\n    \n    \n       \n       \n       \n       \n       \n       \n    \n        \n          Migrating\n          the Web application to containers\n        \n      \n        \n          Run\n          a Serverless Hello World\n        \n      \n        \n          Amazon EKS Workshop\n        \n      \n        \n          Amazon EC2 Workshop\n        \n      \n        \n          Efficient and Resilient Workloads with Amazon Elastic Compute Cloud Auto Scaling\n        \n      \n        \n          Migrating to AWS Graviton with Container Services\n        \n      \n  \u003cdiv\u003e\u003cdiv\u003e\u003cdiv\u003e\u003cdiv id=\"js_error_message\"\u003e\u003cp\u003e\u003cimg src=\"https://d1ge0kk1l5kms0.cloudfront.net/images/G/01/webservices/console/warning.png\" alt=\"Warning\" /\u003e \u003cstrong\u003eJavascript is disabled or is unavailable in your browser.\u003c/strong\u003e\u003c/p\u003e\u003cp\u003eTo use the Amazon Web Services Documentation, Javascript must be enabled. Please refer to your browser's Help pages for instructions.\u003c/p\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003eDocument ConventionsPERF 2. How do you select and use compute resources in your workload?PERF02-BP02 Understand the available compute configuration and\n  features",
  "PERF02-BP02 Understand the available compute configuration and\n  features\n    Understand the available configuration options and features for your\n    compute service to help you provision the right amount of resources\n    and improve performance efficiency.\n  \n    Common anti-patterns:\n  \n     \n     \n  \n      \n        You do not evaluate compute options or available instance\n        families against workload characteristics.\n      \n    \n      \n        You over-provision compute resources to meet peak-demand\n        requirements.\n      \n    \n    Benefits of establishing this best practice: Be familiar with AWS compute features and configurations so that you can use a compute solution optimized to meet your workload characteristics and needs.\n    Level of risk exposed if this best practice\n    is not established: Medium\n  \n\n  Implementation guidance\n    \n      Each compute solution has unique configurations and features\n      available to support different workload characteristics and\n      requirements. Learn how these options complement your workload,\n      and determine which configuration options are best for your\n      application. Examples of these options include instance family,\n      sizes, features (GPU, I/O), bursting, time-outs, function sizes,\n      container instances, and concurrency. If your workload has been\n      using the same compute option for more than four weeks and you\n      anticipate that the characteristics will remain the same in the\n      future, you can\n      use AWS Compute Optimizer  to find out if your current compute option is\n      suitable for the workloads from CPU and memory perspective.\n    \n   \n    \n    Implementation steps\n\n    \n       \n       \n    \n        \n          Understand workload requirements (like CPU need, memory, and\n          latency).\n        \n      \n        \n          Review AWS documentation and best practices to learn about\n          recommended configuration options that can help improve compute performance. Here are some key configuration\n          options to consider:\n        \n        \n              \n                \n                  Configuration option\n                \n                \n                  Examples\n                \n              \n            \n              \n                \n                  Instance type\n                \n                \n                  \n                     \n                     \n                     \n                  \n                      \n                        Compute-optimized instances\n                        are ideal for the workloads that require high higher\n                        vCPU to memory ratio. \n                      \n                    \n                      \n                        Memory-optimized instances\n                        deliver large amounts of memory to support memory\n                        intensive workloads.\n                      \n                    \n                      \n                        Storage-optimized instances\n                        are designed for workloads that require high,\n                        sequential read and write access (IOPS) to local\n                        storage.\n                      \n                    \n                \n              \n              \n                \n                  Pricing model\n                \n                \n                  \n                     \n                     \n                     \n                  \n                      \n                        On-Demand\n                          Instances let you use the compute capacity by\n                        the hour or second with no long-term commitment.\n                        These instances are good for bursting above\n                        performance baseline needs.\n                      \n                    \n                      \n                        Savings Plans offer significant savings over\n                        On-Demand Instances in exchange for a commitment to\n                        use a specific amount of compute power for a one or\n                        three-year period.\n                      \n                    \n                      \n                        Spot\n                          Instances let you take advantage of unused\n                        instance capacity at a discount for your stateless,\n                        fault-tolerant workloads. \n                      \n                    \n                \n              \n              \n                Auto Scaling\n                \n                  Use\n                  Auto Scaling configuration to match compute resources to\n                  traffic patterns.\n                \n              \n              \n                \n                  Sizing\n                \n                \n                  \n                     \n                     \n                  \n                      \n                        Use Compute Optimizer to get a machine-learning powered\n                        recommendations on which compute configuration best\n                        matches your compute characteristics.\n                      \n                    \n                      \n                        Use\n                        AWS Lambda Power Tuning to select the best\n                        configuration for your Lambda function.\n                      \n                    \n                \n              \n              \n                \n                  Hardware-based compute accelerators\n                \n                \n                  \n                     \n                     \n                  \n                      \n                        Accelerated\n                          computing instances perform functions like\n                        graphics processing or data pattern matching more\n                        efficiently than CPU-based alternatives.\n                      \n                    \n                      \n                        For machine learning workloads, take advantage of\n                        purpose-built hardware that is specific to your\n                        workload, such\n                        as AWS                     Trainium, AWS                     Inferentia,\n                        and Amazon EC2 DL1\n                      \n                    \n                \n              \n            \n      \n    \n   \n\n  Resources\n\n      \n    \n      Related documents:\n    \n    \n       \n       \n       \n       \n       \n       \n    \n        \n          Cloud\n          Compute with AWS \n        \n      \n        \n          Amazon EC2\n          Instance Types \n        \n      \n        \n          Processor\n          State Control for Your Amazon EC2 Instance \n        \n      \n        \n          Amazon EKS\n          Containers: Amazon EKS Worker Nodes \n        \n      \n        \n          Amazon ECS Containers: Amazon ECS Container Instances \n        \n      \n        \n          Functions:\n          Lambda Function Configuration\n        \n      \n    \n      Related videos:\n    \n    \n       \n       \n       \n       \n       \n       \n       \n    \n        \n          AWS re:Invent 2023 – AWS Graviton: The best price performance for your AWS workloads\n        \n      \n        \n          AWS re:Invent 2023 – New Amazon EC2 generative AI capabilities in AWS Management Console\n        \n      \n        \n          AWS re:Invent 2023 – What's new with Amazon EC2\n        \n      \n        \n          AWS re:Invent 2023 – Smart savings: Amazon EC2 cost-optimization strategies\n        \n      \n        \n          AWS re:Invent 2021 – Powering next-gen Amazon EC2: Deep dive on the Nitro System\n        \n      \n        \n          AWS re:Invent 2019 – Amazon EC2 foundations\n        \n      \n        \n          AWS re:Invent 2022 – Optimizing Amazon EKS for performance and cost on AWS\n        \n      \n    \n      Related examples:\n    \n    \n       \n       \n       \n       \n       \n       \n       \n       \n    \n        \n          Compute Optimizer demo code\n        \n      \n        \n          Amazon EC2 spot instances workshop\n        \n      \n        \n          Efficient and Resilient Workloads with Amazon EC2 AWS Auto Scaling\n        \n      \n        \n          Graviton developer workshop\n        \n      \n        \n          AWS for Microsoft workloads immersion day\n        \n      \n        \n          AWS for Linux workloads immersion day\n        \n      \n        \n          AWS Compute Optimizer Demo code\n        \n      \n        \n          Amazon EKS workshop\n        \n        \n      \n  \u003cdiv\u003e\u003cdiv\u003e\u003cdiv\u003e\u003cdiv id=\"js_error_message\"\u003e\u003cp\u003e\u003cimg src=\"https://d1ge0kk1l5kms0.cloudfront.net/images/G/01/webservices/console/warning.png\" alt=\"Warning\" /\u003e \u003cstrong\u003eJavascript is disabled or is unavailable in your browser.\u003c/strong\u003e\u003c/p\u003e\u003cp\u003eTo use the Amazon Web Services Documentation, Javascript must be enabled. Please refer to your browser's Help pages for instructions.\u003c/p\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003eDocument ConventionsPERF02-BP01 Select the best compute options for your\n  workloadPERF02-BP03 Collect compute-related metrics",
  "PERF02-BP03 Collect compute-related metrics\n    Record and track compute-related metrics to better understand how\n    your compute resources are performing and improve their performance\n    and their utilization.\n  \n    Common anti-patterns:\n  \n     \n     \n     \n  \n      \n        You only use manual log file searching for metrics. \n      \n    \n      \n        You only use the default metrics recorded by your monitoring\n        software.\n      \n    \n      \n        You only review metrics when there is an issue.\n      \n    \n    Benefits of establishing this best\n    practice: Collecting performance-related metrics will\n    help you align application performance with business requirements to\n    ensure that you are meeting your workload needs. It can also help\n    you continually improve the resource performance and utilization in\n    your workload.\n  \n    Level of risk exposed if this best practice\n    is not established: High\n  \n\n  Implementation guidance\n\n      \n    \n      Cloud workloads can generate large volumes of data such as\n      metrics, logs, and events. In the AWS Cloud, collecting metrics is\n      a crucial step to improve security, cost efficiency, performance,\n      and sustainability. AWS provides a wide range of\n      performance-related metrics using monitoring services such as\n      Amazon CloudWatch to provide you with valuable insights. Metrics\n      such as CPU utilization, memory utilization, disk I/O, and network\n      inbound and outbound can provide insight into utilization levels\n      or performance bottlenecks. Use these metrics as part of a\n      data-driven approach to actively tune and optimize your workload's\n      resources.  In an ideal case, you should collect all metrics\n      related to your compute resources in a single platform with\n      retention policies implemented to support cost and operational\n      goals.\n    \n   \n      \n      Implementation steps\n\n    \n       \n       \n       \n       \n       \n       \n    \n        \n          Identify which performance-related metrics are relevant to\n          your workload. You should collect metrics around resource\n          utilization and the way your cloud workload is operating (like\n          response time and throughput).\n        \n        \n           \n           \n           \n           \n           \n        \n            \n              Amazon EC2\n              default metrics\n            \n          \n            \n              Amazon ECS default metrics\n            \n          \n            \n              Amazon EKS\n              default metrics\n            \n          \n            \n              Lambda\n              default metrics\n            \n          \n            \n              Amazon EC2\n              memory and disk metrics\n            \n          \n      \n        \n          Choose and set up the right logging and monitoring solution\n          for your workload.\n        \n        \n           \n           \n           \n        \n            \n              AWS               native Observability\n            \n          \n            \n              AWS Distro\n              for OpenTelemetry\n            \n          \n            \n              Amazon Managed Service for Prometheus\n            \n          \n      \n        \n          Define the required filter and aggregation for the metrics\n          based on your workload requirements.\n        \n        \n           \n           \n        \n            \n              Quantify\n              custom application metrics with Amazon CloudWatch Logs and\n              metric filters\n            \n          \n            \n              Collect\n              custom metrics with Amazon CloudWatch strategic\n              tagging\n            \n          \n      \n        \n          Configure data retention policies for your metrics to match\n          your security and operational goals.\n        \n        \n           \n           \n        \n            \n              Default\n              data retention for CloudWatch metrics\n            \n          \n            \n              Default\n              data retention for CloudWatch Logs\n            \n          \n      \n        \n          If required, create alarms and notifications for your metrics\n          to help you proactively respond to performance-related issues.\n        \n        \n           \n           \n        \n            \n              Create\n              alarms for custom metrics using Amazon CloudWatch anomaly\n              detection\n            \n          \n            \n              Create\n              metrics and alarms for specific web pages with Amazon CloudWatch RUM\n            \n          \n      \n        \n          Use automation to deploy your metric and log aggregation\n          agents.\n        \n        \n           \n           \n        \n            \n              AWS Systems Manager automation\n            \n          \n            \n              OpenTelemetry\n              Collector\n            \n          \n      \n   \n\n  Resources\n\n      \n    \n      Related documents:\n    \n    \n       \n       \n       \n       \n       \n       \n       \n       \n       \n       \n    \n        \n          Monitoring and observability\n        \n      \n        \n          Best practices: implementing observability with AWS\n        \n      \n        \n          Amazon CloudWatch documentation\n        \n      \n        \n          Collect\n          metrics and logs from Amazon EC2 instances and on-premises\n          servers with the CloudWatch Agent\n        \n      \n        \n          Accessing\n          Amazon CloudWatch Logs for AWS Lambda\n        \n      \n        \n          Using\n          CloudWatch Logs with container instances\n        \n      \n        \n          Publish\n          custom metrics\n        \n      \n        \n          AWS           Answers: Centralized Logging\n        \n      \n        \n          AWS Services That Publish CloudWatch Metrics\n        \n      \n        \n          Monitoring\n          Amazon EKS on AWS Fargate\n        \n      \n    \n      Related videos:\n    \n    \n       \n       \n       \n       \n       \n    \n        \n          AWS re:Invent 2023 – [LAUNCH] Application monitoring for modern workloads\n        \n      \n        \n          AWS re:Invent 2023 – Implementing application observability\n        \n      \n        \n          AWS re:Invent 2023 – Building an effective observability strategy\n        \n      \n        \n          AWS re:Invent 2023 – Seamless observability with AWS Distro for OpenTelemetry\n        \n      \n        \n          Application\n          Performance Management on AWS\n        \n      \n    \n      Related examples:\n    \n    \n       \n       \n       \n       \n    \n        \n          AWS for Linux Workloads Immersion Day- Amazon CloudWatch\n        \n      \n        \n          Monitoring Amazon ECS clusters and containers\n        \n      \n        \n          Monitoring with Amazon CloudWatch dashboards\n        \n      \n        \n          Amazon EKS workshop\n        \n      \n  \u003cdiv\u003e\u003cdiv\u003e\u003cdiv\u003e\u003cdiv id=\"js_error_message\"\u003e\u003cp\u003e\u003cimg src=\"https://d1ge0kk1l5kms0.cloudfront.net/images/G/01/webservices/console/warning.png\" alt=\"Warning\" /\u003e \u003cstrong\u003eJavascript is disabled or is unavailable in your browser.\u003c/strong\u003e\u003c/p\u003e\u003cp\u003eTo use the Amazon Web Services Documentation, Javascript must be enabled. Please refer to your browser's Help pages for instructions.\u003c/p\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003eDocument ConventionsPERF02-BP02 Understand the available compute configuration and\n  featuresPERF02-BP04 Configure and right-size compute resources",
  "PERF02-BP04 Configure and right-size compute resources\n    Configure and right-size compute resources to match your workload’s\n    performance requirements and avoid under- or over-utilized\n    resources.\n  \n    Common anti-patterns:\n  \n     \n     \n     \n     \n     \n     \n  \n      \n        You ignore your workload performance requirements resulting in\n        over-provisioned or under-provisioned compute resources.\n      \n    \n      \n        You only choose the largest or smallest instance available for\n        all workloads.\n      \n    \n      \n        You only use one instance family for ease of management.\n      \n    \n      \n        You ignore recommendations from AWS Cost Explorer or Compute Optimizer for right-sizing.\n      \n    \n      \n        You do not re-evaluate the workload for suitability of new\n        instance types.\n      \n    \n      \n        You certify only a small number of instance configurations for\n        your organization.\n      \n    \n    Benefits of establishing this best\n      practice: \n    Right-sizing compute resources ensures optimal operation in the\n    cloud by avoiding over-provisioning and under-provisioning\n    resources. Properly sizing compute resources typically results in\n    better performance and enhanced customer experience, while also\n    lowering cost.\n  \n    Level of risk exposed if this best practice\n    is not established: Medium\n  \n\n  Implementation guidance\n\n      \n    \n      Right-sizing allows organizations to operate their cloud\n      infrastructure in an efficient and cost-effective manner while\n      addressing their business needs. Over-provisioning cloud resources\n      can lead to extra costs, while under-provisioning can result in\n      poor performance and a negative customer experience. AWS provides\n      tools such as\n      AWS Compute Optimizer and\n      AWS Trusted Advisor that use historical data to provide\n      recommendations to right-size your compute resources.\n    \n     \n\n  Implementation steps\n\n      \n      \n         \n         \n         \n         \n         \n         \n      \n          \n            Choose an instance type to best fit your needs:\n          \n          \n             \n             \n             \n             \n          \n              \n                How\n                do I choose the appropriate Amazon EC2 instance type for\n                my workload?\n              \n            \n              \n                Attribute-based\n                instance type selection for Amazon EC2 Fleet\n              \n            \n              \n                Create\n                an Auto Scaling group using attribute-based instance\n                type selection\n              \n            \n              \n                Optimizing\n                your Kubernetes compute costs with Karpenter\n                consolidation\n              \n            \n        \n          \n            Analyze the various performance characteristics of your\n            workload and how these characteristics relate to memory,\n            network, and CPU usage. Use this data to choose resources\n            that best match your workload's profile and performance\n            goals.\n          \n        \n          \n            Monitor your resource usage using AWS monitoring tools such\n            as Amazon CloudWatch.\n          \n        \n          \n            Select the right configuration for compute resources.\n          \n          \n             \n             \n          \n              \n                For ephemeral workloads,\n                evaluate instance\n                Amazon CloudWatch metrics such\n                as CPUUtilization to identify if the instance is\n                under-utilized or over-utilized.\n              \n            \n              \n                For stable workloads, check AWS rightsizing tools such\n                as AWS Compute Optimizer and AWS Trusted Advisor at\n                regular intervals to identify opportunities to optimize\n                and right-size the compute resource.\n              \n            \n        \n          \n            Test configuration changes in a non-production environment\n            before implementing in a live environment.\n          \n        \n          \n            Continually re-evaluate new compute offerings and compare\n            against your workload’s needs.\n          \n        \n     \n   \n\n  Resources\n      \n    \n      Related documents:\n    \n    \n       \n       \n       \n       \n       \n       \n    \n        \n          Cloud\n          Compute with AWS\n        \n      \n        \n          Amazon EC2\n          Instance Types\n        \n      \n        \n          Amazon ECS\n          Containers: Amazon ECS Container Instances\n        \n      \n        \n          Amazon EKS\n          Containers: Amazon EKS Worker Nodes\n        \n      \n        \n          Functions:\n          Lambda Function Configuration\n        \n      \n        \n          Processor\n          State Control for Your Amazon EC2 Instance\n        \n      \n    \n      Related videos:\n    \n    \n       \n       \n       \n       \n       \n       \n       \n    \n        \n          Amazon EC2 foundations\n        \n      \n        \n          AWS re:Invent 2023 – AWS Graviton: The best price performance for your AWS workloads\n        \n      \n        \n          AWS re:Invent 2023 – New Amazon EC2 generative AI capabilities in AWS Management Console\n        \n      \n        \n          AWS re:Invent 2023 – What’s new with Amazon EC2\n        \n      \n        \n          AWS re:Invent 2023 – Smart savings: Amazon EC2 cost-optimization strategies\n        \n      \n        \n          AWS re:Invent 2021 – Powering next-gen Amazon EC2: Deep dive on the Nitro System\n        \n      \n        \n          AWS re:Invent 2019 – Amazon EC2 foundations\n        \n      \n    \n      Related examples:\n    \n    \n       \n       \n       \n    \n        \n          AWS Compute Optimizer Demo code\n        \n      \n        \n          Amazon EKS workshop \n        \n      \n        \n          Right-sizing recommendations\n        \n      \n  \u003cdiv\u003e\u003cdiv\u003e\u003cdiv\u003e\u003cdiv id=\"js_error_message\"\u003e\u003cp\u003e\u003cimg src=\"https://d1ge0kk1l5kms0.cloudfront.net/images/G/01/webservices/console/warning.png\" alt=\"Warning\" /\u003e \u003cstrong\u003eJavascript is disabled or is unavailable in your browser.\u003c/strong\u003e\u003c/p\u003e\u003cp\u003eTo use the Amazon Web Services Documentation, Javascript must be enabled. Please refer to your browser's Help pages for instructions.\u003c/p\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003eDocument ConventionsPERF02-BP03 Collect compute-related metricsPERF02-BP05 Scale your compute resources dynamically",
  "PERF02-BP05 Scale your compute resources dynamically\n    Use the elasticity of the cloud to scale your compute resources up\n    or down dynamically to match your needs and avoid over- or\n    under-provisioning capacity for your workload.\n  \n    Common anti-patterns:\n  \n     \n     \n     \n  \n      \n        You react to alarms by manually increasing capacity.\n      \n    \n      \n        You use the same sizing guidelines (generally static\n        infrastructure) as in on-premises.\n      \n    \n      \n        You leave increased capacity after a scaling event instead of\n        scaling back down.\n      \n    \n    Benefits of establishing this best\n    practice: Configuring and testing the elasticity of\n    compute resources can help you save money, maintain performance\n    benchmarks, and improve reliability as traffic changes.\n  \n    Level of risk exposed if this best practice\n    is not established: High\n  \n\n  Implementation guidance\n\n      \n    \n      AWS provides the flexibility to scale your resources up or down\n      dynamically through a variety of scaling mechanisms in order to\n      meet changes in demand. Combined with compute-related metrics, a\n      dynamic scaling allows workloads to automatically respond to\n      changes and use the optimal set of compute resources to achieve\n      its goal.\n    \n    \n      You can use a number of different approaches to match supply of\n      resources with demand.\n    \n    \n       \n       \n       \n       \n    \n        \n          Target-tracking approach: Monitor your scaling metric and\n          automatically increase or decrease capacity as you need it. \n      \n        \n          Predictive scaling: Scale in anticipation of daily and\n          weekly trends. \n      \n        \n          Schedule-based approach: Set your own scaling schedule\n          according to predictable load changes. \n      \n        \n          Service scaling: Choose services (like serverless) that\n          that automatically scale by design. \n      \n    \n      You must ensure that workload deployments can handle both scale-up\n      and scale-down events.\n    \n     \n\n  Implementation steps\n\n      \n      \n         \n         \n         \n         \n         \n         \n      \n          \n            Compute instances, containers, and functions provide\n            mechanisms for elasticity, either in combination with\n            autoscaling or as a feature of the service. Here are some\n            examples of automatic scaling mechanisms:\n          \n          \n                \n                  \n                    Autoscaling Mechanism\n                  \n                  \n                    Where to use\n                  \n                \n              \n                \n                  \n                    Amazon EC2 Auto Scaling\n                  \n                  \n                    To ensure you have the correct number of\n                    Amazon EC2 instances available to handle the user load\n                    for your application.\n                  \n                \n                \n                  \n                    Application Auto Scaling\n                  \n                  \n                    To automatically scale the resources for individual AWS\n                    services beyond Amazon EC2 such as\n                    AWS Lambda functions or\n                    Amazon Elastic Container Service (Amazon ECS) services.\n                  \n                \n                \n                  \n                    Kubernetes\n                      Cluster Autoscaler/Karpenter\n                  \n                  \n                    To automatically scale Kubernetes clusters.\n                  \n                \n              \n        \n          \n            Scaling is often discussed related to compute services like\n            Amazon EC2 Instances or AWS Lambda functions. Be sure to\n            also consider the configuration of non-compute services like\n            AWS Glue to match the demand.\n          \n        \n          \n            Verify that the metrics for scaling match the\n            characteristics of the workload being deployed. If you are\n            deploying a video transcoding application, 100% CPU\n            utilization is expected and should not be your primary\n            metric. Use the depth of the transcoding job queue instead.\n            You can use a\n            customized\n            metric for your scaling policy if required. To choose\n            the right metrics, consider the following guidance for Amazon EC2:\n          \n          \n             \n             \n          \n              \n                The metric should be a valid utilization metric and\n                describe how busy an instance is.\n              \n            \n              \n                The metric value must increase or decrease\n                proportionally to the number of instances in the Auto Scaling group.\n              \n            \n        \n          \n            Make sure that you use\n            dynamic\n            scaling instead of\n            manual\n            scaling for your Auto Scaling group. We also\n            recommend that you use\n            target\n            tracking scaling policies in your dynamic scaling.\n          \n        \n          \n            Verify that workload deployments can handle both scaling\n            events (up and down). As an example, you can use\n            Activity\n            history to verify a scaling activity for an Auto Scaling group.\n          \n        \n          \n            Evaluate your workload for predictable patterns and\n            proactively scale as you anticipate predicted and planned\n            changes in demand. With predictive scaling, you can\n            eliminate the need to overprovision capacity. For more detail, \n            see Predictive\n              Scaling with Amazon EC2 Auto Scaling.\n          \n        \n     \n   \n\n  Resources\n\n      \n    \n      Related documents:\n    \n    \n       \n       \n       \n       \n       \n       \n       \n       \n    \n        \n          Cloud\n          Compute with AWS\n        \n      \n        \n          Amazon EC2\n          Instance Types\n        \n      \n        \n          Amazon ECS\n          Containers: Amazon ECS Container Instances\n        \n      \n        \n          Amazon EKS\n          Containers: Amazon EKS Worker Nodes\n        \n      \n        \n          Functions:\n          Lambda Function Configuration\n        \n      \n        \n          Processor\n          State Control for Your Amazon EC2 Instance\n        \n      \n        \n          Deep\n          Dive on Amazon ECS Cluster Auto Scaling\n        \n      \n        \n          Introducing\n          Karpenter – An Open-Source High-Performance Kubernetes Cluster\n          Autoscaler\n        \n      \n    \n      Related videos:\n    \n    \n       \n       \n       \n       \n       \n       \n    \n        \n          AWS re:Invent 2023 – AWS Graviton: The best price performance for your AWS workloads\n        \n      \n        \n          AWS re:Invent 2023 – New Amazon EC2 generative AI capabilities in AWS Management Console\n        \n      \n        \n          AWS re:Invent 2023 – What’s new with Amazon EC2\n        \n      \n        \n          AWS re:Invent 2023 – Smart savings: Amazon EC2 cost-optimization strategies\n        \n      \n        \n          AWS re:Invent 2021 – Powering next-gen Amazon EC2: Deep dive on the Nitro System\n        \n      \n        \n          AWS re:Invent 2019 – Amazon EC2 foundations\n        \n      \n    \n      Related examples:\n    \n    \n       \n       \n       \n    \n        \n          Amazon EC2 Auto Scaling Group Examples\n        \n      \n        \n          Amazon EKS Workshop\n        \n      \n        \n          Scale your Amazon EKS workloads by running on IPv6\n        \n      \n  \u003cdiv\u003e\u003cdiv\u003e\u003cdiv\u003e\u003cdiv id=\"js_error_message\"\u003e\u003cp\u003e\u003cimg src=\"https://d1ge0kk1l5kms0.cloudfront.net/images/G/01/webservices/console/warning.png\" alt=\"Warning\" /\u003e \u003cstrong\u003eJavascript is disabled or is unavailable in your browser.\u003c/strong\u003e\u003c/p\u003e\u003cp\u003eTo use the Amazon Web Services Documentation, Javascript must be enabled. Please refer to your browser's Help pages for instructions.\u003c/p\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003eDocument ConventionsPERF02-BP04 Configure and right-size compute resourcesPERF02-BP06 Use optimized hardware-based compute accelerators",
  "PERF02-BP06 Use optimized\n      hardware-based compute accelerators Use hardware accelerators to perform certain functions more efficiently than CPU-based\n    alternatives. \n    Common anti-patterns:\n  \n     \n     \n     \n  \n       In your workload, you haven't benchmarked a general-purpose instance against a\n        purpose-built instance that can deliver higher performance and lower cost. \n    \n       You are using hardware-based compute accelerators for tasks that can be more efficient\n        using CPU-based alternatives. \n    \n       You are not monitoring GPU usage. \n    Benefits of establishing this best practice: By using\n    hardware-based accelerators, such as graphics processing units (GPUs) and field programmable\n    gate arrays (FPGAs), you can perform certain processing functions more efficiently. \n    Level of risk exposed if this best practice is not established:\n    Medium \n\n    Implementation guidance\n     Accelerated computing instances provide access to hardware-based compute accelerators\n      such as GPUs and FPGAs. These hardware accelerators perform certain functions like graphic\n      processing or data pattern matching more efficiently than CPU-based alternatives. Many\n      accelerated workloads, such as rendering, transcoding, and machine learning, are highly\n      variable in terms of resource usage. Only run this hardware for the time needed, and\n      decommission them with automation when not required to improve overall performance efficiency. \n     \n\n      Implementation steps\n\n\n      \n         \n         \n         \n         \n         \n         \n      \n           Identify which accelerated\n              computing instances can address your requirements. \n        \n           For machine learning workloads, take advantage of purpose-built hardware that is\n            specific to your workload, such as AWS Trainium, AWS Inferentia, and Amazon EC2 DL1. AWS Inferentia\n            instances such as Inf2 instances offer up to 50% better performance/watt over\n              comparable Amazon EC2 instances. \n        \n           Collect usage metrics for your accelerated computing instances. For example, you\n            can use CloudWatch agent to collect metrics such as utilization_gpu and\n              utilization_memory for your GPUs as shown in Collect NVIDIA GPU\n              metrics with Amazon CloudWatch. \n        \n           Optimize the code, network operation, and settings of hardware accelerators to make\n            sure that underlying hardware is fully utilized. \n          \n             \n             \n             \n          \n              \n                Optimize\n                  GPU settings\n              \n            \n              \n                GPU\n                  Monitoring and Optimization in the Deep Learning AMI\n              \n            \n              \n                Optimizing I/O for GPU performance tuning of deep learning training in\n                  Amazon SageMaker AI\n              \n            \n        \n           Use the latest high performant libraries and GPU drivers. \n        \n           Use automation to release GPU instances when not in use. \n        \n     \n   \n\n    Resources\n\n\n    \n      Related documents:\n    \n    \n       \n       \n       \n       \n       \n    \n        \n          Working\n            with GPUs on Amazon Elastic Container Service\n        \n      \n        \n          GPU\n            instances\n        \n      \n        \n          Instances with AWS Trainium\n        \n      \n        \n          Instances with AWS Inferentia\n        \n      \n        \n          Let’s Architect!\n            Architecting with custom chips and accelerators\n        \n      \n    \n       \n       \n       \n       \n    \n        \n          Accelerated\n            Computing\n        \n      \n        \n          Amazon EC2 VT1 Instances\n        \n      \n        \n          How do I\n            choose the appropriate Amazon EC2 instance type for my workload?\n        \n      \n        \n          Choose the best AI accelerator and model compilation for computer vision inference with\n            Amazon SageMaker AI\n        \n      \n    \n      Related videos:\n    \n    \n       \n       \n       \n       \n    \n         AWS re:Invent 2021 - How to select\n            Amazon Elastic Compute Cloud GPU instances for deep learning\n        \n      \n        \n          AWS\n            re:Invent 2022 - [NEW LAUNCH!] Introducing AWS Inferentia2-based Amazon EC2 Inf2\n            instances\n        \n      \n        \n          AWS\n            re:Invent 2022 - Accelerate deep learning and innovate faster with AWS\n            Trainium\n        \n      \n        \n          AWS\n            re:Invent 2022 - Deep learning on AWS with NVIDIA: From training to deployment\n        \n      \n    \n      Related examples:\n    \n    \n       \n       \n       \n    \n        \n          Amazon SageMaker AI\n            and NVIDIA GPU Cloud (NGC)\n        \n      \n        \n          Use SageMaker AI with\n            Trainium and Inferentia for optimized deep learning training and inferencing\n            workloads\n        \n      \n        \n          Optimizing\n            NLP models with Amazon Elastic Compute Cloud Inf1 instances in Amazon SageMaker AI\n        \n      \n  \u003cdiv\u003e\u003cdiv\u003e\u003cdiv\u003e\u003cdiv id=\"js_error_message\"\u003e\u003cp\u003e\u003cimg src=\"https://d1ge0kk1l5kms0.cloudfront.net/images/G/01/webservices/console/warning.png\" alt=\"Warning\" /\u003e \u003cstrong\u003eJavascript is disabled or is unavailable in your browser.\u003c/strong\u003e\u003c/p\u003e\u003cp\u003eTo use the Amazon Web Services Documentation, Javascript must be enabled. Please refer to your browser's Help pages for instructions.\u003c/p\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003eDocument ConventionsPERF02-BP05 Scale your compute resources dynamicallyData management",
  "PERF03-BP01 Use a purpose-built data\n      store that best supports your data access and storage requirements Understand data characteristics (like shareable, size, cache size, access patterns,\n    latency, throughput, and persistence of data) to select the right purpose-built data stores\n    (storage or database) for your workload. \n    Common anti-patterns:\n  \n     \n     \n     \n  \n       You stick to one data store because there is internal experience and knowledge of one\n        particular type of database solution. \n    \n       You assume that all workloads have similar data storage and access requirements.\n      \n    \n       You have not implemented a data catalog to inventory your data assets. \n    \n    Benefits of establishing this best practice: Understanding data\n    characteristics and requirements allows you to determine the most efficient and performant\n    storage technology appropriate for your workload needs. \n    Level of risk exposed if this best practice is not established:\n    High \n\n    Implementation guidance\n\n\n     When selecting and implementing data storage, make sure that the querying, scaling, and\n      storage characteristics support the workload data requirements. AWS provides numerous data\n      storage and database technologies including block storage, object storage, streaming storage,\n      file system, relational, key-value, document, in-memory, graph, time series, and ledger\n      databases. Each data management solution has options and configurations available to you to\n      support your use-cases and data models. By understanding data characteristics and\n      requirements, you can break away from monolithic storage technology and restrictive,\n      one-size-fits-all approaches to focus on managing data appropriately. \n     \n\n      Implementation steps\n\n\n      \n         \n         \n         \n         \n         \n         \n      \n           Conduct an inventory of the various data types that exist in your workload. \n        \n           Understand and document data characteristics and requirements, including: \n          \n             \n             \n             \n             \n             \n             \n             \n             \n             \n          \n               Data type (unstructured, semi-structured, relational) \n            \n               Data volume and growth \n            \n               Data durability: persistent, ephemeral, transient \n            \n               ACID (atomicity, consistency, isolation, durability) requirements \n            \n               Data access patterns (read-heavy or write-heavy) \n            \n               Latency \n            \n               Throughput \n            \n               IOPS (input/output operations per second) \n            \n               Data retention period \n            \n        \n           Learn about different data stores (storage and database services) available for\n            your workload on AWS that can meet your data characteristics, as outlined in PERF01-BP01 Learn\n      about and understand available cloud services and features. Some examples of\n            AWS storage technologies and their key characteristics include: \n          \n                \n                  \n                    Type\n                  \n                  \n                    AWS Services\n                  \n                  \n                    Key characteristics\n                  \n                \n              \n                \n                  Object storage\n                  \n                    Amazon S3\n                  \n                   Unlimited scalability, high availability, and multiple options for\n                    accessibility. Transferring and accessing objects in and out of Amazon S3 can use a\n                    service, such as Transfer Acceleration or Access Points, to support your\n                    location, security needs, and access patterns. \n                \n                \n                  Archiving storage\n                  \n                    Amazon S3 Glacier\n                  \n                   Built for data archiving. \n                \n                \n                  Streaming storage\n                  \n                    \n                      Amazon Kinesis\n                    \n                     Amazon Managed Streaming for Apache Kafka (Amazon MSK) \n                  \n                   Efficient ingestion and storage of streaming data. \n                \n                \n                  Shared file system\n                  \n                    Amazon Elastic File System (Amazon EFS)\n                  \n                   Mountable file system that can be accessed by multiple types of\n                      compute solutions. \n                  \n                \n                \n                  Shared file system\n                  \n                    Amazon FSx\n                  \n                   Built on the latest AWS compute solutions to support four commonly used\n                    file systems: NetApp ONTAP, OpenZFS, Windows File Server, and Lustre.\n                      Amazon FSx latency,\n                      throughput, and IOPS vary per file system and should be considered\n                    when selecting the right file system for your workload needs. \n                \n                \n                  Block storage\n                  \n                    Amazon Elastic Block Store (Amazon EBS)\n                  \n                   Scalable, high-performance block-storage service designed for Amazon Elastic Compute Cloud\n                    (Amazon EC2). Amazon EBS includes SSD-backed storage for transactional, IOPS-intensive\n                    workloads and HDD-backed storage for throughput-intensive workloads. \n                \n                \n                  Relational database\n                  \n                    Amazon Aurora, Amazon RDS, Amazon Redshift. \n                   Designed to support ACID (atomicity, consistency, isolation, durability)\n                    transactions, and maintain referential integrity and strong data consistency.\n                    Many traditional applications, enterprise resource planning (ERP), customer\n                    relationship management (CRM), and ecommerce use relational databases to store\n                    their data. \n                \n                \n                  Key-value database\n                  \n                    Amazon DynamoDB\n                  \n                   Optimized for common access patterns, typically to store and retrieve\n                    large volumes of data. High-traffic web apps, ecommerce systems, and gaming\n                    applications are typical use-cases for key-value databases. \n                \n                \n                  Document database\n                  \n                    Amazon DocumentDB\n                  \n                   Designed to store semi-structured data as JSON-like documents. These\n                    databases help developers build and update applications such as content\n                    management, catalogs, and user profiles quickly.  \n                \n                \n                  In-memory database\n                  \n                    Amazon ElastiCache , Amazon MemoryDB for Redis\n                  \n                   Used for applications that require real-time access to data, lowest\n                    latency and highest throughput. You may use in-memory databases for application\n                    caching, session management, gaming leaderboards, low latency ML feature store,\n                    microservices messaging system, and a high-throughput streaming mechanism\n                  \n                \n                \n                  Graph database\n                  \n                    Amazon Neptune\n                  \n                   Used for applications that must navigate and query millions of\n                    relationships between highly connected graph datasets with millisecond latency\n                    at large scale. Many companies use graph databases for fraud detection, social\n                    networking, and recommendation engines. \n                \n                \n                  Time Series database\n                  \n                    Amazon Timestream\n                  \n                   Used to efficiently collect, synthesize, and derive insights from data\n                    that changes over time. IoT applications, DevOps, and industrial telemetry can\n                    utilize time-series databases. \n                \n                \n                   Wide column \n                  \n                    Amazon Keyspaces (for Apache\n                      Cassandra)\n                  \n                   Uses tables, rows, and columns, but unlike a relational database, the\n                    names and format of the columns can vary from row to row in the same table. You\n                    typically see a wide column store in high scale industrial apps for equipment\n                    maintenance, fleet management, and route optimization.  \n                \n                \n                   Ledger \n                  \n                    Amazon Quantum Ledger Database (Amazon\n                      QLDB)\n                  \n                   Provides a centralized and trusted authority to maintain a scalable,\n                    immutable, and cryptographically verifiable record of transactions for every\n                    application. We see ledger databases used for systems of record, supply chain,\n                    registrations, and even banking transactions.   \n                \n              \n        \n           If you are building a data platform, leverage modern data\n              architecture on AWS to integrate your data lake, data warehouse, and\n            purpose-built data stores. \n        \n           The key questions that you need to consider when choosing a data store for your\n            workload are as follows: \n          \n                \n                   Question \n                   Things to consider \n                \n              \n                \n                   How is the data structured? \n                  \n                    \n                       \n                       \n                    \n                         If the data is unstructured, consider an object-store such as Amazon S3\n                          or a NoSQL database such as Amazon DocumentDB\n                        \n                      \n                         For key-value data, consider DynamoDB, Amazon ElastiCache (Redis OSS) or Amazon MemoryDB\n                        \n                      \n                  \n                \n                \n                   What level of referential integrity is required? \n                  \n                    \n                       \n                       \n                    \n                         For foreign key constraints, relational databases such as Amazon RDS and Aurora can provide this level of integrity.\n                        \n                      \n                         Typically, within a NoSQL data-model, you would de-normalize your\n                          data into a single document or collection of documents to be retrieved in\n                          a single request rather than joining across documents or tables.  \n                      \n                  \n                \n                \n                   Is ACID (atomicity, consistency, isolation, durability) compliance\n                    required? \n                  \n                    \n                       \n                       \n                    \n                         If the ACID properties associated with relational databases are\n                          required, consider a relational database such as Amazon RDS and Aurora. \n                      \n                         If strong consistency is required for NoSQL database, you can use strongly consistent\n                          reads with DynamoDB.\n                        \n                      \n                  \n                \n                \n                   How will the storage requirements change over time? How does this impact\n                    scalability? \n                  \n                    \n                       \n                       \n                    \n                         Serverless databases such as DynamoDB and Amazon Quantum Ledger Database (Amazon QLDB) will scale dynamically. \n                      \n                         Relational databases have upper bounds on provisioned storage, and\n                          often must be horizontally partitioned using mechanisms such as sharding\n                          once they reach these limits. \n                      \n                  \n                \n                \n                   What is the proportion of read queries in relation to write queries? Would\n                    caching be likely to improve performance? \n                  \n                    \n                       \n                       \n                    \n                         Read-heavy workloads can benefit from a caching layer, like ElastiCache or DAX if the database is\n                          DynamoDB. \n                      \n                         Reads can also be offloaded to read replicas with relational\n                          databases such as Amazon RDS.\n                        \n                      \n                  \n                \n                \n                   Does storage and modification (OLTP - Online Transaction Processing) or\n                    retrieval and reporting (OLAP - Online Analytical Processing) have a higher\n                    priority? \n                  \n                    \n                       \n                       \n                       \n                    \n                         For high-throughput read as-is transactional processing, consider a\n                          NoSQL database such as DynamoDB. \n                      \n                         For high-throughput and complex read patterns (like join) with\n                          consistency use Amazon RDS. \n                      \n                         For analytical queries, consider a columnar database such as Amazon Redshift or exporting the data\n                          to Amazon S3 and performing analytics using Athena or Amazon QuickSight. \n                      \n                  \n                \n                \n                   What level of durability does the data require? \n                  \n                    \n                       \n                       \n                       \n                    \n                         Aurora automatically replicates your data across three Availability\n                          Zones within a Region, meaning your data is highly durable with less\n                          chance of data loss. \n                      \n                         DynamoDB is automatically replicated across multiple Availability Zones,\n                          providing high availability and data durability. \n                      \n                         Amazon S3 provides 11 nines of durability. Many database services, such as\n                          Amazon RDS and DynamoDB, support exporting data to Amazon S3 for long-term retention\n                          and archival. \n                      \n                  \n                \n                \n                   Is there a desire to move away from commercial database engines or\n                    licensing costs? \n                  \n                    \n                       \n                       \n                    \n                         Consider open-source engines such as PostgreSQL and MySQL on Amazon RDS or\n                          Aurora. \n                      \n                         Leverage AWS Database Migration Service and\n                            AWS Schema Conversion Tool to perform migrations from commercial database\n                          engines to open-source \n                      \n                  \n                \n                \n                   What is the operational expectation for the database? Is moving to managed\n                    services a primary concern? \n                  \n                    \n                       \n                    \n                         Leveraging Amazon RDS instead of Amazon EC2, and DynamoDB or Amazon DocumentDB instead of\n                          self-hosting a NoSQL database can reduce operational overhead. \n                      \n                  \n                \n                \n                   How is the database currently accessed? Is it only application access, or\n                    are there business intelligence (BI) users and other connected off-the-shelf\n                    applications? \n                  \n                    \n                       \n                    \n                         If you have dependencies on external tooling then you may have to\n                          maintain compatibility with the databases they support. Amazon RDS is fully\n                          compatible with the difference engine versions that it supports including\n                          Microsoft SQL Server, Oracle, MySQL, and PostgreSQL. \n                      \n                  \n                \n              \n        \n           Perform experiments and benchmarking in a non-production environment to identify\n            which data store can address your workload requirements. \n        \n     \n   \n\n    Resources\n\n\n    \n      Related documents:\n    \n    \n       \n       \n       \n       \n       \n       \n       \n       \n       \n       \n       \n       \n       \n       \n       \n       \n       \n       \n       \n    \n        \n          Amazon EBS Volume\n            Types\n        \n      \n        \n          Amazon EC2\n            Storage\n        \n      \n        \n          Amazon EFS: Amazon EFS\n            Performance\n        \n      \n        \n          Amazon FSx for Lustre\n            Performance\n        \n      \n        \n          Amazon FSx for Windows File Server Performance\n        \n      \n        \n          Amazon S3 Glacier:\n            S3 Glacier Documentation\n        \n      \n        \n          Amazon S3: Request Rate and\n            Performance Considerations\n        \n      \n        \n          Cloud Storage with AWS\n        \n      \n        \n          Amazon EBS I/O Characteristics\n        \n      \n        \n          Cloud Databases with\n            AWS \n        \n      \n        \n          AWS Database\n            Caching \n        \n      \n        \n          DynamoDB Accelerator\n        \n      \n        \n          Amazon Aurora\n            best practices \n        \n      \n        \n          Amazon Redshift performance \n        \n      \n        \n          Amazon Athena top 10 performance tips \n        \n      \n        \n          Amazon Redshift Spectrum best practices \n        \n      \n        \n          Amazon DynamoDB best practices\n        \n      \n        \n          Choose between\n            Amazon EC2 and Amazon RDS\n        \n      \n        \n            Best Practices for Implementing Amazon ElastiCache \n      \n    \n      Related videos:\n    \n    \n       \n       \n       \n       \n       \n       \n       \n       \n       \n    \n        \n          AWS re:Invent 2023: Improve\n            Amazon Elastic Block Store efficiency and be more cost-efficient\n        \n      \n        \n          AWS re:Invent 2023: Optimizing\n            storage price and performance with Amazon Simple Storage Service\n        \n      \n        \n          AWS re:Invent 2023: Building\n            and optimizing a data lake on Amazon Simple Storage Service\n        \n      \n        \n          AWS re:Invent 2022: Building\n            modern data architectures on AWS\n        \n      \n        \n          AWS re:Invent 2022: Building\n            data mesh architectures on AWS\n        \n      \n        \n          AWS re:Invent 2023: Deep dive\n            into Amazon Aurora and its innovations\n        \n      \n        \n          AWS re:Invent 2023: Advanced\n            data modeling with Amazon DynamoDB\n        \n      \n        AWS re:Invent 2022:\n            Modernize apps with purpose-built databases\n      \n         Amazon DynamoDB deep dive:\n            Advanced design patterns \n      \n    \n      Related examples:\n    \n    \n       \n       \n       \n       \n       \n       \n       \n       \n       \n       \n    \n        \n          AWS Purpose Built Databases Workshop\n        \n      \n        \n          Databases for Developers\n        \n      \n        \n          AWS Modern Data Architecture Immersion Day\n        \n      \n        \n          Build a Data Mesh on AWS\n        \n      \n        \n          Amazon S3 Examples\n        \n      \n        \n          Optimize Data Pattern using Amazon Redshift Data Sharing\n        \n      \n        \n          Database\n            Migrations\n        \n      \n        \n          MS SQL Server - AWS Database Migration Service\n            (AWS DMS) Replication Demo\n        \n      \n        \n          Database\n            Modernization Hands On Workshop\n        \n      \n        \n          Amazon Neptune\n            Samples\n        \n      \n  \u003cdiv\u003e\u003cdiv\u003e\u003cdiv\u003e\u003cdiv id=\"js_error_message\"\u003e\u003cp\u003e\u003cimg src=\"https://d1ge0kk1l5kms0.cloudfront.net/images/G/01/webservices/console/warning.png\" alt=\"Warning\" /\u003e \u003cstrong\u003eJavascript is disabled or is unavailable in your browser.\u003c/strong\u003e\u003c/p\u003e\u003cp\u003eTo use the Amazon Web Services Documentation, Javascript must be enabled. Please refer to your browser's Help pages for instructions.\u003c/p\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003eDocument ConventionsPERF 3. How do you store, manage, and access data in your workload?PERF03-BP02 Evaluate available configuration options for data store",
  "PERF03-BP02 Evaluate\n      available configuration options for data store Understand and evaluate the various features and configuration options available for your\n    data stores to optimize storage space and performance for your workload. \n    Common anti-patterns:\n  \n     \n     \n     \n     \n     \n  \n       You only use one storage type, such as Amazon EBS, for all workloads. \n    \n       You use provisioned IOPS for all workloads without real-world testing against all\n        storage tiers. \n    \n       You are not aware of the configuration options of your chosen data management solution.\n      \n    \n       You rely solely on increasing instance size without looking at other available\n        configuration options. \n    \n       You are not testing the scaling characteristics of your data store. \n    \n    Benefits of establishing this best practice: By exploring and\n    experimenting with the data store configurations, you may be able to reduce the cost of\n    infrastructure, improve performance, and lower the effort required to maintain your workloads. \n    Level of risk exposed if this best practice is not established:\n    Medium \n\n    Implementation guidance\n\n\n     A workload could have one or more data stores used based on data storage and access\n      requirements. To optimize your performance efficiency and cost, you must evaluate data access\n      patterns to determine the appropriate data store configurations. While you explore data store\n      options, take into consideration various aspects such as the storage options, memory, compute,\n      read replica, consistency requirements, connection pooling, and caching options. Experiment\n      with these various configuration options to improve performance efficiency metrics. \n     \n\n      Implementation steps\n\n\n      \n         \n         \n         \n         \n         \n      \n           Understand the current configurations (like instance type, storage size, or\n            database engine version) of your data store. \n        \n           Review AWS documentation and best practices to learn about recommended\n            configuration options that can help improve the performance of your data store. Key data\n            store options to consider are the following: \n          \n                \n                   Configuration option \n                   Examples \n                \n              \n                \n                   Offloading reads (like read replicas and caching) \n                  \n                    \n                       \n                       \n                       \n                       \n                    \n                         For DynamoDB tables, you can offload reads using DAX for caching.\n                        \n                      \n                         You can create an Amazon ElastiCache (Redis OSS) cluster and configure your application\n                          to read from the cache first, falling back to the database if the\n                          requested item is not present. \n                      \n                         Relational databases such as Amazon RDS and Aurora, and provisioned NoSQL\n                          databases such as Neptune and Amazon DocumentDB all support adding read replicas\n                          to offload the read portions of the workload. \n                      \n                         Serverless databases such as DynamoDB will scale automatically. Ensure\n                          that you have enough read capacity units (RCU) provisioned to handle the\n                          workload. \n                      \n                  \n                \n                \n                   Scaling writes (like partition key sharding or introducing a queue) \n                  \n                    \n                       \n                       \n                       \n                       \n                       \n                    \n                         For relational databases, you can increase the size of the instance\n                          to accommodate an increased workload or increase the provisioned IOPs to\n                          allow for an increased throughput to the underlying storage. \n                      \n                         You can also introduce a queue in front of your database rather than\n                          writing directly to the database. This pattern allows you to decouple the\n                          ingestion from the database and control the flow-rate so the database does\n                          not get overwhelmed.  \n                      \n                         Batching your write requests rather than creating many short-lived\n                          transactions can help improve throughput in high-write volume relational\n                          databases. \n                      \n                         Serverless databases like DynamoDB can scale the write throughput\n                          automatically or by adjusting the provisioned write capacity units (WCU)\n                          depending on the capacity mode.  \n                      \n                         You can still run into issues with hot partitions when you reach the\n                          throughput limits for a given partition key. This can be mitigated by\n                          choosing a more evenly distributed partition key or by write-sharding the\n                          partition key.  \n                      \n                  \n                \n                \n                   Policies to manage the lifecycle of your datasets \n                  \n                    \n                       \n                       \n                    \n                         You can use Amazon S3\n                            Lifecycle to manage your objects throughout their lifecycle. If\n                          your access patterns are unknown, changing, or unpredictable, you can\n                            use Amazon S3\n                            Intelligent-Tiering, which monitors access patterns and\n                          automatically moves objects that have not been accessed to lower-cost\n                          access tiers. You can leverage Amazon S3 Storage\n                            Lens metrics to identify optimization opportunities and gaps in\n                          lifecycle management. \n                      \n                        \n                          Amazon EFS lifecycle\n                            management automatically manages file storage for your file\n                          systems. \n                      \n                  \n                \n                \n                   Connection management and pooling \n                  \n                    \n                       \n                       \n                    \n                         Amazon RDS Proxy can be used with Amazon RDS and Aurora to manage connections to\n                          the database.  \n                      \n                         Serverless databases such as DynamoDB do not have connections associated\n                          with them, but consider the provisioned capacity and automatic scaling\n                          policies to deal with spikes in load. \n                      \n                  \n                \n              \n        \n           Perform experiments and benchmarking in non-production environment to identify\n            which configuration option can address your workload requirements. \n        \n           Once you have experimented, plan your migration and validate your performance\n            metrics. \n        \n           Use AWS monitoring (like Amazon CloudWatch) and optimization (like Amazon S3 Storage Lens) tools to continuously optimize your\n            data store using real-world usage pattern. \n        \n     \n   \n\n    Resources\n    \n      Related documents:\n    \n    \n       \n       \n       \n       \n       \n       \n       \n       \n       \n       \n       \n       \n       \n       \n       \n       \n       \n    \n        \n          Cloud Storage with\n            AWS\n        \n      \n        \n          Amazon EBS Volume\n            Types\n        \n      \n        \n          Amazon EC2\n            Storage\n        \n      \n        \n          Amazon EFS: Amazon EFS\n            Performance\n        \n      \n        \n          Amazon FSx for Lustre\n            Performance\n        \n      \n        \n          Amazon FSx for Windows File Server Performance\n        \n      \n        \n          Amazon S3 Glacier:\n            S3 Glacier Documentation\n        \n      \n        \n          Amazon S3: Request Rate and\n            Performance Considerations\n        \n      \n        \n          Amazon EBS I/O Characteristics\n        \n      \n        \n          Cloud Databases with\n            AWS \n        \n      \n        \n          AWS Database\n            Caching \n        \n      \n        \n          DynamoDB Accelerator\n        \n      \n        \n          Amazon Aurora\n            best practices \n        \n      \n        \n          Amazon Redshift performance \n        \n      \n        \n          Amazon Athena top 10 performance tips \n        \n      \n        \n          Amazon Redshift Spectrum best practices \n        \n      \n        \n          Amazon DynamoDB best practices\n        \n      \n    \n      Related videos:\n    \n    \n       \n       \n       \n       \n       \n    \n        \n          AWS\n            re:Invent 2023: Improve Amazon Elastic Block Store efficiency and be more cost-efficient\n        \n      \n        \n          AWS\n            re:Invent 2023: Optimize storage price and performance with Amazon Simple Storage Service\n        \n      \n        \n          AWS\n            re:Invent 2023: Building and optimizing a data lake on Amazon Simple Storage Service\n        \n      \n        \n          AWS\n            re:Invent 2023: What's new with AWS file storage\n        \n      \n        \n          AWS\n            re:Invent 2023: Dive deep into Amazon DynamoDB\n        \n      \n    \n      Related examples:\n    \n    \n       \n       \n       \n       \n       \n       \n       \n       \n       \n    \n        \n          AWS Purpose Built Databases Workshop\n        \n      \n        \n          Databases for Developers\n        \n      \n        \n          AWS Modern Data Architecture Immersion Day\n        \n      \n        \n          Amazon EBS Autoscale\n        \n      \n        \n          Amazon S3 Examples\n        \n      \n        \n          Amazon DynamoDB\n            Examples\n        \n      \n        \n          AWS Database\n            migration samples\n        \n      \n        \n          Database\n            Modernization Workshop\n        \n      \n        \n          Working with parameters on your Amazon RDS for Postgress DB\n        \n      \n  \u003cdiv\u003e\u003cdiv\u003e\u003cdiv\u003e\u003cdiv id=\"js_error_message\"\u003e\u003cp\u003e\u003cimg src=\"https://d1ge0kk1l5kms0.cloudfront.net/images/G/01/webservices/console/warning.png\" alt=\"Warning\" /\u003e \u003cstrong\u003eJavascript is disabled or is unavailable in your browser.\u003c/strong\u003e\u003c/p\u003e\u003cp\u003eTo use the Amazon Web Services Documentation, Javascript must be enabled. Please refer to your browser's Help pages for instructions.\u003c/p\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003eDocument ConventionsPERF03-BP01 Use a purpose-built data store that best supports your data access and\n      storage requirementsPERF03-BP03 Collect and record data store performance\n  metrics",
  "PERF03-BP03 Collect and record data store performance\n  metrics\n    Track and record relevant performance metrics for your data store to\n    understand how your data management solutions are performing. These\n    metrics can help you optimize your data store, verify that your\n    workload requirements are met, and provide a clear overview on how\n    the workload performs.\n  \n    Common anti-patterns:\n  \n     \n     \n     \n     \n     \n  \n      \n        You only use manual log file searching for metrics.\n      \n    \n      \n        You only publish metrics to internal tools used by your team and\n        don’t have a comprehensive picture of your workload.\n      \n    \n      \n        You only use the default metrics recorded by your selected\n        monitoring software.\n      \n    \n      \n        You only review metrics when there is an issue.\n      \n    \n      \n        You only monitor system-level metrics and do not capture data\n        access or usage metrics.\n      \n    \n    Benefits of establishing this best\n    practice: Establishing a performance baseline helps you\n    understand the normal behavior and requirements of workloads.\n    Abnormal patterns can be identified and debugged faster, improving\n    the performance and reliability of the data store.\n  \n    Level of risk exposed if this best practice\n    is not established: High\n  \n\n  Implementation guidance\n\n      \n    \n      To monitor the performance of your data stores, you must record\n      multiple performance metrics over a period of time. This allows\n      you to detect anomalies, as well as measure performance against\n      business metrics to verify you are meeting your workload needs.\n    \n    \n      Metrics should include both the underlying system that is\n      supporting the data store and the database metrics. The underlying\n      system metrics might include CPU utilization, memory, available\n      disk storage, disk I/O, cache hit ratio, and network inbound and\n      outbound metrics, while the data store metrics might include\n      transactions per second, top queries, average queries rates,\n      response times, index usage, table locks, query timeouts, and\n      number of connections open. This data is crucial to understand how\n      the workload is performing and how the data management solution is\n      used. Use these metrics as part of a data-driven approach to tune\n      and optimize your workload's resources. \n    \n    \n      Use tools, libraries, and systems that record performance\n      measurements related to database performance.\n    \n   \n      \n      Implementation steps\n  \n    \n       \n       \n       \n       \n    \n        \n          Identify the key performance metrics for your data store to\n          track.\n        \n        \n           \n           \n           \n           \n           \n           \n           \n           \n           \n           \n           \n           \n           \n        \n            \n              Amazon S3 Metrics and dimensions\n            \n          \n            \n              Monitoring\n              metrics for in an Amazon RDS instance\n            \n          \n            \n              Monitoring DB load with Performance Insights on Amazon RDS\n            \n          \n            \n              Overview of Enhanced\n              Monitoring\n            \n          \n            \n              DynamoDB\n              Metrics and dimensions\n            \n          \n            \n              Monitoring\n              DynamoDB Accelerator\n            \n          \n            \n              Monitoring\n                Amazon MemoryDB with Amazon CloudWatch\n            \n          \n            \n              Which Metrics Should I Monitor?\n            \n          \n            \n              Monitoring\n              Amazon Redshift cluster performance\n            \n          \n            \n              Timestream\n              metrics and dimensions\n            \n          \n            \n              Amazon CloudWatch metrics for Amazon Aurora\n            \n          \n            \n              Logging and monitoring in Amazon Keyspaces (for Apache Cassandra)\n            \n          \n            \n              Monitoring\n              Amazon Neptune Resources\n            \n          \n      \n        \n          Use an approved logging and monitoring solution to collect\n          these metrics.\n          Amazon CloudWatch can collect metrics across the resources in\n          your architecture. You can also collect and publish custom\n          metrics to surface business or derived metrics. Use CloudWatch\n          or third-party solutions to set alarms that indicate when\n          thresholds are breached.\n        \n      \n        \n          Check if data store monitoring can benefit from a machine\n          learning solution that detects performance anomalies.\n        \n        \n           \n        \n            \n              Amazon DevOps Guru for Amazon RDS provides visibility into\n              performance issues and makes recommendations for\n              corrective actions.\n            \n          \n      \n        \n          Configure data retention in your monitoring and logging\n          solution to match your security and operational goals.\n        \n        \n           \n           \n        \n            \n              Default\n              data retention for CloudWatch metrics\n            \n          \n            \n              Default\n              data retention for CloudWatch Logs\n            \n          \n      \n   \n\n  Resources\n\n      \n    \n      Related documents:\n    \n    \n       \n       \n       \n       \n       \n       \n       \n       \n       \n    \n        \n          AWS           Database Caching\n        \n      \n        \n          Amazon Athena top 10 performance tips\n        \n      \n        \n          Amazon Aurora best practices\n        \n      \n        \n          DynamoDB Accelerator\n        \n      \n        \n          Amazon DynamoDB best practices\n        \n      \n        \n          Amazon Redshift Spectrum best practices\n        \n      \n        \n          Amazon Redshift performance\n        \n      \n        \n          Cloud\n          Databases with AWS\n        \n      \n        \n          Amazon RDS Performance Insights\n        \n      \n    \n      Related videos:\n    \n    \n       \n       \n       \n       \n       \n       \n       \n       \n    \n        AWS re:Invent 2022 - Performance monitoring with Amazon RDS and Aurora, featuring Autodesk\n        \n      \n        \n          Database Performance Monitoring and Tuning with Amazon DevOps Guru for Amazon RDS\n        \n      \n        AWS re:Invent 2023 - What’s new with AWS file storage\n        \n      \n        AWS re:Invent 2023 - Dive deep into Amazon DynamoDB\n        \n      \n        AWS re:Invent 2023 - Building and optimizing a data lake on Amazon S3\n        \n      \n        AWS re:Invent 2023 - What’s new with AWS file storage\n        \n      \n        AWS re:Invent 2023 - Dive deep into Amazon DynamoDB\n        \n      \n        \n          Best\n          Practices for Monitoring Redis Workloads on Amazon ElastiCache\n        \n      \n    \n      Related examples:\n    \n    \n      \n       \n       \n       \n    \n        \n          AWS           Dataset Ingestion Metrics Collection Framework\n        \n      \n        \n          Amazon RDS Monitoring Workshop\n        \n      \n        AWS Purpose Built Databases Workshop\n        \n      \n  \u003cdiv\u003e\u003cdiv\u003e\u003cdiv\u003e\u003cdiv id=\"js_error_message\"\u003e\u003cp\u003e\u003cimg src=\"https://d1ge0kk1l5kms0.cloudfront.net/images/G/01/webservices/console/warning.png\" alt=\"Warning\" /\u003e \u003cstrong\u003eJavascript is disabled or is unavailable in your browser.\u003c/strong\u003e\u003c/p\u003e\u003cp\u003eTo use the Amazon Web Services Documentation, Javascript must be enabled. Please refer to your browser's Help pages for instructions.\u003c/p\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003eDocument ConventionsPERF03-BP02 Evaluate available configuration options for data storePERF03-BP04 Implement strategies to improve query performance\n  in data store",
  "PERF03-BP04 Implement strategies to improve query performance\n  in data store\n    Implement strategies to optimize data and improve data query to\n    enable more scalability and efficient performance for your workload.\n  \n    Common anti-patterns:\n  \n     \n     \n     \n  \n      \n        You do not partition data in your data store.\n      \n    \n      \n        You store data in only one file format in your data store.\n      \n    \n      \n        You do not use indexes in your data store.\n      \n    \n    Benefits of establishing this best\n      practice: Optimizing data and query performance\n    results in more efficiency, lower cost, and improved user\n    experience.\n  \n    Level of risk exposed if this best practice\n    is not established: Medium\n  \n\n  Implementation guidance\n    Data optimization and query tuning are critical aspects of performance efficiency in a data store, as they impact the performance and responsiveness of the entire cloud workload. Unoptimized queries can result in greater resource usage and bottlenecks, which reduce the overall efficiency of a data store. \n    Data optimization includes several techniques to ensure efficient data storage and access. This also help to improve the query performance in a data store. Key strategies include data partitioning, data compression, and data denormalization, which help data to be optimized for both storage and access.\n     \n\n  Implementation steps\n      \n         \n         \n         \n         \n      \n          \n            Understand and analyze the critical data queries which are\n            performed in your data store.\n          \n        \n          \n            Identify the slow-running queries in your data store and use query\n            plans to understand their current state.\n          \n          \n             \n             \n          \n              \n                Analyzing\n                the query plan in Amazon Redshift\n              \n            \n              \n                Using\n                EXPLAIN and EXPLAIN ANALYZE in Athena\n              \n            \n        \n          \n            Implement strategies to improve the query performance. Some\n            of the key strategies include:\n          \n          \n             \n             \n             \n             \n             \n             \n             \n             \n          \n              \n                Using a columnar file format (like Parquet or ORC).\n              \n            \n              Compressing data in the data store to reduce storage space and I/O operation.\n            \n              \n                Data partitioning to split data into smaller parts and\n                reduce data scanning time.\n              \n              \n                 \n                 \n              \n                  \n                    Partitioning data in Athena\n                  \n                \n                  \n                    Partitions and data distribution\n                  \n                \n            \n              \n                Data indexing on the common columns in the query.\n              \n            \n              \n                Use materialized views for frequent queries. \n              \n              \n                 \n                 \n              \n                  \n                    Understanding materialized views\n                  \n                \n                  \n                    Creating materialized views in Amazon Redshift\n                  \n                \n            \n              \n                Choose the right join operation for the query. When you join two tables, specify the larger table on the left side of join and the smaller table on the right side of the join.\n              \n            \n              \n                Distributed caching solution to improve latency and reduce\n                the number of database I/O operation.\n              \n            \n              \n                Regular maintenance such as vacuuming, reindexing, and running statistics.\n              \n            \n        \n          \n            Experiment and test strategies in a non-production\n            environment.\n          \n        \n     \n   \n\n  Resources\n\n      \n    \n      Related documents:\n    \n    \n       \n       \n       \n       \n       \n       \n    \n        \n          Amazon Aurora best practices \n        \n      \n        \n          Amazon Redshift performance \n        \n      \n        \n          Amazon Athena top 10 performance tips\n        \n      \n        \n          AWS           Database Caching \n        \n      \n        \n          Best\n          Practices for Implementing Amazon ElastiCache\n        \n      \n        \n          Partitioning\n          data in Athena\n        \n      \n    \n      Related videos:\n    \n    \n       \n       \n       \n    \n        AWS re:Invent 2023 - AWS storage cost-optimization best practices\n        \n      \n        AWS re:Invent 2022 - Performance monitoring with Amazon RDS and Aurora, featuring Autodesk\n        \n      \n        \n          Optimize\n          Amazon Athena Queries with New Query Analysis Tools \n        \n      \n    \n      Related examples:\n    \n    \n       \n    \n        AWS Purpose Built Databases Workshop\n        \n      \n  \u003cdiv\u003e\u003cdiv\u003e\u003cdiv\u003e\u003cdiv id=\"js_error_message\"\u003e\u003cp\u003e\u003cimg src=\"https://d1ge0kk1l5kms0.cloudfront.net/images/G/01/webservices/console/warning.png\" alt=\"Warning\" /\u003e \u003cstrong\u003eJavascript is disabled or is unavailable in your browser.\u003c/strong\u003e\u003c/p\u003e\u003cp\u003eTo use the Amazon Web Services Documentation, Javascript must be enabled. Please refer to your browser's Help pages for instructions.\u003c/p\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003eDocument ConventionsPERF03-BP03 Collect and record data store performance\n  metricsPERF03-BP05 Implement data access patterns that utilize\n  caching",
  "PERF03-BP05 Implement data access patterns that utilize\n  caching\n    Implement access patterns that can benefit from caching data for\n    fast retrieval of frequently accessed data.\n  \n    Common anti-patterns:\n  \n     \n     \n     \n     \n  \n      \n        You cache data that changes frequently.\n      \n    \n      \n        You rely on cached data as if it is durably stored and always\n        available.\n      \n    \n      \n        You don't consider the consistency of your cached data.\n      \n    \n      \n        You don't monitor the efficiency of your caching implementation.\n      \n    \n    Benefits of establishing this best\n    practice: Storing data in a cache can improve read\n    latency, read throughput, user experience, and overall efficiency,\n    as well as reduce costs.\n  \n    Level of risk exposed if this best practice\n    is not established: Medium\n  \n\n  Implementation guidance\n    \n      A cache is a software or hardware component aimed at storing data\n      so that future requests for the same data can be served faster or\n      more efficiently. The data stored in a cache can be reconstructed\n      if lost by repeating an earlier calculation or fetching it from\n      another data store.\n    \n    \n      Data caching can be one of the most effective strategies to\n      improve your overall application performance and reduce burden on\n      your underlying primary data sources. Data can be cached at\n      multiple levels in the application, such as within the application\n      making remote calls, known as client-side\n      caching, or by using a fast secondary service for\n      storing the data, known as remote caching.\n    \n    \n      Client-side caching\n    \n    \n      With client-side caching, each client (an application or service\n      that queries the backend datastore) can store the results of their\n      unique queries locally for a specified amount of time. This can\n      reduce the number of requests across the network to a datastore by\n      checking the local client cache first. If the results are not\n      present, the application can then query the datastore and store\n      those results locally. This pattern allows each client to store\n      data in the closest location possible (the client itself),\n      resulting in the lowest possible latency. Clients can also\n      continue to serve some queries when the backend datastore is\n      unavailable, increasing the availability of the overall system.\n    \n    \n      One disadvantage of this approach is that when multiple clients are involved, they may store the same cached data locally. This results in both duplicate storage usage and data inconsistency between those clients. One client might cache the results of a query, and one minute later another client can run the same query and get a different result.\n    \n    \n      Remote caching\n    \n    \n      To solve the issue of duplicate data between clients, a fast\n      external service, or remote cache, can be\n      used to store the queried data. Instead of checking a local data\n      store, each client will check the remote cache before querying the\n      backend datastore. This strategy allows for more consistent\n      responses between clients, better efficiency in stored data, and a\n      higher volume of cached data because the storage space scales\n      independently of clients.\n    \n    \n      The disadvantage of a remote cache is that the overall system may\n      see a higher latency, as an additional network hop is required to\n      check the remote cache. Client-side caching can be used alongside\n      remote caching for multi-level caching to improve the latency.\n    \n     \n\n  Implementation steps\n\n      \n      \n         \n         \n         \n         \n         \n         \n         \n      \n          \n            Identify databases, APIs and network services that could\n            benefit from caching. Services that have heavy read\n            workloads, have a high read-to-write ratio, or are expensive\n            to scale are candidates for caching.\n          \n          \n             \n             \n          \n              \n                Database\n                  Caching\n              \n            \n              \n                Enabling\n                  API caching to enhance responsiveness\n              \n            \n        \n          \n            Identify the appropriate type of caching strategy that best\n            fits your access pattern.\n          \n          \n             \n             \n          \n              \n                Caching\n                  strategies\n              \n            \n              \n                AWS             Caching Solutions\n              \n            \n        \n          \n            Follow\n            Caching\n            Best Practices for your data store.\n          \n        \n          \n            Configure a cache invalidation strategy, such as a\n            time-to-live (TTL), for all data that balances freshness of\n            data and reducing pressure on backend datastore.\n          \n        \n          \n            Enable features such as automatic connection retries,\n            exponential backoff, client-side timeouts, and connection\n            pooling in the client, if available, as they can improve\n            performance and reliability.\n          \n          \n             \n          \n              \n                Best\n                  practices: Redis clients and Amazon ElastiCache (Redis OSS)\n              \n            \n        \n          \n            Monitor cache hit rate with a goal of 80% or higher. Lower\n            values may indicate insufficient cache size or an access\n            pattern that does not benefit from caching.\n          \n          \n             \n             \n             \n          \n              \n                Which\n                  metrics should I monitor?\n              \n            \n              \n                Best\n                  practices for monitoring Redis workloads on Amazon ElastiCache\n              \n            \n              \n                Monitoring\n                  best practices with Amazon ElastiCache (Redis OSS) using\n                  Amazon CloudWatch\n              \n            \n        \n          \n            Implement\n            data\n            replication to offload reads to multiple instances\n            and improve data read performance and availability.\n          \n        \n     \n   \n\n  Resources\n    \n      Related documents:\n    \n    \n       \n       \n       \n       \n       \n    \n        \n          Using\n          the Amazon ElastiCache Well-Architected Lens\n        \n      \n        \n          Monitoring\n          best practices with Amazon ElastiCache (Redis OSS) using Amazon CloudWatch\n        \n      \n        \n          Which\n          Metrics Should I Monitor?\n        \n      \n        \n          Performance\n          at Scale with Amazon ElastiCache whitepaper\n        \n      \n        \n          Caching\n          challenges and strategies\n        \n      \n    \n      Related videos:\n    \n    \n       \n       \n       \n       \n       \n       \n    \n        \n          Amazon ElastiCache Learning Path\n        \n      \n        \n          Design for\n          success with Amazon ElastiCache best practices\n        \n      \n        AWS re:Invent 2020 - Design for success with Amazon ElastiCache best practices\n        \n      \n        AWS re:Invent 2023 - [LAUNCH] Introducing Amazon ElastiCache Serverless\n        \n      \n        AWS re:Invent 2022 - 5 great ways to reimagine your data layer with Redis\n        \n      \n        AWS re:Invent 2021 - Deep dive on Amazon ElastiCache (Redis OSS)\n        \n      \n    \n      Related examples:\n    \n    \n       \n    \n        \n          Boosting\n          MySQL database performance with Amazon ElastiCache (Redis OSS)\n        \n      \n  \u003cdiv\u003e\u003cdiv\u003e\u003cdiv\u003e\u003cdiv id=\"js_error_message\"\u003e\u003cp\u003e\u003cimg src=\"https://d1ge0kk1l5kms0.cloudfront.net/images/G/01/webservices/console/warning.png\" alt=\"Warning\" /\u003e \u003cstrong\u003eJavascript is disabled or is unavailable in your browser.\u003c/strong\u003e\u003c/p\u003e\u003cp\u003eTo use the Amazon Web Services Documentation, Javascript must be enabled. Please refer to your browser's Help pages for instructions.\u003c/p\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003eDocument ConventionsPERF03-BP04 Implement strategies to improve query performance\n  in data storeNetworking and content delivery",
  "PERF04-BP01 Understand how networking impacts\n  performance\n    Analyze and understand how network-related decisions impact your\n    workload to provide efficient performance and improved user\n    experience.\n  \n    Common anti-patterns:\n  \n     \n     \n     \n     \n     \n  \n      \n        All traffic flows through your existing data centers.\n      \n    \n      \n        You route all traffic through central firewalls instead of using\n        cloud-native network security tools.\n      \n    \n      \n        You provision AWS Direct Connect connections without understanding\n        actual usage requirements.\n      \n    \n      \n        You don’t consider workload characteristics and encryption\n        overhead when defining your networking solutions.\n      \n    \n      \n        You use on-premises concepts and strategies for networking\n        solutions in the cloud.\n      \n    \n    Benefits of establishing this best\n    practice: Understanding how networking impacts workload\n    performance helps you identify potential bottlenecks, improve user\n    experience, increase reliability, and lower operational maintenance\n    as the workload changes.\n  \n    Level of risk exposed if this best practice\n    is not established: High\n  \n\n  Implementation guidance\n    \n      The network is responsible for the connectivity between\n      application components, cloud services, edge networks, and\n      on-premises data, and therefore it can heavily impact workload\n      performance. In addition to workload performance, user experience\n      can be also impacted by network latency, bandwidth, protocols,\n      location, network congestion, jitter, throughput, and routing\n      rules.\n    \n    \n      Have a documented list of networking requirements from the\n      workload including latency, packet size, routing rules, protocols,\n      and supporting traffic patterns. Review the available networking\n      solutions and identify which service meets your workload\n      networking characteristics. Cloud-based networks can be quickly\n      rebuilt, so evolving your network architecture over time is\n      necessary to improve performance efficiency.\n    \n     \n\n  Implementation steps:\n      \n         \n         \n         \n         \n      \n          \n            Define and document networking performance requirements,\n            including metrics such as network latency, bandwidth,\n            protocols, locations, traffic patterns (spikes and\n            frequency), throughput, encryption, inspection, and routing\n            rules.\n          \n        \n          \n            Learn about key AWS networking services like VPCs, AWS Direct Connect, Elastic Load Balancing (ELB), and Amazon Route 53.\n          \n        \n          \n            Capture the following key networking characteristics:\n          \n          \n                \n                  \n                    Characteristics\n                  \n                  \n                    Tools and metrics\n                  \n                \n              \n                \n                  \n                    Foundational networking characteristics\n                  \n                  \n                    \n                       \n                       \n                       \n                       \n                    \n                        \n                          VPC\n                            Flow Logs \n                        \n                      \n                        \n                          AWS Transit Gateway Flow Logs\n                        \n                      \n                        \n                          AWS Transit Gateway metrics\n                        \n                      \n                        \n                          AWS PrivateLink metrics\n                        \n                      \n                  \n                \n                \n                  \n                    Application networking characteristics\n                  \n                  \n                    \n                       \n                       \n                       \n                    \n                        \n                          Elastic\n                            Fabric Adapter\n                        \n                      \n                        \n                          AWS App Mesh metrics\n                        \n                      \n                        \n                          Amazon API Gateway metrics\n                        \n                      \n                  \n                \n                \n                  \n                    Edge networking characteristics\n                  \n                  \n                    \n                       \n                       \n                       \n                    \n                        \n                          Amazon CloudFront metrics\n                        \n                      \n                        \n                          Amazon Route 53 metrics\n                        \n                      \n                        \n                          AWS Global Accelerator metrics\n                        \n                      \n                  \n                \n                \n                  \n                    Hybrid networking characteristics\n                  \n                  \n                    \n                       \n                       \n                       \n                       \n                    \n                        \n                          AWS Direct Connect metrics\n                        \n                      \n                        \n                          AWS Site-to-Site VPN metrics\n                        \n                      \n                        \n                          AWS Client VPN metrics\n                        \n                      \n                        \n                          AWS Cloud WAN metrics\n                        \n                      \n                  \n                \n                \n                  \n                    Security networking characteristics\n                  \n                  \n                    \n                       \n                    \n                        \n                          AWS Shield, AWS WAF, and AWS Network Firewall metrics\n                        \n                      \n                  \n                \n                \n                  \n                    Tracing characteristics\n                  \n                  \n                    \n                       \n                       \n                       \n                       \n                       \n                    \n                        \n                          AWS X-Ray\n                        \n                      \n                        \n                          VPC\n                            Reachability Analyzer\n                        \n                      \n                        \n                          Network Access Analyzer\n                        \n                      \n                        \n                          Amazon Inspector\n                        \n                      \n                        \n                          Amazon CloudWatch RUM\n                        \n                      \n                  \n                \n              \n        \n          \n            Benchmark and test network performance:\n          \n          \n             \n             \n          \n              \n                Benchmark network\n                throughput, as some factors can affect Amazon EC2 network\n                performance when instances are in the same VPC. Measure\n                the network bandwidth between Amazon EC2 Linux instances in the\n                same VPC.\n              \n            \n              \n                Perform load\n                tests to experiment with networking solutions and\n                options.\n              \n            \n        \n     \n   \n\n  Resources\n    \n      Related documents:\n    \n    \n       \n       \n       \n       \n       \n       \n       \n       \n       \n       \n    \n        \n          Application Load Balancer\n        \n      \n        \n          EC2\n          Enhanced Networking on Linux\n        \n      \n        \n          EC2\n          Enhanced Networking on Windows\n        \n      \n        \n          EC2\n          Placement Groups\n        \n      \n        \n          Enabling\n          Enhanced Networking with the Elastic Network Adapter (ENA) on\n          Linux Instances\n        \n      \n        \n          Network Load Balancer\n        \n      \n        \n          Networking\n          Products with AWS\n        \n      \n        \n          Transit Gateway\n        \n      \n        \n          Transitioning\n          to latency-based routing in Amazon Route 53\n        \n      \n        \n          VPC\n          Endpoints\n        \n      \n    \n      Related videos:\n    \n    \n       \n       \n       \n       \n       \n       \n       \n       \n       \n    \n        AWS re:Invent 2023 - AWS networking foundations\n        \n      \n        AWS re:Invent 2023 - What can networking do for your application?\n        \n      \n        AWS re:Invent 2023 - Advanced VPC designs and new capabilities\n        \n      \n        AWS re:Invent 2023 - A developer’s guide to cloud networking\n        \n      \n        \n          AWS re:Invent 2019 - Connectivity\n          to AWS and hybrid AWS network architectures\n        \n      \n        \n          AWS re:Invent 2019 - Optimizing\n          Network Performance for Amazon EC2 Instances\n        \n      \n        \n          AWS Summit Online - Improve Global\n          Network Performance for Applications\n        \n      \n        \n          AWS re:Invent 2020 - Networking\n          best practices and tips with the Well-Architected\n          Framework\n        \n      \n        \n          AWS re:Invent 2020 - AWS networking\n          best practices in large-scale migrations\n        \n      \n    \n      Related examples:\n    \n    \n       \n       \n       \n       \n       \n    \n        \n          AWS Transit Gateway and Scalable Security Solutions\n        \n      \n        \n          AWS Networking Workshops\n        \n      \n        \n          Hands-on Network Firewall Workshop\n        \n      \n        \n          Observing and Diagnosing your Network on AWS\n      \n        \n          Finding and addressing Network Misconfigurations on AWS\n      \n  \u003cdiv\u003e\u003cdiv\u003e\u003cdiv\u003e\u003cdiv id=\"js_error_message\"\u003e\u003cp\u003e\u003cimg src=\"https://d1ge0kk1l5kms0.cloudfront.net/images/G/01/webservices/console/warning.png\" alt=\"Warning\" /\u003e \u003cstrong\u003eJavascript is disabled or is unavailable in your browser.\u003c/strong\u003e\u003c/p\u003e\u003cp\u003eTo use the Amazon Web Services Documentation, Javascript must be enabled. Please refer to your browser's Help pages for instructions.\u003c/p\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003eDocument ConventionsPERF 4. How do you select and configure networking resources in your workload?PERF04-BP02 Evaluate available networking features",
  "PERF04-BP02 Evaluate available networking featuresEvaluate networking features in the cloud that may increase performance. Measure the impact of these features through testing, metrics, and analysis. For example, take advantage of network-level features that are available to reduce latency, network distance, or jitter.\n    Common anti-patterns:\n  \n     \n     \n     \n     \n  \n      You stay within one Region because that is where your headquarters is physically located.\n    \n      You use firewalls instead of security groups for filtering traffic.\n    \n      You break TLS for traffic inspection rather than relying on security groups, endpoint policies, and other cloud-native functionality.\n    \n      You only use subnet-based segmentation instead of security groups.\n    \n    Benefits of establishing this best\n      practice: Evaluating all service features and options can increase your workload performance, reduce the cost of infrastructure, decrease the effort required to maintain your workload, and increase your overall security posture. You can use the global AWS backbone to provide the optimal networking experience for your customers.\n  \n    Level of risk exposed if this best practice\n    is not established: High\n  \n\n  Implementation guidance\n    \n      AWS offers services like AWS Global Accelerator and Amazon CloudFront that can help improve network performance, while most AWS services have product features (such as the Amazon S3 Transfer Acceleration feature) to optimize network traffic. \n    \n    \n      Review which network-related configuration options are available to you and how they could impact your workload. Performance optimization depends on understanding how these options interact with your architecture and the impact that they will have on both measured performance and user experience.\n    \n     \n      \n      Implementation steps\n      \n         \n         \n         \n         \n      \n          \n            Create a list of workload components.\n          \n          \n             \n             \n             \n             \n          \n              \n                Consider using AWS Cloud WAN to build, manage and monitor your organization's network when building a unified global network.\n              \n            \n              \n                Monitor your global and core networks with Amazon CloudWatch Logs metrics. Leverage Amazon CloudWatch RUM, which provides insights to help to identify, understand, and enhance users’ digital experience.\n              \n            \n              \n                View aggregate network latency between AWS Regions and Availability Zones, as well as within each Availability Zone, using AWS Network Manager to gain insight into how your application performance relates to the performance of the underlying AWS network.\n              \n            \n              \n                Use an existing configuration management database (CMDB) tool or a service such as AWS Config to create an inventory of your workload and how it’s configured.\n              \n            \n        \n          \n            If this is an existing workload, identify and document the benchmark for your performance metrics, focusing on the bottlenecks and areas to improve. Performance-related networking metrics will differ per workload based on business requirements and workload characteristics. As a start, these metrics might be important to review for your workload: bandwidth, latency, packet loss, jitter, and retransmits.\n          \n        \n          \n            If this is a new workload, perform load tests to identify performance bottlenecks.\n          \n        \n          \n            For the performance bottlenecks you identify, review the configuration options for your solutions to identify performance improvement opportunities. Check out the following key networking options and features:\n          \n          \n                \n                  Improvement opportunity\n                  Solution\n                \n              \n                \n                  \n                    Network path or routes\n                  \n                  \n                    Use Network Access Analyzer to identify paths or routes.\n                  \n                \n                \n                  \n                    Network protocols\n                  \n                  \n                    See PERF04-BP05 Choose network protocols to improve\n  performance\n                  \n                \n                \n                  \n                    Network topology\n                  \n                  \n                    Evaluate your operational and performance tradeoffs between VPC Peering and AWS Transit Gateway when connecting multiple accounts. AWS Transit Gateway simplifies how you interconnect all of your VPCs, which can span across thousands of AWS accounts and into on-premises networks. Share your AWS Transit Gateway between multiple accounts using AWS Resource Access Manager.\n                    \n                      See PERF04-BP03 Choose appropriate dedicated connectivity or VPN\n  for your workload\n                    \n                  \n                \n                \n                  \n                    Network services\n                  \n                  \n                    AWS Global Accelerator is a networking service that improves the performance of your users’ traffic by up to 60% using the AWS global network infrastructure. \n                    \n                      Amazon CloudFront can improve the performance of your workload content delivery and latency globally.\n                    \n                    \n                      Use Lambda@edge to run functions that customize the content that CloudFront delivers closer to the users, reduce latency, and improve performance.\n                    \n                    \n                      Amazon Route 53 offers latency-based routing, geolocation routing, geoproximity routing, and IP-based routing options to help you improve your workload’s performance for a global audience. Identify which routing option would optimize your workload performance by reviewing your workload traffic and user location when your workload is distributed globally.\n                    \n                  \n                \n                \n                  \n                    Storage resource features\n                  \n                  \n                    Amazon S3 Transfer Acceleration is a feature that lets external users benefit from the networking optimizations of CloudFront to upload data to Amazon S3. This improves the ability to transfer large amounts of data from remote locations that don’t have dedicated connectivity to the AWS Cloud.\n                    \n                      Amazon S3 Multi-Region Access Points replicates content to multiple Regions and simplifies the workload by providing one access point. When a Multi-Region Access Point is used, you can request or write data to Amazon S3 with the service identifying the lowest latency bucket.\n                    \n                  \n                \n                \n                  \n                    Compute resource features\n                  \n                  \n                    Elastic Network Interfaces (ENA) used by Amazon EC2 instances, containers, and Lambda functions are limited on a per-flow basis. Review your placement groups to optimize your EC2 networking throughput. To avoid a bottleneck on a per flow-basis, design your application to use multiple flows. To monitor and get visibility into your compute related networking metrics, use CloudWatch Metrics and ethtool. The ethtool command is included in the ENA driver and exposes additional network-related metrics that can be published as a custom metric to CloudWatch.\n                    \n                      Amazon Elastic Network Adapters (ENA) provide further optimization by delivering better throughput for your instances within a cluster placement group.\n                    \n                    \n                      Elastic Fabric Adapter (EFA) is a network interface for Amazon EC2 instances that allows you to run workloads requiring high levels of internode communications at scale on AWS. \n                    \n                    \n                      Amazon EBS-optimized instances use an optimized configuration stack and provide additional, dedicated capacity to increase the Amazon EBS I/O. \n                    \n                  \n                \n              \n        \n     \n   \n    Resources\n    \n      Related documents:\n    \n    \n       \n       \n       \n       \n       \n       \n       \n       \n       \n       \n    \n        Application Load Balancer\n      \n        EC2 Enhanced Networking on Linux \n      \n        EC2 Enhanced Networking on Windows \n      \n        EC2 Placement Groups \n      \n        Enabling Enhanced Networking with the Elastic Network Adapter (ENA) on Linux Instances\n        \n      \n        Network Load Balancer\n      \n        Networking Products with AWS\n      \n        Transitioning to Latency-Based Routing in Amazon Route 53\n      \n        VPC Endpoints\n      \n        VPC Flow Logs\n      \n    \n      Related videos:\n    \n    \n      \n       \n       \n       \n       \n      \n       \n       \n       \n    \n        \n          AWS re:Invent 2023 – Ready for what's next? Designing networks for growth and flexibility\n        \n      \n        \n          AWS re:Invent 2023 – Advanced VPC designs and new capabilities\n        \n      \n        \n          AWS re:Invent 2023 – A developer's guide to cloud networking\n        \n      \n        \n          AWS re:Invent 2022 – Dive deep on AWS networking infrastructure\n        \n      \n        AWS re:Invent 2019 – Connectivity to AWS and hybrid AWS network architectures\n      \n        AWS re:Invent 2018 – Optimizing Network Performance for Amazon EC2 Instances\n      \n        AWS Global Accelerator\n      \n    \n      Related examples:\n    \n    \n       \n       \n       \n       \n    \n        AWS Transit Gateway and Scalable Security Solutions \n      \n        AWS Networking Workshops\n      \n        \n          Observing and diagnosing your network\n        \n      \n        \n          Finding and addressing network misconfigurations on AWS\n        \n      \n  \u003cdiv\u003e\u003cdiv\u003e\u003cdiv\u003e\u003cdiv id=\"js_error_message\"\u003e\u003cp\u003e\u003cimg src=\"https://d1ge0kk1l5kms0.cloudfront.net/images/G/01/webservices/console/warning.png\" alt=\"Warning\" /\u003e \u003cstrong\u003eJavascript is disabled or is unavailable in your browser.\u003c/strong\u003e\u003c/p\u003e\u003cp\u003eTo use the Amazon Web Services Documentation, Javascript must be enabled. Please refer to your browser's Help pages for instructions.\u003c/p\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003eDocument ConventionsPERF04-BP01 Understand how networking impacts\n  performancePERF04-BP03 Choose appropriate dedicated connectivity or VPN\n  for your workload",
  "PERF04-BP03 Choose appropriate dedicated connectivity or VPN\n  for your workload\n    When hybrid connectivity is required to connect on-premises and\n    cloud resources, provision adequate bandwidth to meet your\n    performance requirements. Estimate the bandwidth and latency\n    requirements for your hybrid workload. These numbers will drive your\n    sizing requirements.\n  \n    Common anti-patterns:\n  \n     \n     \n     \n  \n      \n        You only evaluate VPN solutions for your network encryption\n        requirements.\n      \n    \n      \n        You do not evaluate backup or redundant connectivity options.\n      \n    \n      \n        You do not identify all workload requirements (encryption,\n        protocol, bandwidth, and traffic needs).\n      \n    \n    Benefits of establishing this best\n    practice: Selecting and configuring appropriate\n    connectivity solutions will increase the reliability of your\n    workload and maximize performance. By identifying workload\n    requirements, planning ahead, and evaluating hybrid solutions, you\n    can minimize expensive physical network changes and operational\n    overhead while increasing your time-to-value.\n  \n    Level of risk exposed if this best practice\n    is not established: High\n  \n\n  Implementation guidance\n\n      \n    \n      Develop a hybrid networking architecture based on your bandwidth\n      requirements. AWS Direct Connect allows you to connect your\n      on-premises network privately with AWS. It is suitable when you\n      need high-bandwidth and low-latency while achieving consistent\n      performance. A VPN connection establishes secure connection over\n      the internet. It is used when only a temporary connection is\n      required, when cost is a factor, or as a contingency while waiting\n      for resilient physical network connectivity to be established when using AWS Direct Connect.\n    \n    \n      If your bandwidth requirements are high, you might consider\n      multiple AWS Direct Connect or VPN services. Traffic can be load\n      balanced across services, although we don't recommend load\n      balancing between AWS Direct Connect and VPN because of the latency\n      and bandwidth differences.\n    \n     \n\n  Implementation steps\n      \n         \n         \n         \n         \n         \n      \n          \n            Estimate the bandwidth and latency requirements of your\n            existing applications.\n          \n          \n             \n             \n          \n              \n                For existing workloads that are moving to AWS, leverage the\n                data from your internal network monitoring systems.\n              \n            \n              \n                For new or existing workloads for which you don’t have\n                monitoring data, consult with the product owners to\n                determine adequate performance metrics and provide a good\n                user experience.\n              \n            \n        \n          \n            Select dedicated connection or VPN as your connectivity\n            option. Based on all workload requirements (encryption,\n            bandwidth, and traffic needs), you can either choose AWS Direct Connect or AWS VPN (or both). The\n            following diagram can help you choose the appropriate\n            connection type.\n          \n          \n             \n             \n          \n              \n                AWS Direct Connect provides dedicated connectivity to the\n                AWS environment, from 50 Mbps up to 100 Gbps, using either\n                dedicated connections or hosted connections. This gives you\n                managed and controlled latency and provisioned bandwidth so\n                your workload can connect efficiently to other environments.\n                Using AWS Direct Connect partners, you can have end-to-end\n                connectivity from multiple environments, providing an\n                extended network with consistent performance. AWS offers\n                scaling direct connect connection bandwidth using either\n                native 100 Gbps, link aggregation group (LAG), or BGP\n                equal-cost multipath (ECMP).\n              \n            \n              \n                The AWS Site-to-Site VPN provides a managed VPN service supporting internet protocol security (IPsec). When a VPN connection is created, each VPN connection includes two tunnels for high availability.\n              \n            \n        \n          \n            Follow AWS documentation to choose an appropriate\n            connectivity option:\n          \n          \n             \n             \n             \n             \n          \n                \n                  If you decide to use AWS Direct Connect, select the\n                  appropriate bandwidth for your connectivity.\n                \n            \n              \n                If you are using an AWS Site-to-Site VPN across multiple\n                locations to connect to an AWS Region, use\n                an accelerated\n                Site-to-Site VPN connection for the opportunity\n                to improve network performance.\n              \n            \n              \n                If your network design consists of IPSec VPN connection\n                over AWS Direct Connect, consider using Private IP VPN to\n                improve security and achieve\n                segmentation. AWS Site-to-Site Private IP VPN is deployed on top of\n                transit virtual interface (VIF).\n              \n            \n              \n                AWS Direct Connect SiteLink allows creating\n                low-latency and redundant connections between your data\n                centers worldwide by sending data over the fastest path\n                between AWS Direct Connect locations, bypassing AWS Regions.\n              \n            \n        \n          \n            Validate your connectivity setup before deploying to\n            production. Perform security and performance testing to\n            assure it meets your bandwidth, reliability, latency, and\n            compliance requirements.\n          \n        \n          \n            Regularly monitor your connectivity performance and usage\n            and optimize if required.\n          \n        \n      \n         \n          \n         \n         \n        Deterministic performance flowchart\n      \n        \n     \n   \n\n  Resources\n\n      \n    \n      Related documents:\n    \n    \n       \n       \n       \n       \n       \n    \n        \n          Networking Products with AWS\n      \n        AWS Transit Gateway\n      \n        \n          VPC Endpoints\n        \n      \n        \n          Building\n          a Scalable and Secure Multi-VPC AWS Network\n          Infrastructure\n        \n      \n        \n          Client\n          VPN\n        \n      \n    \n      Related videos:\n    \n    \n      \n       \n       \n       \n       \n       \n    \n        \n          AWS re:Invent 2023 – Building hybrid network connectivity with AWS\n        \n      \n        \n          AWS re:Invent 2023 – Secure remote connectivity to AWS\n        \n      \n        \n          AWS re:Invent 2022 – Optimizing performance with Amazon CloudFront\n        \n      \n        AWS re:Invent 2019 – Connectivity to AWS and hybrid AWS network architectures\n        \n      \n        \n          AWS re:Invent 2020 – AWS Transit Gateway Connect\n        \n      \n    \n      Related examples:\n    \n    \n       \n       \n    \n        \n          AWS Transit Gateway and Scalable Security Solutions\n        \n      \n        \n          AWS Networking Workshops\n        \n      \n  \u003cdiv\u003e\u003cdiv\u003e\u003cdiv\u003e\u003cdiv id=\"js_error_message\"\u003e\u003cp\u003e\u003cimg src=\"https://d1ge0kk1l5kms0.cloudfront.net/images/G/01/webservices/console/warning.png\" alt=\"Warning\" /\u003e \u003cstrong\u003eJavascript is disabled or is unavailable in your browser.\u003c/strong\u003e\u003c/p\u003e\u003cp\u003eTo use the Amazon Web Services Documentation, Javascript must be enabled. Please refer to your browser's Help pages for instructions.\u003c/p\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003eDocument ConventionsPERF04-BP02 Evaluate available networking featuresPERF04-BP04 Use load balancing to distribute traffic across multiple\n      resources",
  "PERF04-BP04 Use load\n      balancing to distribute traffic across multiple resources Distribute traffic across multiple resources or services to allow your workload to take\n    advantage of the elasticity that the cloud provides. You can also use load balancing for\n    offloading encryption termination to improve performance, reliability and manage and route\n    traffic effectively. \n    Common anti-patterns:\n  \n     \n     \n     \n     \n     \n  \n       You don’t consider your workload requirements when choosing the load balancer type.\n      \n    \n       You don’t leverage the load balancer features for performance optimization. \n    \n       The workload is exposed directly to the internet without a load balancer. \n    \n       You route all internet traffic through existing load balancers. \n    \n       You use generic TCP load balancing and making each compute node handle SSL encryption.\n      \n    \n    Benefits of establishing this best practice: A load balancer\n    handles the varying load of your application traffic in a single Availability Zone or across\n    multiple Availability Zones and enables high availability, automatic scaling, and better\n    utilization for your workload. \n    Level of risk exposed if this best practice is not established:\n    High \n\n    Implementation guidance\n     Load balancers act as the entry point for your workload, from which point they distribute\n      the traffic to your backend targets, such as compute instances or containers, to improve\n      utilization. \n     Choosing the right load balancer type is the first step to optimize your architecture.\n      Start by listing your workload characteristics, such as protocol (like TCP, HTTP, TLS, or\n      WebSockets), target type (like instances, containers, or serverless), application requirements\n      (like long running connections, user authentication, or stickiness), and placement (like\n      Region, Local Zone, Outpost, or zonal isolation). \n     AWS provides multiple models for your applications to use load balancing. Application Load Balancer is best suited for load balancing of HTTP and HTTPS traffic and provides\n      advanced request routing targeted at the delivery of modern application architectures,\n      including microservices and containers. \n    \n      Network Load Balancer is best suited for load balancing of TCP traffic where extreme performance is\n      required. It is capable of handling millions of requests per second while maintaining\n      ultra-low latencies, and it is optimized to handle sudden and volatile traffic patterns. \n    \n      Elastic Load Balancing provides integrated\n      certificate management and SSL/TLS decryption, allowing you the flexibility to centrally\n      manage the SSL settings of the load balancer and offload CPU intensive work from your\n      workload. \n     After choosing the right load balancer, you can start leveraging its features to reduce\n      the amount of effort your backend has to do to serve the traffic. \n     For example, using both Application Load Balancer (ALB) and Network Load Balancer (NLB), you can perform SSL/TLS encryption\n      offloading, which is an opportunity to avoid the CPU-intensive TLS handshake from being\n      completed by your targets and also to improve certificate management. \n     When you configure SSL/TLS offloading in your load balancer, it becomes responsible for\n      the encryption of the traffic from and to clients while delivering the traffic unencrypted to\n      your backends, freeing up your backend resources and improving the response time for the\n      clients. \n     Application Load Balancer can also serve HTTP/2 traffic without needing to support it on your targets. This\n      simple decision can improve your application response time, as HTTP/2 uses TCP connections\n      more efficiently. \n     Your workload latency requirements should be considered when defining the architecture.\n      As an example, if you have a latency-sensitive application, you may decide to use Network Load Balancer, which\n      offers extremely low latencies. Alternatively, you may decide to bring your workload closer to\n      your customers by leveraging Application Load Balancer in AWS Local Zones or even AWS Outposts. \n     Another consideration for latency-sensitive workloads is cross-zone load balancing. With\n      cross-zone load balancing, each load balancer node distributes traffic across the registered\n      targets in all allowed Availability Zones. \n     Use Auto Scaling integrated with your load balancer. One of the key aspects of a performance\n      efficient system has to do with right-sizing your backend resources. To do this, you can\n      leverage load balancer integrations for backend target resources. Using the load balancer\n      integration with Auto Scaling groups, targets will be added or removed from the load balancer as\n      required in response to incoming traffic. Load balancers can also integrate with Amazon ECS and Amazon EKS for containerized workloads. \n    \n       \n       \n       \n    \n        \n          Amazon ECS - Service load\n            balancing\n        \n      \n        \n          Application load\n            balancing on Amazon EKS\n        \n      \n        \n          Network\n            load balancing on Amazon EKS\n        \n      \n     \n\n      Implementation steps\n\n\n      \n         \n         \n         \n         \n         \n         \n         \n      \n           Define your load balancing requirements including traffic volume, availability and\n            application scalability. \n        \n           Choose the right load balancer type for your application. \n          \n             \n             \n             \n             \n          \n               Use Application Load Balancer for HTTP/HTTPS workloads. \n            \n               Use Network Load Balancer for non-HTTP workloads that run on TCP or UDP. \n            \n               Use a combination of both (ALB as a target of NLB) if you want to leverage features of both\n                products. For example, you can do this if you want to use the static IPs of NLB\n                together with HTTP header based routing from ALB, or if you want to expose your HTTP\n                workload to an AWS PrivateLink. \n            \n               For a full comparison of load balancers, see ELB product comparison. \n            \n        \n           Use SSL/TLS offloading if possible. \n          \n             \n             \n             \n          \n               Configure HTTPS/TLS listeners with both Application Load Balancer and Network Load Balancer integrated with AWS Certificate Manager. \n            \n               Note that some workloads may require end-to-end encryption for compliance\n                reasons. In this case, it is a requirement to allow encryption at the targets.\n              \n            \n               For security best practices, see SEC09-BP02 Enforce encryption in transit. \n            \n        \n           Select the right routing algorithm (only ALB). \n          \n             \n             \n             \n          \n               The routing algorithm can make a difference in how well-used your backend\n                targets are and therefore how they impact performance. For example, ALB\n                  provides two options for routing algorithms: \n            \n              \n                Least outstanding requests: Use to achieve a better\n                load distribution to your backend targets for cases when the requests for your\n                application vary in complexity or your targets vary in processing capability.\n              \n            \n              \n                Round robin: Use when the requests and targets are\n                similar, or if you need to distribute requests equally among targets. \n            \n        \n           Consider cross-zone or zonal isolation. \n          \n             \n             \n          \n               Use cross-zone turned off (zonal isolation) for latency improvements and zonal\n                failure domains. It is turned off by default in NLB and in ALB you can\n                  turn it off per target group. \n            \n               Use cross-zone turned on for increased availability and flexibility. By\n                default, cross-zone is turned on for ALB and in NLB you can\n                  turn it on per target group. \n            \n        \n           Turn on HTTP keep-alives for your HTTP workloads (only ALB). With this feature, the\n            load balancer can reuse backend connections until the keep-alive timeout expires,\n            improving your HTTP request and response time and also reducing resource utilization on\n            your backend targets. For detail on how to do this for Apache and Nginx, see What are\n              the optimal settings for using Apache or NGINX as a backend server for ELB?\n          \n        \n           Turn on monitoring for your load balancer. \n          \n             \n             \n             \n             \n             \n          \n               Turn on access logs for your Application Load Balancer and Network Load Balancer. \n            \n               The main fields to consider for ALB\n                  are request_processing_time, request_processing_time,\n                  and response_processing_time. \n            \n               The main fields to consider for NLB\n                  are connection_time and tls_handshake_time. \n            \n               Be ready to query the logs when you need them. You can use Amazon Athena to query\n                  both ALB\n                  logs and NLB logs.\n              \n            \n               Create alarms for performance related metrics such as TargetResponseTime for ALB. \n            \n        \n     \n   \n\n    Resources\n\n\n    \n      Related documents:\n    \n    \n       \n       \n       \n       \n       \n       \n       \n       \n    \n        \n          ELB product\n            comparison \n        \n      \n        \n          AWS Global\n            Infrastructure \n        \n      \n        \n          Improving Performance and Reducing Cost Using Availability Zone Affinity \n        \n      \n        \n          Step by step for Log Analysis with Amazon Athena \n        \n      \n        \n          Querying Application Load Balancer logs\n        \n      \n        \n          Monitor your\n            Application Load Balancers\n        \n      \n        \n          Monitor your\n            Network Load Balancer\n        \n      \n        \n          Use Elastic Load Balancing to distribute traffic across the instances in your Auto Scaling group\n        \n      \n    \n      Related videos:\n    \n    \n       \n       \n       \n       \n       \n    \n        \n          AWS re:Invent 2023: What can\n            networking do for your application?\n        \n      \n        \n          AWS re:Inforce 20: How to use\n            Elastic Load Balancing to enhance your security posture at scale\n        \n      \n        \n          AWS re:Invent 2018: Elastic Load Balancing: Deep\n            Dive and Best Practices\n        \n      \n        \n          AWS re:Invent 2021 - How to\n            choose the right load balancer for your AWS workloads \n        \n      \n        \n          AWS re:Invent 2019: Get the\n            most from Elastic Load Balancing for different workloads\n        \n      \n    \n      Related examples:\n    \n    \n       \n       \n    \n        \n          Gateway Load Balancer\n        \n      \n        \n          CDK and AWS CloudFormation samples for Log Analysis with Amazon Athena \n        \n      \n  \u003cdiv\u003e\u003cdiv\u003e\u003cdiv\u003e\u003cdiv id=\"js_error_message\"\u003e\u003cp\u003e\u003cimg src=\"https://d1ge0kk1l5kms0.cloudfront.net/images/G/01/webservices/console/warning.png\" alt=\"Warning\" /\u003e \u003cstrong\u003eJavascript is disabled or is unavailable in your browser.\u003c/strong\u003e\u003c/p\u003e\u003cp\u003eTo use the Amazon Web Services Documentation, Javascript must be enabled. Please refer to your browser's Help pages for instructions.\u003c/p\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003eDocument ConventionsPERF04-BP03 Choose appropriate dedicated connectivity or VPN\n  for your workloadPERF04-BP05 Choose network protocols to improve\n  performance",
  "PERF04-BP05 Choose network protocols to improve\n  performance\n    Make decisions about protocols for communication between systems and\n    networks based on the impact to the workload’s performance.\n  \n    There is a relationship between latency and bandwidth to achieve\n    throughput. If your file transfer is using Transmission Control\n    Protocol (TCP), higher latencies will most likely reduce overall\n    throughput. There are approaches to fix this with TCP tuning and\n    optimized transfer protocols, but one solution is to use User\n    Datagram Protocol (UDP).\n  \n    Common anti-patterns:\n  \n     \n  \n      \n        You use TCP for all workloads regardless of performance\n        requirements.\n      \n    \n    Benefits of establishing this best\n      practice: Verifying that an appropriate protocol is used for communication\n    between users and workload components helps improve overall user\n    experience for your applications. For instance, connection-less UDP\n    allows for high speed, but it doesn't offer retransmission or high\n    reliability. TCP is a full featured protocol, but it requires\n    greater overhead for processing the packets.  \n  \n    Level of risk exposed if this best practice\n    is not established: Medium\n  \n\n  Implementation guidance\n    \n      If you have the ability to choose different protocols for your\n      application and you have expertise in this area, optimize your\n      application and end-user experience by using a different protocol.\n      Note that this approach comes with significant difficulty and\n      should only be attempted if you have optimized your application in\n      other ways first.\n    \n    \n      A primary consideration for improving your workload’s performance\n      is to understand the latency and throughput requirements, and then\n      choose network protocols that optimize performance.\n    \n    \n      When to consider using TCP\n    \n    \n      TCP provides reliable data delivery, and can be used for\n      communication between workload components where reliability and\n      guaranteed delivery of data is important. Many web-based\n      applications rely on TCP-based protocols, such as HTTP and HTTPS,\n      to open TCP sockets for communication between application\n      components. Email and file data transfer are common applications\n      that also make use of TCP, as it is a simple and reliable transfer\n      mechanism between application components. Using TLS with TCP can\n      add some overhead to the communication, which can result in\n      increased latency and reduced throughput, but it comes with the\n      advantage of security. The overhead comes mainly from the added\n      overhead of the handshake process, which can take several\n      round-trips to complete. Once the handshake is complete, the\n      overhead of encrypting and decrypting data is relatively small.\n    \n    \n      When to consider using UDP\n    \n    \n      UDP is a connection-less-oriented protocol and is therefore\n      suitable for applications that need fast, efficient transmission,\n      such as log, monitoring, and VoIP data. Also, consider using UDP\n      if you have workload components that respond to small queries from\n      large numbers of clients to ensure optimal performance of the\n      workload. Datagram Transport Layer Security (DTLS) is the UDP\n      equivalent of Transport Layer Security (TLS). When using DTLS with\n      UDP, the overhead comes from encrypting and decrypting the data,\n      as the handshake process is simplified. DTLS also adds a small\n      amount of overhead to the UDP packets, as it includes additional\n      fields to indicate the security parameters and to detect\n      tampering.\n    \n    \n      When to consider using SRD\n    \n    \n      Scalable reliable datagram (SRD) is a network transport protocol\n      optimized for high-throughput workloads due to its ability to\n      load-balancer traffic across multiple paths and quickly recover\n      from packet drops or link failures. SRD is therefore best used for\n      high performance computing (HPC) workloads that require high\n      throughput and low latency communication between compute nodes.\n      This might include parallel processing tasks such as simulation,\n      modelling, and data analysis that involve a large amount of data\n      transfer between nodes.\n    \n     \n\n  Implementation steps\n      \n         \n         \n         \n         \n         \n         \n      \n          \n            Use\n            the AWS Global Accelerator and AWS Transfer Family services to improve the throughput of\n            your online file transfer applications. The AWS Global Accelerator service helps you achieve lower latency between\n            your client devices and your workload on AWS. With AWS Transfer Family, you can use TCP-based protocols such as\n            Secure Shell File Transfer Protocol (SFTP) and File Transfer\n            Protocol over SSL (FTPS) to securely scale and manage your\n            file transfers to AWS storage services.\n          \n        \n          \n            Use network latency to determine if TCP is appropriate for\n            communication between workload components. If the network\n            latency between your client application and server is high,\n            then the TCP three-way handshake can take some time, thereby\n            impacting on the responsiveness of your application. Metrics\n            such as time to first byte (TTFB) and round-trip time (RTT)\n            can be used to measure network latency. If your workload\n            serves dynamic content to users, consider\n            using Amazon CloudFront, which establishes a persistent connection\n            to each origin for dynamic content to remove the connection\n            setup time that would otherwise slow down each client\n            request.\n          \n        \n          \n            Using TLS with TCP or UDP can result in increased latency\n            and reduced throughput for your workload due to the impact\n            of encryption and decryption. For such workloads, consider\n            SSL/TLS offloading\n            on Elastic Load Balancing to improve workload performance by\n            allowing the load balancer to handle SSL/TLS encryption and\n            decryption process instead of having backend instances do\n            it. This can help reduce the CPU utilization on the backend\n            instances, which can improve performance and increase\n            capacity.\n          \n        \n          \n            Use\n            the Network Load Balancer (NLB) to deploy services that rely on\n            the UDP protocol, such as authentication and authorization,\n            logging, DNS, IoT, and streaming media, to improve the\n            performance and reliability of your workload. The NLB\n            distributes incoming UDP traffic across multiple targets,\n            allowing you to scale your workload horizontally, increase\n            capacity, and reduce the overhead of a single target.\n          \n        \n          \n            For your High Performance Computing (HPC) workloads,\n            consider using\n            the Elastic\n            Network Adapter (ENA) Express functionality that uses\n            the SRD protocol to improve network performance by providing\n            a higher single flow bandwidth (25 Gbps) and lower tail\n            latency (99.9 percentile) for network traffic between EC2\n            instances.\n          \n        \n          \n            Use\n            the Application Load Balancer (ALB) to route and load balance your\n            gRPC (Remote Procedure Calls) traffic between workload\n            components or between gRPC clients and services. gRPC uses\n            the TCP-based HTTP/2 protocol for transport and it provides\n            performance benefits such as lighter network footprint,\n            compression, efficient binary serialization, support for\n            numerous languages, and bi-directional streaming.\n          \n        \n     \n   \n\n  Resources\n\n    \n      Related documents:\n    \n    \n       \n       \n       \n       \n       \n       \n       \n       \n       \n       \n    \n        \n          How to route UDP traffic into Kubernetes\n        \n      \n        \n          Application Load Balancer\n        \n      \n        \n          EC2\n          Enhanced Networking on Linux\n        \n      \n        \n          EC2\n          Enhanced Networking on Windows\n        \n      \n        \n          EC2\n          Placement Groups\n        \n      \n        \n          Enabling\n          Enhanced Networking with the Elastic Network Adapter (ENA) on\n          Linux Instances\n        \n      \n        \n          Network Load Balancer\n        \n      \n        \n          Networking\n          Products with AWS\n        \n      \n        \n          Transitioning\n          to Latency-Based Routing in Amazon Route 53\n        \n      \n        \n          VPC\n          Endpoints\n        \n      \n    \n      Related videos:\n    \n    \n       \n       \n    \n        \n          AWS re:Invent 2022 – Scaling network performance on next-gen Amazon Elastic Compute Cloud instances\n        \n      \n        \n          AWS re:Invent 2022 – Application networking foundations\n        \n      \n    \n      Related examples:\n    \n    \n       \n       \n    \n        \n          AWS Transit Gateway and Scalable Security Solutions\n        \n      \n        \n          AWS           Networking Workshops\n        \n      \n  \u003cdiv\u003e\u003cdiv\u003e\u003cdiv\u003e\u003cdiv id=\"js_error_message\"\u003e\u003cp\u003e\u003cimg src=\"https://d1ge0kk1l5kms0.cloudfront.net/images/G/01/webservices/console/warning.png\" alt=\"Warning\" /\u003e \u003cstrong\u003eJavascript is disabled or is unavailable in your browser.\u003c/strong\u003e\u003c/p\u003e\u003cp\u003eTo use the Amazon Web Services Documentation, Javascript must be enabled. Please refer to your browser's Help pages for instructions.\u003c/p\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003eDocument ConventionsPERF04-BP04 Use load balancing to distribute traffic across multiple\n      resourcesPERF04-BP06 Choose your workload's location based on network requirements",
  "PERF04-BP06 Choose your workload's location based on network requirementsEvaluate options for resource placement to reduce network latency and improve throughput, providing an optimal user experience by reducing page load and data transfer times.\n    Common anti-patterns:\n  \n     \n     \n  \n      \n        You consolidate all workload resources into one geographic location.\n      \n    \n      \n        You chose the closest Region to your location but not to the workload end user.\n      \n    \n    Benefits of establishing this best\n      practice: User experience is greatly affected by the latency between the user and your application. By using appropriate AWS Regions and the AWS private global network, you can reduce latency and deliver a better experience to remote users.\n  \n    Level of risk exposed if this best practice\n    is not established: Medium\n  \n\n  Implementation guidance\n\n      \n    \n      Resources, such as Amazon EC2 instances, are placed into Availability Zones within AWS Regions, AWS Local Zones, AWS Outposts, or AWS Wavelength zones. Selection of this location influences network latency and throughput from a given user location. Edge services like Amazon CloudFront and AWS Global Accelerator can also be used to improve network performance by either caching content at edge locations or providing users with an optimal path to the workload through the AWS global network.\n    \n    \n      Amazon EC2 provides placement groups for networking. A placement group is a logical grouping of instances to decrease latency. Using placement groups with supported instance types and an Elastic Network Adapter (ENA) enables workloads to participate in a low-latency, reduced jitter 25 Gbps network. Placement groups are recommended for workloads that benefit from low network latency, high network throughput, or both.\n    \n    \n      Latency-sensitive services are delivered at edge locations using AWS global network, such as Amazon CloudFront. These edge locations commonly provide services like content delivery network (CDN) and domain name system (DNS). By having these services at the edge, workloads can respond with low latency to requests for content or DNS resolution. These services also provide geographic services, such as geotargeting of content (providing different content based on the end users’ location) or latency-based routing to direct end users to the nearest Region (minimum latency). \n    \n    \n      Use edge services to reduce latency and to enable content caching. Configure cache control correctly for both DNS and HTTP/HTTPS to gain the most benefit from these approaches.\n    \n     \n\n  Implementation steps\n      \n         \n         \n         \n         \n         \n         \n         \n         \n         \n      \n          \n            Capture information about the IP traffic going to and from network interfaces.\n          \n          \n             \n             \n          \n              \n                Logging IP traffic using VPC Flow Logs\n              \n            \n              \n                How the client IP address is preserved in AWS Global Accelerator\n            \n        \n          \n            Analyze network access patterns in your workload to identify how users use your application.\n          \n          \n             \n             \n          \n              \n                Use monitoring tools, such as Amazon CloudWatch and AWS CloudTrail, to gather data on network activities.\n              \n            \n              \n                Analyze the data to identify the network access pattern.\n              \n            \n        \n          \n            Select Regions for your workload deployment based on the following key elements:\n          \n          \n             \n             \n             \n          \n              \n                Where your data is located: For data-heavy\n                applications (such as big data and machine learning), application code should run as\n                close to the data as possible. \n            \n              \n                Where your users are located: For user-facing\n                applications, choose a Region (or Regions) close to your workload’s users. \n            \n              \n                Other constraints: Consider constraints such as\n                cost and compliance as explained in What to Consider when Selecting a Region for\n                your Workloads.\n            \n        \n          \n            Use AWS Local Zones to run workloads like video rendering. Local Zones allow you to benefit from having compute and storage resources closer to end users.\n          \n        \n          \n            Use AWS Outposts for workloads that need to remain on-premises and where you want that workload to run seamlessly with the rest of your other workloads in AWS.\n          \n        \n          \n            Applications like high-resolution live video streaming, high-fidelity audio, and augmented reality or virtual reality (AR/VR) require ultra-low-latency for 5G devices. For such applications, consider AWS Wavelength. AWS Wavelength embeds AWS compute and storage services within 5G networks, providing mobile edge computing infrastructure for developing, deploying, and scaling ultra-low-latency applications.\n          \n        \n          \n            Use local caching or AWS Caching Solutions for frequently used assets to improve performance, reduce data movement, and lower environmental impact.\n          \n          \n                \n                  Service\n                  When to use\n                \n              \n                \n                  \n                    Amazon CloudFront\n                  \n                  \n                    Use to cache static content such as images, scripts, and videos, as well as dynamic content such as API responses or web applications.\n                  \n                \n                \n                  \n                    Amazon ElastiCache\n                  \n                  \n                    Use to cache content for web applications.\n                  \n                \n                \n                  \n                    DynamoDB Accelerator\n                  \n                  \n                    Use to add in-memory acceleration to your DynamoDB tables.\n                  \n                \n              \n        \n          \n            Use services that can help you run code closer to users of your workload like the following:\n          \n          \n                \n                  Service\n                  When to use\n                \n              \n                \n                  \n                    Lambda@edge\n                  \n                  \n                    Use for compute-heavy operations that are initiated when objects are not in the cache.\n                  \n                \n                \n                  \n                    Amazon CloudFront Functions\n                  \n                  \n                    Use for simple use cases like HTTP(s) requests or response manipulations that can be initiated by short-lived functions.\n                  \n                \n                  \n                    AWS IoT Greengrass\n                  \n                  \n                    Use to run local compute, messaging, and data caching for connected devices.\n                  \n                \n              \n        \n          \n            Some applications require fixed entry points or higher performance by reducing first byte latency and jitter, and increasing throughput. These applications can benefit from networking services that provide static anycast IP addresses and TCP termination at edge locations. AWS Global Accelerator can improve performance for your applications by up to 60% and provide quick failover for multi-region architectures. AWS Global Accelerator provides you with static anycast IP addresses that serve as a fixed entry point for your applications hosted in one or more AWS Regions. These IP addresses permit traffic to ingress onto the AWS global network as close to your users as possible. AWS Global Accelerator reduces the initial connection setup time by establishing a TCP connection between the client and the AWS edge location closest to the client. Review the use of AWS Global Accelerator to improve the performance of your TCP/UDP workloads and provide quick failover for multi-Region architectures.\n          \n        \n     \n   \n\n  Resources\n\n    \n      Related best practices:\n    \n    \n       \n       \n       \n       \n       \n       \n       \n    \n        \n          COST07-BP02 Implement Regions based on cost\n        \n      \n        \n          COST08-BP03 Implement services to reduce data transfer costs\n        \n      \n        \n          REL10-BP01 Deploy the workload to multiple locations\n        \n      \n        \n          REL10-BP02 Select the appropriate locations for your multi-location deployment\n        \n      \n        \n          SUS01-BP01 Choose Region based on both business requirements and sustainability goals\n        \n      \n        \n          SUS02-BP04 Optimize geographic placement of workloads based on their networking requirements\n        \n      \n        \n          SUS04-BP07 Minimize data movement across networks\n        \n      \n    \n      Related documents:\n    \n    \n       \n       \n       \n       \n       \n       \n       \n       \n       \n       \n       \n    \n        AWS Global Infrastructure \n        \n      \n        AWS Local Zones and AWS Outposts, choosing the right technology for your edge workload \n        \n      \n        \n          Placement groups\n        \n      \n        AWS Local Zones\n        \n      \n        AWS Outposts\n      \n        AWS Wavelength\n      \n        \n          Amazon CloudFront\n        \n      \n        AWS Global Accelerator\n      \n        AWS Direct Connect\n      \n        AWS Site-to-Site VPN\n      \n        \n          Amazon Route 53\n        \n      \n    \n      Related videos:\n    \n    \n       \n       \n       \n       \n       \n       \n       \n       \n       \n       \n    \n        AWS Local Zones Explainer Video\n        \n      \n        AWS Outposts: Overview and How it Works\n        \n      \n        AWS re:Invent 2023 - A migration strategy for edge and on-premises workloads \n        \n      \n        AWS re:Invent 2021 - AWS Outposts: Bringing the AWS experience on premises \n        \n      \n        AWS re:Invent 2020: AWS Wavelength: Run apps with ultra-low latency at 5G edge \n        \n      \n        AWS re:Invent 2022 - AWS Local Zones: Building applications for a distributed edge \n        \n      \n        AWS re:Invent 2021 - Building low-latency websites with Amazon CloudFront \n        \n      \n        AWS re:Invent 2022 - Improve performance and availability with AWS Global Accelerator\n      \n        AWS re:Invent 2022 - Build your global wide area network using AWS\n      \n        AWS re:Invent 2020: Global traffic management with Amazon Route 53 \n        \n      \n    \n      Related examples:\n    \n    \n       \n       \n    \n        AWS Global Accelerator Custom Routing Workshop\n        \n      \n        \n          Handling Rewrites and Redirects using Edge Functions\n        \n      \n  \u003cdiv\u003e\u003cdiv\u003e\u003cdiv\u003e\u003cdiv id=\"js_error_message\"\u003e\u003cp\u003e\u003cimg src=\"https://d1ge0kk1l5kms0.cloudfront.net/images/G/01/webservices/console/warning.png\" alt=\"Warning\" /\u003e \u003cstrong\u003eJavascript is disabled or is unavailable in your browser.\u003c/strong\u003e\u003c/p\u003e\u003cp\u003eTo use the Amazon Web Services Documentation, Javascript must be enabled. Please refer to your browser's Help pages for instructions.\u003c/p\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003eDocument ConventionsPERF04-BP05 Choose network protocols to improve\n  performancePERF04-BP07 Optimize network configuration based on\n  metrics",
  "PERF04-BP07 Optimize network configuration based on\n  metrics\n    Use collected and analyzed data to make informed decisions about\n    optimizing your network configuration.\n  \n    Common anti-patterns:\n  \n     \n     \n     \n     \n  \n      \n        You assume that all performance-related issues are\n        application-related.\n      \n    \n      \n        You only test your network performance from a location close to\n        where you have deployed the workload.\n      \n    \n      \n        You use default configurations for all network services.\n      \n    \n      \n        You overprovision the network resource to provide sufficient\n        capacity.\n      \n    \n    Benefits of establishing this best\n    practice: Collecting necessary metrics of your AWS\n    network and implementing network monitoring tools allows you to\n    understand network performance and optimize network configurations.\n  \n    Level of risk exposed if this best practice\n    is not established: Low\n  \n\n  Implementation guidance\n\n      \n    \n      Monitoring traffic to and from VPCs, subnets, or network\n      interfaces is crucial to understand how to utilize AWS network\n      resources and optimize network configurations. By using the\n      following AWS networking tools, you can further inspect\n      information about the traffic usage, network access and logs.\n    \n     \n\n  Implementation steps\n\n      \n      \n         \n         \n         \n         \n      \n          \n            Identify the key performance metrics such as latency or packet\n            loss to collect. AWS provides several tools that can\n            help you to collect these metrics. By using the following\n            tools, you can further inspect information about the traffic\n            usage, network access, and logs:\n          \n          \n                \n                  \n                    AWS tool\n                  \n                  \n                    Where to use\n                  \n                \n              \n                \n                  \n                    Amazon VPC IP Address Manager.\n                  \n                  \n                    Use IPAM to plan, track, and monitor IP addresses for\n                    your AWS and on-premises workloads. This is a best\n                    practice to optimize IP address usage and allocation.\n                  \n                \n                \n                  \n                    VPC\n                      Flow logs\n                  \n                  \n                    Use VPC Flow Logs to capture detailed information about\n                    traffic to and from network interfaces in your VPCs.\n                    With VPC Flow Logs, you can diagnose overly restrictive\n                    or permissive security group rules and determine the\n                    direction of the traffic to and from the network\n                    interfaces.\n                  \n                \n                \n                  \n                    AWS Transit Gateway Flow Logs\n                  \n                  \n                    Use AWS Transit Gateway Flow Logs to capture information\n                    about the IP traffic going to and from your transit\n                    gateways.\n                  \n                \n                \n                  \n                    DNS\n                      query logging\n                  \n                  \n                    Log information about public or private DNS queries\n                    Route 53 receives. With DNS logs, you can optimize DNS\n                    configurations by understanding the domain or subdomain\n                    that was requested or Route 53 EDGE locations that\n                    responded to DNS queries.\n                  \n                \n                \n                  \n                    Reachability Analyzer\n                  \n                  \n                    Reachability Analyzer helps you analyze and debug\n                    network reachability. Reachability Analyzer is a\n                    configuration analysis tool that allows you to perform\n                    connectivity testing between a source resource and a\n                    destination resource in your VPCs. This tool helps you\n                    verify that your network configuration matches your\n                    intended connectivity.\n                  \n                \n                \n                  \n                    Network Access Analyzer\n                  \n                  \n                    Network Access Analyzer helps you understand network\n                    access to your resources. You can use Network Access Analyzer to specify your network access requirements and\n                    identify potential network paths that do not meet your\n                    specified requirements. By optimizing your corresponding\n                    network configuration, you can understand and verify the\n                    state of your network and demonstrate if your network on\n                    AWS meets your compliance requirements.\n                  \n                \n                \n                  \n                    Amazon CloudWatch\n                  \n                  \n                    Use Amazon CloudWatch and turn on the appropriate metrics\n                    for network options. Make sure to choose the right\n                    network metric for your workload. For example, you can\n                    turn on metrics for VPC Network Address Usage, VPC NAT\n                    Gateway, AWS Transit Gateway, VPN tunnel, AWS Network Firewall, Elastic Load Balancing, and AWS Direct Connect. Continually monitoring metrics is a good\n                    practice to observe and understand your network status\n                    and usage, which helps you optimize network\n                    configuration based on your observations.\n                  \n                \n                \n                  \n                    AWS Network Manager\n                  \n                  \n                    Using AWS Network Manager, you can monitor the real-time\n                    and historical performance of\n                    the AWS Global Network for operational and planning\n                    purposes. Network Manager provides aggregate network\n                    latency between AWS Regions and Availability Zones and\n                    within each Availability Zone, allowing you to better\n                    understand how your application performance relates to\n                    the performance of the underlying AWS network.\n                  \n                \n                \n                  \n                    Amazon CloudWatch RUM\n                  \n                  \n                    Use Amazon CloudWatch RUM to collect the metrics that\n                    give you the insights that help you identify,\n                    understand, and improve user experience.\n                  \n                \n              \n        \n          \n            Identify top talkers and application traffic patterns using\n            VPC and AWS Transit Gateway Flow Logs.\n          \n        \n          \n            Assess and optimize your current network architecture\n            including VPCs, subnets, and routing. As an example, you can\n            evaluate how different VPC peering or AWS Transit Gateway\n            can help you improve the networking in your architecture.\n          \n        \n          \n            Assess the routing paths in your network to verify that the\n            shortest path between destinations is always used. Network Access Analyzer can help you do this.\n          \n        \n     \n   \n\n  Resources\n\n      \n    \n      Related documents:\n    \n    \n       \n       \n       \n       \n       \n       \n       \n       \n    \n        \n          Public\n          DNS query logging\n        \n      \n        \n          What\n          is IPAM?\n        \n      \n        \n          What\n          is Reachability Analyzer?\n        \n      \n        \n          What\n          is Network Access Analyzer?\n        \n      \n        \n          CloudWatch\n          metrics for your VPCs\n        \n      \n        \n          Optimize\n          performance and reduce costs for network analytics with VPC\n          Flow Logs in Apache Parquet format \n        \n      \n        \n          Monitoring\n          your global and core networks with Amazon CloudWatch\n          metrics\n        \n      \n        \n          Continuously\n          monitor network traffic and resources\n        \n      \n    \n      Related videos:\n    \n      \n       \n       \n       \n       \n       \n       \n    \n        \n          AWS re:Invent 2023 – A developer's guide to cloud networking\n        \n      \n        \n          AWS re:Invent 2023 – Ready for what’s next? Designing networks for growth and flexibility\n        \n      \n        \n          AWS re:Invent 2023 – Advanced VPC designs and new capabilities\n        \n      \n        \n          AWS re:Invent 2022 – Dive deep on AWS networking infrastructure\n        \n      \n        \n          AWS re:Invent 2020 – Networking\n          best practices and tips with the AWS Well-Architected\n          Framework \n        \n      \n        \n          AWS re:Invent 2020 – Monitoring\n          and troubleshooting network traffic \n        \n      \n    \n      Related examples:\n    \n    \n       \n       \n       \n       \n    \n        \n          AWS           Networking Workshops\n        \n      \n        \n          AWS           Network Monitoring\n        \n      \n        \n          Observing and diagnosing your network on AWS\n        \n      \n        \n          Finding and addressing network misconfigurations on AWS\n        \n      \n  \u003cdiv\u003e\u003cdiv\u003e\u003cdiv\u003e\u003cdiv id=\"js_error_message\"\u003e\u003cp\u003e\u003cimg src=\"https://d1ge0kk1l5kms0.cloudfront.net/images/G/01/webservices/console/warning.png\" alt=\"Warning\" /\u003e \u003cstrong\u003eJavascript is disabled or is unavailable in your browser.\u003c/strong\u003e\u003c/p\u003e\u003cp\u003eTo use the Amazon Web Services Documentation, Javascript must be enabled. Please refer to your browser's Help pages for instructions.\u003c/p\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003eDocument ConventionsPERF04-BP06 Choose your workload's location based on network requirementsProcess and culture",
  "PERF05-BP01 Establish key performance indicators (KPIs) to\n  measure workload health and performance\n    Identify the KPIs that quantitatively and qualitatively measure\n    workload performance. KPIs help you measure the health and\n    performance of a workload related to a business goal.\n  \n    Common anti-patterns:\n  \n     \n     \n     \n     \n  \n      \n        You only monitor system-level metrics to gain insight into your\n        workload and don’t understand business impacts to those metrics.\n      \n    \n      \n        You assume that your KPIs are already being published and shared\n        as standard metric data.\n      \n    \n      \n        You do not define a quantitative, measurable KPI.\n      \n    \n      \n        You do not align KPIs with business goals or strategies.\n      \n    \n    Benefits of establishing this best\n    practice: Identifying specific KPIs that represent\n    workload health and performance helps align teams on their\n    priorities and define successful business outcomes. Sharing those\n    metrics with all departments provides visibility and alignment on\n    thresholds, expectations, and business impact.\n  \n    Level of risk exposed if this best practice\n    is not established: High\n  \n\n  Implementation guidance\n\n      \n    \n      KPIs allow business and engineering teams to align on the\n      measurement of goals and strategies and how these factors combine\n      to produce business outcomes. For example, a website workload\n      might use page load time as an indication of overall performance.\n      This metric would be one of multiple data points that measures user experience. In addition to identifying the page load time\n      thresholds, you should document the expected outcome or business\n      risk if ideal performance is not met. A long page load time\n      affects your end users directly, decreases their user experience\n      rating, and can lead to a loss of customers. When you define your\n      KPI thresholds, combine both industry benchmarks and your end user\n      expectations. For example, if the current industry benchmark is a\n      webpage loading within a two-second time period, but your end\n      users expect a webpage to load within a one-second time period,\n      then you should take both of these data points into consideration\n      when establishing the KPI.\n    \n    \n      Your team must evaluate your workload KPIs using real-time\n      granular data and historical data for reference and create\n      dashboards that perform metric math on your KPI data to derive\n      operational and utilization insights. KPIs should be documented\n      and include thresholds that support business goals and strategies,\n      and should be mapped to metrics being monitored. KPIs should be\n      revisited when business goals, strategies, or end user\n      requirements change.  \n    \n   \n    \n    Implementation steps\n    \n       \n       \n       \n       \n       \n       \n       \n       \n       \n    \n        Identify stakeholders: Identify and document key business stakeholders, including development and operation teams.\n        \n      \n        Define objectives:\n          Work with these stakeholders to define and document objectives\n          of your workload. Consider the critical performance aspects of your workloads, such as throughput, response time, and cost, as well as business goals, such as user satisfaction.\n        \n      \n        Review industry best practices:\n          Review industry best practices to identify relevant KPIs\n          aligned with your workload objectives.\n        \n      \n        \n          Identify metrics: Identify metrics that are aligned with your workload objectives and can help you measure performance and business goals. Establish KPIs based on these metrics. Example metrics are measurements like average response time or number of concurrent users.\n        \n      \n        Define and document KPIs:\n          Use industry best practices and your workload objectives to\n          set targets for your workload KPI. Use this information to set\n          KPI thresholds for severity or alarm level. Identify and document the risk and impact of a KPI is not met.\n        \n      \n        Implement monitoring:\n          Use monitoring tools such as\n          Amazon CloudWatch or\n          AWS Config to collect metrics and measure KPIs.\n        \n      \n        Visually communicate KPIs:\n          Use dashboard tools like Amazon QuickSight to visualize and communicate KPIs with\n          stakeholders.\n        \n      \n        Analyze and optimize:\n          Regularly review and analyze KPIs to identify areas of your\n          workload that need to be improved. Work with stakeholders to implement these improvements.\n        \n      \n        Revisit and refine:\n          Regularly  review metrics and KPIs to assess their effectiveness, especially when business goals or workload performance change.\n        \n      \n   \n\n  Resources\n\n      \n    \n      Related documents:\n    \n    \n       \n       \n       \n       \n       \n       \n       \n       \n    \n        \n          CloudWatch\n          documentation\n        \n      \n        \n          Monitoring,\n          Logging, and Performance AWS Partners\n        \n      \n        AWS observability tools\n        \n      \n        \n          The Importance of Key Performance Indicators (KPIs) for Large-Scale Cloud Migrations\n        \n      \n        \n          How to track your cost optimization KPIs with the KPI Dashboard\n        \n      \n        \n          X-Ray\n          Documentation\n        \n      \n        \n          Using\n          Amazon CloudWatch dashboards\n        \n      \n        \n          QuickSight KPIs\n        \n      \n    \n      Related videos:\n    \n    \n       \n       \n       \n       \n       \n       \n       \n       \n       \n    \n        AWS re:Invent 2023 - Optimize cost and performance and track progress toward mitigation\n        \n      \n        AWS re:Invent 2023 - Manage resource lifecycle events at scale with AWS Health\n      \n        AWS re:Invent 2023 - Performance \u0026 efficiency at Pinterest: Optimizing the latest instances\n        \n      \n        AWS re:Invent 2022 - AWS optimization: Actionable steps for immediate results\n        \n      \n        AWS re:Invent 2023 - Building an effective observability strategy\n        \n      \n        AWS Summit SF 2022 - Full-stack observability and application monitoring with AWS\n      \n        AWS re:Invent 2023 - Scaling on AWS for the first 10 million users\n        \n      \n        AWS re:Invent 2022 - How Amazon uses better metrics for improved website performance\n        \n      \n        \n          Creating an Effective Metrics Strategy for Your Business | AWS Events\n        \n      \n    \n      Related examples:\n    \n    \n       \n    \n        \n          Creating\n          a dashboard with QuickSight\n        \n      \n  \u003cdiv\u003e\u003cdiv\u003e\u003cdiv\u003e\u003cdiv id=\"js_error_message\"\u003e\u003cp\u003e\u003cimg src=\"https://d1ge0kk1l5kms0.cloudfront.net/images/G/01/webservices/console/warning.png\" alt=\"Warning\" /\u003e \u003cstrong\u003eJavascript is disabled or is unavailable in your browser.\u003c/strong\u003e\u003c/p\u003e\u003cp\u003eTo use the Amazon Web Services Documentation, Javascript must be enabled. Please refer to your browser's Help pages for instructions.\u003c/p\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003eDocument ConventionsPERF 5. How do your organizational practices and culture contribute to performance efficiency in your workload?PERF05-BP02 Use monitoring solutions to understand the areas\n  where performance is most critical",
  "PERF05-BP02 Use monitoring solutions to understand the areas\n  where performance is most critical\n    Understand and identify areas where increasing the performance of\n    your workload will have a positive impact on efficiency or customer\n    experience. For example, a website that has a large amount of\n    customer interaction can benefit from using edge services to move\n    content delivery closer to customers.\n  \n    Common anti-patterns:\n  \n     \n     \n     \n  \n      \n        You assume that standard compute metrics such as CPU utilization\n        or memory pressure are enough to catch performance issues.\n      \n    \n      \n        You only use the default metrics recorded by your selected\n        monitoring software.\n      \n    \n      \n        You only review metrics when there is an issue.\n      \n    \n    Benefits of establishing this best\n    practice: Understanding critical areas of performance\n    helps workload owners monitor KPIs and prioritize high-impact\n    improvements.\n  \n    Level of risk exposed if this best practice\n    is not established: High\n  \n\n  Implementation guidance\n    \n      Set up end-to-end tracing to identify traffic patterns, latency,\n      and critical performance areas. Monitor your data access patterns\n      for slow queries or poorly fragmented and partitioned data.\n      Identify the constrained areas of the workload using load testing\n      or monitoring.\n    \n    \n      Increase performance efficiency by understanding your\n      architecture, traffic patterns, and data access patterns, and\n      identify your latency and processing times. Identify the potential\n      bottlenecks that might affect the customer experience as the\n      workload grows. After investigating these areas, look at which\n      solution you could deploy to remove those performance concerns.\n    \n     \n\n  Implementation steps\n      \n         \n         \n         \n         \n      \n          \n            Set up end-to-end monitoring to capture all workload\n            components and metrics. Here are examples of monitoring\n            solutions on AWS.\n          \n          \n                \n                  \n                    Service\n                  \n                  \n                    Where to use\n                  \n                \n              \n                \n                  \n                    Amazon CloudWatch Real-User Monitoring (RUM)\n                  \n                  \n                    To capture application performance metrics from real\n                    user client-side and frontend sessions.\n                  \n                \n                \n                  \n                    AWS X-Ray \n                  \n                  \n                    To trace traffic through the application layers and\n                    identify latency between components and dependencies.\n                    Use X-Ray service maps to see relationships and latency\n                    between workload components.\n                  \n                \n                \n                  \n                    Amazon Relational Database Service Performance Insights \n                  \n                  \n                    To view database performance metrics and identify\n                    performance improvements.\n                  \n                \n                \n                  \n                    Amazon RDS Enhanced Monitoring \n                  \n                  \n                    To view database OS performance metrics.\n                  \n                \n                \n                  \n                    Amazon\n                      DevOps Guru\n                  \n                  \n                    To detect abnormal operating patterns so you can\n                    identify operational issues before they impact your\n                    customers.\n                  \n                \n              \n        \n          \n            Perform tests to generate metrics, identify traffic\n            patterns, bottlenecks, and critical performance areas. Here\n            are some examples of how to perform testing:\n          \n          \n             \n             \n          \n              \n                Set\n                up CloudWatch\n                Synthetic Canaries to mimic browser-based user\n                activities programmatically using Linux cron jobs or\n                rate expressions to generate consistent metrics over\n                time.\n              \n            \n              \n                Use the AWS Distributed Load Testing solution to generate\n                peak traffic or test the workload at the expected growth\n                rate.\n              \n            \n        \n          \n            Evaluate the metrics and telemetry to identify your critical\n            performance areas. Review these areas with your team to\n            discuss monitoring and solutions to avoid bottlenecks.\n          \n        \n          \n            Experiment with performance improvements and measure those\n            changes with data. As an example, you can\n            use CloudWatch\n            Evidently to test new improvements and performance\n            impacts to your workload.\n          \n        \n     \n   \n\n  Resources\n\n      \n    \n      Related documents:\n    \n    \n       \n       \n       \n       \n       \n    \n        \n          What's new in AWS Observability at re:Invent 2023\n        \n      \n        \n          Amazon\n          Builders’ Library\n        \n      \n        \n          X-Ray\n          Documentation\n        \n      \n        \n          Amazon CloudWatch RUM\n        \n      \n        \n          Amazon DevOps Guru\n        \n      \n    \n      Related videos:\n    \n    \n       \n       \n       \n       \n       \n       \n       \n       \n    \n        AWS re:Invent 2023 - [LAUNCH] Application monitoring for modern workloads\n        \n      \n        AWS re:Invent 2023 - Implementing application observability\n        \n      \n        AWS re:Invent 2023 - Building an effective observability strategy\n        \n      \n        AWS Summit SF 2022 - Full-stack observability and application monitoring with AWS\n      \n        AWS re:Invent 2022 - AWS optimization: Actionable steps for immediate results\n        \n      \n        \n          AWS re:Invent 2022 - The\n          Amazon Builders’ Library: 25 years of Amazon operational\n          excellence\n        \n      \n        AWS re:Invent 2022 - How Amazon uses better metrics for improved website performance\n        \n      \n        \n          Visual\n          Monitoring of Applications with Amazon CloudWatch\n          Synthetics\n        \n      \n    \n      Related examples:\n    \n    \n       \n       \n       \n       \n    \n        \n          Measure\n          page load time with Amazon CloudWatch Synthetics\n        \n      \n        \n          Amazon CloudWatch RUM Web Client\n        \n      \n        \n          X-Ray\n          SDK for Python\n        \n      \n        \n          Distributed\n          Load Testing on AWS\n        \n      \n  \u003cdiv\u003e\u003cdiv\u003e\u003cdiv\u003e\u003cdiv id=\"js_error_message\"\u003e\u003cp\u003e\u003cimg src=\"https://d1ge0kk1l5kms0.cloudfront.net/images/G/01/webservices/console/warning.png\" alt=\"Warning\" /\u003e \u003cstrong\u003eJavascript is disabled or is unavailable in your browser.\u003c/strong\u003e\u003c/p\u003e\u003cp\u003eTo use the Amazon Web Services Documentation, Javascript must be enabled. Please refer to your browser's Help pages for instructions.\u003c/p\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003eDocument ConventionsPERF05-BP01 Establish key performance indicators (KPIs) to\n  measure workload health and performancePERF05-BP03 Define a process to improve workload\n  performance",
  "PERF05-BP03 Define a process to improve workload\n  performance\n    Define a process to evaluate new services, design patterns, resource\n    types, and configurations as they become available. For example, run\n    existing performance tests on new instance offerings to determine\n    their potential to improve your workload.\n  \n    Common anti-patterns:\n  \n     \n     \n  \n      \n        You assume your current architecture is static and won’t be\n        updated over time.\n      \n    \n      \n        You introduce architecture changes over time with no metric\n        justification.\n      \n    \n    Benefits of establishing this best\n    practice: By defining your process for making\n    architectural changes, you can use gathered data to influence your\n    workload design over time.\n  \n    Level of risk exposed if this best practice\n    is not established: Medium\n  \n\n  Implementation guidance\n    \n      Your workload's performance has a few key constraints. Document\n      these so that you know what kinds of innovation might improve the\n      performance of your workload. Use this information when learning\n      about new services or technology as it becomes available to\n      identify ways to alleviate constraints or bottlenecks.\n    \n    \n      Identify the key performance constraints for your workload.\n      Document your workload’s performance constraints so that you know\n      what kinds of innovation might improve the performance of your\n      workload.\n    \n     \n\n  Implementation steps\n      \n         \n         \n         \n         \n         \n         \n      \n          Identify KPIs:\n            Identify your workload performance KPIs as outlined in PERF05-BP01 Establish key performance indicators (KPIs) to\n  measure workload health and performance to\n            baseline your workload.\n          \n        \n          Implement monitoring:\n            Use\n            AWS             observability tools to collect performance metrics\n            and measure KPIs.\n          \n        \n          Conduct analysis:\n            Conduct in-depth analysis to identify the areas (like\n            configuration and application code) in your workload that is\n            under-performing as outlined in\n            PERF05-BP02 Use monitoring solutions to understand the areas\n  where performance is most critical. Use your analysis and performance tools to identify the performance improvement strategies.\n          \n        \n          Validate improvements:\n            Use sandbox or pre-production environments to validate the\n            effectiveness of improvement strategies.\n          \n        \n          Implement changes:\n            Implement the changes in production and continually monitor\n            the workload’s performance. Document the improvements, and communicate the changes to stakeholders.\n          \n        \n          Revisit and refine: Regularly review your performance improvement process to identify areas for enhancement.\n          \n        \n     \n   \n\n  Resources\n\n      \n    \n      Related documents:\n    \n    \n       \n       \n       \n    \n        \n          AWS           Blog\n        \n      \n        \n          What's\n          New with AWS\n        \n      \n        \n          AWS Skill Builder\n        \n      \n    \n      Related videos:\n    \n    \n       \n       \n       \n       \n    \n        AWS re:Invent 2022 - Delivering sustainable, high-performing architectures\n        \n      \n        AWS re:Invent 2023 - Optimize cost and performance and track progress toward mitigation\n        \n      \n        AWS re:Invent 2022 - AWS optimization: Actionable steps for immediate results\n        \n      \n        AWS re:Invent 2022 - Optimize your AWS workloads with best-practice guidance\n        \n      \n    \n      Related examples:\n    \n    \n       \n    \n        \n          AWS Github\n        \n      \n  \u003cdiv\u003e\u003cdiv\u003e\u003cdiv\u003e\u003cdiv id=\"js_error_message\"\u003e\u003cp\u003e\u003cimg src=\"https://d1ge0kk1l5kms0.cloudfront.net/images/G/01/webservices/console/warning.png\" alt=\"Warning\" /\u003e \u003cstrong\u003eJavascript is disabled or is unavailable in your browser.\u003c/strong\u003e\u003c/p\u003e\u003cp\u003eTo use the Amazon Web Services Documentation, Javascript must be enabled. Please refer to your browser's Help pages for instructions.\u003c/p\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003eDocument ConventionsPERF05-BP02 Use monitoring solutions to understand the areas\n  where performance is most criticalPERF05-BP04 Load test your workload",
  "PERF05-BP04 Load test your workload\n    Load test your workload to verify it can handle production load and\n    identify any performance bottleneck.\n  \n    Common anti-patterns:\n  \n     \n     \n     \n     \n  \n      \n        You load test individual parts of your workload but not your\n        entire workload.\n      \n    \n      \n        You load test on infrastructure that is not the same as your\n        production environment.\n      \n    \n      \n        You only conduct load testing to your expected load and not\n        beyond, to help foresee where you may have future problems.\n      \n    \n      \n        You perform load testing without consulting the Amazon EC2 Testing Policy and submitting a Simulated Event Submissions Form. This results in your test failing to run, as it looks like a denial-of-service event.\n      \n    \n    Benefits of establishing this best\n    practice: Measuring your performance under a load test\n    will show you where you will be impacted as load increases. This can\n    provide you with the capability of anticipating needed changes\n    before they impact your workload.\n  \n    Level of risk exposed if this best practice\n    is not established: Low\n  \n\n  Implementation guidance\n\n      \n    \n      Load testing in the cloud is a process to measure the performance\n      of cloud workload under realistic conditions with expected user\n      load. This process involves provisioning a production-like cloud\n      environment, using load testing tools to generate load, and\n      analyzing metrics to assess the ability of your workload handling\n      a realistic load. Load tests must be run using synthetic or\n      sanitized versions of production data (remove sensitive or\n      identifying information). Automatically carry out load tests as\n      part of your delivery pipeline, and compare the results against\n      pre-defined KPIs and thresholds. This process helps you continue\n      to achieve required performance.\n    \n     \n\n  Implementation steps\n\n      \n      \n         \n         \n         \n         \n         \n         \n         \n         \n         \n      \n          Define your testing objectives: Identify the performance aspects of your workload that you want to evaluate, such as throughput and response time.\n            \n          \n        \n          Select a testing tool: Choose and configure the load testing tool that suits your workload.\n            \n          \n        \n          Set up your environment:\n            Set up the test environment based on your production\n            environment. You can use AWS services to run\n            production-scale environments to test your architecture.\n          \n        \n          Implement monitoring: Use monitoring tools such as Amazon CloudWatch to collect metrics across the resources in your architecture. You can also collect and publish custom metrics. \n            \n          \n        \n          Define scenarios:\n            Define the load testing scenarios and parameters (like test\n            duration and number of users).\n          \n        \n          Conduct load testing:\n            Perform test scenarios at scale. Take advantage of the AWS Cloud to test your workload to discover where it fails to\n            scale, or if it scales in a non-linear way. For example, use\n            Spot Instances to generate loads at low cost and discover\n            bottlenecks before they are experienced in production.\n          \n        \n          Analyze test results:\n            Analyze the results to identify performance bottlenecks and\n            areas for improvements.\n          \n        \n          Document and share findings:\n            Document and report on findings and recommendations. Share this information with stakeholders to help them make informed decision regarding performance optimization strategies.\n          \n        \n          Continually iterate: Load testing should be performed at regular cadence, especially after a system change of update.\n            \n          \n        \n     \n   \n\n  Resources\n\n      \n    \n      Related documents:\n    \n    \n       \n       \n       \n    \n        \n          Amazon CloudWatch RUM\n        \n      \n        \n          Amazon CloudWatch Synthetics\n        \n      \n        \n          Distributed\n          Load Testing on AWS\n        \n      \n    \n      Related videos:\n    \n    \n       \n       \n       \n    \n        AWS Summit ANZ 2023: Accelerate with confidence through AWS Distributed Load Testing\n        \n      \n        AWS re:Invent 2022 - Scaling on AWS for your first 10 million users\n        \n      \n        \n          Solving\n          with AWS Solutions: Distributed Load Testing\n        \n      \n    \n       \n       \n    \n        AWS re:Invent 2021 - Optimize applications through end user insights with Amazon CloudWatch RUM\n        \n      \n        \n          Demo\n          of Amazon CloudWatch Synthetics\n        \n      \n    \n      Related examples:\n    \n    \n       \n    \n        \n          Distributed\n          Load Testing on AWS\n        \n      \n  \u003cdiv\u003e\u003cdiv\u003e\u003cdiv\u003e\u003cdiv id=\"js_error_message\"\u003e\u003cp\u003e\u003cimg src=\"https://d1ge0kk1l5kms0.cloudfront.net/images/G/01/webservices/console/warning.png\" alt=\"Warning\" /\u003e \u003cstrong\u003eJavascript is disabled or is unavailable in your browser.\u003c/strong\u003e\u003c/p\u003e\u003cp\u003eTo use the Amazon Web Services Documentation, Javascript must be enabled. Please refer to your browser's Help pages for instructions.\u003c/p\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003eDocument ConventionsPERF05-BP03 Define a process to improve workload\n  performancePERF05-BP05 Use automation to proactively remediate\n  performance-related issues",
  "PERF05-BP05 Use automation to proactively remediate\n  performance-related issues\n    Use key performance indicators (KPIs), combined with monitoring and\n    alerting systems, to proactively address performance-related issues.\n  \n    Common anti-patterns:\n  \n     \n     \n  \n      \n        You only allow operations staff the ability to make operational\n        changes to the workload.\n      \n    \n      \n        You let all alarms filter to the operations team with no\n        proactive remediation.\n      \n    \n    Benefits of establishing this best\n    practice: Proactive remediation of alarm actions allows\n    support staff to concentrate on those items that are not\n    automatically actionable. This helps operations staff handle all\n    alarms without being overwhelmed and instead focus only on critical\n    alarms.\n  \n    Level of risk exposed if this best practice\n    is not established: Low\n  \n\n  Implementation guidance\n\n      \n    \n      Use alarms to trigger automated actions to remediate issues where\n      possible. Escalate the alarm to those able to respond if automated\n      response is not possible. For example, you may have a system that\n      can predict expected key performance indicator (KPI) values and\n      alarm when they breach certain thresholds, or a tool that can\n      automatically halt or roll back deployments if KPIs are outside of\n      expected values.\n    \n    \n      Implement processes that provide visibility into performance as\n      your workload is running. Build monitoring dashboards and\n      establish baseline norms for performance expectations to determine\n      if the workload is performing optimally.\n    \n     \n\n  Implementation steps\n\n      \n      \n         \n         \n         \n         \n         \n         \n         \n         \n      \n          Identify remediation workflow:\n            Identify and understand the performance issue that can be\n            remediated automatically. Use AWS monitoring solutions such\n            as\n            Amazon CloudWatch or AWS X-Ray to help you better understand\n            the root cause of the issue.\n          \n        \n          Define the automation process:\n            Create a step-by-step remediation process that can\n            be used to automatically fix the issue.\n          \n        \n          Configure the initiation event:\n            Configure the event to automatically initiate the\n            remediation process. For example, you can define a trigger\n            to automatically restart an instance when it reaches a\n            certain threshold of CPU utilization.\n          \n        \n          Automate the remediation:\n            Use AWS services and technologies to automate the\n            remediation process. For example,\n            AWS Systems Manager Automation provides a secure and\n            scalable way to automate the remediation process. Make sure to use self-healing logic to revert changes if they do not successfully resolve the issue.\n          \n        \n          Test the workflow\n            Test the automated remediation process in a pre-production\n            environment.\n          \n        \n          Implement the workflow: Implement the automated remediation in the production environment.\n          \n        \n          Develop a playbook: Develop and document a playbook that outlines the steps for the remediation plan, including the initiation events, remediation logic, and actions taken. Make sure to train stakeholders to help them effectively respond to automated remediation events.\n          \n        \n          Review and refine: Regularly assess the effectiveness of the automated remediation workflow. Adjust initiation events and remediation logic if necessary.\n          \n        \n     \n   \n\n  Resources\n\n      \n    \n      Related documents:\n    \n    \n       \n       \n       \n       \n       \n       \n    \n        \n          CloudWatch\n          Documentation\n        \n      \n        \n          Monitoring,\n          Logging, and Performance AWS Partner Network Partners\n        \n      \n        \n          X-Ray\n          Documentation\n        \n      \n        \n          Using\n          Alarms and Alarm Actions in CloudWatch\n        \n      \n        \n          Build a Cloud Automation Practice for Operational Excellence: Best Practices from AWS Managed Services\n      \n        \n          Automate your Amazon Redshift performance tuning with automatic table optimization\n        \n      \n    \n      Related videos:\n    \n    \n       \n       \n       \n       \n       \n       \n       \n       \n       \n       \n    \n        AWS re:Invent 2023 - Strategies for automated scaling, remediation, and smart self-healing\n        \n      \n        AWS re:Invent 2023 - [LAUNCH] Application monitoring for modern workloads\n        \n      \n        AWS re:Invent 2023 - Implementing application observability\n        \n      \n        \n          AWS re:Invent 2021 - Intelligently\n          automating cloud operations\n        \n      \n        \n          AWS re:Invent 2022 - Setting\n          up controls at scale in your AWS environment\n        \n      \n        \n          AWS re:Invent 2022 - Automating\n          patch management and compliance using AWS\n        \n      \n        \n          AWS re:Invent 2022 - How\n          Amazon uses better metrics for improved website\n          performance\n        \n      \n        AWS re:Invent 2023 - Take a load off: Diagnose \u0026 resolve performance issues with Amazon RDS\n        \n      \n        AWS re:Invent 2021 -{New Launch} Automatically detect and resolve issues with Amazon DevOps Guru\n        \n      \n        AWS re:Invent 2023 - Centralize your operations\n        \n      \n    \n      Related examples:\n    \n    \n       \n    \n        \n          CloudWatch Logs Customize Alarms\n        \n      \n  \u003cdiv\u003e\u003cdiv\u003e\u003cdiv\u003e\u003cdiv id=\"js_error_message\"\u003e\u003cp\u003e\u003cimg src=\"https://d1ge0kk1l5kms0.cloudfront.net/images/G/01/webservices/console/warning.png\" alt=\"Warning\" /\u003e \u003cstrong\u003eJavascript is disabled or is unavailable in your browser.\u003c/strong\u003e\u003c/p\u003e\u003cp\u003eTo use the Amazon Web Services Documentation, Javascript must be enabled. Please refer to your browser's Help pages for instructions.\u003c/p\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003eDocument ConventionsPERF05-BP04 Load test your workloadPERF05-BP06 Keep your workload and services up-to-date",
  "PERF05-BP06 Keep your workload and services up-to-date\n    Stay up-to-date on new cloud services and features to adopt\n    efficient features, remove issues, and improve the overall\n    performance efficiency of your workload.\n  \n    Common anti-patterns:\n  \n     \n     \n  \n      \n        You assume your current architecture is static and will not be\n        updated over time.\n      \n    \n      \n        You do not have any systems or a regular cadence to evaluate if\n        updated software and packages are compatible with your workload.\n      \n    \n    Benefits of establishing this best\n    practice: By establishing a process to stay up-to-date on\n    new services and offerings, you can adopt new features and\n    capabilities, resolve issues, and improve workload performance.\n  \n    Level of risk exposed if this best practice\n    is not established: Low\n  \n\n  Implementation guidance\n\n      \n    \n      Evaluate ways to improve performance as new services, design\n      patterns, and product features become available. Determine which\n      of these could improve performance or increase the efficiency of\n      the workload through evaluation, internal discussion, or external\n      analysis. Define a process to evaluate updates, new features, and\n      services relevant to your workload. For example, build a proof of\n      concept that uses new technologies or consult with an internal\n      group. When trying new ideas or services, run performance tests to\n      measure the impact that they have on the performance of the\n      workload.\n    \n   \n    \n    Implementation steps\n\n    \n       \n       \n       \n       \n       \n       \n    \n        Inventory your workload:\n          Inventory your workload software and architecture and identify\n          components that need to be updated.\n        \n      \n        Identify update sources:\n          Identify news and update sources related to your workload\n          components. As an example, you can subscribe to\n          the What’s New\n          at AWS blog for the products that match your workload\n          component. You can subscribe to the RSS feed or manage\n          your email\n          subscriptions.\n        \n      \n        Define an update schedule:\n          Define a schedule to evaluate new services and features for\n          your workload.\n        \n        \n           \n        \n            \n              You can\n              use AWS Systems Manager Inventory to collect operating\n              system (OS), application, and instance metadata from your\n              Amazon EC2 instances and quickly understand which\n              instances are running the software and configurations\n              required by your software policy and which instances need\n              to be updated.\n            \n          \n      \n        Assess the new update:\n          Understand how to update the components of your workload. Take\n          advantage of agility in the cloud to quickly test how new\n          features can improve your workload to gain performance\n          efficiency.\n        \n      \n        Use automation:\n          Use automation for the update process to reduce the level of\n          effort to deploy new features and limit errors caused by\n          manual processes.\n        \n        \n           \n           \n        \n            \n              You can\n              use CI/CD to\n              automatically update AMIs, container images, and other\n              artifacts related to your cloud application.\n            \n          \n            \n              You can use tools such\n              as AWS Systems Manager Patch Manager to automate the\n              process of system updates, and schedule the activity\n              using AWS Systems Manager Maintenance Windows.\n            \n          \n      \n        Document the process:\n          Document your process for evaluating updates and new services.\n          Provide your owners the time and space needed to research,\n          test, experiment, and validate updates and new services. Refer\n          back to the documented business requirements and KPIs to help\n          prioritize which update will make a positive business impact.\n        \n      \n   \n\n  Resources\n\n      \n    \n      Related documents:\n    \n    \n       \n       \n       \n    \n        \n          AWS           Blog\n        \n      \n        \n          What's\n          New with AWS\n        \n      \n        \n          Implementing up-to-date images with automated EC2 Image Builder pipelines\n        \n      \n    \n      Related videos:\n    \n    \n       \n       \n    \n        AWS re:Inforce 2022 - Automating patch management and compliance using AWS\n      \n        \n          All Things Patch: AWS Systems Manager | AWS Events\n        \n      \n    \n      Related examples:\n    \n    \n       \n       \n    \n        \n          Inventory and Patch Management\n        \n      \n        \n          One Observability Workshop\n        \n      \n  \u003cdiv\u003e\u003cdiv\u003e\u003cdiv\u003e\u003cdiv id=\"js_error_message\"\u003e\u003cp\u003e\u003cimg src=\"https://d1ge0kk1l5kms0.cloudfront.net/images/G/01/webservices/console/warning.png\" alt=\"Warning\" /\u003e \u003cstrong\u003eJavascript is disabled or is unavailable in your browser.\u003c/strong\u003e\u003c/p\u003e\u003cp\u003eTo use the Amazon Web Services Documentation, Javascript must be enabled. Please refer to your browser's Help pages for instructions.\u003c/p\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003eDocument ConventionsPERF05-BP05 Use automation to proactively remediate\n  performance-related issuesPERF05-BP07 Review metrics at regular intervals",
  "PERF05-BP07 Review metrics at regular intervals\n    As part of routine maintenance or in response to events or\n    incidents, review which metrics are collected. Use these reviews to\n    identify which metrics were essential in addressing issues and which\n    additional metrics, if they were being tracked, could help identify,\n    address, or prevent issues.\n  \n    Common anti-patterns:\n  \n     \n     \n  \n      \n        You allow metrics to stay in an alarm state for an extended\n        period of time.\n      \n    \n      \n        You create alarms that are not actionable by an automation\n        system.\n      \n    \n    Benefits of establishing this best\n    practice: Continually review metrics that are being\n    collected to verify that they properly identify, address, or prevent\n    issues. Metrics can also become stale if you let them stay in an\n    alarm state for an extended period of time.\n  \n    Level of risk exposed if this best practice\n    is not established: Medium\n  \n\n  Implementation guidance\n\n      \n    \n      Constantly improve metric collection and monitoring. As part of\n      responding to incidents or events, evaluate which metrics were\n      helpful in addressing the issue and which metrics could have\n      helped that are not currently being tracked. Use this method to\n      improve the quality of metrics you collect so that you can prevent,\n      or more quickly resolve future incidents.\n    \n    \n      As part of responding to incidents or events, evaluate which\n      metrics were helpful in addressing the issue and which metrics\n      could have helped that are not currently being tracked. Use this\n      to improve the quality of metrics you collect so that you can\n      prevent or more quickly resolve future incidents.\n    \n     \n      Implementation steps\n      \n         \n         \n         \n         \n         \n         \n         \n      \n          Define metrics: Define critical performance metrics to monitor that are aligned to your workload\n            objective, including metrics such as response time and resource utilization.\n        \n          Establish baselines: Set a baseline and desirable value for each metric. The baseline should provide reference points to identify deviation or anomalies.\n        \n          Set up a cadence: Set a cadence (like weekly or monthly) to review critical metrics. \n        \n          Identify performance issues: During each review, assess trends and deviation from the baseline values. Look for\n            any performance bottlenecks or anomalies. For identified issues, conduct in-depth root cause analysis to understand the main\n            reason behind the issue.\n        \n          Identify corrective actions: Use your analysis to identify corrective actions. This may include parameter tuning, fixing bugs, and scaling resources.\n        \n          Document findings: Document your findings, including identified issues, root causes, and corrective actions.\n        \n          Iterate and improve: Continually assess and improve the metrics review process. Use the lesson learned from previous review to enhance the process over time.\n        \n     \n   \n\n  Resources\n\n      \n    \n      Related documents:\n    \n    \n       \n       \n       \n       \n       \n    \n        \n          CloudWatch\n          Documentation\n        \n      \n        \n          Collect\n          metrics and logs from Amazon EC2 Instances and on-premises\n          servers with the CloudWatch Agent\n        \n      \n        \n          Query your metrics with CloudWatch Metrics Insights\n        \n      \n        \n          Monitoring,\n          Logging, and Performance AWS Partner Network Partners\n        \n      \n        \n          X-Ray\n          Documentation\n        \n      \n    \n      Related videos:\n    \n    \n       \n       \n       \n       \n       \n    \n        \n          AWS re:Invent 2022 - Setting\n          up controls at scale in your AWS environment\n        \n      \n        \n          AWS re:Invent 2022 - How\n          Amazon uses better metrics for improved website\n          performance\n        \n      \n        AWS re:Invent 2023 - Building an effective observability strategy\n        \n      \n        AWS Summit SF 2022 - Full-stack observability and application monitoring with AWS\n      \n        AWS re:Invent 2023 - Take a load off: Diagnose \u0026 resolve performance issues with Amazon RDS\n        \n      \n    \n      Related examples:\n    \n    \n       \n       \n    \n        \n          Creating\n          a dashboard with QuickSight\n        \n      \n        \n          CloudWatch Dashboards\n        \n      \n  \u003cdiv\u003e\u003cdiv\u003e\u003cdiv\u003e\u003cdiv id=\"js_error_message\"\u003e\u003cp\u003e\u003cimg src=\"https://d1ge0kk1l5kms0.cloudfront.net/images/G/01/webservices/console/warning.png\" alt=\"Warning\" /\u003e \u003cstrong\u003eJavascript is disabled or is unavailable in your browser.\u003c/strong\u003e\u003c/p\u003e\u003cp\u003eTo use the Amazon Web Services Documentation, Javascript must be enabled. Please refer to your browser's Help pages for instructions.\u003c/p\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003eDocument ConventionsPERF05-BP06 Keep your workload and services up-to-dateCost optimization",
  "COST01-BP01 Establish ownership of cost optimization\n    Create a team (Cloud Business Office, Cloud Center of Excellence, or FinOps team) that is responsible for establishing and maintaining cost awareness across your organization. The owner of cost optimization can be an individual or a team (requires people from finance, technology, and business teams) that understands the entire organization and cloud finance.\n  \n    Level of risk exposed if this best practice\n    is not established: High\n  \n\n  Implementation guidance\n    \n      This is the introduction of a Cloud Business Office (CBO) or Cloud Center of Excellence (CCOE) function or team that is responsible for establishing and maintaining a culture of cost awareness in cloud computing. This function can be an existing individual, a team within your organization, or a new team of key finance, technology, and organization stakeholders from across the organization.\n    \n    \n      The function (individual or team) prioritizes and spends the required percentage of their\n      time on cost management and cost optimization activities. For a small organization, the\n      function might spend a smaller percentage of time compared to a full-time function for a\n      larger enterprise.\n    \n    \n      The function requires a multi-disciplinary approach, with capabilities in project management, data science, financial analysis, and software or infrastructure development. They can improve workload efficiency by running cost optimizations within three different ownerships:\n    \n    \n       \n       \n       \n    \n        \n          Centralized: Through designated teams such as FinOps team, Cloud Financial Management (CFM) team, Cloud Business Office (CBO), or Cloud Center of Excellence (CCoE), customers can design and implement governance mechanisms and drive best practices company-wide.\n        \n      \n        \n          Decentralized: Influencing technology teams to run cost optimizations.\n        \n      \n        \n          Hybrid: Combination of both centralized and decentralized teams can work together to run cost optimizations.\n        \n      \n    \n      The function may be measured against their ability to run and deliver against cost optimization goals (for example, workload efficiency metrics). \n    \n    \n      You must secure executive sponsorship for this function, which is a key success factor. The sponsor is regarded as a champion for cost efficient cloud consumption, and provides escalation support for the team to ensure that cost optimization activities are treated with the level of priority defined by the organization. Otherwise, guidance can be ignored and cost saving opportunities will not be prioritized. Together, the sponsor and team help your organization consume the cloud efficiently and deliver business value.\n    \n    \n      If you have the Business, Enterprise-On-Ramp or Enterprise support plan and need help building this team or function, reach out to your Cloud Financial Management (CFM) experts through your account team.\n    \n     \n      \n      Implementation steps\n\n    \n       \n       \n       \n    \n        \n          Define key members: All relevant parts of your organization must contribute and be interested in cost management. Common teams within organizations typically include: finance, application or product owners, management, and technical teams (DevOps). Some are engaged full time (finance or technical), while others are engaged periodically as required.  Individuals or teams performing CFM need the following set of skills:\n        \n        \n           \n           \n           \n        \n            \n                Software development: in the case where scripts and\n                automation are being built out. \n          \n            \n                Infrastructure engineering: to deploy scripts,\n                automate processes, and understand how services or resources are provisioned. \n          \n            \n                Operations acumen: CFM is about operating on the\n                cloud efficiently by measuring, monitoring, modifying, planning, and scaling\n                efficient use of the cloud. \n          \n      \n        \n          Define goals and metrics: The function needs to deliver value to the organization in different ways. These goals are defined and continually evolve as the organization evolves. Common activities include: creating and running education programs on cost optimization across the organization, developing organization-wide standards, such as monitoring and reporting for cost optimization, and setting workload goals on optimization. This function also needs to regularly report to the organization on their cost optimization capability.\n        \n        \n          You can define value- or cost-based key performance indicators (KPIs). When you define the KPIs, you can calculate expected cost in terms of efficiency and expected business outcome. Value-based KPIs tie cost and usage metrics to business value drivers and help rationalize changes in AWS spend.  The first step to deriving value-based KPIs is working together, cross-organizationally, to select and agree upon a standard set of KPIs.\n        \n      \n        \n          Establish regular cadence: The group (finance, technology and business teams) should come together regularly to review their goals and metrics. A typical cadence involves reviewing the state of the organization, reviewing any programs currently running, and reviewing overall financial and optimization metrics. Afterwards, key workloads are reported on in greater detail.\n        \n        \n          During these regular reviews, you can review workload efficiency (cost) and business outcome. For example, a 20% cost increase for a workload may align with increased customer usage. In this case, this 20% cost increase can be interpreted as an investment. These regular cadence calls can help teams to identify value KPIs that provide meaning to the entire organization. \n        \n      \n     \n   \n\n  Resources\n\n      \n    \n      Related documents:\n    \n    \n       \n       \n       \n    \n        \n          AWS CCOE Blog\n        \n      \n        \n          Creating Cloud Business Office\n        \n      \n        \n          CCOE - Cloud Center of Excellence\n        \n      \n    \n      Related videos:\n    \n    \n       \n    \n        Vanguard CCOE Success\n            Story\n      \n    \n      Related examples:\n    \n    \n       \n       \n       \n    \n        Using a Cloud Center of Excellence (CCOE) to Transform the Entire\n          Enterprise\n      \n        Building a CCOE to transform the entire enterprise\n      \n        7 Pitfalls to Avoid When Building CCOE\n      \n  \u003cdiv\u003e\u003cdiv\u003e\u003cdiv\u003e\u003cdiv id=\"js_error_message\"\u003e\u003cp\u003e\u003cimg src=\"https://d1ge0kk1l5kms0.cloudfront.net/images/G/01/webservices/console/warning.png\" alt=\"Warning\" /\u003e \u003cstrong\u003eJavascript is disabled or is unavailable in your browser.\u003c/strong\u003e\u003c/p\u003e\u003cp\u003eTo use the Amazon Web Services Documentation, Javascript must be enabled. Please refer to your browser's Help pages for instructions.\u003c/p\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003eDocument ConventionsCOST 1. How do you implement cloud financial management?COST01-BP02 Establish a partnership between finance and\n  technology",
  "COST01-BP02 Establish a partnership between finance and\n  technologyInvolve finance and technology teams in cost and usage discussions at all stages of your\n    cloud journey. Teams regularly meet and discuss topics such as organizational goals and targets,\n    current state of cost and usage, and financial and accounting practices. \n    Level of risk exposed if this best practice\n    is not established: High\n  \n\n  Implementation guidance\n    Technology teams innovate faster in the cloud due to shortened approval, procurement, and\n      infrastructure deployment cycles. This can be an adjustment for finance organizations\n      previously used to running time-consuming and resource-intensive processes for procuring and\n      deploying capital in data center and on-premises environments, and cost allocation only at\n      project approval. \n    From a finance and procurement organization perspective, the process for capital\n      budgeting, capital requests, approvals, procurement, and installing physical infrastructure is\n      one that has been learned and standardized over decades:\n    \n       \n       \n       \n    Engineering or IT teams are typically the requestersVarious finance teams act as approvers and procurersOperations teams rack, stack, and hand off ready-to-use infrastructure\n    \n    \n       \n        \n       \n    \n    With the adoption of cloud, infrastructure procurement and consumption are no longer\n      beholden to a chain of dependencies. In the cloud model, technology and product teams are no\n      longer just builders, but operators and owners of their products, responsible for most of the\n      activities historically associated with finance and operations teams, including procurement\n      and deployment.\n    All it really takes to provision cloud resources is an account, and the right set of\n      permissions. This is also what reduces IT and finance risk; which means teams are always a\n      just few clicks or API calls away from terminating idle or unnecessary cloud resources. This\n      is also what allows technology teams to innovate faster – the agility and ability to spin up\n      and then tear down experiments. While the variable nature of cloud consumption may impact\n      predictability from a capital budgeting and forecasting perspective, cloud provides\n      organizations with the ability to reduce the cost of over-provisioning, as well as reduce the\n      opportunity cost associated with conservative under-provisioning.\n  \n    \n       \n        \n       \n    \n    \n    Establish a partnership between key finance and technology stakeholders to create a shared understanding of organizational goals and develop mechanisms to succeed financially in the variable spend model of cloud computing. Relevant teams within your organization must be involved in cost and usage discussions at all stages of your cloud journey, including:   \n    \n       \n       \n    \n         Financial leads: CFOs, financial controllers,\n          financial planners, business analysts, procurement, sourcing, and accounts payable must\n          understand the cloud model of consumption, purchasing options, and the monthly invoicing\n          process. Finance needs to partner with technology teams to create and socialize an IT\n          value story, helping business teams understand how technology spend is linked to business\n          outcomes. This way, technology expenditures are viewed not as costs, but rather as\n          investments. Due to the fundamental differences between the cloud (such as the rate of\n          change in usage, pay as you go pricing, tiered pricing, pricing models, and detailed\n          billing and usage information) compared to on-premises operation, it is essential that the\n          finance organization understands how cloud usage can impact business aspects including\n          procurement processes, incentive tracking, cost allocation and financial\n          statements.\n      \n        \n          Technology leads: Technology leads (including product and\n          application owners) must be aware of the financial requirements (for example, budget\n          constraints) as well as business requirements (for example, service level agreements).\n          This allows the workload to be implemented to achieve the desired goals of the\n          organization. \n      \n    The partnership of finance and technology provides the following benefits: \n    \n       \n       \n       \n       \n       \n       \n    Finance and technology teams have near real-time visibility into cost and usage. Finance and technology teams establish a standard operating procedure to handle cloud spend variance. Finance stakeholders act as strategic advisors with respect to how capital is used to purchase commitment discounts (for example, Reserved Instances or AWS Savings Plans), and how the cloud is used to grow the organization.  Existing accounts payable and procurement processes are used with the cloud. Finance and technology teams collaborate on forecasting future AWS cost and usage to align and build organizational budgets. Better cross-organizational communication through a shared language, and common understanding of financial concepts. \n    \n    Additional stakeholders within your organization that should be involved in cost and usage discussions include: \n    \n       \n       \n       \n    Business unit owners: Business unit owners must understand\n          the cloud business model so that they can provide direction to both the business units and\n          the entire company. This cloud knowledge is critical when there is a need to forecast\n          growth and workload usage, and when assessing longer-term purchasing options, such as\n          Reserved Instances or Savings Plans. Engineering team: Establishing a partnership between finance\n          and technology teams is essential for building a cost-aware culture that encourages\n          engineers to take action on Cloud Financial Management (CFM). One of the common problems\n          of CFM or finance operations practitioners and finance teams is getting engineers to\n          understand the whole business on cloud, follow best practices, and take recommended\n          actions.Third parties: If your organization uses third parties (for\n          example, consultants or tools), ensure that they are aligned to your financial goals and\n          can demonstrate both alignment through their engagement models and a return on investment\n          (ROI). Typically, third parties will contribute to reporting and analysis of any workloads\n          that they manage, and they will provide cost analysis of any workloads that they\n          design. \n    \n    Implementing CFM and achieving success requires collaboration across finance, technology,\n      and business teams, and a shift in how cloud spend is communicated and evaluated across the\n      organization. Include engineering teams so that they can be part of these cost and usage\n      discussions at all stages, and encourage them to follow best practices and take agreed-upon\n      actions accordingly.\n    \n    Implementation steps\n    \n       \n       \n       \n    Define key members: Verify that all relevant members of your\n          finance and technology teams participate in the partnership. Relevant finance members will\n          be those having interaction with the cloud bill. This will typically be CFOs, financial\n          controllers, financial planners, business analysts, procurement, and sourcing. Technology\n          members will typically be product and application owners, technical managers and\n          representatives from all teams that build on the cloud. Other members may include business\n          unit owners, such as marketing, that will influence usage of products, and third parties\n          such as consultants, to achieve alignment to your goals and mechanisms, and to assist with\n          reporting.Define topics for discussion: Define the topics that are\n          common across the teams, or will need a shared understanding. Follow cost from that time\n          it is created, until the bill is paid. Note any members involved, and organizational\n          processes that are required to be applied. Understand each step or process it goes through\n          and the associated information, such as pricing models available, tiered pricing, discount\n          models, budgeting, and financial requirements.Establish regular cadence: To create a finance and technology\n          partnership, establish a regular communication cadence to create and maintain alignment.\n          The group needs to come together regularly against their goals and metrics. A typical\n          cadence involves reviewing the state of the organization, reviewing any programs currently\n          running, and reviewing overall financial and optimization metrics. Then key workloads are\n          reported on in greater detail.\n   \n\n  Resources\n\n      \n    \n      Related documents:\n    \n    \n       \n    \n        \n          AWS News Blog\n        \n      \n  \u003cdiv\u003e\u003cdiv\u003e\u003cdiv\u003e\u003cdiv id=\"js_error_message\"\u003e\u003cp\u003e\u003cimg src=\"https://d1ge0kk1l5kms0.cloudfront.net/images/G/01/webservices/console/warning.png\" alt=\"Warning\" /\u003e \u003cstrong\u003eJavascript is disabled or is unavailable in your browser.\u003c/strong\u003e\u003c/p\u003e\u003cp\u003eTo use the Amazon Web Services Documentation, Javascript must be enabled. Please refer to your browser's Help pages for instructions.\u003c/p\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003eDocument ConventionsCOST01-BP01 Establish ownership of cost optimizationCOST01-BP03 Establish cloud budgets and forecasts",
  "COST01-BP03 Establish cloud budgets and forecasts\n    Adjust existing organizational budgeting and forecasting processes\n    to be compatible with the highly variable nature of cloud costs and\n    usage. Processes must be dynamic, using trend-based or business\n    driver-based algorithms or a combination of both.\n  \n    Level of risk exposed if this best practice\n    is not established: High\n  \n\n  Implementation guidance\n\n      \n    \n      In traditional on-premises IT setups, customers often face the\n      challenge of planning for fixed costs that change only\n      occasionally, typically with the purchase of new IT hardware and\n      services to meet peak demand. In contrast, AWS Cloud adopts a\n      different approach, where customers pay for the resources they use\n      as dictated by their actual IT and business needs. In the cloud\n      environment, demand can fluctuate on a monthly, daily, or even\n      hourly basis.\n    \n    \n      Using the cloud brings efficiency, speed, and agility, which\n      results in a highly-variable cost and usage pattern. Costs can\n      decrease or sometimes increase in response to greater workload\n      efficiency or the deployment of new workloads and features. As\n      workloads scale to serve an expanding customer base, cloud usage\n      and costs correspondingly rise due to the increased accessibility\n      of resources. This flexibility in cloud services extends to the\n      costs and forecasts, which creates a degree of elasticity.\n    \n    \n      It's essential to align closely with these shifting business needs\n      and demand drivers, and aim for the most accurate planning\n      possible. Traditional organizational budget processes need to\n      adapt to accommodate this variability.\n    \n    \n      Consider cost modelling while you forecast the cost for new\n      workloads. Cost modelling creates a baseline understanding of\n      expected cloud costs, which helps you perform total cost of\n      ownership (TCO), return on investment (ROI), and other financial\n      analysis, set targets and expectations with stakeholders, and\n      identify opportunities for cost optimization.\n    \n    \n      Your organization should understand the cost definitions and\n      accepted groupings. The level of detail at which you forecast can\n      vary based on your organization's structure and internal\n      workflows. Select a granularity that suits your specific\n      requirements and organizational setup. It is important to\n      understand at what level the forecast is performed:\n    \n    \n       \n       \n       \n       \n       \n       \n    \n        \n          Management account or AWS Organizations\n            level: The management account is the account that\n          you use to create AWS Organizations. Organizations have one\n          management account by default.\n        \n      \n        \n          Linked or\n            member account: An account in\n          Organizations is a standard AWS account that contains your AWS\n          resources and the identities that can access those resources.\n        \n      \n        \n          Environment: An environment\n          is a collection of AWS resources that runs an application\n          version. An environment can be made with multiple linked or\n          member accounts.\n        \n      \n        \n          Project: A project is a\n          combination of set objectives or tasks to be accomplished\n          within a fixed period. It is important to consider the project\n          lifecycle during your forecast.\n        \n      \n        \n          AWS services: Groups or\n          categories such as compute or storage services where you can\n          group AWS services for your forecast.\n        \n      \n        \n          Custom grouping: You can\n          create custom groups based on your organization's needs, such\n          as business units, cost centers, teams, cost allocation tags,\n          cost categories, linked accounts, or combination of these.\n        \n      \n    \n      Identify the business drivers that can impact your usage cost, and\n      forecast for each of them separately to calculate expected usage\n      in advance. Some of the drivers might be linked to IT and product\n      teams within the organization. Other business drivers, such as\n      marketing events, promotions, geographic expansions, mergers, and\n      acquisitions, are known by your sales, marketing, and business\n      leaders, and it's important to collaborate and account for all\n      those demand drivers as well.\n    \n    \n      You can\n      use AWS Cost Explorer for trend-based forecasting in a defined\n      future time range based on your past spend. AWS Cost Explorer's\n      forecasting engine segments your historical data based on charge\n      types (for example, Reserved Instances) and uses a combination of\n      machine learning and rule-based models to predict spend across all\n      charge types individually.\n    \n    \n      Once you've established your forecast process and built models,\n      you can\n      use AWS Budgets to set custom budgets at a granular level by\n      specifying the time period, recurrence, or amount (fixed or\n      variable) and add filters such as service, AWS Region, and tags.\n      The budget is usually prepared for a single year and remains\n      fixed, which requires strict adherence from everyone involved. In\n      contrast, forecasts are more flexible, which allows for\n      readjustments throughout the year and provides dynamic projections\n      over a period of one, two, or three years. Both budgets and\n      forecasts play a crucial role when you establish financial\n      expectations among various technology and business stakeholders.\n      Accurate forecasts and implementation also provides accountability\n      to stakeholders who are directly responsible for provisioning cost\n      in the first place, and it can also raise their overall cost\n      awareness.\n    \n    \n      To stay informed on the performance of your existing budgets, you\n      can create and schedule AWS Budgets reports to email you and your\n      stakeholders on a regular cadence. You can also create AWS Budgets\n      alerts based on actual costs, which are reactive in nature, or on\n      forecasted costs, which provides time to implement mitigations\n      against potential cost overruns. You can be alerted when your cost\n      or usage actually exceeds a certain level or if they are\n      forecasted to exceed your budgeted amount.\n    \n    \n      Adjust existing budget and forecast processes to be more dynamic\n      using trend-based algorithms (with historical costs as inputs) and\n      driver-based algorithms (for example, new product launches,\n      Regional expansion, or new environments for workloads), which are\n      ideal for a dynamic and variable spending environment. Once you've\n      determined your trend-based forecast using Cost Explorer or any\n      other tools, use\n      the AWS Pricing Calculator to estimate your AWS use case and future costs\n      based on the expected usage (traffic, requests-per-second, or\n      required Amazon EC2 instances).\n    \n    \n      Track the accuracy of that forecast, as budgets should be set\n      based on these forecast calculations and estimations. Monitor the\n      accuracy and effectiveness of the integrated cloud cost forecasts.\n      Regularly review actual spend compared to your forecast, and\n      adjust as needed to improve forecast precision. Track forecast\n      variance, and perform root cause analysis on reported variance to\n      act and adjust forecasts.\n    \n    \n      As mentioned in\n      the COST01-BP02 Establish a partnership between finance and\n  technology, it is important to foster a\n      partnership and cadence between IT, finance, and other\n      stakeholders to verify that they are all using the same tools or\n      processes for consistency. In cases where budgets may need to\n      change, increase cadence touch points to react to those changes\n      more quickly.\n    \n     \n\n  Implementation steps\n\n      \n      \n         \n         \n         \n         \n         \n         \n      \n          \n            Define the cost language within the\n            organization: Create a common AWS cost language\n            within the Organization with multiple dimension and\n            grouping. Make sure stakeholders understand forecast\n            granularity, pricing models, and the level of your cost\n            forecasts.\n          \n        \n          \n            Analyze trend-based\n            forecasts: Use trend-based forecast tools such as\n            AWS Cost Explorer and Amazon Forecast. Analyze your usage\n            cost on multiple dimensions like service, account, tags, and\n            cost categories.\n          \n        \n          \n            Analyze driver-based forecasts: Identify the\n            impact of business drivers on your cloud usage, and forecast\n            for each of them separately to calculate expected usage cost\n            in advance. Work closely with business unit owners and\n            stakeholders to understand the impact on new drivers, and\n            calculate expected cost changes to define accurate budgets.\n          \n        \n          \n            Update existing forecast and budget\n            processes: Based on adopted forecast methods such\n            as trend-based, business driver-based, or a combination of\n            both forecasting methods, define your forecast and budget\n            processes. Budgets should be calculated, realistic, and\n            based on your forecasts.\n          \n        \n          \n            Configure alerts and\n            notifications: Use AWS Budgets alerts and cost\n            anomaly detection to get alerts and notifications.\n          \n        \n          \n            Perform regular reviews with key\n            stakeholders: For example, align on changes in\n            business direction and usage with stakeholders in IT,\n            finance, platform teams, and other areas of the business.\n          \n        \n     \n   \n\n  Resources\n\n      \n    \n      Related documents:\n    \n    \n       \n       \n       \n       \n       \n    \n        \n          AWS Cost Explorer\n        \n      \n        \n          AWS Cost and Usage Report\n        \n      \n        \n          Forecasting\n          with Cost Explorer\n        \n      \n        \n          QuickSight Forecasting\n        \n      \n        \n          AWS Budgets\n        \n      \n    \n      Related videos:\n    \n    \n       \n       \n    \n        \n          How\n          can I use AWS Budgets to track my spending and usage\n        \n      \n        \n          AWS           Cost Optimization Series: AWS Budgets\n        \n      \n    \n      Related examples:\n    \n    \n       \n       \n       \n       \n    \n        \n          Understand\n          and build driver-based forecasting\n        \n      \n        \n          How\n          to establish and drive a forecasting culture\n        \n      \n        \n          How\n          to improve your cloud cost forecasting\n        \n      \n        \n          Using\n          the right tools for your cloud cost forecasting\n        \n      \n  \u003cdiv\u003e\u003cdiv\u003e\u003cdiv\u003e\u003cdiv id=\"js_error_message\"\u003e\u003cp\u003e\u003cimg src=\"https://d1ge0kk1l5kms0.cloudfront.net/images/G/01/webservices/console/warning.png\" alt=\"Warning\" /\u003e \u003cstrong\u003eJavascript is disabled or is unavailable in your browser.\u003c/strong\u003e\u003c/p\u003e\u003cp\u003eTo use the Amazon Web Services Documentation, Javascript must be enabled. Please refer to your browser's Help pages for instructions.\u003c/p\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003eDocument ConventionsCOST01-BP02 Establish a partnership between finance and\n  technologyCOST01-BP04 Implement cost awareness in your organizational\n  processes",
  "COST01-BP04 Implement cost awareness in your organizational\n  processesImplement cost awareness, create transparency, and accountability of costs into new or\n    existing processes that impact usage, and leverage existing processes for cost awareness.\n    Implement cost awareness into employee training. \n    Level of risk exposed if this best practice\n    is not established: High\n  \n\n  Implementation guidance\n    Cost awareness must be implemented in new and existing organizational processes. It is one\n      of the foundational, prerequisite capabilities for other best practices. It is recommended to\n      reuse and modify existing processes where possible — this minimizes the impact to agility and\n      velocity. Report cloud costs to the technology teams and the decision makers in the business\n      and finance teams to raise cost awareness, and establish efficiency key performance indicators\n      (KPIs) for finance and business stakeholders. The following recommendations will help\n      implement cost awareness in your workload:\n    \n       \n       \n       \n       \n       \n       \n    Verify that change management includes a cost measurement to quantify the financial impact of\n          your changes. This helps proactively address cost-related concerns and highlight cost\n          savings.Verify that cost optimization is a core component of your operating capabilities. For example,\n          you can leverage existing incident management processes to investigate and identify root\n          causes for cost and usage anomalies or cost overruns.Accelerate cost savings and business value realization through automation or tooling. When\n          thinking about the cost of implementing, frame the conversation to include an return on\n          investment (ROI) component to justify the investment of time or money.Allocate cloud costs by implementing showbacks or chargebacks for cloud spend, including spend\n          on commitment-based purchase options, shared services and marketplace purchases to drive\n          most cost-aware cloud consumption.Extend existing training and development programs to include cost-awareness training\n          throughout your organization. It is recommended that this includes continuous training and\n          certification. This will build an organization that is capable of self-managing cost and\n          usage.Take advantage of free AWS native tools such as AWS Cost Anomaly Detection, AWS Budgets, and\n            AWS Budgets\n            Reports.\n    When organizations consistently adopt Cloud Financial Management (CFM)\n      practices, those behaviours become ingrained in the way of working and decision-making. The\n      result is a culture that is more cost-aware, from developers architecting a new\n      born-in-the-cloud application, to finance managers analyzing the ROI on these new cloud\n      investments.\n    Implementation steps\n    \n       \n       \n       \n    \n         Identify relevant organizational processes: Each\n          organizational unit reviews their processes and identifies processes that impact cost and\n          usage. Any processes that result in the creation or termination of a resource need to be\n          included for review. Look for processes that can support cost awareness in your business,\n          such as incident management and training. \n      \n        Establish self-sustaining cost-aware culture: Make\n          sure all the relevant stakeholders align with cause-of-change and impact as a cost so that\n          they understand cloud cost. This will allow your organization to establish a\n          self-sustaining cost-aware culture of innovation.\n      \n         Update processes with cost awareness: Each process\n          is modified to be made cost aware. The process may require additional pre-checks, such as\n          assessing the impact of cost, or post-checks validating that the expected changes in cost\n          and usage occurred. Supporting processes such as training and incident management can be\n          extended to include items for cost and usage. \n      \n    To get help, reach out to CFM experts through your Account team, or explore the resources\n      and related documents below.\n   \n\n  Resources\n\n      \n    \n      Related documents:\n    \n    \n       \n      \n    AWS Cloud Financial Management\n    \n    \n      Related examples:\n    \n    \n       \n       \n       \n    \n        \n          Strategy for Efficient Cloud Cost Management\n        \n      \n        Cost Control Blog Series #3: How to Handle Cost Shock\n      \n        A Beginner’s Guide to AWS Cost Management\n      \n  \u003cdiv\u003e\u003cdiv\u003e\u003cdiv\u003e\u003cdiv id=\"js_error_message\"\u003e\u003cp\u003e\u003cimg src=\"https://d1ge0kk1l5kms0.cloudfront.net/images/G/01/webservices/console/warning.png\" alt=\"Warning\" /\u003e \u003cstrong\u003eJavascript is disabled or is unavailable in your browser.\u003c/strong\u003e\u003c/p\u003e\u003cp\u003eTo use the Amazon Web Services Documentation, Javascript must be enabled. Please refer to your browser's Help pages for instructions.\u003c/p\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003eDocument ConventionsCOST01-BP03 Establish cloud budgets and forecastsCOST01-BP05 Report and notify on cost optimization",
  "COST01-BP05 Report and notify on cost optimization\n    Set up cloud budgets and configure mechanisms to detect anomalies in\n    usage. Configure related tools for cost and usage alerts against\n    pre-defined targets and receive notifications when any usage exceeds\n    those targets. Have regular meetings to analyze the\n    cost-effectiveness of your workloads and promote cost awareness.\n  \n    Level of risk exposed if this best practice\n    is not established: Low\n  \n    \n    Implementation guidance\n    \n    \n    \n      You must regularly report on cost and usage optimization within\n      your organization. You can implement dedicated sessions to discuss\n      cost performance, or include cost optimization in your regular\n      operational reporting cycles for your workloads. Use services and\n      tools to monitor your cost performances regularly and implement\n      cost savings opportunities. \n    \n    \n      View your cost and usage with multiple filters and granularity by\n      using\n      AWS Cost Explorer, which provides dashboards and reports such as\n      costs by service or by account, daily costs, or marketplace costs.\n      Track your progress of cost and usage against configured budgets\n      with AWS Budgets Reports.\n    \n    \n      Use AWS Budgets to set custom budgets to track your costs and usage\n      and respond quickly to alerts received from email or Amazon Simple Notification Service (Amazon SNS) notifications if you exceed your\n      threshold. Set\n        your preferred budget period to daily, monthly, quarterly,\n      or annually, and create specific budget limits to stay informed on\n      how actual or forecasted costs and usage progress toward your\n      budget threshold. You can also set\n      up alerts and actions against\n      those alerts to run automatically or through an approval process\n      when a budget target is exceeded.\n    \n    \n      Implement notifications on cost and usage to ensure that changes\n      in cost and usage can be acted upon quickly if they are\n      unexpected. AWS Cost Anomaly Detection allows you to reduce cost surprises\n      and enhance control without slowing innovation. AWS Cost Anomaly Detection identifies anomalous spend and root causes, which helps\n      to reduce the risk of billing surprises. With three simple steps,\n      you can create your own contextualized monitor and receive alerts\n      when any anomalous spend is detected.\n    \n    \n      You can also\n      use QuickSight with AWS Cost and Usage Report (CUR) data, to\n      provide highly customized reporting with more granular data.\n      QuickSight allows you to schedule reports and receive\n      periodic Cost Report emails for historical cost and usage or\n      cost-saving opportunities. Check our\n      Cost\n        Intelligence Dashboard (CID) solution built on QuickSight, which gives you advanced visibility.\n    \n    \n      Use AWS Trusted Advisor, which provides guidance to verify whether\n      provisioned resources are aligned with AWS best practices for cost\n      optimization.\n    \n    \n      Check your Savings Plans recommendations through visual graphs\n      against your granular cost and usage. Hourly graphs show On-Demand\n      spend alongside the recommended Savings Plans commitment,\n      providing insight into estimated savings, Savings Plans coverage,\n      and Savings Plans utilization. This helps organizations to\n      understand how their Savings Plans apply to each hour of spend\n      without having to invest time and resources into building models to\n      analyze their spend.\n    \n    \n      Periodically create reports containing a highlight of Savings Plans, Reserved Instances, and Amazon EC2 rightsizing\n      recommendations from AWS Cost Explorer to start reducing the cost\n      associated with steady-state workloads, idle, and underutilized\n      resources. Identify and recoup spend associated with cloud waste\n      for resources that are deployed. Cloud waste occurs when\n      incorrectly-sized resources are created or different usage\n      patterns are observed instead what is expected. Follow AWS best\n      practices to reduce your waste or ask your account team and\n      partner to help you\n      to optimize\n        and save your cloud costs.\n    \n    \n      Generate reports regularly for better purchasing options for your\n      resources to drive down unit costs for your workloads. Purchasing\n      options such as Savings Plans, Reserved Instances, or Amazon EC2\n      Spot Instances offer the deepest cost savings for fault-tolerant\n      workloads and allow stakeholders (business owners, finance, and\n      tech teams) to be part of these commitment discussions.\n    \n    \n      Share the reports that contain opportunities or new release\n      announcements that may help you to reduce total cost of ownership\n      (TCO) of the cloud. Adopt new services, Regions, features,\n      solutions, or new ways to achieve further cost reductions.\n    \n     \n      \n      Implementation steps\n      \n      \n      \n         \n         \n      \n          \n            Configure AWS Budgets:\n            Configure AWS Budgets on all accounts for your workload. Set\n            a budget for the overall account spend, and a budget for the\n            workload by using tags.\n          \n          \n             \n          \n              \n                Well-Architected\n                  Labs: Cost and Governance Usage\n              \n            \n        \n          \n            Report on cost\n              optimization: Set up a regular cycle to discuss\n            and analyze the efficiency of the workload. Using the\n            metrics established, report on the metrics achieved and the\n            cost of achieving them. Identify and fix any negative\n            trends, as well as positive trends that you can promote\n            across your organization. Reporting should involve\n            representatives from the application teams and owners,\n            finance, and key decision makers with respect to cloud\n            expenditure.\n          \n        \n     \n   \n    \n    Resources\n    \n    \n    \n      Related documents:\n    \n    \n       \n       \n       \n       \n       \n       \n    \n        \n          AWS Cost Explorer\n        \n      \n        \n          AWS Trusted Advisor\n        \n      \n        \n          AWS Budgets\n        \n      \n        \n          AWS Cost and Usage Report\n        \n      \n        \n          AWS Budgets Best Practices\n        \n      \n        \n          Amazon S3 Analytics\n        \n      \n    \n      Related examples:\n    \n    \n       \n    \n        \n          Key\n            ways to start optimizing your AWS cloud costs\n        \n      \n  \u003cdiv\u003e\u003cdiv\u003e\u003cdiv\u003e\u003cdiv id=\"js_error_message\"\u003e\u003cp\u003e\u003cimg src=\"https://d1ge0kk1l5kms0.cloudfront.net/images/G/01/webservices/console/warning.png\" alt=\"Warning\" /\u003e \u003cstrong\u003eJavascript is disabled or is unavailable in your browser.\u003c/strong\u003e\u003c/p\u003e\u003cp\u003eTo use the Amazon Web Services Documentation, Javascript must be enabled. Please refer to your browser's Help pages for instructions.\u003c/p\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003eDocument ConventionsCOST01-BP04 Implement cost awareness in your organizational\n  processesCOST01-BP06 Monitor cost proactively",
  "COST01-BP06 Monitor cost proactivelyImplement tooling and dashboards to monitor cost proactively for the workload. Regularly\n    review the costs with configured tools or out of the box tools, do not just look at costs and\n    categories when you receive notifications. Monitoring and analyzing costs proactively helps to\n    identify positive trends and allows you to promote them throughout your organization. \n    Level of risk exposed if this best practice\n    is not established: Medium\n  \n\n  Implementation guidance\n\n    It is recommended to monitor cost and usage proactively within your organization, not just\n      when there are exceptions or anomalies. Highly visible dashboards throughout your office\n      or work environment ensure that key people have access to the information they need, and\n      indicate the organization’s focus on cost optimization. Visible dashboards allow you to\n      actively promote successful outcomes and implement them throughout your organization.\n    Create a daily or frequent routine to use AWS Cost Explorer or any other dashboard such\n      as Amazon QuickSight to see the costs and analyze\n      proactively. Analyze AWS service usage and costs at the AWS account-level, workload-level,\n      or specific AWS service-level with grouping and filtering, and validate whether they are\n      expected or not. Use the hourly- and resource-level granularity and tags to filter and\n      identify incurring costs for the top resources. You can also build your own reports with the\n        Cost\n        Intelligence Dashboard, an Amazon QuickSight solution built by AWS Solutions Architects, and compare your budgets\n      with the actual cost and usage.\n    Implementation steps\n    \n       \n       \n       \n       \n    \n        \n          Report on cost optimization: Set up a regular cycle to\n          discuss and analyze the efficiency of the workload. Using the metrics established, report\n          on the metrics achieved and the cost of achieving them. Identify and fix any negative\n          trends, and identify positive trends to promote across your organization. Reporting should\n          involve representatives from the application teams and owners, finance, and management. \n      \n        Create and activate daily granularity AWS Budgets for the cost and usage to take timely actions to prevent any\n            potential cost overruns:  AWS Budgets allow you to configure alert\n          notifications, so you stay informed if any of your budget types fall out of your\n          pre-configured thresholds. The best way to leverage AWS Budgets is to set your expected\n          cost and usage as your limits, so that anything above your budgets can be considered\n          overspend.\n      \n        Create AWS Cost Anomaly Detection for cost monitor: \n          AWS Cost Anomaly Detection uses advanced Machine Learning technology to identify anomalous\n          spend and root causes, so you can quickly take action. It allows you to configure cost\n          monitors that define spend segments you want to evaluate (for example, individual AWS\n          services, member accounts, cost allocation tags, and cost categories), and lets you set\n          when, where, and how you receive your alert notifications. For each monitor, attach\n          multiple alert subscriptions for business owners and technology teams, including a name, a\n          cost impact threshold, and alerting frequency (individual alerts, daily summary, weekly\n          summary) for each subscription.\n      Use AWS Cost Explorer or integrate your AWS Cost and Usage Report (CUR) data with Amazon QuickSight\n            dashboards to visualize your organization’s costs: AWS Cost Explorer has an\n          easy-to-use interface that lets you visualize, understand, and manage your AWS costs and\n          usage over time. The Cost\n            Intelligence Dashboard is a customizable and accessible dashboard to help create\n          the foundation of your own cost management and optimization tool.\n   \n\n  Resources\n\n      \n    \n      Related documents:\n    \n    \n       \n       \n       \n       \n    \n        AWS Budgets\n      \n        AWS Cost Explorer\n      \n        Daily Cost and Usage Budgets\n      \n        AWS Cost Anomaly Detection\n      \n    \n      Related examples:\n    \n    \n       \n    \n        AWS Cost Anomaly Detection Alert with Slack\n      \n  \u003cdiv\u003e\u003cdiv\u003e\u003cdiv\u003e\u003cdiv id=\"js_error_message\"\u003e\u003cp\u003e\u003cimg src=\"https://d1ge0kk1l5kms0.cloudfront.net/images/G/01/webservices/console/warning.png\" alt=\"Warning\" /\u003e \u003cstrong\u003eJavascript is disabled or is unavailable in your browser.\u003c/strong\u003e\u003c/p\u003e\u003cp\u003eTo use the Amazon Web Services Documentation, Javascript must be enabled. Please refer to your browser's Help pages for instructions.\u003c/p\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003eDocument ConventionsCOST01-BP05 Report and notify on cost optimizationCOST01-BP07 Keep up-to-date with new service releases",
  "COST01-BP07 Keep up-to-date with new service releases Consult regularly with experts or AWS Partners to consider which services and features\n    provide lower cost. Review AWS blogs and other information sources. \n    Level of risk exposed if this best practice\n    is not established: Medium\n  \n\n  Implementation guidance\n\n    AWS is constantly adding new capabilities so you can leverage the latest technologies to\n      experiment and innovate more quickly. You may be able to implement new AWS services and\n      features to increase cost efficiency in your workload. Regularly review AWS Cost Management, the AWS News Blog, the AWS Cost Management blog, and What’s New with AWS for\n      information on new service and feature releases. What's New posts provide a brief overview of\n      all AWS service, feature, and Region expansion announcements as they are released.\n    Implementation steps\n    \n       \n       \n       \n       \n       \n    \n        \n          Subscribe to blogs: Go to the AWS blogs pages and\n          subscribe to the What's New Blog and other relevant blogs. You can sign up on the communication preference page with your email address.\n      Subscribe to AWS News: Regularly review the AWS News Blog and What’s New with AWS for information on new\n          service and feature releases. Subscribe to the RSS feed, or with your email to follow\n          announcements and releases.Follow AWS Price Reductions: Regular price cuts on all our\n          services has been a standard way for AWS to pass on the economic efficiencies to our\n          customers gained from our scale. As of September 20, 2023, AWS has reduced prices 134 times since 2006. If you have any pending business decisions due to price\n          concerns, you can review them again after price reductions and new service integrations.\n          You can learn about the previous price reductions efforts, including Amazon Elastic Compute Cloud (Amazon EC2)\n          instances, in the price-reduction category of the AWS News Blog.\n         AWS events and meetups: Attend your local AWS\n          summit, and any local meetups with other organizations from your local area. If you cannot\n          attend in person, try to attend virtual events to hear more from AWS experts and other\n          customers’ business cases.\n      \n         Meet with your account team: Schedule a regular\n          cadence with your account team, meet with them and discuss industry trends and AWS\n          services. Speak with your account manager, Solutions Architect, and support team. \n      \n   \n\n  Resources\n\n      \n    \n      Related documents:\n    \n    \n \n       \n       \n       \n    \n        \n          AWS Cost Management\n        \n      \n        What’s New with AWS\n      \n        \n          AWS News Blog\n        \n      \n    \n      Related examples:\n    \n    \n       \n       \n    \n        \n          Amazon EC2 – 15 Years of Optimizing and Saving Your IT Costs\n        \n      \n        AWS News Blog - Price Reduction\n      \n  \u003cdiv\u003e\u003cdiv\u003e\u003cdiv\u003e\u003cdiv id=\"js_error_message\"\u003e\u003cp\u003e\u003cimg src=\"https://d1ge0kk1l5kms0.cloudfront.net/images/G/01/webservices/console/warning.png\" alt=\"Warning\" /\u003e \u003cstrong\u003eJavascript is disabled or is unavailable in your browser.\u003c/strong\u003e\u003c/p\u003e\u003cp\u003eTo use the Amazon Web Services Documentation, Javascript must be enabled. Please refer to your browser's Help pages for instructions.\u003c/p\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003eDocument ConventionsCOST01-BP06 Monitor cost proactivelyCOST01-BP08 Create a cost-aware culture",
  "COST01-BP08 Create a cost-aware culture\n    Implement changes or programs across your organization to create a cost-aware culture. It\n    is recommended to start small, then as your capabilities increase and your organization’s\n    use of the cloud increases, implement large and wide ranging programs.\n  \n    Level of risk exposed if this best practice\n    is not established: Low\n  \n\n  Implementation guidance\n\n    A cost-aware culture allows you to scale cost optimization and Cloud Financial Management\n      (financial operations, cloud center of excellence, cloud operations teams, and so on) through\n      best practices that are performed in an organic and decentralized manner across your\n      organization. Cost awareness allows you to create high levels of capability across your\n      organization with minimal effort, compared to a strict top-down, centralized approach.\n    Creating cost awareness in cloud computing, especially for primary cost drivers in cloud\n      computing, allows teams to understand expected outcomes of any changes in cost perspective.\n      Teams who access the cloud environments should be aware of pricing models and the\n      difference between traditional on-premesis datacenters and cloud computing.\n    The main benefit of a cost-aware culture is that technology teams optimize costs\n      proactively and continually (for example, they are considered a non-functional requirement\n      when architecting new workloads, or making changes to existing workloads) rather than\n      performing reactive cost optimizations as needed.\n    Small changes in culture can have large impacts on the efficiency of your current and future\n      workloads. Examples of this include:\n    \n       \n       \n       \n       \n       \n       \n    Giving visibility and creating awareness in engineering teams to understand what they do, and\n          what they impact in terms of cost.Gamifying cost and usage across your organization. This can be done through a publicly visible\n          dashboard, or a report that compares normalized costs and usage across teams (for example,\n          cost-per-workload and cost-per-transaction).Recognizing cost efficiency. Reward voluntary or unsolicited cost optimization\n        accomplishments publicly or privately, and learn from mistakes to avoid repeating\n        them in the future.Creating top-down organizational requirements for workloads to run at pre-defined\n          budgets.Questioning business requirements of changes, and the cost impact of requested changes to the\n          architecture infrastructure or workload configuration to make sure you pay only what you\n          need.Making sure the change planner is aware of expected changes that have a cost impact, and that\n          they are confirmed by the stakeholders to deliver business outcomes\n          cost-effectively.\n    Implementation steps\n    \n       \n       \n       \n       \n       \n       \n       \n    Report cloud costs to technology teams: To raise cost\n          awareness, and establish efficiency KPIs for finance and business stakeholders.Inform stakeholders or team members about planned changes: Create an agenda item to discuss planned changes and the cost-benefit impact on the\n          workload during weekly change meetings.\n         Meet with your account team: Establish a regular\n          meeting cadence with your account team, and discuss industry trends and AWS\n          services. Speak with your account manager, architect, and support team. \n      Share success stories: Share success stories about cost\n          reduction for any workload, AWS account, or organization to create a positive attitude\n          and encouragement around cost optimization.Training: Ensure technical teams or team members are trained\n          for awareness of resource costs on AWS Cloud.\n         AWS events and meetups: Attend local AWS\n          summits, and any local meetups with other organizations from your local area. \n      \n        \n          Subscribe to blogs: Go to the AWS blogs pages and\n          subscribe to the What's New Blog and other relevant\n          blogs to follow new releases, implementations, examples, and changes shared by AWS.\n        \n      \n   \n\n  Resources\n\n      \n    \n      Related documents:\n    \n    \n       \n       \n\n       \n    \n        \n          AWS Blog\n        \n      \n        \n          AWS Cost Management\n        \n      \n        \n          AWS News Blog\n        \n      \n    \n      Related examples:\n    \n    \n       \n    \n        \n          AWS Cloud Financial Management\n        \n      \n  \u003cdiv\u003e\u003cdiv\u003e\u003cdiv\u003e\u003cdiv id=\"js_error_message\"\u003e\u003cp\u003e\u003cimg src=\"https://d1ge0kk1l5kms0.cloudfront.net/images/G/01/webservices/console/warning.png\" alt=\"Warning\" /\u003e \u003cstrong\u003eJavascript is disabled or is unavailable in your browser.\u003c/strong\u003e\u003c/p\u003e\u003cp\u003eTo use the Amazon Web Services Documentation, Javascript must be enabled. Please refer to your browser's Help pages for instructions.\u003c/p\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003eDocument ConventionsCOST01-BP07 Keep up-to-date with new service releasesCOST01-BP09 Quantify business value from cost optimization",
  "COST01-BP09 Quantify business value from cost optimization\n    Quantifying business value from cost optimization allows you to understand the entire set\n    of benefits to your organization. Because cost optimization is a necessary investment,\n    quantifying business value allows you to explain the return on investment to stakeholders.\n    Quantifying business value can help you gain more buy-in from stakeholders on future cost\n    optimization investments, and provides a framework to measure the outcomes for your\n    organization’s cost optimization activities.\n  \n    Level of risk exposed if this best practice\n    is not established: Medium\n  \n\n  Implementation guidance\n    \n      Quantifying the business value means measuring the benefits that businesses gain from the actions and decisions they take. Business value can be tangible (like reduced expenses or increased profits) or intangible (like improved brand reputation or increased customer satisfaction).\n    \n    \n      To quantify business value from cost optimization means determining how much value or benefit you’re getting from your efforts to spend more efficiently. For example, if a company spends $100,000 to deploy a workload on AWS and later optimizes it, the new cost becomes only $80,000 without sacrificing the quality or output. In this scenario, the quantified business value from cost optimization would be a savings of $20,000. But beyond just savings, the business might also quantify value in terms of faster delivery times, improved customer satisfaction, or other metrics that result from the cost optimization efforts. Stakeholders need to make decisions about the potential value of cost optimization, the cost of optimizing the workload, and return value. \n    \n    \n      In addition to reporting savings from cost optimization, it is recommended that you\n      quantify the additional value delivered. Cost optimization benefits are typically quantified in terms of lower costs per business outcome. For example, you can quantify\n     Amazon Elastic Compute Cloud(Amazon EC2) cost savings when you purchase Savings Plans, which reduce cost and maintain workload output levels. You can quantify cost\n      reductions in AWS spending when idle Amazon EC2 instances are removed, or\n      unattached Amazon Elastic Block Store (Amazon EBS) volumes are deleted.\n    \n    \n      The benefits from cost optimization, however, go above and beyond cost reduction or\n      avoidance. Consider capturing additional data to measure efficiency improvements and\n      business value.\n    \n     \n      \n      Implementation steps\n    \n      \n         \n         \n      \n          \n            Evaluate business benefits: This is the process of analyzing and adjusting AWS Cloud cost in ways that maximize the benefit received from each dollar spent. Instead of focusing on cost reduction without business value, consider business benefits and return on investments for cost optimization, which may bring more value out of the money you spend. It's about spending wisely and making investments and expenditures in areas that yield the best return.\n          \n        \n          \n            Analyze forecasting AWS costs: Forecasting helps finance stakeholders set expectations with other internal and external organization stakeholders, and can improve your organization’s financial predictability. AWS Cost Explorer can be used to perform forecasting for your cost and usage.\n          \n        \n     \n    \n   \n\n  Resources\n\n      \n    \n      Related documents:\n    \n    \n       \n       \n       \n       \n       \n       \n    \n        AWS Cloud Economics\n        \n      \n        \n          AWS Blog\n        \n      \n        \n          AWS Cost Management\n        \n      \n        \n          AWS News Blog\n        \n      \n        \n          Well-Architected Reliability Pillar whitepaper\n        \n      \n        \n          AWS Cost Explorer\n        \n      \n    \n      Related videos:\n    \n    \n       \n    \n        \n          Unlock Business Value with Windows on AWS\n      \n    \n      Related examples:\n    \n    \n       \n       \n       \n       \n       \n    \n        \n          Measuring and Maximizing the Business Value of Customer 360\n        \n      \n        \n          The Business Value of Adopting Amazon Web Services Managed Databases\n        \n      \n        \n          The Business Value of Amazon Web Services for Independent Software Vendors\n        \n      \n        \n          Business Value of Cloud Modernization\n        \n      \n        \n          The Business Value of Migration to Amazon Web Services\n        \n      \n  \u003cdiv\u003e\u003cdiv\u003e\u003cdiv\u003e\u003cdiv id=\"js_error_message\"\u003e\u003cp\u003e\u003cimg src=\"https://d1ge0kk1l5kms0.cloudfront.net/images/G/01/webservices/console/warning.png\" alt=\"Warning\" /\u003e \u003cstrong\u003eJavascript is disabled or is unavailable in your browser.\u003c/strong\u003e\u003c/p\u003e\u003cp\u003eTo use the Amazon Web Services Documentation, Javascript must be enabled. Please refer to your browser's Help pages for instructions.\u003c/p\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003eDocument ConventionsCOST01-BP08 Create a cost-aware cultureExpenditure and usage awareness",
  "COST02-BP01 Develop policies based on your organization\n  requirementsDevelop policies that define how resources are managed by your organization and inspect them periodically. Policies should cover the cost aspects of resources and workloads, including creation, modification, and decommissioning over a resource’s lifetime.\n    Level of risk exposed if this best practice\n    is not established: High\n  \n\n  Implementation guidance\n    Understanding your organization’s costs and drivers is critical for managing your cost and usage effectively and identifying cost reduction opportunities. Organizations typically operate multiple workloads run by multiple teams. These teams can be in different organization units, each with its own revenue stream. The capability to attribute resource costs to the workloads, individual organization, or product owners drives efficient usage behaviour and helps reduce waste. Accurate cost and usage monitoring helps you understand how optimized a workload is, as well as how profitable organization units and products are. This knowledge allows for more informed decision making about where to allocate resources within your organization. Awareness of usage at all levels in the organization is key to driving change, as change in usage drives changes in cost. Consider taking a multi-faceted approach to becoming aware of your usage and expenditures.\n    The first step in performing governance is to use your organization’s requirements to develop policies for your cloud usage. These policies define how your organization uses the cloud and how resources are managed. Policies should cover all aspects of resources and workloads that relate to cost or usage, including creation, modification, and decommissioning over a resource’s lifetime. Verify that policies and procedures are followed and implemented for any change in a cloud environment. During your IT change management meetings, raise questions to find out the cost impact of planned changes, whether increasing or decreasing, the business justification, and the expected outcome. \n    Policies should be simple so that they are easily understood and can be implemented effectively throughout the organization. Policies also need to be easy to follow and interpret (so they are used) and specific (no misinterpretation between teams). Moreover, they need to be inspected periodically (like our mechanisms) and updated as customers business conditions or priorities change, which would make the policy outdated.\n    \n      Start with broad, high-level policies, such as which geographic Region to use or times of the day that resources should be running. Gradually refine the policies for the various organizational units and workloads. Common policies include which services and features can be used (for example, lower performance storage in test and development environments), which types of resources can be used by different groups (for example, the largest size of resource in a development account is medium) and how long these resources will be in use (whether temporary, short term, or for a specific period of time). \n    \n    \n      Policy example\n    \n    \n      The following is a sample policy you can review to create your own cloud governance policies, which focus on cost optimization. Make sure you adjust policy based on your organization’s requirements and your stakeholders’ requests. \n    \n    \n       \n       \n       \n    \n        \n          Policy name: Define a clear policy name, such as Resource Optimization and Cost Reduction Policy.\n        \n      \n        \n          Purpose: Explain why this policy should be used and what is the expected outcome. The objective of this policy is to verify that there is a minimum cost required to deploy and run the desired workload to meet business requirements.\n        \n      \n        \n          Scope: Clearly define who should use this policy and when it should be used, such as DevOps X Team to use this policy in us-east customers for X environment (production or non-production).\n        \n      \n    \n      Policy statement\n    \n    \n       \n       \n       \n       \n       \n       \n       \n       \n       \n       \n       \n       \n    \n        \n          Select us-east-1or multiple us-east regions based on your workload’s environment and business requirement (development, user acceptance testing, pre-production, or production).\n        \n      \n        \n          Schedule Amazon EC2 and Amazon RDS instances to run between six in the morning and eight at night (Eastern Standard Time (EST)).\n        \n      \n        \n          Stop all unused Amazon EC2 instances after eight hours and unused Amazon RDS instances after 24 hours of inactivity.\n        \n      \n        \n          Terminate all unused Amazon EC2 instances after 24 hours of inactivity in non-production environments. Remind Amazon EC2 instance owner (based on tags) to review their stopped Amazon EC2 instances in production and inform them that their Amazon EC2 instances will be terminated within 72 hours if they are not in use.\n        \n      \n        \n          Use generic instance family and size such as m5.large and then resize the instance based on CPU and memory utilization using AWS Compute Optimizer.\n        \n      \n        \n          Prioritize using auto scaling to dynamically adjust the number of running instances based on traffic. \n        \n      \n        \n          Use spot instances for non-critical workloads.\n        \n      \n        \n          Review capacity requirements to commit saving plans or reserved instances for predictable workloads and inform Cloud Financial Management Team. \n        \n      \n        \n          Use Amazon S3 lifecycle policies to move infrequently accessed data to cheaper storage tiers. If no retention policy defined, use Amazon S3 Intelligent Tiering to move objects to archived tier automatically. \n        \n      \n        \n          Monitor resource utilization and set alarms to trigger scaling events using Amazon CloudWatch.\n        \n      \n        \n          For each AWS account, use AWS Budgets to set cost and usage budgets for your account based on cost center and business units.\n        \n      \n        \n          Using AWS Budgets to set cost and usage budgets for your account can help you stay on top of your spending and avoid unexpected bills, allowing you to better control your costs.\n        \n      \n    \n      Procedure: Provide detailed procedures for implementing this policy or refer to other documents that describe how to implement each policy statement. This section should provide step-by-step instructions for carrying out the policy requirements.\n    \n    \n      To implement this policy, you can use various third-party tools or AWS Config rules to check for compliance with the policy statement and trigger automated remediation actions using AWS Lambda functions. You can also use AWS Organizations to enforce the policy. Additionally, you should regularly review your resource usage and adjust the policy as necessary to verify that it continues to meet your business needs.\n    \n   \n      \n      Implementation steps\n    \n       \n       \n       \n       \n       \n       \n       \n       \n       \n    \n        \n          Meet with stakeholders: To develop policies, ask stakeholders (cloud business office, engineers, or functional decision makers for policy enforcement) within your organization to specify their requirements and document them. Take an iterative approach by starting broadly and continually refine down to the smallest units at each step. Team members include those with direct interest in the workload, such as organization units or application owners, as well as supporting groups, such as security and finance teams.\n      \n        \n          Get confirmation: Make sure teams agree on policies who \n          can access and deploy to the AWS Cloud. Make sure they follow your organization’s policies \n          and confirm that their resource creations align with the agreed policies and procedures.\n        \n      \n        \n          Create onboarding training sessions: Ask new organization \n          members to complete onboarding training courses to create cost awareness and organization \n          requirements. They may assume different policies from their previous experience or not think \n          of them at all. \n        \n      \n         Define locations for your workload: Define where\n          your workload operates, including the country and the area within the country. This\n          information is used for mapping to AWS Regions and Availability Zones. \n      \n         Define and group services and resources: Define the\n          services that the workloads require. For each service, specify the types, the size, and\n          the number of resources required. Define groups for the resources by function, such as\n          application servers or database storage. Resources can belong to multiple groups. \n      \n        \n          Define and group the users by function: Define the users\n          that interact with the workload, focusing on what they do and how they use the workload,\n          not on who they are or their position in the organization. Group similar users or\n          functions together. You can use the AWS managed policies as a guide. \n      \n         Define the actions: Using the locations, resources,\n          and users identified previously, define the actions that are required by each to achieve\n          the workload outcomes over its life time (development, operation, and decommission).\n          Identify the actions based on the groups, not the individual elements in the groups, in\n          each location. Start broadly with read or write, then refine down to specific actions to\n          each service. \n      \n         Define the review period: Workloads and\n          organizational requirements can change over time. Define the workload review schedule to\n          ensure it remains aligned with organizational priorities. \n      \n        \n          Document the policies: Verify the policies that have been\n          defined are accessible as required by your organization. These policies are used to\n          implement, maintain, and audit access of your environments. \n      \n   \n\n  Resources\n\n      \n    \n      Related documents:\n    \n    \n       \n       \n       \n       \n       \n       \n       \n    \n        \n          Change Management in the Cloud\n        \n      \n        \n          AWS Managed Policies for Job Functions\n        \n      \n        \n          AWS multiple account billing strategy\n        \n      \n        \n          Actions,\n          Resources, and Condition Keys for AWS Services\n        \n      \n        \n          AWS Management and Governance\n        \n      \n        \n          Control\n          access to AWS Regions using IAM policies\n        \n      \n        \n          Global\n          Infrastructures Regions and AZs\n        \n      \n    \n      Related videos:\n    \n    \n       \n    \n        \n          AWS Management and Governance at Scale\n        \n      \n  \u003cdiv\u003e\u003cdiv\u003e\u003cdiv\u003e\u003cdiv id=\"js_error_message\"\u003e\u003cp\u003e\u003cimg src=\"https://d1ge0kk1l5kms0.cloudfront.net/images/G/01/webservices/console/warning.png\" alt=\"Warning\" /\u003e \u003cstrong\u003eJavascript is disabled or is unavailable in your browser.\u003c/strong\u003e\u003c/p\u003e\u003cp\u003eTo use the Amazon Web Services Documentation, Javascript must be enabled. Please refer to your browser's Help pages for instructions.\u003c/p\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003eDocument Conventions COST 2. How do you govern usage?COST02-BP02 Implement goals and targets",
  "COST02-BP02 Implement goals and targets\n    Implement both cost and usage goals and targets for your workload.\n    Goals provide direction to your organization on expected outcomes,\n    and targets provide specific measurable outcomes to be achieved for\n    your workloads.\n  \n    Level of risk exposed if this best practice\n    is not established: High\n  \n\n  Implementation guidance\n\n      \n    \n      Develop cost and usage goals and targets for your organization. As\n      a growing organization on AWS, it is important to set and track\n      goals for cost optimization. These goals or\n      key\n      performance indicators (KPIs) can include things like\n      percent of spend on-demand or adoption of certain optimized\n      services such as AWS Graviton instances or gp3 EBS volume types.\n      Set measurable and achievable goals to help you measure efficiency\n      improvements, which is important for your business operations.\n      Goals provide guidance and direction to your organization on\n      expected outcomes.\n    \n    \n      Targets provide specific, measurable outcomes to be achieved. In\n      short, a goal is the direction you want to go, and a target is how\n      far in that direction and when that goal should be achieved (use\n      guidance of specific, measurable, assignable, realistic and\n      timely, or SMART). An example of a goal is that platform usage\n      should increase significantly, with only a minor (non-linear)\n      increase in cost. An example target is a 20% increase in platform\n      usage, with less than a five percent increase in costs. Another\n      common goal is that workloads need to be more efficient every six\n      months. The accompanying target would be that the cost per\n      business metrics needs to decrease by five percent every six\n      months. Use the right metrics, and set calculated KPIs for your\n      organization. You can start with basic KPIs and evolve later based\n      on business needs.\n    \n    \n      A goal for cost optimization is to increase workload efficiency,\n      which corresponds to a decrease in the cost per business outcome\n      of the workload over time. Implement this goal for all workloads,\n      and set a target like a five percent increase in efficiency every\n      six months to a year. In the cloud, you can achieve this through\n      the establishment of capability in cost optimization, as well as\n      new service and feature releases.\n    \n    \n      Targets are the quantifiable benchmarks you want to reach to meet\n      your goals and benchmarks compare your actual results against a\n      target. Establish benchmarks\n      with KPIs for the cost per unit of compute services (such as Spot\n      adoption, Graviton adoption, latest instance types, and On-Demands\n      coverage), storage services (such as EBS GP3 adoption, obsolete\n      EBS snapshots, and Amazon S3 standard storage), or database\n      service usage (such as RDS open-source engines, Graviton adoption,\n      and On-demand coverage). These benchmarks and KPIs can help you\n      verify that you use AWS services in the most cost-effective\n      manner.\n    \n    \n      The following table provides a list of standard AWS metrics for\n      reference. Each organization can have different target values for\n      these KPIs.\n    \n    \n          \n            \n              Category\n            \n            \n              KPI (%)\n            \n            \n              Description\n            \n          \n        \n          \n            \n              Compute\n            \n            \n              EC2 usage Coverage\n            \n            \n              EC2 instances (in cost or hours) using SP+RI+Spot compared\n              to total (in cost or hours) of EC2 instances\n            \n          \n          \n            \n              Compute\n            \n            \n              Compute SP/RI utilization\n            \n            \n              Utilized SP or RI hours compared to total available SP or\n              RI hours\n            \n          \n          \n            \n              Compute\n            \n            \n              EC2/Hour cost\n            \n            \n              EC2 cost divided by the number of EC2 instances running in\n              that hour\n            \n          \n          \n            \n              Compute\n            \n            \n              vCPU cost\n            \n            \n              Cost per vCPU for all instances\n            \n          \n          \n            \n              Compute\n            \n            \n              Latest Instance Generation\n            \n            \n              Percentage of instances on Graviton (or other modern\n              generation instance types)\n            \n          \n          \n            \n              Database\n            \n            \n              RDS coverage\n            \n            \n              RDS instances (in cost or hours) using RI compared to\n              total (in cost or hours) of RDS instances\n            \n          \n          \n            \n              Database\n            \n            \n              RDS utilization\n            \n            \n              Utilized RI hours compared to total available RI hours\n            \n          \n          \n            \n              Database\n            \n            \n              RDS uptime\n            \n            \n              RDS cost divided by the number of RDS instances running in\n              that hour\n            \n          \n          \n            \n              Database\n            \n            \n              Latest Instance Generation\n            \n            \n              Percentage of instances on Graviton (or other modern\n              instance types)\n            \n          \n          \n            \n              Storage\n            \n            \n              Storage utilization\n            \n            \n              Optimized storage cost (for example Glacier, deep archive,\n              or Infrequent Access) divided by total storage cost\n            \n          \n          \n            \n              Tagging\n            \n            \n              Untagged resources\n            \n            \n              \n                Cost Explorer:\n              \n              \n                1. Filter out credits, discounts, taxes, refunds,\n                marketplace, and copy the latest monthly cost\n              \n              \n                2. Select Show only untagged\n                resources in Cost Explorer\n              \n              \n                3. Divide the amount in untagged\n                resources with your monthly cost.\n              \n            \n          \n        \n    \n      Using this table, include target or benchmark values, which should\n      be calculated based on your organizational goals. You need to\n      measure certain metrics for your business and understand business\n      outcome for that workload to define accurate and realistic KPIs.\n      When you evaluate performance metrics within an organization,\n      distinguish between different types of metrics that serve distinct\n      purposes. These metrics primarily measure the performance and\n      efficiency of the technical infrastructure rather than directly\n      the overall business impact. For instance, they might track server\n      response times, network latency, or system uptime. These metrics\n      are crucial to assess how well the infrastructure supports the\n      organization's technical operations. However, they don't provide\n      direct insight into broader business objectives like customer\n      satisfaction, revenue growth, or market share. To gain a\n      comprehensive understanding of business performance, complement\n      these efficiency metrics with strategic business metrics that\n      directly correlate with business outcomes.\n    \n    \n      Establish near real-time visibility over your KPIs and related\n      savings opportunities and track your progress over time. To get\n      started with the definition and tracking of KPI goals, we\n      recommend the KPI dashboard from\n      Cloud\n      Intelligence Dashboards (CID). Based on the data from Cost\n      and Usage Report (CUR), the KPI dashboard provides a series of\n      recommended cost optimization KPIs, with the ability to set custom\n      goals and track progress over time.\n    \n    \n      If you have other solutions to set and track KPI goals, make sure\n      these methods are adopted by all cloud financial management\n      stakeholders in your organization.\n    \n     \n      \n      Implementation steps\n    \n    \n       \n       \n       \n       \n    \n        \n          Define expected usage\n          levels: To begin, focus on usage levels. Engage\n          with the application owners, marketing, and greater business\n          teams to understand what the expected usage levels are for the\n          workload. How might customer demand change over time, and what\n          can change due to seasonal increases or marketing campaigns?\n        \n      \n        \n          Define workload resourcing and\n          costs: With usage levels defined, quantify the\n          changes in workload resources required to meet those usage\n          levels. You may need to increase the size or number of\n          resources for a workload component, increase data transfer, or\n          change workload components to a different service at a\n          specific level. Specify the costs at each of these major\n          points, and predict the change in cost when there is a change\n          in usage.\n        \n      \n        \n          Define business goals: Take\n          the output from the expected changes in usage and cost,\n          combine this with expected changes in technology, or any\n          programs that you are running, and develop goals for the\n          workload. Goals must address usage and cost, as well as the\n          relationship between the two. Goals must be simple,\n          high-level, and help people understand what the business\n          expects in terms of outcomes (such as making sure unused\n          resources are kept below certain cost level). You don't need\n          to define goals for each unused resource type or define costs\n          that can cause losses in goals and targets. Verify that there\n          are organizational programs (for example, capability building\n          like training and education) if there are expected changes in\n          cost without changes in usage.\n        \n      \n        \n          Define targets: For each of\n          the defined goals, specify a measurable target. If the goal is\n          to increase efficiency in the workload, the target should\n          quantify the amount of improvement (typically in business\n          outputs for each dollar spent) and when it should be\n          delivered. For example, you could set a goal to minimize waste\n          due to over-provisioning. With this goal, your target can be\n          that waste due to compute over-provisioning in the first tier\n          of production workloads should not exceed ten percent of tier\n          compute cost. Additionally, a second target could be that\n          waste due to compute over-provisioning in the second tier of\n          production workloads should not exceed five percent of tier\n          compute cost.\n        \n      \n     \n   \n\n  Resources\n\n      \n    \n      Related documents:\n    \n    \n       \n       \n       \n       \n       \n    \n        \n          AWS           managed policies for job functions\n        \n      \n        \n          AWS           multiple account billing strategy\n        \n      \n        \n          Control\n          access to AWS Regions using IAM policies\n        \n      \n        \n          S.M.A.R.T.\n          Goals\n        \n      \n        \n          How\n          to track your cost optimization KPIs with the CID KPI\n          Dashboard\n        \n      \n    \n      Related videos:\n    \n    \n       \n    \n        \n          Well-Architected\n          Labs: Goals and Targets (Level 100)\n        \n      \n    \n      Related examples:\n    \n    \n       \n       \n       \n       \n    \n        \n          What\n          is a unit metric?\n        \n      \n        \n          Selecting\n          a unit metric to support your business\n        \n      \n        \n          Unit\n          metrics in practice – lessons learned\n        \n      \n        \n          How\n          unit metrics help create alignment between business\n          functions\n        \n      \n  \u003cdiv\u003e\u003cdiv\u003e\u003cdiv\u003e\u003cdiv id=\"js_error_message\"\u003e\u003cp\u003e\u003cimg src=\"https://d1ge0kk1l5kms0.cloudfront.net/images/G/01/webservices/console/warning.png\" alt=\"Warning\" /\u003e \u003cstrong\u003eJavascript is disabled or is unavailable in your browser.\u003c/strong\u003e\u003c/p\u003e\u003cp\u003eTo use the Amazon Web Services Documentation, Javascript must be enabled. Please refer to your browser's Help pages for instructions.\u003c/p\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003eDocument ConventionsCOST02-BP01 Develop policies based on your organization\n  requirementsCOST02-BP03 Implement an account structure",
  "COST02-BP03 Implement an account structure\n    Implement a structure of accounts that maps to your organization.\n    This assists in allocating and managing costs throughout your\n    organization.\n  \n    Level of risk exposed if this best practice\n    is not established: High\n  \n\n  Implementation guidance\n    \n      AWS Organizations allows you to create multiple AWS accounts which can help you centrally \n      govern your environment as you scale your workloads on AWS. You can model your organizational \n      hierarchy by grouping AWS accounts in organizational unit (OU) structure and creating multiple \n      AWS accounts under each OU. To create an account structure, you need to decide which of your AWS accounts \n      will be the management account first. After that, you can create \n      new AWS accounts or select existing accounts as member accounts based on your designed account \n      structure by following management account best practices \n      and member account best practices.\n    \n    \n      It is advised to always have at least one management account with one member account linked to it, \n      regardless of your organization size or usage. All workload resources should reside only within member \n      accounts and no resource should be created in management account. There is no one size fits all answer \n      for how many AWS accounts you should have. Assess your current and future operational and cost models \n      to ensure that the structure of your AWS accounts reflects your organization’s goals. Some companies \n      create multiple AWS accounts for business reasons, for example: \n    \n    \n       \n       \n       \n    Administrative or fiscal and billing isolation is required between organization\n        units, cost centers, or specific workloads.AWS service limits are set to be specific to particular workloads.There is a requirement for isolation and separation between workloads and\n        resources.\n    \n      Within AWS Organizations, \n      consolidated billing \n      creates the construct between one or more member accounts and\n      the management account. Member accounts allow you to isolate and distinguish your cost and\n      usage by groups. A common practice is to have separate member accounts for each organization\n      unit (such as finance, marketing, and sales), or for each environment lifecycle (such as\n      development, testing and production), or for each workload (workload a, b, and c), and then\n      aggregate these linked accounts using consolidated billing.\n    \n    \n      Consolidated billing allows you to consolidate payment for multiple member AWS accounts\n      under a single management account, while still providing visibility for each linked account’s\n      activity. As costs and usage are aggregated in the management account, this allows you to\n      maximize your service volume discounts, and maximize the use of your commitment\n      discounts (Savings Plans and Reserved Instances) to achieve the highest discounts.\n    \n    \n      The following diagram shows how you can use AWS Organizations with organizational units (OU) \n      to group multiple accounts, and place multiple AWS accounts under each OU. It is recommended \n      to use OUs for various use cases and workloads which provides patterns for organizing accounts.\n    \n    \n       \n        \n       \n       \n      Example of grouping multiple AWS accounts under organizational units.\n    \n    \n      AWS Control Tower\n      can quickly set up and configure multiple AWS accounts, ensuring that governance is aligned\n      with your organization’s requirements.\n    Implementation steps\n    \n    \n       \n       \n       \n    \n        \n          Define separation requirements: Requirements for separation \n          are a combination of multiple factors, including security, reliability, and financial constructs. \n          Work through each factor in order and specify whether the workload or workload environment \n          should be separate from other workloads. Security promotes adhesion to access and data \n          requirements. Reliability manages limits so that environments and workloads do not impact \n          others. Review the security and reliability pillars of the Well-Architected Framework \n          periodically and follow the provided best practices. Financial constructs create strict \n          financial separation (different cost center, workload ownerships and accountability). \n          Common examples of separation are production and test workloads being run in separate \n          accounts, or using a separate account so that the invoice and billing data can be provided \n          to the individual business units or departments in the organization or stakeholder who \n          owns the account. \n      \n        \n          Define grouping requirements: Requirements for grouping \n          do not override the separation requirements, but are used to assist management. Group \n          together similar environments or workloads that do not require separation. An example \n          of this is grouping multiple test or development environments from one or more workloads \n          together.\n      \n        \n          Define account structure: Using these separations and \n          groupings, specify an account for each group and maintain separation requirements. \n          These accounts are your member or linked accounts. By grouping these member accounts \n          under a single management or payer account, you combine usage, which allows for greater \n          volume discounts across all accounts, which provides a single bill for all accounts. \n          It's possible to separate billing data and provide each member account with an individual \n          view of their billing data. If a member account must not have its usage or billing data \n          visible to any other account, or if a separate bill from AWS is required, define multiple \n          management or payer accounts. In this case, each member account has its own management or \n          payer account. Resources should always be placed in member or linked accounts. The management \n          or payer accounts should only be used for management. \n      \n   \n\n  Resources\n\n      \n    \n      Related documents:\n    \n    \n       \n       \n       \n       \n       \n       \n       \n       \n       \n       \n       \n    \n        \n          Using Cost Allocation Tags\n        \n      \n        \n          AWS           managed policies for job functions\n        \n      \n        \n          AWS           multiple account billing strategy\n        \n      \n        \n          Control\n          access to AWS Regions using IAM policies\n        \n      \n        \n          AWS Control Tower\n        \n      \n        \n          AWS Organizations\n        \n      \n        \n          Best practices for management accounts \n          and member accounts\n        \n      \n        \n          Organizing Your AWS Environment Using Multiple Accounts\n        \n      \n        \n          Turning on shared reserved instances and Savings Plans discounts\n        \n      \n        \n          Consolidated billing\n        \n      \n        \n          Consolidated billing\n        \n      \n    \n      Related examples:\n    \n    \n       \n    \n        \n          Splitting\n          the CUR and Sharing Access\n        \n      \n    \n      Related videos:\n    \n    \n       \n       \n    \n         Introducing AWS Organizations\n      \n         Set Up a Multi-Account AWS\n            Environment that Uses Best Practices for AWS Organizations\n      \n    \n      Related examples:\n    \n    \n       \n       \n       \n    \n        \n          Defining an AWS Multi-Account Strategy for telecommunications companies\n        \n      \n        \n          Best Practices for Optimizing AWS accounts\n        \n      \n        \n          Best Practices for Organizational Units with AWS Organizations\n        \n      \n  \u003cdiv\u003e\u003cdiv\u003e\u003cdiv\u003e\u003cdiv id=\"js_error_message\"\u003e\u003cp\u003e\u003cimg src=\"https://d1ge0kk1l5kms0.cloudfront.net/images/G/01/webservices/console/warning.png\" alt=\"Warning\" /\u003e \u003cstrong\u003eJavascript is disabled or is unavailable in your browser.\u003c/strong\u003e\u003c/p\u003e\u003cp\u003eTo use the Amazon Web Services Documentation, Javascript must be enabled. Please refer to your browser's Help pages for instructions.\u003c/p\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003eDocument ConventionsCOST02-BP02 Implement goals and targetsCOST02-BP04 Implement groups and roles",
  "COST02-BP04 Implement groups and roles\n    Implement groups and roles that align to your policies and control\n    who can create, modify, or decommission instances and resources in\n    each group. For example, implement development, test, and production\n    groups. This applies to AWS services and third-party solutions.\n  \n    Level of risk exposed if this best practice\n    is not established: Low\n  \n\n  Implementation guidance\n    User roles and groups are fundamental building blocks in the design and implementation of secure and efficient systems. Roles and groups help organizations balance the need for control with the requirement for flexibility and productivity, ultimately supporting organizational objectives and user needs. As recommended in Identity and access management section of AWS Well-Architected Framework Security Pillar, you need robust identity management and permissions in place to provide access to the right resources for the right people under the right conditions. Users receive only the access necessary to complete their tasks. This minimizes the risk associated with unauthorized access or misuse.\n    \n      After you develop policies, you can create logical groups and user roles within your organization. This allows you to assign permissions, control usage, and help implement robust access control mechanisms, preventing unauthorized access to sensitive information. Begin with high-level groupings of people. Typically, this aligns with organizational units and job roles (for example, a systems administrator in the IT Department, financial controller, or business analysts). The groups categorize people that do similar tasks and need similar access. Roles define what a group must do. It is easier to manage permissions for groups and roles than for individual users. Roles and groups assign permissions consistently and systematically across all users, preventing errors and inconsistencies. \n    \n    \n      When a user’s role changes, administrators can adjust access at the role or group level, rather than reconfiguring individual user accounts. For example, a systems administrator in IT requires access to create all resources, but an analytics team member only needs to create analytics resources.\n    \n    \n     \n      \n      Implementation steps\n    \n      \n    \n       \n       \n    \n         Implement groups: Using the groups of users defined\n          in your organizational policies, implement the corresponding groups, if necessary. For best practices on users, groups and authentication, see the Security Pillar of the AWS Well-Architected Framework.\n      \n         Implement roles and policies: Using the actions\n          defined in your organizational policies, create the required roles and access policies.\n          For best practices on roles and policies, see the Security Pillar of the AWS Well-Architected Framework.\n      \n     \n   \n\n  Resources\n\n      \n    \n      Related documents:\n    \n    \n       \n       \n       \n       \n       \n    \n        \n          AWS           managed policies for job functions\n        \n      \n        \n          AWS           multiple account billing strategy\n        \n      \n        \n          AWS Well-Architected Framework\n          Security Pillar\n        \n      \n        AWS Identity and Access Management (IAM)\n        \n      \n        AWS Identity and Access Management policies\n        \n      \n    \n      Related videos:\n    \n    \n       \n    \n        \n    Why use Identity and Access Management      \n        \n      \n    \n      Related examples:\n    \n    \n       \n       \n    \n        \n          Control\n            access to AWS Regions using IAM policies\n        \n      \n        \n          Starting your Cloud Financial Management journey: Cloud cost operations\n        \n      \n  \u003cdiv\u003e\u003cdiv\u003e\u003cdiv\u003e\u003cdiv id=\"js_error_message\"\u003e\u003cp\u003e\u003cimg src=\"https://d1ge0kk1l5kms0.cloudfront.net/images/G/01/webservices/console/warning.png\" alt=\"Warning\" /\u003e \u003cstrong\u003eJavascript is disabled or is unavailable in your browser.\u003c/strong\u003e\u003c/p\u003e\u003cp\u003eTo use the Amazon Web Services Documentation, Javascript must be enabled. Please refer to your browser's Help pages for instructions.\u003c/p\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003eDocument ConventionsCOST02-BP03 Implement an account structureCOST02-BP05 Implement cost controls",
  "COST02-BP05 Implement cost controls\n    Implement controls based on organization policies and defined groups and roles. \n    These certify that costs are only incurred as defined by organization requirements \n    such as control access to regions or resource types. \n  \n    Level of risk exposed if this best practice\n    is not established: Medium\n  \n\n  Implementation guidance\n    A common first step in implementing cost controls is to set up notifications when cost or\n      usage events occur outside of policies. You can act quickly and\n      verify if corrective action is required without restricting or negatively impacting workloads\n      or new activity. After you know the workload and environment limits, you can enforce\n      governance. AWS Budgets allows you to set notifications and define monthly budgets for your AWS costs, usage, and commitment discounts (Savings Plans\n      and Reserved Instances). You can create budgets at an aggregate cost level (for example, all\n      costs), or at a more granular level where you include only specific dimensions such as linked\n      accounts, services, tags, or Availability Zones.\n    \n      Once you set up your budget limits with AWS Budgets, use AWS Cost Anomaly Detection to reduce your unexpected \n      cost. AWS Cost Anomaly Detection is a cost management services that uses machine learning to continually monitor \n      your cost and usage to detect unusual spends. It helps you identify anomalous spend and root causes, so you \n      can quickly take action. First, create a cost monitor in AWS Cost Anomaly Detection, then choose your \n      alerting preference by setting up a dollar threshold (such as an alert on anomalies with impact greater \n      than $1,000). Once you receive alerts, you can analyze the root cause behind the anomaly and impact on \n      your costs. You can also monitor and perform your own anomaly analysis in AWS Cost Explorer.\n    \n    \n      Enforce governance policies in AWS through AWS Identity and Access Management and AWS Organizations Service Control Policies (SCP). \n      IAM allows you to securely manage access to AWS services and resources. Using IAM, you can control who can create or manage AWS resources, \n      the type of resources that can be created, and where they can be created. This minimizes the possibility of resources being created outside \n      of the defined policy. Use the roles and groups created previously and assign IAM policies to enforce the correct usage. SCP offers central \n      control over the maximum available permissions for all accounts in your organization, keeping your accounts stay within your access control \n      guidelines. SCPs are available only in an organization that has all features turned on, and you can configure the SCPs to either deny or allow \n      actions for member accounts by default. For more details on implementing access management, see the Well-Architected Security Pillar whitepaper. \n    \n    \n      Governance can also be implemented through management of AWS service quotas. By ensuring\n      service quotas are set with minimum overhead and accurately maintained, you can minimize\n      resource creation outside of your organization’s requirements. To achieve this, you must\n      understand how quickly your requirements can change, understand projects in progress (both\n      creation and decommission of resources), and factor in how fast quota changes can be\n      implemented. Service\n        quotas can be used to increase your quotas when required.\n    \n    Implementation steps\n    \n       \n       \n       \n    \n         Implement notifications on spend: Using your defined\n          organization policies, create AWS Budgets to notify you when spending is\n          outside of your policies. Configure multiple cost budgets, one for each account, which\n          notify you about overall account spending. Configure additional cost budgets within\n          each account for smaller units within the account. These units vary depending on your\n          account structure. Some common examples are AWS Regions, workloads (using tags), or\n          AWS services. Configure an email distribution list as the recipient for\n          notifications, and not an individual's email account. You can configure an actual budget\n          for when an amount is exceeded, or use a forecasted budget for notifying on forecasted\n          usage. You can also preconfigure AWS Budget Actions that can enforce specific IAM or SCP \n          policies, or stop target Amazon EC2 or Amazon RDS instances. Budget Actions can be started automatically \n          or require workflow approval.\n      \n      \n        Implement notifications on anomalous spend: Use AWS Cost Anomaly Detection \n        to reduce your surprise costs in your organization and analyze root cause of potential anomalous spend. \n        Once you create cost monitor to identify unusual spend at your specified granularity and configure \n        notifications in AWS Cost Anomaly Detection, it sends you alert when unusual spend is detected. \n        This will allow you to analyze root case behind the anomaly and understand the impact on your cost. \n        Use AWS Cost Categories while configuring AWS Cost Anomaly Detection to identify which project \n        team or business unit team can analyze the root cause of the unexpected cost and take timely necessary actions.  \n      \n      \n         Implement controls on usage: Using your defined\n          organization policies, implement IAM policies and roles to specify which actions users\n          can perform and which actions they cannot. Multiple organizational policies may be\n          included in an AWS policy. In the same way that you defined policies, start broadly and\n          then apply more granular controls at each step. Service limits are also an effective\n          control on usage. Implement the correct service limits on all your accounts. \n        \n      \n   \n\n  Resources\n\n      \n    \n      Related documents:\n    \n    \n       \n       \n       \n       \n       \n       \n    \n        \n          AWS           managed policies for job functions\n        \n      \n        \n          AWS           multiple account billing strategy\n        \n      \n        \n          Control\n          access to AWS Regions using IAM policies\n        \n      \n        \n          AWS Budgets\n        \n      \n        \n          AWS Cost Anomaly Detection\n        \n      \n        \n          Control Your AWS Costs\n        \n      \n     \n      Related videos:\n    \n    \n       \n    \n        \n          How can I use AWS Budgets to track my spending and usage\n        \n      \n    \n      Related examples:\n    \n    \n       \n       \n       \n       \n       \n       \n    \n        \n          Example IAM access management policies \n        \n      \n        \n          Example service control policies\n        \n      \n        \n          AWS Budgets Actions\n        \n      \n        \n          Create IAM Policy to control access to Amazon EC2 resources using Tags\n        \n      \n        \n          Restrict the access of IAM Identity to specific Amazon EC2 resources\n        \n      \n        \n          Slack integrations for Cost Anomaly Detection using Amazon Q Developer in chat applications\n        \n      \n  \u003cdiv\u003e\u003cdiv\u003e\u003cdiv\u003e\u003cdiv id=\"js_error_message\"\u003e\u003cp\u003e\u003cimg src=\"https://d1ge0kk1l5kms0.cloudfront.net/images/G/01/webservices/console/warning.png\" alt=\"Warning\" /\u003e \u003cstrong\u003eJavascript is disabled or is unavailable in your browser.\u003c/strong\u003e\u003c/p\u003e\u003cp\u003eTo use the Amazon Web Services Documentation, Javascript must be enabled. Please refer to your browser's Help pages for instructions.\u003c/p\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003eDocument ConventionsCOST02-BP04 Implement groups and rolesCOST02-BP06 Track project lifecycle",
  "COST02-BP06 Track project lifecycle\n    Track, measure, and audit the lifecycle of projects, teams, and\n    environments to avoid using and paying for unnecessary resources.\n  \n    Level of risk exposed if this best practice\n      is not established: Low\n  \n    \n    Implementation guidance\n    \n    \n    \n      By effectively tracking the project lifecycle, organizations can\n      achieve better cost control through enhanced planning, management,\n      and resource optimization. The insights gained through tracking\n      are invaluable for making informed decisions that contribute to\n      the cost-effectiveness and overall success of the project.\n    \n    \n      Tracking the entire lifecycle of the workload helps you understand\n      when workloads or workload components are no longer required. The\n      existing workloads and components may appear to be in use, but\n      when AWS releases new services or features, they can be\n      decommissioned or adopted. Check the previous stages of workloads.\n      After a workload is in production, previous environments can be\n      decommissioned or greatly reduced in capacity until they are\n      required again.\n    \n    \n      You can tag resources with a timeframe or reminder to pin the time\n      that the workload was reviewed. For example, if the development\n      environment was last reviewed months ago, it could be a good time\n      to review it again to explore if new services can be adopted or if\n      the environment is in use. You can group and tag your applications\n      with\n      myApplications\n      on AWS to manage and track metadata such as criticality,\n      environment, last reviewed, and cost center. You can both track\n      your workload's lifecycle and monitor and manage the cost, health,\n      security posture, and performance of your applications.\n    \n    \n      AWS provides various management and governance services you can\n      use for entity lifecycle tracking. You can use\n      AWS Config or\n      AWS Systems Manager to provide a detailed inventory\n      of your AWS resources and configuration. It is recommended that\n      you integrate with your existing project or asset management\n      systems to keep track of active projects and products within your\n      organization. Combining your current system with the rich set of\n      events and metrics provided by AWS allows you to build a view of\n      significant lifecycle events and proactively manage resources to\n      reduce unnecessary costs.\n    \n    \n      Similar to\n      Application\n        Lifecycle Management (ALM), tracking project lifecycle\n      should involve multiple processes, tools, and teams working\n      together, such as design and development, testing, production,\n      support, and workload redundancy.\n    \n    \n      By carefully monitoring each phase of a project's lifecycle,\n      organizations gain crucial insights and enhanced control,\n      facilitating successful project planning, implementation, and\n      completion. This careful oversight verifies that projects not only\n      meet quality standards, but are delivered on time and within\n      budget, promoting overall cost efficiency.\n    \n    \n      For more details on implementing entity lifecycle tracking, see\n      AWS       Well-Architected Operational Excellence Pillar\n        whitepaper.\n    \n     \n      \n      Implementation steps\n      \n      \n      \n         \n         \n      \n          \n            Establish project lifecycle\n              monitoring process:\n            The\n              Cloud Center of Excellence team must establish\n            project lifecycle monitoring process. Establish a structured\n            and systematic approach to monitoring workloads in order to\n            improve control, visibility, and performance of the\n            projects. Make the monitoring process transparent,\n            collaborative, and focused on continuous improvement to\n            maximize its effectiveness and value.\n          \n        \n          \n            Perform workload reviews:\n            As defined by your organizational policies, set up a regular\n            cadence to audit your existing projects and perform workload\n            reviews. The amount of effort spent in the audit should be\n            proportional to the approximate risk, value, or cost to the\n            organization. Key areas to include in the audit would be\n            risk to the organization of an incident or outage, value, or\n            contribution to the organization (measured in revenue or\n            brand reputation), cost of the workload (measured as total\n            cost of resources and operational costs), and usage of the\n            workload (measured in number of organization outcomes per\n            unit of time). If these areas change over the lifecycle,\n            adjustments to the workload are required, such as full or\n            partial decommissioning.\n          \n        \n     \n   \n    \n    Resources\n    \n    \n    \n      Related documents:\n    \n    \n       \n       \n       \n    \n        \n          Guidance\n            for Tagging on AWS\n        \n      \n        \n          What\n            Is ALM (Application Lifecycle Management)?\n        \n      \n        \n          AWS           managed policies for job functions\n        \n      \n    \n      Related examples:\n    \n    \n       \n    \n        \n          Control\n            access to AWS Regions using IAM policies\n        \n      \n    \n      Related Tools\n    \n    \n       \n       \n       \n       \n       \n    \n        \n          AWS Config\n        \n      \n        \n          AWS Systems Manager\n        \n      \n        \n          AWS Budgets\n        \n      \n        \n          AWS Organizations\n        \n      \n        \n          AWS CloudFormation\n        \n      \n  \u003cdiv\u003e\u003cdiv\u003e\u003cdiv\u003e\u003cdiv id=\"js_error_message\"\u003e\u003cp\u003e\u003cimg src=\"https://d1ge0kk1l5kms0.cloudfront.net/images/G/01/webservices/console/warning.png\" alt=\"Warning\" /\u003e \u003cstrong\u003eJavascript is disabled or is unavailable in your browser.\u003c/strong\u003e\u003c/p\u003e\u003cp\u003eTo use the Amazon Web Services Documentation, Javascript must be enabled. Please refer to your browser's Help pages for instructions.\u003c/p\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003eDocument ConventionsCOST02-BP05 Implement cost controlsCOST 3. How do you monitor your cost and usage?",
  "COST03-BP01 Configure detailed information sourcesSet up cost management and reporting tools for enhanced analysis and transparency of cost and usage data. Configure your workload to create log entries that facilitate the tracking and segregation of costs and usage.\n    Level of risk exposed if this best practice\n    is not established: High\n  \n    \n    Implementation guidance\n    \n    \n      Detailed billing information such as hourly granularity in cost management tools allow organizations to track their consumptions with further details and help them to identify some of the cost increase reasons. These data sources provide the most accurate view of cost and usage across your entire organization.\n    \n    \n      You can use AWS Data Exports to create exports of the AWS Cost and Usage Report (CUR) 2.0. This is the new and recommended way to receive your detailed cost and usage data from AWS. It provides daily or hourly usage granularity, rates, costs, and usage attributes for all chargeable AWS services (the same information as CUR), along with some improvements. All possible dimensions are in the CUR such as tagging, location, resource attributes, and account IDs.\n    \n    \n      There are three export types based on the type of export you want to create: a standard data export, an export to a cost and usage dashboard with QuickSight integration, or a legacy data export. \n    \n    \n       \n       \n       \n    \n        \n          Standard data export: A customized export of a table that delivers to Amazon S3 on a recurring basis.\n        \n      \n        \n          Cost and usage dashboard: An export and integration to QuickSight to deploy a pre-built cost and usage dashboard.\n        \n      \n        \n          Legacy data export: An export of the legacy AWS Cost and Usage Report (CUR).\n        \n      \n    \n      You can create data exports with the following customizations:\n    \n    \n       \n       \n       \n       \n       \n    \n        \n          Include resource IDs\n        \n      \n        \n         Split cost allocation data\n        \n      \n        \n          Hourly granularity\n        \n      \n        \n          Versioning\n        \n      \n        \n          Compression type and file format\n        \n      \n    \n      For your workloads that run containers on Amazon ECS or Amazon EKS, enable split cost allocation data so that you can allocate your container costs to individual business units and teams, based on how your container workloads consume shared compute and memory resources. Split cost allocation data introduces cost and usage data for new container-level resources to AWS Cost and Usage Report. Split cost allocation data is calculated by computing the cost of individual ECS services and tasks running on the cluster.\n    \n    \n      A cost and usage dashboard exports the cost and usage dashboard table to an S3 bucket on a recurring basis and deploys a prebuilt cost and usage dashboard to QuickSight. Use this option if you want to quickly deploy a dashboard of your cost and usage data without the ability for customization.\n    \n    \n      If desired, you can still export CUR in legacy mode, where you can integrate other processing services such as AWS Glue to prepare the data for analysis and perform data analysis with Amazon Athena using SQL to query the data.\n    \n     \n      \n      Implementation steps\n      \n      \n      \n         \n         \n         \n         \n      \n          \n            Create data exports: Create customized exports with the data you want and control the schema of your exports. Create billing and cost management data exports using basic SQL, and visualize your billing and cost management data by integrating with QuickSight. You can also export your data in standard mode to analyze your data with other processing tools like Amazon Athena.\n          \n        \n          \n            Configure the cost and usage\n              report: Using the billing console, configure at\n            least one cost and usage report. Configure a report with\n            hourly granularity that includes all identifiers and\n            resource IDs. You can also create other reports with\n            different granularities to provide higher-level summary\n            information.\n          \n        \n          \n            Configure hourly granularity in Cost Explorer: To access cost and usage data with hourly granularity for the past 14 days, consider enabling hourly and resource level data in the billing console.\n          \n        \n          \n            Configure application\n              logging: Verify that your application logs each\n            business outcome that it delivers so it can be tracked and\n            measured. Ensure that the granularity of this data is at\n            least hourly so it matches with the cost and usage data. For\n            more details on logging and monitoring,\n            see Well-Architected\n              Operational Excellence Pillar.\n          \n        \n     \n   \n    \n    Resources\n    \n    \n    \n      Related documents:\n    \n    \n       \n       \n       \n       \n       \n       \n       \n       \n    \n        \n          AWS Data Exports\n        \n      \n        \n          AWS Glue\n        \n      \n        \n          QuickSight\n        \n      \n        \n          AWS           Cost Management Pricing\n        \n      \n        \n          Tagging\n            AWS resources\n        \n      \n        \n          Analyzing\n            your costs with Cost Explorer\n        \n      \n        \n          Managing\n            AWS Cost and Usage Reports\n        \n      \n        \n          Well-Architected\n            Operational Excellence Pillar\n        \n      \n    \n      Related examples:\n    \n    \n       \n       \n       \n    \n        \n          AWS           Account Setup\n        \n      \n        \n          Data Exports for AWS Billing and Cost Management\n        \n      \n        \n          AWS Cost Explorer Common Use Cases\n        \n      \n  \u003cdiv\u003e\u003cdiv\u003e\u003cdiv\u003e\u003cdiv id=\"js_error_message\"\u003e\u003cp\u003e\u003cimg src=\"https://d1ge0kk1l5kms0.cloudfront.net/images/G/01/webservices/console/warning.png\" alt=\"Warning\" /\u003e \u003cstrong\u003eJavascript is disabled or is unavailable in your browser.\u003c/strong\u003e\u003c/p\u003e\u003cp\u003eTo use the Amazon Web Services Documentation, Javascript must be enabled. Please refer to your browser's Help pages for instructions.\u003c/p\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003eDocument ConventionsCOST 3. How do you monitor your cost and usage?COST03-BP02 Add organization information to cost and\n  usage",
  "COST03-BP02 Add organization information to cost and\n  usageDefine a tagging schema based on your organization, workload attributes, and cost allocation categories \n    so that you can filter and search for resources or monitor cost and usage in cost management tools. Implement \n    consistent tagging across all resources where possible by purpose, team, environment, or other criteria \n    relevant to your business. \n    Level of risk exposed if this best practice\n    is not established: Medium\n  \n\n  Implementation guidance\n    Implement tagging in AWS to add organization information to your resources, which will then\n      be added to your cost and usage information. A tag is a key-value pair — the key is defined and\n      must be unique across your organization, and the value is unique to a group of resources. An\n      example of a key-value pair is the key is Environment, with a value of Production. All\n      resources in the production environment will have this key-value pair. Tagging allows you\n      categorize and track your costs with meaningful, relevant organization information. You can\n      apply tags that represent organization categories (such as cost centers, application names,\n      projects, or owners), and identify workloads and characteristics of workloads (such as test\n      or production) to attribute your costs and usage throughout your organization.\n    When you apply tags to your AWS resources (such as Amazon Elastic Compute Cloud instances or Amazon Simple Storage Service buckets)\n      and activate the tags, AWS adds this information to your Cost and Usage Reports. You can\n      run reports and perform analysis on tagged and untagged resources to allow greater\n      compliance with internal cost management policies and ensure accurate attribution.\n    Creating and implementing an AWS tagging standard across your organization’s accounts\n      helps you manage and govern your AWS environments in a consistent and uniform manner.\n      Use Tag Policies \n      in AWS Organizations to define rules for how tags can be used on AWS resources in your accounts in AWS Organizations. \n      Tag Policies allow you to easily adopt a standardized approach for tagging AWS resources\n    AWS Tag\n        Editor allows you to add, delete, and manage tags of multiple resources.\n      With Tag Editor, you search for the resources that you want to tag, and then manage tags \n      for the resources in your search results.\n    AWS Cost\n        Categories allows you to assign organization meaning to your costs, without\n      requiring tags on resources. You can map your cost and usage information to unique internal\n      organization structures. You define category rules to map and categorize costs using billing\n      dimensions, such as accounts and tags. This provides another level of management capability in\n      addition to tagging. You can also map specific accounts and tags to multiple projects.\n    Implementation steps\n      \n    \n       \n       \n       \n       \n       \n    \n        \n          Define a tagging schema: Gather all stakeholders from\n          across your business to define a schema. This typically includes people in technical,\n          financial, and management roles. Define a list of tags that all resources must have, as\n          well as a list of tags that resources should have. Verify that the tag names and values\n          are consistent across your organization. \n      \n         Tag resources: Using your defined cost attribution\n          categories, place tags on all resources in your workloads according to the categories. Use\n          tools such as the CLI, Tag Editor, or AWS Systems Manager to increase efficiency. \n      \n        \n          Implement AWS Cost Categories: You can create Cost Categories\n          without implementing tagging. Cost categories use the existing cost and usage dimensions.\n          Create category rules from your schema and implement them into cost categories. \n      \n        \n          Automate tagging: To verify that you maintain high levels\n          of tagging across all resources, automate tagging so that resources are automatically\n          tagged when they are created. Use services such as AWS CloudFormation to verify that resources \n          are tagged when created. You can also create a custom solution to tag automatically using\n          Lambda functions or use a microservice that scans the workload periodically and removes any \n          resources that are not tagged, which is ideal for test and development environments. \n      \n         Monitor and report on tagging: To verify that you\n          maintain high levels of tagging across your organization, report and monitor the tags\n          across your workloads. You can use AWS Cost Explorer to view the cost of tagged and untagged\n          resources, or use services such as Tag Editor. Regularly review the number of untagged\n          resources and take action to add tags until you reach the desired level of tagging. \n      \n   \n\n  Resources\n\n      \n    \n      Related documents:\n    \n    \n       \n       \n       \n       \n       \n       \n       \n    \n        \n          Tagging Best Practices\n        \n      \n        \n          AWS CloudFormation Resource Tag\n        \n      \n        \n          AWS           Cost Categories\n        \n      \n        \n          Tagging AWS resources\n        \n      \n        \n          Analyzing\n          your costs with AWS Budgets\n        \n      \n        \n          Analyzing\n          your costs with Cost Explorer\n        \n      \n        \n          Managing\n          AWS Cost and Usage Reports\n        \n      \n    \n      Related videos:\n    \n    \n       \n       \n    \n        \n          How can I tag my AWS resources to divide up my bill by cost center or project\n        \n      \n        \n          Tagging AWS Resources\n        \n      \n  \u003cdiv\u003e\u003cdiv\u003e\u003cdiv\u003e\u003cdiv id=\"js_error_message\"\u003e\u003cp\u003e\u003cimg src=\"https://d1ge0kk1l5kms0.cloudfront.net/images/G/01/webservices/console/warning.png\" alt=\"Warning\" /\u003e \u003cstrong\u003eJavascript is disabled or is unavailable in your browser.\u003c/strong\u003e\u003c/p\u003e\u003cp\u003eTo use the Amazon Web Services Documentation, Javascript must be enabled. Please refer to your browser's Help pages for instructions.\u003c/p\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003eDocument ConventionsCOST03-BP01 Configure detailed information sourcesCOST03-BP03 Identify cost attribution categories",
  "COST03-BP03 Identify cost attribution categories\n    Identify organization categories such as business units, departments\n    or projects that could be used to allocate cost within your\n    organization to the internal consuming entities. Use those\n    categories to enforce spend accountability, create cost awareness\n    and drive effective consumption behaviors.\n  \n    Level of risk exposed if this best practice\n    is not established: High\n  \n    \n    Implementation guidance \n    \n    \n    \n      The process of categorizing costs is crucial in budgeting,\n      accounting, financial reporting, decision making, benchmarking,\n      and project management. By classifying and categorizing expenses,\n      teams can gain a better understanding of the types of costs they\n      incur throughout their cloud journey helping teams make\n      informed decisions and manage budgets effectively.\n    \n    \n      Cloud spend accountability establishes a strong incentive for\n      disciplined demand and cost management. The result is\n      significantly greater cloud cost savings for organizations that\n      allocate most of their cloud spend to consuming business units or\n      teams. Moreover, allocating cloud spend helps organizations\n      adopt more best practices of centralized cloud governance.\n    \n    \n      Work with your finance team and other relevant stakeholders to\n      understand the requirements of how costs must be allocated within\n      your organization during your regular cadence calls. Workload\n      costs must be allocated throughout the entire lifecycle, including\n      development, testing, production, and decommissioning. Understand\n      how the costs incurred for learning, staff development, and idea\n      creation are attributed in the organization. This can be helpful\n      to correctly allocate accounts used for this purpose to training\n      and development budgets instead of generic IT cost budgets.\n    \n    \n      After defining your cost attribution categories with stakeholders\n      in your organization, use\n      AWS       Cost Categories to group your cost and usage information\n      into meaningful categories in the AWS Cloud, such as cost for\n      a specific project, or AWS accounts for departments or business\n      units. You can create custom categories and map your cost and\n      usage information into these categories based on rules you define\n      using various dimensions such as account, tag, service, or charge\n      type. Once cost categories are set up, you can view your cost and\n      usage information by these categories, which allows your organization\n      to make better strategic and purchasing decisions. These\n      categories are visible in AWS Cost Explorer, AWS Budgets, and AWS Cost and Usage Report as well.\n    \n    \n      For example, create cost categories for your business units\n      (DevOps team), and under each category create multiple rules\n      (rules for each sub category) with multiple dimensions (AWS accounts, cost allocation tags, services or charge type) based on\n      your defined groupings. With cost categories, you can organize\n      your costs using a rule-based engine. The rules that you configure\n      organize your costs into categories. Within these rules, you can\n      filter with using multiple dimensions for each category such as\n      specific AWS accounts, AWS services, or charge types. You can then\n      use these categories across multiple products in the\n      AWS Billing and Cost Management and Cost Management\n      console.\n      This includes AWS Cost Explorer, AWS Budgets, AWS Cost and Usage Report, and AWS Cost Anomaly Detection.\n    \n    \n      As an example, the following diagram displays how to group\n      your costs and usage information in your organization by having\n      multiple teams (cost category), multiple environments (rules), and\n      each environment having multiple resources or assets (dimensions).\n    \n    \n    \n       \n        \n       \n       \n      Cost and usage organization chart\n    \n      \n    \n      You can create groupings of costs using cost categories as well.\n      After you create the cost categories (allowing up to 24 hours\n      after creating a cost category for your usage records to be\n      updated with values), they appear in\n      AWS Cost Explorer,\n      AWS Budgets,\n      AWS Cost and Usage Report, and\n      AWS Cost Anomaly Detection. In AWS Cost Explorer and\n      AWS Budgets, a cost category appears as an additional billing\n      dimension. You can use this to filter for the specific cost\n      category value, or group by the cost category.\n    \n     \n      \n      Implementation steps\n      \n      \n      \n         \n         \n         \n      \n          \n            Define your organization\n              categories: Meet with internal stakeholders and\n            business units to define categories that reflect your\n            organization's structure and requirements. These categories\n            should directly map to the structure of existing financial\n            categories, such as business unit, budget, cost center, or\n            department. Look at the outcomes the cloud delivers for your\n            business such as training or education, as these are also\n            organization categories.\n          \n        \n          \n            Define your functional\n              categories: Meet with internal stakeholders and\n            business units to define categories that reflect the\n            functions that you have within your business. This may be\n            the workload or application names, and the type of\n            environment, such as production, testing, or development.\n          \n        \n          \n            Define AWS Cost\n              Categories: Create cost categories to organize\n            your cost and usage information with using\n            AWS Cost Categories and map your AWS cost and\n            usage into\n            meaningful\n              categories. Multiple categories can be assigned to a\n            resource, and a resource can be in multiple different\n            categories, so define as many categories as needed so that\n            you can\n            manage\n              your costs within the categorized structure using AWS\n            Cost Categories.\n          \n        \n     \n   \n    \n    Resources \n    \n    \n    \n      Related documents:\n    \n    \n       \n       \n       \n       \n       \n       \n       \n       \n       \n       \n       \n    \n        \n          Tagging\n            AWS resources\n        \n      \n        \n          Using\n            Cost Allocation Tags\n        \n      \n        \n          Analyzing\n            your costs with AWS Budgets\n        \n      \n        \n          Analyzing\n            your costs with Cost Explorer\n        \n      \n        \n          Managing\n            AWS Cost and Usage Reports\n        \n      \n        \n          AWS           Cost Categories\n        \n      \n        \n          Managing\n            your costs with AWS Cost Categories\n        \n      \n        \n          Creating\n            cost categories\n        \n      \n        \n          Tagging\n            cost categories\n        \n      \n        \n          Splitting\n            charges within cost categories\n        \n      \n        \n          AWS           Cost Categories Features\n        \n      \n    \n      Related examples:\n    \n    \n       \n       \n    \n        \n          Organize\n            your cost and usage data with AWS Cost Categories\n        \n      \n        \n          Managing\n            your costs with AWS Cost Categories\n        \n      \n  \u003cdiv\u003e\u003cdiv\u003e\u003cdiv\u003e\u003cdiv id=\"js_error_message\"\u003e\u003cp\u003e\u003cimg src=\"https://d1ge0kk1l5kms0.cloudfront.net/images/G/01/webservices/console/warning.png\" alt=\"Warning\" /\u003e \u003cstrong\u003eJavascript is disabled or is unavailable in your browser.\u003c/strong\u003e\u003c/p\u003e\u003cp\u003eTo use the Amazon Web Services Documentation, Javascript must be enabled. Please refer to your browser's Help pages for instructions.\u003c/p\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003eDocument ConventionsCOST03-BP02 Add organization information to cost and\n  usageCOST03-BP04 Establish organization metrics",
  "COST03-BP04 Establish organization metrics\n    Establish the organization metrics that are required for this\n    workload. Example metrics of a workload are customer reports\n    produced, or web pages served to customers.\n  \n    Level of risk exposed if this best practice\n    is not established: High\n  \n\n  Implementation guidance\n    Understand how your workload’s output is measured against business success. Each\n      workload typically has a small set of major outputs that indicate performance. If you have a\n      complex workload with many components, then you can prioritize the list, or define and\n      track metrics for each component. Work with your teams to understand which metrics to\n      use. This unit will be used to understand the efficiency of the workload, or the cost for each\n      business output.\n    Implementation steps\n      \n    \n       \n       \n    \n        \n          Define workload outcomes: Meet with the stakeholders in\n          the business and define the outcomes for the workload. These are a primary measure of\n          customer usage and must be business metrics and not technical metrics. There should be a\n          small number of high-level metrics (less than five) per workload. If the workload produces\n          multiple outcomes for different use cases, then group them into a single metric. \n      \n        \n          Define workload component outcomes: Optionally, if you\n          have a large and complex workload, or can easily break your workload into components (such\n          as microservices) with well-defined inputs and outputs, define metrics for each component.\n          The effort should reflect the value and cost of the component. Start with the largest\n          components and work towards the smaller components. \n      \n   \n\n  Resources\n\n      \n    \n      Related documents:\n    \n    \n       \n       \n       \n       \n    \n        \n          Tagging AWS resources\n        \n      \n        \n          Analyzing\n          your costs with AWS Budgets\n        \n      \n        \n          Analyzing\n          your costs with Cost Explorer\n        \n      \n        \n          Managing\n          AWS Cost and Usage Reports\n        \n      \n  \u003cdiv\u003e\u003cdiv\u003e\u003cdiv\u003e\u003cdiv id=\"js_error_message\"\u003e\u003cp\u003e\u003cimg src=\"https://d1ge0kk1l5kms0.cloudfront.net/images/G/01/webservices/console/warning.png\" alt=\"Warning\" /\u003e \u003cstrong\u003eJavascript is disabled or is unavailable in your browser.\u003c/strong\u003e\u003c/p\u003e\u003cp\u003eTo use the Amazon Web Services Documentation, Javascript must be enabled. Please refer to your browser's Help pages for instructions.\u003c/p\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003eDocument ConventionsCOST03-BP03 Identify cost attribution categoriesCOST03-BP05 Configure billing and cost management tools",
  "COST03-BP05 Configure billing and cost management tools\n      Configure cost management tools that meet your\n      organization's policies to manage and optimize cloud spending.\n      This includes services, tools, and resources to organize and track\n      cost and usage data, enhance control through consolidated billing\n      and access permission, improve planning through budgeting and\n      forecasts, receive notifications or alerts, and lower cost with\n      resources and pricing optimizations.\n    \n        Level of risk exposed if this best\n        practice is not established: High\n      \n\n  Implementation guidance \n\n      \n    \n      To establish strong accountability, consider your account strategy\n      first as part of your cost allocation strategy. Get this right,\n      and you may not need to go any further. Otherwise, there can be\n      unawareness and further pain points.\n    \n    \n      To encourage accountability of cloud spend, grant users access to\n      tools that provide visibility into their costs and usage. AWS\n      recommends that you configure all workloads and teams for the\n      following purposes:\n    \n    \n       \n       \n       \n       \n       \n       \n       \n       \n       \n       \n       \n    \n        \n          Organize: Establish your\n          cost allocation and governance baseline with your own tagging\n          strategy and taxonomy. Create multiple AWS Accounts with tools\n          such as AWS Control Tower or AWS Organization. Tag the\n          supported AWS resources and categorize them meaningfully based\n          on your organization structure (business units, departments,\n          or projects). Tag account names for specific cost centers and\n          map them with AWS Cost Categories to group accounts for\n          business units to their cost centers so that business unit\n          owner can see multiple accounts' consumption in one place.\n        \n      \n        \n          Access: Track\n          organization-wide billing information in consolidated billing.\n          Verify the right stakeholders and business owners have access.\n        \n      \n        \n          Control: Build effective\n          governance mechanisms with the right guardrails to prevent\n          unexpected scenarios when using Service Control Policies\n          (SCP), tag policies, IAM policies and budget alerts. For\n          example, you can allow teams to create specific resources in\n          preferred regions only by using effective control mechanisms\n          and prevent resource creations without specific tag (such as\n          cost-center).\n        \n      \n        \n          Current state: Configure a\n          dashboard that shows current levels of cost and usage. The\n          dashboard should be available in a highly visible place within\n          the work environment like an operations dashboard. You can\n          export data and use the Cost and Usage Dashboard from the AWS\n          Cost Optimization Hub or any supported product to create this\n          visibility. You may need to create different dashboards for\n          different personas. For example, manager dashboard may differ\n          from an engineering dashboard.\n        \n      \n        \n          Notifications: Provide\n          notifications when cost or usage exceeds defined limits and\n          anomalies occur with AWS Budgets or AWS Cost Anomaly\n          Detection.\n        \n      \n        \n          Reports: Summarize all cost\n          and usage information. Raise awareness and accountability of\n          your cloud spend with detailed, attributable cost data. Create\n          reports that are relevant to the team consuming them and\n          contain recommendations.\n        \n      \n        \n          Tracking: Show the current\n          cost and usage against configured goals or targets.\n        \n      \n        \n          Analysis: Allow team\n          members to perform custom and deep analysis down to the\n          hourly, daily or monthly granularity with different filters\n          (resource, account, tag, etc.).\n        \n      \n        \n          Inspect: Stay up to date\n          with your resource deployment and cost optimization\n          opportunities. Get notifications using Amazon CloudWatch,\n          Amazon SNS, or Amazon SES for resource deployments at the\n          organization level. Review cost optimization recommendations\n          with AWS Trusted Advisor or AWS Compute Optimizer.\n        \n      \n        \n          Trend reports: Display the\n          variability in cost and usage over the required period with\n          the required granularity.\n        \n      \n        \n          Forecasts: Show estimated\n          future costs, estimate your resource usage, and spend with\n          forecast dashboards you create.\n        \n      \n    \n      You can use\n      AWS       Cost Optimization Hub to understand potential cost-saving\n      opportunities consolidated from a centralized location and create\n      data exports for integration with Amazon Athena. You can also use\n      the AWS Cost Optimization Hub to deploy the Cost and Usage\n      Dashboard, which utilizes QuickSight for interactive cost\n      analysis and secure cost insight sharing.\n    \n    \n      If you don't have essential skills or bandwidth in your\n      organization, you can work with\n      AWS       ProServ,\n      AWS Managed Services (AMS), or\n      AWS       Partners. You can also use third-party tools but\n      ensure you validate the value proposition.\n    \n     \n\n  Implementation steps \n\n      \n      \n         \n         \n         \n         \n         \n         \n         \n      \n          \n            Allow team-based access to\n            tools: Configure your accounts and create groups\n            that have access to the required cost and usage reports for\n            their consumptions and use\n            AWS Identity and Access Management to\n            control\n            access to the tools such as AWS Cost Explorer. These groups must include representatives from all\n            teams that own or manage an application. This certifies that\n            every team has access to their cost and usage information to\n            track their consumption.\n          \n        \n          \n            Organize Costs Tags and\n            Categories: organize your costs across teams,\n            business units, applications, environments, and projects.\n            Use resource tags to organize costs, by cost allocation\n            tags. Create Cost Categories based on the dimensions with\n            using tags, accounts, services, etc. to map your costs.\n          \n        \n          \n            Configure AWS Budgets:\n            Configure\n            AWS Budgets on all accounts for your\n            workloads. Set budgets for the overall account spend, and\n            budgets for the workloads by using tags and cost categories.\n            Configure notifications in AWS Budgets to receive alerts for\n            when you exceed your budgeted amounts, or when your\n            estimated costs exceed your budgets.\n          \n        \n          \n            Configure AWS Cost Anomaly\n            Detection: Use\n            AWS             Cost Anomaly Detection for your accounts,\n            core services or cost categories you created to monitor your\n            cost and usage and detect unusual spends. You can receive\n            alerts individually in aggregated reports and receive alerts\n            in an email or an Amazon SNS topic which allows you to\n            analyze and determine the root cause of the anomaly and\n            identify the factor that is driving the cost increase.\n          \n        \n          \n            Use cost analysis tools:\n            Configure\n            AWS Cost Explorer for your workload and\n            accounts to visualize your cost data for further analysis.\n            Create a dashboard for the workload that tracks overall\n            spend, key usage metrics for the workload, and forecast of\n            future costs based on your historical cost data.\n          \n        \n          \n            Use cost-saving analysis\n            tools: Use AWS Cost Optimization Hub to identify\n            savings opportunities with tailored recommendations\n            including deleting unused resources, rightsizing, savings\n            Plans, reservations and compute optimizer recommendations.\n          \n        \n          \n            Configure advanced tools:\n            You can optionally create visuals to facilitate interactive\n            analysis and sharing of cost insights. With Data Exports on\n            AWS Cost Optimization Hub, you can create cost and usage\n            dashboard powered by QuickSight for your organization that\n            provides additional detail and granularity. You can also\n            implement advanced analysis capability with using data\n            exports in\n            Amazon Athena for advanced queries, and create\n            dashboards on\n            QuickSight. Work with\n            AWS             Partners to adopt cloud management\n            solutions for consolidated cloud bill monitoring and\n            optimization.\n          \n        \n     \n   \n\n  Resources \n\n      \n    \n      Related documents:\n    \n    \n       \n       \n       \n       \n       \n       \n       \n       \n    \n        \n          What\n          is AWS Billing and Cost Management and Cost Management?\n        \n      \n        \n          Establishing\n          your best practice AWS environment\n        \n      \n        \n          Best\n          Practices for Tagging AWS Resources\n        \n      \n        \n          Tagging\n          your AWS resources\n        \n      \n        \n          AWS           Cost Categories \n        \n      \n        \n          Analyzing\n          your costs with AWS Budgets\n        \n      \n        \n          Analyzing\n          your costs with AWS Cost Explorer\n        \n      \n        \n          What\n          is AWS Data Exports?\n        \n      \n    \n      Related videos:\n    \n    \n       \n       \n    \n        \n          Deploying\n          Cloud Intelligence Dashboards \n        \n      \n        \n          Get\n          Alerts on any FinOps or Cost Optimization Metric or\n          KPI \n        \n      \n    \n      Related examples:\n    \n    \n       \n       \n    \n        \n          Cost\n          and Usage Dashboard powered by QuickSight\n        \n      \n        \n          AWS           Cost and Usage Governance Workshop\n        \n      \n  \u003cdiv\u003e\u003cdiv\u003e\u003cdiv\u003e\u003cdiv id=\"js_error_message\"\u003e\u003cp\u003e\u003cimg src=\"https://d1ge0kk1l5kms0.cloudfront.net/images/G/01/webservices/console/warning.png\" alt=\"Warning\" /\u003e \u003cstrong\u003eJavascript is disabled or is unavailable in your browser.\u003c/strong\u003e\u003c/p\u003e\u003cp\u003eTo use the Amazon Web Services Documentation, Javascript must be enabled. Please refer to your browser's Help pages for instructions.\u003c/p\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003eDocument ConventionsCOST03-BP04 Establish organization metricsCOST03-BP06 Allocate costs based on workload metrics",
  "COST03-BP06 Allocate costs based on workload metrics\n    Allocate the workload's costs based on usage metrics or business outcomes to measure workload cost efficiency. Implement a process to analyze the cost and usage data with analytics services, which can provide insight and charge back capability.\n  \n    Level of risk exposed if this best practice\n    is not established: Low\n  \n\n  Implementation guidance\n    \n      Cost optimization means delivering business outcomes at the lowest price point, which can only be achieved by allocating workload costs based on workload metrics (measured by workload efficiency). Monitor the defined workload metrics through log files or other application monitoring. Combine this data with the workload’s costs, which can be obtained by looking at costs with a specific tag value or account ID. Perform this analysis at the hourly level. Your efficiency typically changes if you have static cost components (for example, a backend database running permanently) with a varying request rate (for example, usage peaks at nine in the morning to five in the evening, with few requests at night). Understanding the relationship between the static and variable costs helps you focus your optimization activities. \n    \n    \n      Creating workload metrics for shared resources may be challenging compared to resources like containerized applications on Amazon Elastic Container Service (Amazon ECS) and Amazon API Gateway. However, there are certain ways you can categorize usage and track cost. If you need to track Amazon ECS and AWS Batch shared resources, you can enable split cost allocation data in AWS Cost Explorer. With split cost allocation data, you can understand and optimize the cost and usage of your containerized applications and allocate application costs back to individual business entities based on how shared compute and memory resources are consumed. \n    \n     \n      \n      Implementation steps\n      \n         \n      \n          \n            Allocate costs to workload metrics: Using the defined metrics and configured tags, create a metric that combines the workload output and workload cost. Use analytics services such as Amazon Athena and Amazon QuickSight to create an efficiency dashboard for the overall workload and any components.\n          \n        \n     \n   \n\n  Resources\n\n      \n    \n      Related documents:\n    \n    \n       \n       \n       \n       \n    \n        \n          Tagging AWS resources\n          \n        \n      \n        \n          Analyzing\n          your costs with AWS Budgets\n        \n      \n        \n          Analyzing\n          your costs with Cost Explorer\n        \n      \n        \n          Managing\n          AWS Cost and Usage Reports\n        \n      \n    \n      Related examples:\n    \n    \n       \n    \n        \n          Improve cost visibility of Amazon ECS and AWS Batch with AWS Split Cost Allocation Data\n        \n      \n    \n  \u003cdiv\u003e\u003cdiv\u003e\u003cdiv\u003e\u003cdiv id=\"js_error_message\"\u003e\u003cp\u003e\u003cimg src=\"https://d1ge0kk1l5kms0.cloudfront.net/images/G/01/webservices/console/warning.png\" alt=\"Warning\" /\u003e \u003cstrong\u003eJavascript is disabled or is unavailable in your browser.\u003c/strong\u003e\u003c/p\u003e\u003cp\u003eTo use the Amazon Web Services Documentation, Javascript must be enabled. Please refer to your browser's Help pages for instructions.\u003c/p\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003eDocument ConventionsCOST03-BP05 Configure billing and cost management toolsCOST 4. How do you decommission resources?",
  "COST04-BP01 Track resources over their lifetime\n    Define and implement a method to track resources and their\n    associations with systems over their lifetime. You can use tagging\n    to identify the workload or function of the resource.\n  \n    Level of risk exposed if this best practice\n    is not established: High\n  \n\n  Implementation guidance\n\n    Decommission workload resources that are no longer required. A common example is resources\n      used for testing: after testing has been completed, the resources can be removed. Tracking\n      resources with tags (and running reports on those tags) can help you identify assets for\n      decommission, as they will not be in use or the license on them will expire. Using tags is an effective way to track resources, by labeling the resource with\n      its function, or a known date when it can be decommissioned. Reporting can then be run on\n      these tags. Example values for feature tagging are feature-X testing to identify the purpose\n      of the resource in terms of the workload lifecycle. Another example is using LifeSpan or TTL for the resources, \n      such as to-be-deleted tag key name and value to define the time period or specific time for decommissioning.  \n    Implementation steps\n    \n       \n       \n       \n    \n         Implement a tagging scheme: Implement a tagging\n          scheme that identifies the workload the resource belongs to, verifying that all resources\n          within the workload are tagged accordingly. Tagging helps you categorize resources by purpose, \n          team, environment, or other criteria relevant to your business. For more detail on tagging \n          uses cases, strategies, and techniques, see AWS Tagging Best Practices.\n      \n         Implement workload throughput or output monitoring:\n          Implement workload throughput monitoring or alarming, initiating on either\n          input requests or output completions. Configure it to provide notifications when workload\n          requests or outputs drop to zero, indicating the workload resources are no longer used.\n          Incorporate a time factor if the workload periodically drops to zero under normal\n          conditions. For more detail on unused or underutilized resources, see \n          AWS Trusted Advisor Cost Optimization checks.\n      \n        \n          Group AWS resources: Create groups for AWS resources. \n          You can use AWS Resource Groups to organize and manage your AWS resources that are in the \n          same AWS Region. You can add tags to most of your resources to help identify and sort your \n          resources within your organization. Use Tag Editor add tags to supported resources in bulk. \n          Consider using AWS Service Catalog to create, manage, and distribute portfolios of approved \n          products to end users and manage the product lifecycle. \n        \n      \n   \n\n  Resources\n\n      \n    \n      Related documents:\n    \n    \n       \n       \n       \n       \n       \n    \n        \n          AWS Auto Scaling\n        \n      \n        \n          AWS Trusted Advisor\n        \n      \n        \n          AWS Trusted Advisor Cost Optimization Checks\n        \n      \n        \n          Tagging AWS resources\n        \n      \n        \n          Publishing\n          Custom Metrics\n        \n      \n    \n      Related videos:\n    \n    \n       \n    \n        \n          How to optimize costs using AWS Trusted Advisor\n        \n      \n    \n      Related examples:\n    \n    \n       \n       \n    \n        \n          Organize AWS resources\n        \n      \n        \n          Optimize cost using AWS Trusted Advisor\n        \n      \n  \u003cdiv\u003e\u003cdiv\u003e\u003cdiv\u003e\u003cdiv id=\"js_error_message\"\u003e\u003cp\u003e\u003cimg src=\"https://d1ge0kk1l5kms0.cloudfront.net/images/G/01/webservices/console/warning.png\" alt=\"Warning\" /\u003e \u003cstrong\u003eJavascript is disabled or is unavailable in your browser.\u003c/strong\u003e\u003c/p\u003e\u003cp\u003eTo use the Amazon Web Services Documentation, Javascript must be enabled. Please refer to your browser's Help pages for instructions.\u003c/p\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003eDocument ConventionsCOST 4. How do you decommission resources?COST04-BP02 Implement a decommissioning process",
  "COST04-BP02 Implement a decommissioning process\n    Implement a process to identify and decommission unused resources.\n  \n    Level of risk exposed if this best practice\n    is not established: High\n  \n\n  Implementation guidance\n    \n    Implement a standardized process across your organization to identify and remove unused\n      resources. The process should define the frequency searches are performed and the\n      processes to remove the resource to verify that all organization requirements are met.\n    Implementation steps\n    \n       \n    \n        \n          Create and implement a decommissioning process: Work with the workload \n          developers and owners to build a decommissioning process for the workload and its resources. The process \n          should cover the method to verify if the workload is in use, and also if each of the workload resources \n          are in use. Detail the steps necessary to decommission the resource, removing them from service while \n          ensuring compliance with any regulatory requirements. Any associated resources should be included, such \n          as licenses or attached storage. Notify the workload owners that the decommissioning process has been \n          started. \n        \n        \n          Use the following decommission steps to guide you on what should be checked as part of your process:\n        \n        \n           \n           \n           \n           \n           \n           \n        \n            \n              Identify resources to be decommissioned: Identify resources that are eligible \n              for decommissioning in your AWS Cloud. Record all necessary information and schedule the decommission. In your \n              timeline, be sure to account for if (and when) unexpected issues arise during the process.\n            \n          \n            \n              Coordinate and communicate: Work with workload owners to confirm the resource to be decommissioned\n            \n          \n            \n              Record metadata and create backups: Record metadata (such as public IPs, Region, \n              AZ, VPC, Subnet, and Security Groups) and create backups (such as Amazon Elastic Block Store snapshots or taking AMI, keys export, and \n              Certificate export) if it is required for the resources in the production environment or if they are critical resources. \n            \n          \n            \n              Validate infrastructure-as-code: Determine whether resources were deployed \n              with AWS CloudFormation, Terraform, AWS Cloud Development Kit (AWS CDK), or any other infrastructure-as-code deployment tool so they can be re-deployed if necessary. \n            \n          \n            \n              Prevent access: Apply restrictive controls for a period of time, to prevent \n              the use of resources while you determine if the resource is required. Verify that the resource environment \n              can be reverted to its original state if required. \n            \n          \n            \n              Follow your internal decommissioning process: Follow the administrative tasks and \n              decommissioning process of your organization, like removing the resource from your organization domain, removing the \n              DNS record, and removing the resource from your configuration management tool, monitoring tool, automation tool and \n              security tools.\n            \n          \n        \n          If the resource is an Amazon EC2 instance, consult the following list.\n          For more detail, see How do I delete or terminate my Amazon EC2 resources? \n        \n        \n           \n           \n           \n           \n           \n           \n           \n        \n            \n              Stop or terminate all your Amazon EC2 instances and load balancers. \n              Amazon EC2 instances are visible in the console for a short time after \n              they're terminated. You aren't billed for any instances that aren't \n              in the running state\n            \n          \n            \n              Delete your Auto Scaling infrastructure.\n            \n          \n            \n              Release all Dedicated Hosts.\n            \n          \n            \n              Delete all Amazon EBS volumes and Amazon EBS snapshots.\n            \n          \n            \n              Release all Elastic IP addresses.\n            \n          \n            \n              Deregister all Amazon Machine Images (AMIs).\n            \n          \n            \n              Terminate all AWS Elastic Beanstalk environments.   \n            \n          \n        \n          If the resource is an object in Amazon S3 Glacier storage and if you delete an archive before meeting the minimum \n          storage duration, you will be charged a prorated early deletion fee. Amazon S3 Glacier minimum storage duration \n          depends on the storage class used. For a summary of minimum storage duration for each storage class, see \n          Performance \n            across the Amazon S3 storage classes. For detail on how early deletion fees are calculated, see Amazon S3 pricing.\n        \n      \n    \n      The following simple decommissioning process flowchart outlines the decommissioning steps. \n      Before decommissioning resources, verify that resources you have identified for decommissioning \n      are not being used by the organization. \n    \n    \n       \n        \n       \n       \n      Resource decommissioning flow.\n    \n      \n   \n\n  Resources\n\n      \n    \n      Related documents:\n    \n    \n       \n       \n       \n    \n        \n          AWS Auto Scaling\n        \n      \n        \n          AWS Trusted Advisor\n        \n      \n        \n          AWS CloudTrail\n        \n      \n    \n      Related videos:\n    \n    \n       \n       \n    \n        \n          Delete CloudFormation stack but retain some resources\n        \n      \n        \n          Find out which user launched Amazon EC2 instance\n        \n      \n    \n      Related examples:\n    \n    \n       \n       \n    \n        \n          Delete or terminate Amazon EC2 resources\n        \n      \n        \n          Find out which user launched an Amazon EC2 instance\n        \n      \n  \u003cdiv\u003e\u003cdiv\u003e\u003cdiv\u003e\u003cdiv id=\"js_error_message\"\u003e\u003cp\u003e\u003cimg src=\"https://d1ge0kk1l5kms0.cloudfront.net/images/G/01/webservices/console/warning.png\" alt=\"Warning\" /\u003e \u003cstrong\u003eJavascript is disabled or is unavailable in your browser.\u003c/strong\u003e\u003c/p\u003e\u003cp\u003eTo use the Amazon Web Services Documentation, Javascript must be enabled. Please refer to your browser's Help pages for instructions.\u003c/p\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003eDocument ConventionsCOST04-BP01 Track resources over their lifetimeCOST04-BP03 Decommission resources",
  "COST04-BP03 Decommission resources\n    Decommission resources initiated by events such as periodic audits,\n    or changes in usage. Decommissioning is typically performed\n    periodically and can be manual or automated.\n  \n    Level of risk exposed if this best practice\n    is not established: Medium\n  \n\n  Implementation guidance\n    The frequency and effort to search for unused resources should reflect the potential savings,\n      so an account with a small cost should be analyzed less frequently than an account with\n      larger costs. Searches and decommission events can be initiated by state changes in the\n      workload, such as a product going end of life or being replaced. Searches and decommission\n      events may also be initiated by external events, such as changes in market conditions or\n      product termination.\n    Implementation steps\n    \n       \n    \n        \n          Decommission resources: This is the depreciation stage of \n          AWS resources that are no longer needed or ending of a licensing agreement. Complete all \n          final checks completed before moving to the disposal stage and decommissioning resources \n          to prevent any unwanted disruptions like taking snapshots or backups. Using the \n          decommissioning process, decommission each of the resources that have been identified \n          as unused.\n      \n      \n   \n\n  Resources\n\n      \n    \n      Related documents:\n    \n    \n       \n       \n    \n        \n          AWS Auto Scaling\n        \n      \n        \n          AWS Trusted Advisor\n        \n      \n  \u003cdiv\u003e\u003cdiv\u003e\u003cdiv\u003e\u003cdiv id=\"js_error_message\"\u003e\u003cp\u003e\u003cimg src=\"https://d1ge0kk1l5kms0.cloudfront.net/images/G/01/webservices/console/warning.png\" alt=\"Warning\" /\u003e \u003cstrong\u003eJavascript is disabled or is unavailable in your browser.\u003c/strong\u003e\u003c/p\u003e\u003cp\u003eTo use the Amazon Web Services Documentation, Javascript must be enabled. Please refer to your browser's Help pages for instructions.\u003c/p\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003eDocument ConventionsCOST04-BP02 Implement a decommissioning processCOST04-BP04 Decommission resources automatically",
  "COST04-BP04 Decommission resources automatically\n    Design your workload to gracefully handle resource termination as\n    you identify and decommission non-critical resources, resources that\n    are not required, or resources with low utilization.\n  \n    Level of risk exposed if this best practice\n    is not established: Low\n  \n\n  Implementation guidance\n    Use automation to reduce or remove the associated costs of the decommissioning process.\n      Designing your workload to perform automated decommissioning will reduce the overall workload\n      costs during its lifetime. You can use Amazon EC2 Auto Scaling or Application Auto Scaling to perform the decommissioning process. You can also implement custom code\n      using the API or SDK to decommission\n      workload resources automatically.\n    \n      Modern applications are built serverless-first, a strategy that prioritizes the adoption of \n      serverless services. AWS developed serverless services for all three layers of your stack: \n      compute, integration, and data stores. Using serverless architecture will allow you to save \n      costs during low-traffic periods with scaling up and down automatically. \n    \n    Implementation steps\n    \n       \n       \n       \n       \n    \n         Implement Amazon EC2 Auto Scaling or Application Auto Scaling: For resources that\n          are supported, configure them with Amazon EC2 Auto Scaling or Application Auto Scaling.  These services can help you\n          optimize your utilization and cost efficiencies when consuming AWS services. When demand\n          drops, these services will automatically remove any excess resource capacity so you avoid\n          overspending.\n      \n         Configure CloudWatch to terminate instances: Instances can\n          be configured to terminate using CloudWatch alarms. Using the metrics from the decommissioning\n          process, implement an alarm with an Amazon Elastic Compute Cloud action. Verify the operation in a\n          non-production environment before rolling out. \n      \n        \n          Implement code within the workload: You can use the AWS\n          SDK or AWS CLI to decommission workload resources. Implement code within the application\n          that integrates with AWS and terminates or removes resources that are no longer used. \n      \n        \n          Use serverless services: Prioritize building serverless \n            architectures and event-driven architecture on AWS to build and run your applications. \n          AWS offers multiple serverless technology services that inherently provide automatically \n          optimized resource utilization and automated decommissioning (scale in and scale out). \n          With serverless applications, resource utilization is automatically optimized and you \n          never pay for over-provisioning.\n        \n      \n   \n\n  Resources\n\n      \n    \n      Related documents:\n    \n    \n       \n       \n       \n       \n       \n       \n       \n    \n        \n          Amazon EC2 Auto Scaling\n        \n      \n        \n          Getting Started with Amazon EC2 Auto Scaling\n        \n      \n        \n          Application Auto Scaling\n        \n      \n        \n          AWS Trusted Advisor\n        \n      \n        \n          Serverless on AWS\n        \n      \n        \n          Create\n          Alarms to Stop, Terminate, Reboot, or Recover an\n          Instance\n        \n      \n        \n          Adding terminate actions to Amazon CloudWatch alarms\n        \n      \n    \n      Related examples:\n    \n    \n       \n      \n    \n        \n          Scheduling automatic deletion of AWS CloudFormation stacks\n        \n      \n  \u003cdiv\u003e\u003cdiv\u003e\u003cdiv\u003e\u003cdiv id=\"js_error_message\"\u003e\u003cp\u003e\u003cimg src=\"https://d1ge0kk1l5kms0.cloudfront.net/images/G/01/webservices/console/warning.png\" alt=\"Warning\" /\u003e \u003cstrong\u003eJavascript is disabled or is unavailable in your browser.\u003c/strong\u003e\u003c/p\u003e\u003cp\u003eTo use the Amazon Web Services Documentation, Javascript must be enabled. Please refer to your browser's Help pages for instructions.\u003c/p\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003eDocument ConventionsCOST04-BP03 Decommission resourcesCOST04-BP05 Enforce data retention policies",
  "COST04-BP05 Enforce data retention policies\n    Define data retention policies on supported resources to handle object deletion \n    per your organizations’ requirements. Identify and delete unnecessary or orphaned \n    resources and objects that are no longer required.\n  \n    Level of risk exposed if this best practice\n    is not established: Medium\n  \n    \n      Use data retention policies and lifecycle policies to reduce the associated costs of the \n      decommissioning process and storage costs for the identified resources. Defining your data \n      retention policies and lifecycle policies to perform automated storage class migration and \n      deletion will reduce the overall storage costs during its lifetime. You can use Amazon Data Lifecycle Manager \n      to automate the creation and deletion of Amazon Elastic Block Store snapshots and Amazon EBS-backed Amazon Machine Images (AMIs), and use \n      Amazon S3 Intelligent-Tiering or an Amazon S3 lifecycle configuration to manage the lifecycle of your Amazon S3 objects. \n      You can also implement custom code using the API or SDK to \n      create lifecycle policies and policy rules for objects to be deleted automatically.\n    \n    \n      Implementation steps\n    \n    \n       \n       \n    \n        \n          Use Amazon Data Lifecycle Manager: Use lifecycle policies on Amazon Data Lifecycle Manager to automate deletion of Amazon EBS snapshots and Amazon EBS-backed AMIs.\n        \n      \n        \n          Set up lifecycle configuration on a bucket: Use Amazon S3 lifecycle configuration on a bucket to define actions for\n          Amazon S3 to take during an object's lifecycle, as well as deletion at the end of the object's lifecycle, based on your business requirements.\n        \n      \n   \n\n  Resources\n      \n    \n      Related documents:\n    \n    \n       \n       \n       \n    \n        \n          AWS Trusted Advisor\n        \n      \n        \n          Amazon Data Lifecycle Manager\n        \n      \n        \n          How to set lifecycle configuration on Amazon S3 bucket\n        \n      \n    \n      Related videos:\n    \n    \n       \n       \n    \n        \n          Automate Amazon EBS Snapshots with Amazon Data Lifecycle Manager\n        \n      \n        \n          Empty an Amazon S3 bucket using a lifecycle configuration rule\n        \n      \n    \n      Related examples:\n    \n    \n       \n    \n        \n          Empty an Amazon S3 bucket using a lifecycle configuration rule\n        \n      \n    \n  \u003cdiv\u003e\u003cdiv\u003e\u003cdiv\u003e\u003cdiv id=\"js_error_message\"\u003e\u003cp\u003e\u003cimg src=\"https://d1ge0kk1l5kms0.cloudfront.net/images/G/01/webservices/console/warning.png\" alt=\"Warning\" /\u003e \u003cstrong\u003eJavascript is disabled or is unavailable in your browser.\u003c/strong\u003e\u003c/p\u003e\u003cp\u003eTo use the Amazon Web Services Documentation, Javascript must be enabled. Please refer to your browser's Help pages for instructions.\u003c/p\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003eDocument ConventionsCOST04-BP04 Decommission resources automaticallyCost-effective resources",
  "COST05-BP01 Identify organization requirements for cost\n    Work with team members to define the balance between cost\n    optimization and other pillars, such as performance and reliability,\n    for this workload.\n  \n    Level of risk exposed if this best practice\n    is not established: High\n  \n\n  Implementation guidance\n    \n      In most organizations, the information technology (IT) department is comprised of multiple small teams, each with its own agenda and focus area, that reflects the specialisies and skills of its team members. You need to understand your organization’s overall objectives, priorities, goals and how each department or project contributes to these objectives. Categorizing all essential resources, including personnel, equipment, technology, materials, and external services, is crucial for achieving organizational objectives and comprehensive budget planning. Adopting this systematic approach to cost identification and understanding is fundamental for establishing a realistic and robust cost plan for the organization.\n    \n    \n      When selecting services for your workload, it is key that you understand your organization priorities. Create a balance between cost optimization and other AWS Well-Architected Framework pillars, such as performance and reliability. This process should be conducted systematically and regularly to reflect changes in the organization's objectives, market conditions, and operational dynamics. A fully cost-optimized workload is the solution that is most aligned to your organization’s requirements, not necessarily the lowest cost. Meet with all teams in your organization, such as product, business, technical, and finance to collect information. Evaluate the impact of tradeoffs between competing interests or alternative approaches to help make informed decisions when determining where to focus efforts or choosing a course of action.\n    \n    \n      For example, accelerating speed to market for new features may be emphasized over cost optimization, or you may choose a relational database for non-relational data to simplify the effort to migrate a system, rather than migrating to a database optimized for your data type and updating your application.\n    \n     \n      \n      Implementation steps\n  \n    \n       \n       \n    \n         Identify organization requirements for cost: Meet with team members from your organization, including those in product management, application owners, development and operational teams, management, and financial roles. Prioritize the Well-Architected pillars for this workload and its components. The output should be a list of the pillars in order. You can also add a weight to each pillar to indicate how much additional focus it has, or how similar the focus is between two pillars.\n      \n        \n          Address the technical debt and document it: During the workload review, address the technical debt. Document a backlog item to revisit the workload in the future, with the goal of refactoring or re-architecting to optimize it further. It's essential to clearly communicate the trade-offs that were made to other stakeholders.\n        \n      \n     \n   \n\n  Resources\n\n    \n      Related best practices:\n    \n    \n       \n       \n    \n        \n          REL11-BP07 Architect your product to meet availability targets and uptime service level agreements (SLAs)\n        \n      \n        \n          OPS01-BP06 Evaluate tradeoffs\n        \n      \n    \n      Related documents:\n    \n    \n       \n       \n       \n    \n        \n          AWS           Total Cost of Ownership (TCO) Calculator\n        \n      \n        \n          Amazon S3 storage classes\n        \n      \n        \n          Cloud\n          products\n        \n      \n   \n  \u003cdiv\u003e\u003cdiv\u003e\u003cdiv\u003e\u003cdiv id=\"js_error_message\"\u003e\u003cp\u003e\u003cimg src=\"https://d1ge0kk1l5kms0.cloudfront.net/images/G/01/webservices/console/warning.png\" alt=\"Warning\" /\u003e \u003cstrong\u003eJavascript is disabled or is unavailable in your browser.\u003c/strong\u003e\u003c/p\u003e\u003cp\u003eTo use the Amazon Web Services Documentation, Javascript must be enabled. Please refer to your browser's Help pages for instructions.\u003c/p\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003eDocument ConventionsCOST 5. How do you evaluate cost when you select services?COST05-BP02 Analyze all components of the workload",
  "COST05-BP02 Analyze all components of the workload\n    Verify every workload component is analyzed, regardless of current\n    size or current costs. The review effort should reflect the\n    potential benefit, such as current and projected costs.\n  \n    Level of risk exposed if this best practice\n      is not established: High\n  \n    \n    Implementation guidance\n    \n    \n    \n      Workload components, which are designed to deliver business value\n      to the organization, may encompass various services. For each\n      component, one might choose specific AWS Cloud services to address\n      business needs. This selection could be influenced by factors such\n      as familiarity with or prior experience using these services.\n    \n    \n      After identifying your organization's requirements as mentioned in\n      COST05-BP01\n        Identify organization requirements for cost, perform a\n      thorough analysis on all components in your workload. Analyze each\n      component considering current and projected costs and sizes.\n      Consider the cost of analysis against any potential workload\n      savings over its lifecycle. The effort expended on the analysis of\n      all components of this workload should correspond to the potential\n      savings or improvements anticipated from optimization of that\n      specific component. For example, if the cost of the proposed\n      resource is $10 per month, and under forecasted loads would not\n      exceed $15 per month, spending a day of effort to reduce costs by\n      50% (five dollars per month) could exceed the potential benefit\n      over the life of the system. Use a faster and more efficient\n      data-based estimation to create the best overall outcome for this\n      component.\n    \n    \n      Workloads can change over time, and the right set of services may\n      not be optimal if the workload architecture or usage changes.\n      Analysis for selection of services must incorporate current and\n      future workload states and usage levels. Implementing a service\n      for future workload state or usage may reduce overall costs by\n      reducing or removing the effort required to make future changes.\n      For example, using EMR Serverless might be the appropriate choice\n      initially. However, as consumption for that service increases,\n      transitioning to EMR on EC2 could reduce costs for that component\n      of the workload.\n    \n    \n      AWS Cost Explorer and the AWS Cost and Usage Reports (CUR)\n      can analyze the cost of a proof of concept (PoC) or running\n      environment. You can also use AWS Pricing Calculator to estimate workload costs.\n    \n    \n      Write a workflow to be followed by technical teams to review their\n      workloads. Keep this workflow simple, but also cover all the\n      necessary steps to make sure the teams understand each component\n      of the workload and its pricing. Your organization can then follow\n      and customize this workflow based on the specific needs of each\n      team.\n    \n    \n       \n       \n       \n       \n       \n       \n    \n        \n          List each service in use for your\n            workload: This is a good starting point. Identify\n          all of the services currently in use and where costs are\n          originate from.\n        \n      \n        \n          Understand how pricing works for those\n            services:  Understand the\n          pricing\n            model of each service. Different AWS services have\n          different pricing models based on factors like usage volume,\n          data transfer, and feature-specific pricing.\n        \n      \n        \n          Focus on the services that have\n            unexpected workload costs and that do not align with your\n            expected usage and business outcome: Identify\n          outliers or services where the cost is not proportional to the\n          value or usage with using AWS Cost Explorer or AWS Cost and Usage Reports. It's important to correlate costs with business\n          outcomes to prioritize optimization efforts.\n        \n      \n        \n          AWS Cost Explorer, CloudWatch Logs,\n            VPC Flow Logs, and Amazon S3 Storage Lens to understand the\n            root cause of those high costs: These tools are\n          instrumental in the diagnosis of high costs. Each service\n          offers a different lens to view and analyze usage and costs.\n          For instance, Cost Explorer helps determine overall cost\n          trends, CloudWatch Logs provides operational insights, VPC\n          Flow Logs displays IP traffic, and Amazon S3 Storage Lens is\n          useful for storage analytics.\n        \n      \n        \n          Use AWS Budgets to set budgets for\n            certain amounts for services or accounts: Setting\n          budgets is a proactive way to manage costs. Use AWS Budgets to\n          set custom budget thresholds and receive alerts when costs\n          exceed those thresholds.\n        \n      \n        \n          Configure Amazon CloudWatch alarms to\n            send billing and usage alerts: Set up monitoring\n          and alerts for cost and usage metrics. CloudWatch alarms can\n          notify you when certain thresholds are breached, which\n          improves intervention response time.\n        \n      \n    \n      Facilitate notable enhancement and financial savings over time\n      through strategic review of all workload components and\n      irrespective of their present attributes. The effort invested in\n      this review process should be deliberate, with careful\n      consideration of the potential advantages that might be realized.\n    \n     \n      \n      Implementation steps\n      \n      \n      \n         \n         \n         \n      \n          \n            List the workload\n              components: Build a list of your workload's\n            components. Use this list to verify that each component was\n            analyzed. The effort spent should reflect the criticality to\n            the workload as defined by your organization's priorities.\n            Group together resources functionally to improve efficiency\n            (for example, production database storage, if there are\n            multiple databases).\n          \n        \n          \n            Prioritize the component\n              list: Take the component list and prioritize it\n            in order of effort. This is typically in order of the cost\n            of the component, from most expensive to least expensive or\n            the criticality as defined by your organization's\n            priorities.\n          \n        \n          \n            Perform the analysis: For\n            each component on the list, review the options and services\n            available, and choose the option that aligns best with your\n            organizational priorities.\n          \n        \n     \n   \n    \n    Resources\n    \n    \n    \n      Related documents:\n    \n    \n       \n       \n       \n       \n    \n        \n          AWS Pricing Calculator\n        \n      \n        \n          AWS Cost Explorer\n        \n      \n        \n          Amazon S3 storage classes\n        \n      \n        \n          AWS Cloud\n            products\n        \n      \n    \n      Related videos:\n    \n    \n       \n    \n        \n          AWS           Cost Optimization Series: CloudWatch\n        \n      \n  \u003cdiv\u003e\u003cdiv\u003e\u003cdiv\u003e\u003cdiv id=\"js_error_message\"\u003e\u003cp\u003e\u003cimg src=\"https://d1ge0kk1l5kms0.cloudfront.net/images/G/01/webservices/console/warning.png\" alt=\"Warning\" /\u003e \u003cstrong\u003eJavascript is disabled or is unavailable in your browser.\u003c/strong\u003e\u003c/p\u003e\u003cp\u003eTo use the Amazon Web Services Documentation, Javascript must be enabled. Please refer to your browser's Help pages for instructions.\u003c/p\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003eDocument ConventionsCOST05-BP01 Identify organization requirements for costCOST05-BP03 Perform a thorough analysis of each\n  component",
  "COST05-BP03 Perform a thorough analysis of each\n  component\n    Look at overall cost to the organization of each component. \n    Calculate the total cost of ownership by factoring in cost \n    of operations and management, especially when using managed \n    services by cloud provider. The review effort should reflect \n    potential benefit (for example, time spent analyzing is \n    proportional to component cost).\n  \n    Level of risk exposed if this best practice\n    is not established: High\n  \n\n  Implementation guidance\n    \n      Consider the time savings that will allow your team to focus on retiring technical \n      debt, innovation, value-adding features and building what differentiates the business. \n      For example, you might need to lift and shift (also known as rehost) your databases \n      from your on-premises environment to the cloud as rapidly as possible and optimize later. \n      It is worth exploring the possible savings attained by using managed services on AWS \n      that may remove or reduce license costs. Managed services on AWS remove the operational \n      and administrative burden of maintaining a service, such as patching or upgrading the \n      OS, and allow you to focus on innovation and business. \n    \n    \n      Since managed services operate at cloud scale, they can offer a lower cost per transaction \n      or service. You can make potential optimizations in order to achieve some tangible benefit, \n      without changing the core architecture of the application. For example, you may be looking \n      to reduce the amount of time you spend managing database instances by migrating to a \n      database-as-a-service platform like Amazon Relational Database Service (Amazon RDS) or \n      migrating your application to a fully managed platform like AWS Elastic Beanstalk.\n    \n    Usually, managed services have attributes that you can set to ensure sufficient capacity. You\n      must set and monitor these attributes so that your excess capacity is kept to a minimum and\n      performance is maximized. You can modify the attributes of AWS Managed Services using\n      the AWS Management Console or AWS APIs and SDKs to align resource needs with changing\n      demand. For example, you can increase or decrease the number of nodes on an Amazon EMR cluster (or an Amazon Redshift cluster) to scale out or in.\n    \n    You can also pack multiple instances on an AWS resource to activate higher density usage.\n      For example, you can provision multiple small databases on a single Amazon Relational Database Service (Amazon RDS) database instance. As usage grows, you can migrate one of the\n      databases to a dedicated Amazon RDS database instance using a snapshot and restore process.\n    \n    When provisioning workloads on managed services, you must understand the requirements\n      of adjusting the service capacity. These requirements are typically time, effort, and any\n      impact to normal workload operation. The provisioned resource must allow time for any\n      changes to occur, provision the required overhead to allow this. The ongoing effort required\n      to modify services can be reduced to virtually zero by using APIs and SDKs that are\n      integrated with system and monitoring tools, such as Amazon CloudWatch.\n    \n    Amazon RDS, Amazon Redshift, and Amazon ElastiCache provide a managed database\n      service. Amazon Athena, Amazon EMR, and Amazon OpenSearch Service provide a managed\n      analytics service.\n    AMS is a service that\n      operates AWS infrastructure on behalf of enterprise customers and partners. It provides a\n      secure and compliant environment that you can deploy your workloads onto. AMS uses\n      enterprise cloud operating models with automation to allow you to meet your organization\n      requirements, move into the cloud faster, and reduce your on-going management costs.\n    Implementation steps\n    \n       \n       \n    \n         Perform a thorough analysis: Using the component\n          list, work through each component from the highest priority to the lowest priority. For\n          the higher priority and more costly components, perform additional analysis and assess all\n          available options and their long term impact. For lower priority components, assess if\n          changes in usage would change the priority of the component, and then perform an analysis\n          of appropriate effort. \n      \n        \n          Compare managed and unmanaged resources: Consider the \n          operational cost for the resources you manage and compare them with AWS managed resources. \n          For example, review your databases running on Amazon EC2 instances and compare with Amazon RDS options \n          (an AWS managed service) or Amazon EMR compared to running Apache Spark on Amazon EC2. When moving from a \n          self-managed workload to a AWS fully managed workload, research your options carefully. The \n          three most important factors to consider are the type of managed service you want to use, \n          the process you will use to migrate your data and understand the \n          AWS shared responsibility model.\n        \n       \n    \n   \n\n  Resources\n\n      \n    \n      Related documents:\n    \n    \n       \n       \n       \n       \n    \n        \n          AWS           Total Cost of Ownership (TCO) Calculator\n        \n      \n        \n          Amazon S3 storage classes\n        \n      \n        \n          AWS Cloud\n          products\n        \n      \n        AWS Shared Responsibility Model\n        \n      \n    \n      Related videos:\n    \n    \n       \n       \n    \n        \n          Why move to a managed database?\n        \n      \n        \n          What is Amazon EMR and how can I use it for processing data?\n        \n      \n    \n      Related examples:\n    \n    \n       \n       \n       \n       \n    \n        \n          Why to move to a managed database\n        \n      \n        \n          Consolidate data from identical SQL Server databases into a single Amazon RDS for SQL Server database using AWS DMS\n      \n        \n          Deliver data at scale to Amazon Managed Streaming for Apache Kafka (Amazon MSK)\n        \n      \n        \n          Migrate an ASP.NET web application to AWS Elastic Beanstalk\n      \n  \u003cdiv\u003e\u003cdiv\u003e\u003cdiv\u003e\u003cdiv id=\"js_error_message\"\u003e\u003cp\u003e\u003cimg src=\"https://d1ge0kk1l5kms0.cloudfront.net/images/G/01/webservices/console/warning.png\" alt=\"Warning\" /\u003e \u003cstrong\u003eJavascript is disabled or is unavailable in your browser.\u003c/strong\u003e\u003c/p\u003e\u003cp\u003eTo use the Amazon Web Services Documentation, Javascript must be enabled. Please refer to your browser's Help pages for instructions.\u003c/p\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003eDocument ConventionsCOST05-BP02 Analyze all components of the workloadCOST05-BP04 Select software with cost-effective\n  licensing",
  "COST05-BP04 Select software with cost-effective\n  licensing\n    Open-source software eliminates software licensing costs, which can\n    contribute significant costs to workloads. Where licensed software\n    is required, avoid licenses bound to arbitrary attributes such as\n    CPUs, look for licenses that are bound to output or outcomes. The\n    cost of these licenses scales more closely to the benefit they\n    provide.\n  \n    Level of risk exposed if this best practice\n    is not established: Low\n  \n\n  Implementation guidance\n\n    \n      Open source originated in the context of software development to indicate that the software complies with certain free distribution criteria. Open source software is composed of source code that anyone can inspect, modify, and enhance. Based on business requirements, skill of engineers, forecasted usage, or other technology dependencies, organizations can consider using open source software on AWS to minimize their license costs. In other words, the cost of software licenses can be reduced through the use of open source software. This can have significant impact on workload costs as the size of the workload scales. \n    \n    \n      Measure the benefits of licensed software against the total cost to optimize your workload. Model any changes in licensing and how they would impact your workload costs. If a vendor changes the cost of your database license, investigate how that impacts the overall efficiency of your workload. Consider historical pricing announcements from your vendors for trends of licensing changes across their products. Licensing costs may also scale independently of throughput or usage, such as licenses that scale by hardware (CPU bound licenses). These licenses should be avoided because costs can rapidly increase without corresponding outcomes. \n    \n    \n      For instance, operating an Amazon EC2 instance in us-east-1 with a Linux operating system allows you to cut costs by approximately 45%, compared to running another Amazon EC2 instance that runs on Windows. \n    \n    \n      The AWS Pricing Calculator offers a comprehensive way to compare the costs of various resources with different license options, such as Amazon RDS instances and different database engines. Additionally, the AWS Cost Explorer provides an invaluable perspective for the costs of existing workloads, especially those that come with different licenses. For license management, AWS License Manager offers a streamlined method to oversee and handle software licenses. Customers can deploy and operationalize their preferred open source software in the AWS Cloud. \n    \n     \n      \n      Implementation steps\n    \n    \n       \n       \n    \n         Analyze license options: Review the licensing terms of available software. Look for open source versions that have the required functionality, and whether the benefits of licensed software outweigh the cost. Favorable terms align the cost of the software to the benefits it provides.\n      \n         Analyze the software provider: Review any historical pricing or licensing changes from the vendor. Look for any changes that do not align to outcomes, such as punitive terms for running on specific vendors hardware or platforms. Additionally, look for how they perform audits, and penalties that could be imposed.\n      \n     \n   \n\n  Resources\n\n      \n    \n      Related documents:\n    \n    \n       \n       \n       \n       \n    \n        \n          Open Source at AWS\n      \n        \n          AWS           Total Cost of Ownership (TCO) Calculator\n        \n      \n        \n          Amazon S3 storage classes\n        \n      \n        \n          Cloud\n          products\n        \n      \n    \n      Related examples:\n    \n    \n       \n       \n       \n    \n        \n          Open Source Blogs\n        \n      \n        AWS Open Source Blogs\n        \n      \n        \n          Optimization and Licensing Assessment\n        \n      \n  \u003cdiv\u003e\u003cdiv\u003e\u003cdiv\u003e\u003cdiv id=\"js_error_message\"\u003e\u003cp\u003e\u003cimg src=\"https://d1ge0kk1l5kms0.cloudfront.net/images/G/01/webservices/console/warning.png\" alt=\"Warning\" /\u003e \u003cstrong\u003eJavascript is disabled or is unavailable in your browser.\u003c/strong\u003e\u003c/p\u003e\u003cp\u003eTo use the Amazon Web Services Documentation, Javascript must be enabled. Please refer to your browser's Help pages for instructions.\u003c/p\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003eDocument ConventionsCOST05-BP03 Perform a thorough analysis of each\n  componentCOST05-BP05 Select components of this workload to optimize cost\n  in line with organization priorities",
  "COST05-BP05 Select components of this workload to optimize cost\n  in line with organization priorities\n    Factor in cost when selecting all components for your workload. This\n    includes using application-level and managed services or serverless,\n    containers, or event-driven architecture to reduce overall cost.\n    Minimize license costs by using open-source software, software that\n    does not have license fees, or alternatives to reduce spending.\n  \n    Level of risk exposed if this best practice\n    is not established: Medium\n  \n    \n    Implementation guidance\n    \n    \n    \n      Consider the cost of services and options when selecting all\n      components. This includes using application level and managed\n      services, such as\n      Amazon Relational Database Service (Amazon RDS),\n      Amazon DynamoDB,\n      Amazon Simple Notification Service (Amazon SNS), and\n      Amazon Simple Email Service (Amazon SES) to reduce overall organization cost.\n    \n    \n      Use serverless and containers for compute, such as\n      AWS Lambda and\n      Amazon Simple Storage Service (Amazon S3) for static websites.\n      Containerize your application if possible and use AWS Managed\n      Container Services such as\n      Amazon Elastic Container Service (Amazon ECS) or\n      Amazon Elastic Kubernetes Service (Amazon EKS).\n    \n    \n      Minimize license costs by using open-source software, or software\n      that does not have license fees (for example, Amazon Linux for\n      compute workloads or migrate databases to Amazon Aurora).\n    \n    \n      You can use serverless or application-level services such as\n      Lambda,\n      Amazon Simple Queue Service (Amazon SQS),\n      Amazon SNS, and\n      Amazon SES. These services remove the need for\n      you to manage a resource and provide the function of code\n      execution, queuing services, and message delivery. The other\n      benefit is that they scale in performance and cost in line with\n      usage, allowing efficient cost allocation and attribution.\n    \n    \n      Using\n      event-driven\n        architecture is also possible with serverless services.\n      Event-driven architectures are push-based, so everything happens\n      on demand as the event presents itself in the router. This way,\n      you’re not paying for continuous polling to check for an event.\n      This means less network bandwidth consumption, less CPU\n      utilization, less idle fleet capacity, and fewer SSL/TLS\n      handshakes.\n    \n    \n      For more information on serverless, see\n      Well-Architected\n        Serverless Application lens whitepaper.\n    \n     \n      \n      Implementation steps\n      \n      \n      \n         \n         \n      \n          \n            Select each service to optimize\n              cost: Using your prioritized list and analysis,\n            select each option that provides the best match with your\n            organizational priorities. Instead of increasing the\n            capacity to meet the demand, consider other options which\n            may give you better performance with lower cost. For\n            example, if you need to review expected traffic for your\n            databases on AWS, consider either increasing the instance\n            size or using Amazon ElastiCache services (Redis or Memcached)\n            to provide cached mechanisms for your databases.\n          \n        \n          \n            Evaluate event-driven\n              architecture: Using serverless architecture also\n            allows you to build event-driven architecture for\n            distributed microservice-based applications, which helps you\n            build scalable, resilient, agile and cost-effective\n            solutions.\n          \n        \n     \n   \n    \n    Resources\n    \n    \n    \n      Related documents:\n    \n    \n       \n       \n       \n       \n       \n       \n    \n        \n          AWS           Total Cost of Ownership (TCO) Calculator\n        \n      \n        \n          AWS           Serverless\n        \n      \n        \n          What is\n            Event-Driven Architecture\n        \n      \n        \n          Amazon S3 storage classes\n        \n      \n        \n          Cloud\n            products\n        \n      \n        \n          Amazon ElastiCache (Redis OSS)\n        \n      \n    \n      Related examples:\n    \n    \n       \n       \n       \n       \n    \n        \n          Getting\n            started with event-driven architecture\n        \n      \n        \n          Event-driven\n            architecture\n        \n      \n        \n          How\n            Statsig runs 100x more cost-effectively using Amazon ElastiCache (Redis OSS)\n        \n      \n        \n          Best\n            practices for working with AWS Lambda functions\n        \n      \n  \u003cdiv\u003e\u003cdiv\u003e\u003cdiv\u003e\u003cdiv id=\"js_error_message\"\u003e\u003cp\u003e\u003cimg src=\"https://d1ge0kk1l5kms0.cloudfront.net/images/G/01/webservices/console/warning.png\" alt=\"Warning\" /\u003e \u003cstrong\u003eJavascript is disabled or is unavailable in your browser.\u003c/strong\u003e\u003c/p\u003e\u003cp\u003eTo use the Amazon Web Services Documentation, Javascript must be enabled. Please refer to your browser's Help pages for instructions.\u003c/p\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003eDocument ConventionsCOST05-BP04 Select software with cost-effective\n  licensingCOST05-BP06 Perform cost analysis for different usage over\n  time",
  "COST05-BP06 Perform cost analysis for different usage over\n  time\n    Workloads can change over time. Some services or features are more\n    cost effective at different usage levels. By performing the analysis\n    on each component over time and at projected usage, the workload\n    remains cost-effective over its lifetime.\n  \n    Level of risk exposed if this best practice\n    is not established: Medium\n  \n\n  Implementation guidance\n\n    As AWS releases new services and features, the optimal services for your workload may\n      change. Effort required should reflect potential benefits. Workload review frequency\n      depends on your organization requirements. If it is a workload of significant cost,\n      implementing new services sooner will maximize cost savings, so more frequent review can\n      be advantageous. Another initiation for review is change in usage patterns. Significant changes\n      in usage can indicate that alternate services would be more optimal.\n    \n      If you need to move data into AWS Cloud, you can select any wide variety of services AWS \n      offers and partner tools to help you migrate your data sets, whether they are files, \n      databases, machine images, block volumes, or even tape backups. For example, to move a \n      large amount of data to and from AWS or process data at the edge, you can use one of \n      the AWS purpose-built devices to cost effectively move petabytes of data offline. \n      Another example is for higher data transfer rates, a direct connect service may be \n      cheaper than a VPN which provides the required consistent connectivity for your \n      business. \n    \n    \n      Based on the cost analysis for different usage over time, review your scaling activity. \n      Analyze the result to see if the scaling policy can be tuned to add instances with multiple \n      instance types and purchase options. Review your settings to see if the minimum can be \n      reduced to serve user requests but with a smaller fleet size, and add more resources to \n      meet the expected high demand. \n    \n    \n      Perform cost analysis for different usage over time by discussing with stakeholders in \n      your organization and use AWS Cost Explorer’s forecast feature to predict the potential \n      impact of service changes. Monitor usage level launches using AWS Budgets, CloudWatch \n      billing alarms and AWS Cost Anomaly Detection to identify and implement the most cost-effective \n      services sooner. \n    \n    Implementation steps\n    \n       \n       \n    \n         Define predicted usage patterns: Working with your\n          organization, such as marketing and product owners, document what the expected and\n          predicted usage patterns will be for the workload. Discuss with business stakeholders \n          about both historical and forecasted cost and usage increases and make sure increases \n          align with business requirements. Identify calendar days, weeks, or months where you \n          expect more users to use your AWS resources, which indicate that you should increase \n          the capacity of the existing resources or adopt additional services to reduce the cost \n          and increase performance. \n      \n         Perform cost analysis at predicted usage: Using the usage \n          patterns defined, perform analysis at each of these points. The analysis effort should reflect \n          the potential outcome. For example, if the change in usage is large, a thorough analysis should \n          be performed to verify any costs and changes. In other words, when cost increases, usage should \n          increase for business as well. \n      \n   \n\n  Resources\n\n      \n    \n      Related documents:\n    \n    \n       \n       \n       \n       \n       \n       \n    \n        \n          AWS           Total Cost of Ownership (TCO) Calculator\n        \n      \n        \n          Amazon S3 storage classes\n        \n      \n        \n          Cloud\n          products\n        \n      \n        \n          Amazon EC2 Auto Scaling\n        \n      \n        \n          Cloud Data Migration\n        \n      \n        AWS Snow Family\n      \n    \n      Related videos:\n    \n    \n       \n    \n        AWS OpsHub for Snow Family\n      \n  \u003cdiv\u003e\u003cdiv\u003e\u003cdiv\u003e\u003cdiv id=\"js_error_message\"\u003e\u003cp\u003e\u003cimg src=\"https://d1ge0kk1l5kms0.cloudfront.net/images/G/01/webservices/console/warning.png\" alt=\"Warning\" /\u003e \u003cstrong\u003eJavascript is disabled or is unavailable in your browser.\u003c/strong\u003e\u003c/p\u003e\u003cp\u003eTo use the Amazon Web Services Documentation, Javascript must be enabled. Please refer to your browser's Help pages for instructions.\u003c/p\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003eDocument ConventionsCOST05-BP05 Select components of this workload to optimize cost\n  in line with organization priorities COST 6. How do you meet cost targets when you select resource type, size\n                  and number? ",
  "COST06-BP01 Perform cost modelingIdentify organization requirements (such as business needs and existing commitments) \n    and perform cost modeling (overall costs) of the workload and each of its components. \n    Perform benchmark activities for the workload under different predicted loads and compare \n    the costs. The modeling effort should reflect the potential benefit. For example, time \n    spent is proportional to component cost.\n    Level of risk exposed if this best practice\n    is not established: High\n  \n\n  Implementation guidance\n    \n      Perform cost modelling for your workload and each of its components to understand the balance \n      between resources, and find the correct size for each resource in the workload, given a specific \n      level of performance. Understanding cost considerations can inform your organizational business \n      case and decision-making process when evaluating the value realization outcomes for planned \n      workload deployment.\n    \n    \n      Perform benchmark activities for the workload under different predicted loads and compare the costs. \n      The modelling effort should reflect potential benefit; for example, time spent is proportional to \n      component cost or predicted saving. For best practices, refer to the \n      Review section of the \n      Performance Efficiency Pillar of the AWS Well-Architected Framework. \n    \n    \n      As an example, to create cost modeling for a workload consisting of compute resources, \n      AWS Compute Optimizer can assist with cost modelling for running workloads. It provides \n      right-sizing recommendations for compute resources based on historical usage. Make sure \n      CloudWatch Agents are deployed to the Amazon EC2 instances to collect memory metrics which help \n      you with more accurate recommendations within AWS Compute Optimizer. This is the ideal \n      data source for compute resources because it is a free service that uses machine learning \n      to make multiple recommendations depending on levels of risk. \n    \n    \n      There are multiple services you can use with custom logs as data sources for rightsizing \n      operations for other services and workload components, such as AWS Trusted Advisor, \n      Amazon CloudWatch and \n      Amazon CloudWatch Logs. AWS Trusted Advisor checks resources and flags \n      resources with low utilization which can help you right size your resources and create \n      cost modelling. \n    \n    \n      The following are recommendations for cost modelling data and metrics:\n    \n    \n       \n       \n       \n    \n        \n          The monitoring must accurately reflect the user experience. Select the correct \n          granularity for the time period and thoughtfully choose the maximum or 99th \n          percentile instead of the average. \n        \n      \n        \n          Select the correct granularity for the time period of \n          analysis that is required to cover any workload cycles. \n          For example, if a two-week analysis is performed, you\n          might be overlooking a monthly cycle of high utilization, \n          which could lead to under-provisioning. \n        \n      \n        \n          Choose the right AWS services for your planned workload by \n          considering your existing commitments, selected pricing models \n          for other workloads, and ability to innovate faster and focus \n          on your core business value. \n        \n      \n    \n    Implementation steps \n    \n       \n    \n         Perform cost modeling for resources: Deploy the \n          workload or a proof of concept into a separate account with the specific resource types \n          and sizes to test. Run the workload with the test data and record the output results, \n          along with the cost data for the time the test was run. Afterwards, redeploy the workload \n          or change the resource types and sizes and run the test again. Include license fees of \n          any products you may use with these resources and estimated operations (labor or engineer) \n          costs for deploying and managing these resources while creating cost modeling. Consider \n          cost modeling for a period (hourly, daily, monthly, yearly or three years).\n      \n      \n   \n\n  Resources\n\n      \n    \n      Related documents:\n    \n    \n       \n       \n       \n       \n       \n       \n    \n        \n          AWS Auto Scaling\n        \n      \n        \n          Identifying Opportunities to Right Size\n        \n      \n        \n          Amazon CloudWatch features\n        \n      \n        \n          Cost\n          Optimization: Amazon EC2 Right Sizing\n        \n      \n        \n          AWS Compute Optimizer\n        \n      \n        AWS Pricing Calculator\n        \n      \n    \n      Related examples:\n    \n    \n       \n       \n       \n    \n        \n          Perform a Data-Driven Cost Modelling\n        \n      \n        \n          Estimate the cost of planned AWS resource configurations\n        \n      \n        \n          Choose the right AWS tools\n        \n      \n  \u003cdiv\u003e\u003cdiv\u003e\u003cdiv\u003e\u003cdiv id=\"js_error_message\"\u003e\u003cp\u003e\u003cimg src=\"https://d1ge0kk1l5kms0.cloudfront.net/images/G/01/webservices/console/warning.png\" alt=\"Warning\" /\u003e \u003cstrong\u003eJavascript is disabled or is unavailable in your browser.\u003c/strong\u003e\u003c/p\u003e\u003cp\u003eTo use the Amazon Web Services Documentation, Javascript must be enabled. Please refer to your browser's Help pages for instructions.\u003c/p\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003eDocument Conventions COST 6. How do you meet cost targets when you select resource type, size\n                  and number? COST06-BP02 Select resource type, size, and number based on\n  data",
  "COST06-BP02 Select resource type, size, and number based on\n  dataSelect resource size or type based on data about the workload and resource characteristics.\n    For example, compute, memory, throughput, or write intensive. This selection is typically made\n    using a previous (on-premises) version of the workload, using documentation, or using other\n    sources of information about the workload.\n    Level of risk exposed if this best practice\n    is not established: Medium\n  \n\n  Implementation guidance\n    \n      Amazon EC2 provides a wide selection of instance types with different levels of CPU, memory, storage, and networking capacity to fit different use cases. These instance types feature different blends of CPU, memory, storage, and networking capabilities, giving you versatility when selecting the right resource combination for your projects. Every instance type comes in multiple sizes, so that you can adjust your resources based on your workload’s demands. To determine which instance type you need, gather details about the system requirements of the application or software that you plan to run on your instance. These details should include the following:\n    \n    \n       \n       \n       \n       \n       \n       \n    \n        \n          Operating system\n        \n      \n        \n          Number of CPU cores\n        \n      \n        \n          GPU cores\n        \n      \n        \n          Amount of system memory (RAM)\n        \n      \n        \n          Storage type and space\n        \n      \n        \n          Network bandwidth requirement\n        \n      \n    \n      Identify the purpose of compute requirements and which instance is needed, and then explore the various Amazon EC2 instance families. Amazon offers the following instance type families:\n    \n    \n       \n       \n       \n       \n       \n       \n    \n        \n          General Purpose\n        \n      \n        \n          Compute Optimized\n        \n      \n        \n          Memory Optimized\n        \n      \n        \n          Storage Optimized\n        \n      \n        \n          Accelerated Computing\n        \n      \n        \n          HPC Optimized \n        \n      \n    \n      For a deeper understanding of the specific purposes and use cases that a particular Amazon EC2 instance family can fulfill, see AWS Instance types.\n    \n    \n      System requirements gathering is critical for you to select the specific instance family and instance type that best serves your needs. Instance type names are comprised of the family name and the instance size. For example, the t2.micro instance is from the T2 family and is micro-sized.\n    \n    \n      Select resource size or type based on workload and resource characteristics (for example, compute, memory, throughput, or write intensive). This selection is typically made using cost modelling, a previous version of the workload (such as an on-premises version), using documentation, or using other sources of information about the workload (whitepapers or published solutions). Using AWS pricing calculators or cost management tools can assist in making informed decisions about instance types, sizes, and configurations.\n    \n     \n      \n      Implementation steps\n  \n    \n       \n    \n        Select resources based on data: Use your cost modeling data to select the anticipated workload usage level, and choose the specified resource type and size. Relying on the cost modeling data, determine the number of virtual CPUs, total memory (GiB), the local instance store volume (GB), Amazon EBS volumes, and the network performance level, taking into account the data transfer rate required for the instance. Always make selections based on detailed analysis and accurate data to optimize performance while managing costs effectively.\n      \n     \n   \n\n  Resources\n\n      \n    \n      Related documents:\n    \n    \n       \n       \n       \n       \n    \n        AWS Instance types\n      \n        \n          AWS Auto Scaling\n        \n      \n        \n          Amazon CloudWatch features\n        \n      \n        \n          Cost\n          Optimization: EC2 Right Sizing\n        \n      \n    \n      Related videos:\n    \n    \n       \n       \n    \n        \n          Selecting the right Amazon EC2 instance for your workloads\n        \n      \n        \n          Right size your service\n        \n      \n    \n      Related examples:\n    \n    \n       \n    \n        \n          It just got easier to discover and compare Amazon EC2 instance types\n        \n      \n  \u003cdiv\u003e\u003cdiv\u003e\u003cdiv\u003e\u003cdiv id=\"js_error_message\"\u003e\u003cp\u003e\u003cimg src=\"https://d1ge0kk1l5kms0.cloudfront.net/images/G/01/webservices/console/warning.png\" alt=\"Warning\" /\u003e \u003cstrong\u003eJavascript is disabled or is unavailable in your browser.\u003c/strong\u003e\u003c/p\u003e\u003cp\u003eTo use the Amazon Web Services Documentation, Javascript must be enabled. Please refer to your browser's Help pages for instructions.\u003c/p\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003eDocument ConventionsCOST06-BP01 Perform cost modelingCOST06-BP03 Select resource type, size, and number\n  automatically based on metrics",
  "COST06-BP03 Select resource type, size, and number\n  automatically based on metricsUse metrics from the currently running workload to select the right size and type to optimize for cost. \n    Appropriately provision throughput, sizing, and storage for compute, storage, data, and networking services. \n    This can be done with a feedback loop such as automatic scaling or by custom code in the workload.\n    Level of risk exposed if this best practice\n    is not established: Low\n  \n\n  Implementation guidance\n\n    Create a feedback loop within the workload that uses active metrics from the running\n      workload to make changes to that workload. You can use a managed service, such as AWS Auto Scaling, which you configure to perform\n      the right sizing operations for you. AWS also provides APIs, SDKs, and features that allow\n      resources to be modified with minimal effort. You can program a workload to stop-and-start an\n      Amazon EC2 instance to allow a change of instance size or instance type. This\n      provides the benefits of right-sizing while removing almost all the operational cost required\n      to make the change.\n    Some AWS services have built in automatic type or size selection, such as Amazon Simple Storage Service Intelligent-Tiering. Amazon S3\n      Intelligent-Tiering automatically moves your data between two access tiers, frequent access\n      and infrequent access, based on your usage patterns.\n    Implementation steps\n\n    \n       \n       \n       \n    \n        Increase your observability by configuring workload metrics: Capture key \n          metrics for the workload. These metrics provide an indication of the customer experience, such as workload output, \n          and align to the differences between resource types and sizes, such as CPU and memory usage. For compute resource, \n          analyze performance data to right size your Amazon EC2 instances. Identify idle instances and ones that are underutilized. \n          Key metrics to look for are CPU usage and memory utilization (for example, 40% CPU utilization at 90% of the time \n          as explained in Rightsizing with AWS Compute Optimizer and Memory Utilization Enabled). Identify instances with a \n          maximum CPU usage and memory utilization of less than 40% over a four-week period. These are the instances to \n          right size to reduce costs. For storage resources such as Amazon S3, you can use Amazon S3 Storage Lens, which \n          allows you to see 28 metrics across various categories at the bucket level, and 14 days of historical data in \n          the dashboard by default. You can filter your Amazon S3 Storage Lens dashboard by summary and cost optimization or \n          events to analyze specific metrics. \n        \n      \n        View rightsizing recommendations: Use the rightsizing recommendations in \n          AWS Compute Optimizer and the Amazon EC2 rightsizing tool in the Cost Management console, or review AWS Trusted Advisor \n          right-sizing your resources to make adjustments on your workload. It is important to use the right tools when \n          right-sizing different resources and follow right-sizing guidelines whether it is an Amazon EC2 instance, \n          AWS storage classes, or Amazon RDS instance types. For storage resources, you can use Amazon S3 Storage Lens, which \n          gives you visibility into object storage usage, activity trends, and makes actionable recommendations to optimize \n          costs and apply data protection best practices. Using the contextual recommendations that Amazon S3 Storage Lens derives \n          from analysis of metrics across your organization, you can take immediate steps to optimize your storage.    \n        \n      \n        Select resource type and size automatically based on metrics:  Using the workload \n          metrics, manually or automatically select your workload resources. For compute resources, configuring AWS Auto Scaling \n          or implementing code within your application can reduce the effort required if frequent changes are needed, and it can \n          potentially implement changes sooner than a manual process. You can launch and automatically scale a fleet of On-Demand \n          Instances and Spot Instances within a single Auto Scaling group. In addition to receiving discounts for using Spot \n          Instances, you can use Reserved Instances or a Savings Plan to receive discounted rates of the regular On-Demand Instance \n          pricing. All of these factors combined help you optimize your cost savings for Amazon EC2 instances and determine the desired \n          scale and performance for your application. You can also use an attribute-based instance type selection (ABS) strategy \n          in Auto Scaling Groups (ASG), which lets you express your instance requirements as a set of attributes, such as vCPU, \n          memory, and storage. You can automatically use newer generation instance types when they are released and access a \n          broader range of capacity with Amazon EC2 Spot Instances. Amazon EC2 Fleet and Amazon EC2 Auto Scaling select and launch instances that fit \n          the specified attributes, removing the need to manually pick instance types. For storage resources, you can use the\n          Amazon S3 Intelligent Tiering and Amazon EFS Infrequent Access features, which allow you to select storage classes automatically \n          that deliver automatic storage cost savings when data access patterns change, without performance impact or operational overhead. \n        \n      \n   \n\n  Resources\n\n      \n    \n      Related documents:\n    \n    \n       \n       \n       \n       \n       \n       \n       \n       \n       \n       \n       \n    \n        \n          AWS Auto Scaling\n        \n      \n        \n          AWS Right-Sizing\n        \n      \n        \n          AWS Compute Optimizer\n        \n      \n        \n          Amazon CloudWatch features\n        \n      \n        \n          CloudWatch\n          Getting Set Up\n        \n      \n        \n          CloudWatch\n          Publishing Custom Metrics\n        \n      \n        \n          Getting\n          Started with Amazon EC2 Auto Scaling\n        \n      \n        \n          Amazon S3 Storage Lens\n        \n      \n        \n          Amazon S3 Intelligent-Tiering\n        \n      \n        \n          Amazon EFS Infrequent Access\n        \n      \n        \n          Launch\n          an Amazon EC2 Instance Using the SDK\n        \n      \n    \n      Related videos:\n    \n    \n       \n    \n        \n          Right Size Your Services\n        \n      \n    \n      Related examples:\n    \n    \n       \n       \n       \n       \n    \n        \n          Attribute based Instance Type Selection for Auto Scaling for Amazon EC2 Fleet\n        \n      \n        \n          Optimizing Amazon Elastic Container Service for cost using scheduled scaling \n        \n      \n        \n          Predictive scaling with Amazon EC2 Auto Scaling\n        \n      \n        \n          Optimize Costs and Gain Visibility into Usage with Amazon S3 Storage Lens\n        \n      \n  \u003cdiv\u003e\u003cdiv\u003e\u003cdiv\u003e\u003cdiv id=\"js_error_message\"\u003e\u003cp\u003e\u003cimg src=\"https://d1ge0kk1l5kms0.cloudfront.net/images/G/01/webservices/console/warning.png\" alt=\"Warning\" /\u003e \u003cstrong\u003eJavascript is disabled or is unavailable in your browser.\u003c/strong\u003e\u003c/p\u003e\u003cp\u003eTo use the Amazon Web Services Documentation, Javascript must be enabled. Please refer to your browser's Help pages for instructions.\u003c/p\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003eDocument ConventionsCOST06-BP02 Select resource type, size, and number based on\n  dataCOST06-BP04 Consider using shared resources",
  "COST06-BP04 Consider using shared resources\n    For already-deployed services at the organization level for multiple business units, consider using shared resources to increase utilization and reduce total cost of ownership (TCO). Using shared resources can be a cost-effective option to centralize the management and costs by using existing solutions, sharing components, or both. Manage common functions like monitoring, backups, and connectivity either within an account boundary or in a dedicated account. You can also reduce cost by implementing standardization, reducing duplication, and reducing complexity.\n  \n    Level of risk exposed if this best practice\n    is not established: Medium\n  \n\n  Implementation guidance\n    \n      Where multiple workloads cause the same function, use existing solutions and shared components to improve management and optimize costs. Consider using existing resources (especially shared ones), such as non-production database servers or directory services, to mitigate cloud costs by following security best practices and organizational regulations. For optimal value realization and efficiency, it is crucial to allocate costs back (using showback and chargeback) to the pertinent areas of the business driving consumption.\n    \n    \n      Showback refers to reports that break down cloud costs into attributable categories, such as consumers, business units, general ledger accounts, or other responsible entities. The goal of showback is to show teams, business units, or individuals the cost of their consumed cloud resources.\n    \n        Chargeback means to allocate central service spend to cost units based on a strategy suitable for a specific financial management process. For customers, chargeback charges the cost incurred from one shared services account to different financial cost categories suitable for a customer reporting process. By establishing chargeback mechanisms, you can report costs incurred by different business units, products, and teams.\n    \n    \n      Workloads can be categorized as critical and non-critical. Based on this classification, use shared resources with general configurations for less critical workloads. To further optimize costs, reserve dedicated servers solely for critical workloads. Share resources or provision them across several accounts to manage them efficiently. Even with distinct development, testing, and production environments, secure sharing is feasible and does not compromise organizational structure.\n    \n    \n      To improve your understanding and optimize cost and usage for containerized applications, use  split cost allocation data which helps you allocate costs to individual business entities based on how the application consumes shared compute and memory resources. Split cost allocation data helps you achieve task-level showback and chargeback in container workloads running on Amazon Elastic Container Service (Amazon ECS) or Amazon Elastic Kubernetes Service (Amazon EKS). \n    \n    \n      For distributed architectures, build a shared services VPC, which provides centralized access to shared services required by workloads in each of the VPCs. These shared services can include resources such as directory services or VPC endpoints. To reduce administrative overhead and cost, share resources from a central location instead of building them in each VPC.\n    \n    \n      When you use shared resources, you can save on operational costs, maximize resource utilization, and improve consistency. In a multi-account design, you can host some AWS services centrally and access them using several applications and accounts in a hub to save cost. You can use AWS Resource Access Manager (AWS RAM) to share other common resources, such as VPC subnets and AWS Transit Gateway attachments, AWS Network Firewall, or Amazon SageMaker AI pipelines. In a multi-account environment, use AWS RAM to create a resource once and share it with other accounts.\n    \n    \n      Organizations should tag shared costs effectively and verify that they do not have a significant portion of their costs untagged or unallocated. If you do not allocate shared costs effectively and no one takes accountability for shared costs management, shared cloud costs can spiral. You should know where you have incurred costs at the resource, workload, team, or organization level, as this knowledge enhances your understanding of the value delivered at the applicable level when compared to the business outcomes achieved. Ultimately, organizations benefit from cost savings as a result of sharing cloud infrastructure. Encourage cost allocation on shared cloud resources to optimize cloud spend.\n    \n     \n      \n      Implementation steps\n      \n         \n         \n         \n      \n          \n            Evaluate existing resources: Review existing workloads that use similar services for your workload. Depending on the workload’s components, consider existing platforms if business logic or technical requirement allow.\n          \n        \n          \n            Use resource sharing in AWS RAM and restrict accordingly: Use AWS RAM to share resources with other AWS accounts within your organization. When you share resources, you don’t need to duplicate resources in multiple accounts, which minimizes the operational burden of resource maintenance. This process also helps you securely share the resources that you have created with roles and users in your account, as well as with other AWS accounts. \n          \n        \n          \n            Tag resources: Tag resources that are candidates for cost reporting and categorize them within cost categories. Activate these cost related resource tags for cost allocation to provide visibility of AWS resources usage. Focus on creating an appropriate level of granularity with respect to cost and usage visibility, and inﬂuence cloud consumption behaviors through cost allocation reporting and KPI tracking.\n          \n        \n     \n   \n    \n    Resources\n    \n      Related best practices:\n    \n    \n       \n    \n        \n          SEC03-BP08 Share resources securely within your organization\n        \n      \n    \n      Related documents:\n    \n    \n       \n       \n       \n       \n    \n        \n          What is AWS Resource Access Manager?\n        \n      \n        AWS services that you can use with AWS Organizations\n      \n        \n          Shareable AWS resources\n        \n      \n        AWS Cost and Usage (CUR) Queries\n        \n      \n    \n      Related videos:\n    \n    \n       \n       \n       \n    \n        AWS Resource Access Manager - granular access control with managed permissions\n        \n      \n        \n          How to design your AWS cost allocation strategy\n        \n      \n        AWS Cost Categories\n        \n      \n    \n      Related examples:\n    \n    \n       \n       \n       \n       \n       \n    \n        \n          How-to chargeback shared services: An AWS Transit Gateway example\n        \n      \n        \n          How to build a chargeback/showback model for Savings Plans using the CUR\n        \n      \n        \n          Using VPC Sharing for a Cost-Effective Multi-Account Microservice Architecture\n        \n      \n        \n          Improve cost visibility of Amazon EKS with AWS Split Cost Allocation Data\n        \n      \n        \n          Improve cost visibility of Amazon ECS and AWS Batch with AWS Split Cost Allocation Data\n        \n      \n  \u003cdiv\u003e\u003cdiv\u003e\u003cdiv\u003e\u003cdiv id=\"js_error_message\"\u003e\u003cp\u003e\u003cimg src=\"https://d1ge0kk1l5kms0.cloudfront.net/images/G/01/webservices/console/warning.png\" alt=\"Warning\" /\u003e \u003cstrong\u003eJavascript is disabled or is unavailable in your browser.\u003c/strong\u003e\u003c/p\u003e\u003cp\u003eTo use the Amazon Web Services Documentation, Javascript must be enabled. Please refer to your browser's Help pages for instructions.\u003c/p\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003eDocument ConventionsCOST06-BP03 Select resource type, size, and number\n  automatically based on metricsCOST 7. How do you use pricing models to reduce cost?",
  "COST07-BP01 Perform pricing model analysisAnalyze each component of the workload. Determine if the \n    component and resources will be running for extended periods \n    (for commitment discounts) or dynamic and short-running (for \n    spot or on-demand). Perform an analysis on the workload using \n    the recommendations in cost management tools and apply business \n    rules to those recommendations to achieve high returns.\n    Level of risk exposed if this best practice\n    is not established: High\n  \n\n  Implementation guidance\n    AWS has multiple pricing models\n      that allow you to pay for your resources in the most cost-effective way that suits your\n      organization’s needs and depending on product. Work with your teams to \n      determine the most appropriate pricing model. Often your pricing model \n      consists of a combination of multiple options, as determined by your \n      availability \n    \n      On-Demand Instances allow you pay for \n      compute or database capacity by the hour or by the second (60 seconds \n      minimum) depending on which instances you run, without long-term \n      commitments or upfront payments.\n    \n    \n      Savings Plans are a flexible pricing \n      model that offers low prices on Amazon EC2, Lambda, and AWS Fargate \n      usage, in exchange for a commitment to a consistent amount of usage \n      (measured in dollars per hour) over one year or three years terms.\n    \n    \n      Spot Instances are an Amazon EC2 \n      pricing mechanism that allows you request spare compute capacity \n      at discounted hourly rate (up to 90% off the on-demand price) \n      without upfront commitment.\n    \n    \n      Reserved Instances allow you up \n      to 75 percent discount by prepaying for capacity. For more details, \n      see Optimizing costs with reservations.\n    \n    \n      You might choose to include a Savings Plan for the resources \n      associated with the production, quality, and development \n      environments. Alternatively, because sandbox resources are \n      only powered on when needed, you might choose an on-demand \n      model for the resources in that environment. Use Amazon \n      Spot Instances \n      to reduce Amazon EC2 costs or use \n      Compute Savings Plans \n      to reduce Amazon EC2, Fargate, and Lambda cost. The \n      AWS Cost Explorer \n      recommendations tool provides opportunities for commitment \n      discounts with Saving plans. \n    \n    \n      If you have been purchasing  \n      Reserved Instances for Amazon EC2 in \n      the past or have established cost allocation practices inside \n      your organization, you can continue using Amazon EC2 Reserved Instances \n      for the time being. However, we recommend working on a strategy \n      to use Savings Plans in the future as a more flexible cost \n      savings mechanism. You can refresh Savings Plans (SP) \n      Recommendations in AWS Cost Management to generate new Savings \n      Plans Recommendations at any time. Use Reserved Instances \n      (RI) to reduce Amazon RDS, Amazon Redshift, Amazon ElastiCache, \n      and Amazon OpenSearch Service costs. Saving Plans and Reserved \n      Instances are available in three options: all upfront, partial \n      upfront and no upfront payments. Use the recommendations provided \n      in AWS Cost Explorer RI and SP purchase recommendations. \n    \n    \n      To find opportunities for Spot workloads, use an hourly view \n      of your overall usage, and look for regular periods of changing \n      usage or elasticity. You can use Spot Instances for various \n      fault-tolerant and flexible applications. Examples include \n      stateless web servers, API endpoints, big data and analytics \n      applications, containerized workloads, CI/CD, and other \n      flexible workloads.\n    \n    \n      Analyze your Amazon EC2 and Amazon RDS instances whether they can be turned \n      off when you don’t use (after hours and weekends). This approach \n      will allow you to reduce costs by 70% or more compared to using \n      them 24/7. If you have Amazon Redshift clusters that only need to be \n      available at specific times, you can pause the cluster and later \n      resume it. When the Amazon Redshift cluster or Amazon EC2 and Amazon RDS Instance is \n      stopped, the compute billing halts and only the storage charge \n      applies.\n    \n    \n      Note that On-Demand Capacity reservations (ODCR) are not a pricing \n      discount. Capacity Reservations are charged at the equivalent \n      On-Demand rate, whether you run instances in reserved capacity or \n      not. They should be considered when you need to provide enough \n      capacity for the resources you plan to run. ODCRs don't have to \n      be tied to long-term commitments, as they can be cancelled when \n      you no longer need them, but they can also benefit from the \n      discounts that Savings Plans or Reserved Instances provide.\n    \n    Implementation steps\n\n      \n    \n       \n       \n       \n      \n    \n        \n          Analyze workload elasticity: Using the hourly granularity\n          in Cost Explorer or a custom dashboard, analyze your workload's elasticity. Look for regular changes\n          in the number of instances that are running. Short duration instances are candidates for\n          Spot Instances or Spot Fleet. \n        \n           \n           \n        \n            \n              Well-Architected Lab: Cost Explorer\n            \n          \n            \n              Well-Architected Lab: Cost Visualization\n            \n          \n      \n        \n          Review existing pricing contracts: Review \n          current contracts or commitments for long term needs. Analyze what you currently \n          have and how much those commitments are in use. Leverage pre-existing \n          contractual discounts or enterprise agreements. Enterprise Agreements \n          give customers the option to tailor agreements that best suit their \n          needs. For long term commitments, consider reserved pricing discounts,\n          Reserved Instances or Savings Plans for the specific instance type, \n          instance family, AWS Region, and Availability Zones. \n        \n      \n         Perform a commitment discount analysis: Using Cost Explorer\n          in your account, review the Savings Plans and Reserved Instance recommendations. To verify that\n          you implement the correct recommendations with the required discounts and risk, follow the\n            Well-Architected labs. \n      \n   \n\n  Resources\n\n      \n    \n      Related documents:\n    \n    \n       \n       \n       \n    \n        \n          Accessing\n          Reserved Instance recommendations\n        \n      \n        \n          Instance\n          purchasing options\n        \n      \n        AWS Enterprise\n        \n      \n    \n      Related videos:\n    \n    \n       \n    \n        \n          Save\n          up to 90% and run production workloads on Spot\n        \n      \n    \n      Related examples:\n    \n    \n       \n       \n       \n    \n        \n          Well-Architected\n          Lab: Cost Explorer\n        \n      \n        \n          Well-Architected\n          Lab: Cost Visualization\n        \n      \n        \n          Well-Architected\n          Lab: Pricing Models\n        \n      \n  \u003cdiv\u003e\u003cdiv\u003e\u003cdiv\u003e\u003cdiv id=\"js_error_message\"\u003e\u003cp\u003e\u003cimg src=\"https://d1ge0kk1l5kms0.cloudfront.net/images/G/01/webservices/console/warning.png\" alt=\"Warning\" /\u003e \u003cstrong\u003eJavascript is disabled or is unavailable in your browser.\u003c/strong\u003e\u003c/p\u003e\u003cp\u003eTo use the Amazon Web Services Documentation, Javascript must be enabled. Please refer to your browser's Help pages for instructions.\u003c/p\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003eDocument ConventionsCOST 7. How do you use pricing models to reduce cost?COST07-BP02 Choose Regions based on cost",
  "COST07-BP02 Choose Regions based on costResource pricing may be different in each Region. Identify Regional cost differences and only deploy in Regions with higher costs to meet latency, data residency and data sovereignty requirements. Factoring in Region cost helps you pay the lowest overall price for this workload.\n    Level of risk exposed if this best practice\n    is not established: Medium\n  \n\n  Implementation guidance\n    The AWS Cloud Infrastructure is global, hosted in \n      multiple locations world-wide, and built \n      around AWS Regions, Availability Zones, Local Zones, AWS Outposts, and Wavelength Zones. A Region \n      is a physical location in the world and each Region is a separate geographic area where AWS has \n      multiple Availability Zones. Availability Zones which are multiple isolated locations within each \n      Region consist of one or more discrete data centers, each with redundant power, networking, and \n      connectivity. \n    Each AWS Region operates within local market conditions, and resource pricing is different in \n      each Region due to differences in the cost of land, fiber, electricity, and taxes, for example. \n      Choose a specific Region to operate a component of or your entire solution so that you can run \n      at the lowest possible price globally. Use AWS Calculator to estimate the costs of your workload \n      in various Regions by searching services by location type (Region, wave length zone and local zone) \n      and Region. \n    When you architect your solutions, a best practice is to seek to place computing resources\n      closer to users to provide lower latency and strong data sovereignty. Select the geographic \n      location based on your business, data privacy, performance, and security requirements. For \n      applications with global end users, use multiple locations.\n     Use Regions that provide lower prices for AWS services to deploy your workloads if you\n      have no obligations in data privacy, security and business requirements. For example, if your\n      default Region is Asia Pacific (Sydney) (ap-southwest-2), and if there are no\n      restrictions (data privacy, security, for example) to use other Regions, deploying\n      non-critical (development and test) Amazon EC2 instances in US East (N. Virginia)\n        (us-east-1)  will cost you less. \n    \n       \n        \n       \n       \n      Region feature matrix table\n    \n     \n    \n      The preceding matrix table shows us that Region 6 is the best option for this given scenario because latency \n      is low compared to other Regions, service is available, and it is the least expensive Region.\n    \n   \n    \n    Implementation steps\n    \n       \n       \n       \n    \n         Review AWS Region pricing: Analyze the workload costs in\n          the current Region. Starting with the highest costs by service and usage type, calculate\n          the costs in other Regions that are available. If the forecasted saving outweighs the cost\n          of moving the component or workload, migrate to the new Region. \n      \n        \n          Review requirements for multi-Region deployments: Analyze your \n          business requirements and obligations (data privacy, security, or performance) to find out if \n          there are any restrictions for you to not to use multiple Regions. If there are no obligations \n          to restrict you to use single Region, then use multiple Regions. \n        \n      \n        \n          Analyze required data transfer: Consider data transfer costs \n          when selecting Regions. Keep your data close to your customer and close to the resources. Select \n          less costly AWS Regions where data flows and where there is minimal data transfer. Depending on \n          your business requirements for data transfer, you can use Amazon CloudFront, AWS PrivateLink, \n          AWS Direct Connect, and AWS Virtual Private Network to reduce your networking costs, improve performance, and enhance \n          security. \n        \n      \n  \n   \n\n  Resources\n\n      \n    \n      Related documents:\n    \n    \n       \n       \n       \n       \n    \n        \n          Accessing\n          Reserved Instance recommendations\n        \n      \n        \n          Amazon EC2 pricing\n        \n      \n        \n          Instance\n          purchasing options\n        \n      \n        \n          Region\n          Table\n        \n      \n    \n      Related videos:\n    \n    \n       \n    \n        \n          Save\n          up to 90% and run production workloads on Spot\n        \n      \n    \n      Related examples:\n    \n    \n       \n       \n       \n    \n        \n          Overview of Data Transfer Costs for Common Architectures\n        \n      \n        \n          Cost Considerations for Global Deployments\n        \n      \n        \n          What to Consider when Selecting a Region for your Workloads\n        \n      \n  \u003cdiv\u003e\u003cdiv\u003e\u003cdiv\u003e\u003cdiv id=\"js_error_message\"\u003e\u003cp\u003e\u003cimg src=\"https://d1ge0kk1l5kms0.cloudfront.net/images/G/01/webservices/console/warning.png\" alt=\"Warning\" /\u003e \u003cstrong\u003eJavascript is disabled or is unavailable in your browser.\u003c/strong\u003e\u003c/p\u003e\u003cp\u003eTo use the Amazon Web Services Documentation, Javascript must be enabled. Please refer to your browser's Help pages for instructions.\u003c/p\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003eDocument ConventionsCOST07-BP01 Perform pricing model analysisCOST07-BP03 Select third-party agreements with cost-efficient\n  terms",
  "COST07-BP03 Select third-party agreements with cost-efficient\n  terms\n    Cost efficient agreements and terms ensure the cost of these\n    services scales with the benefits they provide. Select agreements\n    and pricing that scale when they provide additional benefits to your\n    organization.\n  \n    Level of risk exposed if this best practice\n    is not established: Medium\n  \n\n  Implementation guidance\n    \n      There are multiple products on the market that can help you manage costs in your cloud environments. They may have some differences in terms of features that depend on customer requirements, such as some focusing on cost governance or cost visibility and others on cost optimization. One key factor for effective cost optimization and governance is using the right tool with necessary features and the right pricing model. These products have different pricing models. Some charge you a certain percentage of your monthly bill, while others charge a percentage of your realized savings. Ideally, you should pay only for what you need. \n    \n    \n      When you use third-party solutions or services in the cloud, it's important that the\n      pricing structures are aligned to your desired outcomes. Pricing should scale with the\n      outcomes and value it provides. For example, in software that takes a percentage of\n      savings it provides, the more you save (outcome), the more it charges. License agreements where you pay more as your expenses increase might not always be in your best interest for optimizing costs. However, if the vendor offers clear benefits for all parts of your bill, this scaling fee might be justified.\n    \n    \n      For example, a solution that provides recommendations for Amazon EC2 and charges a percentage of your entire bill can become more expensive if you use other services that provide no benefit. Another example is a managed service that is charged at a percentage of the cost of managed resources. A larger instance size may not necessarily require more management effort, but can be charged more. Verify that these service pricing arrangements include a cost optimization program or features in their service to drive efficiency. \n    \n    \n      Customers may find these products on the market more advanced or easier to use. You need to consider the cost of these products and think about potential cost optimization outcomes in the long term. \n    \n     \n      \n      Implementation steps\n    \n    \n       \n    \n        \n           Analyze third-party agreements and terms: Review the pricing in third party agreements. Perform modeling for different levels of your usage, and factor in new costs such as new service usage, or increases in current services due to workload growth. Decide if the additional costs provide the required benefits to your business.\n        \n           \n     \n   \n\n  Resources\n\n      \n    \n      Related documents:\n    \n    \n       \n       \n    \n        \n          Accessing\n          Reserved Instance recommendations\n        \n      \n        \n          Instance\n          purchasing options\n        \n      \n    \n      Related videos:\n    \n    \n       \n    \n        \n          Save\n          up to 90% and run production workloads on Spot\n        \n      \n  \u003cdiv\u003e\u003cdiv\u003e\u003cdiv\u003e\u003cdiv id=\"js_error_message\"\u003e\u003cp\u003e\u003cimg src=\"https://d1ge0kk1l5kms0.cloudfront.net/images/G/01/webservices/console/warning.png\" alt=\"Warning\" /\u003e \u003cstrong\u003eJavascript is disabled or is unavailable in your browser.\u003c/strong\u003e\u003c/p\u003e\u003cp\u003eTo use the Amazon Web Services Documentation, Javascript must be enabled. Please refer to your browser's Help pages for instructions.\u003c/p\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003eDocument ConventionsCOST07-BP02 Choose Regions based on costCOST07-BP04 Implement pricing models for all components of this\n  workload",
  "COST07-BP04 Implement pricing models for all components of this\n  workload\n    Permanently running resources should utilize reserved capacity such\n    as Savings Plans or Reserved Instances. Short-term capacity is\n    configured to use Spot Instances, or Spot Fleet. On-Demand Instances are only\n    used for short-term workloads that cannot be interrupted and do not\n    run long enough for reserved capacity, between 25% to 75% of the\n    period, depending on the resource type.\n  \n    Level of risk exposed if this best practice\n    is not established: Low\n  \n\n  Implementation guidance\n\n    \n      To improve cost efficiency, AWS provides multiple commitment recommendations based on your past usage. You can use these recommendations to understand what you can save, and how the commitment will be used. You can use these services as On-Demand, Spot, or make a commitment for a certain period of time and reduce your on-demand costs with Reserved Instances (RIs) and Savings Plans (SPs). You need to understand not only each workload components and multiple AWS services, but also commitment discounts, purchase options, and Spot Instances for these services to optimize your workload. \n    \n    \n      Consider the requirements of your workload’s components, and understand the different pricing models for these services. Define the availability requirement of these components. Determine if there are multiple independent resources that perform the function in the workload, and what the workload requirements are over time. Compare the cost of the resources using the default On-Demand pricing model and other applicable models. Factor in any potential changes in resources or workload components. \n    \n    \n      For example, let’s look at this Web Application Architecture on AWS. This sample workload consists of multiple AWS services, such as Amazon Route 53, AWS WAF, Amazon CloudFront, Amazon EC2 instances, Amazon RDS instances, Load Balancers, Amazon S3 storage, and Amazon Elastic File System (Amazon EFS). You need to review each of these services, and identify potential cost saving opportunities with different pricing models. Some of them may be eligible for RIs or SPs, while some of them may be only available by on-demand. As the following image shows, some of the AWS services can be committed using RIs or SPs.\n    \n    \n       \n        \n       \n       \n      AWS services committed using Reserved Instances and Savings Plans\n    \n     \n      \n      Implementation steps\n   \n    \n       \n       \n    \n        \n          Implement pricing models: Using your analysis results, purchase Savings Plans, Reserved Instances, or implement Spot Instances. If it is your first commitment purchase, choose the top five or ten recommendations in the list, then monitor and analyze the results over the next month or two. AWS Cost Management Console guides you through the process. Review the RI or SP recommendations from the console, customize the recommendations (type, payment, and term), and review hourly commitment (for example $20 per hour), and then add to cart. Discounts apply automatically to eligible usage. Purchase a small amount of commitment discounts in regular cycles (for example every 2 weeks or monthly). Implement Spot Instances for workloads that can be interrupted or are stateless. Finally, select on-demand Amazon EC2 instances and allocate resources for the remaining requirements.\n      \n        \n          Workload review cycle: Implement a review cycle for the workload that specifically analyzes pricing model coverage. Once the workload has the required coverage, purchase additional commitment discounts partially (every few months), or as your organization usage changes.\n      \n       \n   \n\n  Resources\n\n      \n    \n      Related documents:\n    \n    \n       \n       \n       \n       \n       \n       \n       \n    \n        \n          Understanding your Savings Plans recommendations\n        \n      \n        \n          Accessing\n          Reserved Instance recommendations\n        \n      \n        \n          How\n          to Purchase Reserved Instances\n        \n      \n        \n          Instance\n          purchasing options\n        \n      \n        \n          Spot\n          Instances\n        \n      \n        \n          Reservation models for other AWS services\n        \n      \n        \n          Savings Plans Supported Services\n        \n      \n    \n      Related videos:\n    \n    \n       \n    \n        \n          Save\n          up to 90% and run production workloads on Spot\n        \n      \n    \n      Related examples:\n    \n    \n       \n       \n    \n        \n          What should you consider before purchasing Savings Plans?\n        \n      \n        \n          How can I use Cost Explorer to analyze my spending and usage?\n        \n      \n  \u003cdiv\u003e\u003cdiv\u003e\u003cdiv\u003e\u003cdiv id=\"js_error_message\"\u003e\u003cp\u003e\u003cimg src=\"https://d1ge0kk1l5kms0.cloudfront.net/images/G/01/webservices/console/warning.png\" alt=\"Warning\" /\u003e \u003cstrong\u003eJavascript is disabled or is unavailable in your browser.\u003c/strong\u003e\u003c/p\u003e\u003cp\u003eTo use the Amazon Web Services Documentation, Javascript must be enabled. Please refer to your browser's Help pages for instructions.\u003c/p\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003eDocument ConventionsCOST07-BP03 Select third-party agreements with cost-efficient\n  termsCOST07-BP05 Perform pricing model analysis at the management\n  account level",
  "COST07-BP05 Perform pricing model analysis at the management\n  account level\n    Check billing and cost management tools and see recommended\n    discounts with commitments and reservations to perform regular\n    analysis at the management account level.\n  \n    Level of risk exposed if this best practice\n    is not established: Low\n  \n    \n    Implementation guidance\n    \n    \n    \n      Performing regular cost modeling helps you implement opportunities\n      to optimize across multiple workloads. For example, if multiple\n      workloads use On-Demand Instances at an aggregate level, the risk\n      of change is lower, and implementing a commitment-based discount\n      can achieve a lower overall cost. It is recommended to perform\n      analysis in regular cycles of two weeks to one month. This allows\n      you to make small adjustment purchases, so the coverage of your\n      pricing models continues to evolve with your changing workloads\n      and their components.\n    \n    \n      Use the\n      AWS Cost Explorer recommendations tool to find opportunities\n      for commitment discounts in your management account.\n      Recommendations at the management account level are calculated\n      considering usage across all of the accounts in your AWS\n      organization that have Reserve Instances (RI) or Savings Plans (SP). They're also calculated when\n      discount sharing is activated to recommend a commitment that\n      maximizes savings across accounts.\n    \n    \n      While purchasing at the management account level optimizes for max\n      savings in many cases, there may be situations where you might\n      consider purchasing SPs at the linked account level, like when you\n      want the discounts to apply first to usage in that particular\n      linked account. Member account recommendations are calculated at\n      the individual account level, to maximize savings for each\n      isolated account. If your account owns both RI and SP commitments,\n      they will be applied in this order:\n    \n    \n       \n       \n       \n       \n       \n    \n        \n          Zonal RI\n        \n      \n        \n          Standard RI\n        \n      \n        \n          Convertible RI\n        \n      \n        \n          Instance Savings Plan\n        \n      \n        \n          Compute Savings\n          Plan\n        \n      \n    \n      If you purchase an SP at the management account level, the savings\n      will be applied based on highest to lowest discount percentage.\n      SPs at the management account level look across all linked\n      accounts and apply the savings wherever the discount will be the\n      highest. If you wish to restrict where the savings are applied,\n      you can purchase a Savings Plan at the linked account level and\n      any time that account is running eligible compute services, the\n      discount will be applied there first. When the account is not\n      running eligible compute services, the discount will be shared\n      across the other linked accounts under the same management\n      account. Discount sharing is turned on by default, but can be\n      turned off if needed.\n    \n    \n      In a Consolidated Billing Family, Savings Plans are applied first\n      to the owner account's usage, and then to other accounts' usage.\n      This occurs only if you have sharing enabled. Your Savings Plans\n      are applied to your highest savings percentage first. If there are\n      multiple usages with equal savings percentages, Savings Plans are\n      applied to the first usage with the lowest Savings Plans rate.\n      Savings Plans continue to apply until there are no more remaining\n      uses or your commitment is exhausted. Any remaining usage is\n      charged at the On-Demand rates. You can refresh Savings Plans\n      Recommendations in AWS Cost Management to generate new Savings Plans Recommendations at any time.\n    \n    \n      After analyzing flexibility of instances, you can commit by\n      following recommendations. Create cost modeling by analyzing the\n      workload’s short-term costs with potential different resource\n      options, analyzing AWS pricing models, and aligning them with your\n      business requirements to find out total cost of ownership and\n      cost\n        optimization opportunities.\n    \n     \n      \n      Implementation steps\n      \n      \n      \n        Perform a commitment discount\n          analysis: Use Cost Explorer in your account review\n        the Savings Plans and Reserved Instance recommendations. Make\n        sure you understand Saving Plan recommendations, and estimate\n        your monthly spend and monthly savings. Review recommendations\n        at the management account level, which are calculated\n        considering usage across all of the member accounts in your AWS\n        organization that have RI or Savings Plans discount sharing\n        enabled for maximum savings across accounts. You can verify that\n        you implemented the correct recommendations with the required\n        discounts and risk by following the Well-Architected labs.\n      \n     \n   \n    \n    Resources\n    \n    \n    \n      Related documents:\n    \n    \n       \n       \n       \n       \n       \n       \n       \n       \n       \n    \n        \n          How\n            does AWS pricing work?\n        \n      \n        \n          Instance\n            purchasing options\n        \n      \n        \n          Saving\n            Plan Overview\n        \n      \n        \n          Saving\n            Plan recommendations\n        \n      \n        \n          Accessing\n            Reserved Instance recommendations\n        \n      \n        \n          Understanding\n            your Saving Plans recommendation\n        \n      \n        \n          How\n            Savings Plans apply to your AWS usage\n        \n      \n        \n          Saving\n            Plans with Consolidated Billing\n        \n      \n        \n          Turning\n            on shared reserved instances and Savings Plans\n            discounts\n        \n      \n    \n      Related videos:\n    \n    \n       \n    \n        \n          Save\n            up to 90% and run production workloads on Spot\n        \n      \n    \n      Related examples:\n    \n    \n       \n       \n       \n    \n        \n          What\n            should I consider before purchasing a Savings Plan?\n        \n      \n        \n          How\n            can I use rolling Savings Plans to reduce commitment\n            risk?\n        \n      \n        \n          When\n            to Use Spot Instances\n        \n      \n  \u003cdiv\u003e\u003cdiv\u003e\u003cdiv\u003e\u003cdiv id=\"js_error_message\"\u003e\u003cp\u003e\u003cimg src=\"https://d1ge0kk1l5kms0.cloudfront.net/images/G/01/webservices/console/warning.png\" alt=\"Warning\" /\u003e \u003cstrong\u003eJavascript is disabled or is unavailable in your browser.\u003c/strong\u003e\u003c/p\u003e\u003cp\u003eTo use the Amazon Web Services Documentation, Javascript must be enabled. Please refer to your browser's Help pages for instructions.\u003c/p\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003eDocument ConventionsCOST07-BP04 Implement pricing models for all components of this\n  workloadCOST 8. How do you plan for data transfer charges?",
  "COST08-BP01 Perform data transfer modeling\n    Gather organization requirements and perform data transfer modeling\n    of the workload and each of its components. This identifies the\n    lowest cost point for its current data transfer requirements.\n  \n    Level of risk exposed if this best practice\n    is not established: High\n  \n\n  Implementation guidance\n    \n      When designing a solution in the cloud, data transfer fees are usually neglected due to habits of designing architecture using on-premises data centers or lack of knowledge. Data transfer charges in AWS are determined by the source, destination, and volume of traffic. Factoring in these fees during the design phase can lead to cost savings. Understanding where the data transfer occurs in your workload, the cost of the transfer, and its associated benefit is very important to accurately estimate total cost of ownership (TCO). This allows you to make an informed decision to modify or accept the architectural decision. For example, you may have a Multi-Availability Zone configuration where you replicate data between the Availability Zones. \n    \n    \n      You model the components of services which transfer the data in your workload, and decide that this is an acceptable cost (similar to paying for compute and storage in both Availability Zones) to achieve the required reliability and resilience.  Model the costs over different usage levels. Workload usage can change over time, and different services may be more cost effective at different levels. \n    \n    \n      While modelling your data transfer, think about how much data is ingested and where that data comes from. Additionally, consider how much data is processed and how much storage or compute capacity is needed. During modelling, follow networking best practices for your workload architecture to optimize your potential data transfer costs.\n    \n    \n      The AWS Pricing Calculator can help you see estimated costs for specific AWS services and expected data transfer. If you have a workload already running (for test purposes or in a pre-production environment), use AWS Cost Explorer or AWS Cost and Usage Report (CUR) to understand and model your data transfer costs. Configure a proof\n      of concept (PoC) or test your workload, and run a test with a realistic simulated load. You\n      can model your costs at different workload demands.\n    \n     \n      \n      Implementation steps\n   \n    \n       \n       \n       \n       \n       \n       \n       \n    \n        \n          Identify requirements: What is the primary goal and business requirements for the planned data transfer between source and destination? What is the expected business outcome at the end? Gather business requirements and define expected outcome.\n        \n      \n        \n          Identify source and destination: What is the data source and destination for the data transfer, such as within AWS Regions, to AWS services, or out to the internet?\n        \n        \n           \n           \n           \n        \n            \n              Data transfer within an AWS Region\n          \n            \n              Data transfer between AWS Regions\n          \n            \n              Data transfer out to the internet\n            \n          \n      \n        \n          Identify data classifications: What is the data classification for this data transfer? What kind of data is it? How big is the data? How frequently must data be transferred? Is data sensitive? \n        \n      \n        \n          Identify AWS services or tools to use: Which AWS services are used for this data transfer? Is it possible to use an already-provisioned service for another workload?  \n        \n      \n        \n           Calculate data transfer costs: Use AWS Pricing the data transfer modeling you created previously to calculate the data\n          transfer costs for the workload. Calculate the data transfer costs at different usage\n          levels, for both increases and reductions in workload usage. Where there are multiple\n          options for the workload architecture, calculate the cost for each option for comparison.\n        \n      \n        \n           Link costs to outcomes: For each data transfer cost\n          incurred, specify the outcome that it achieves for the workload. If it is transfer between\n          components, it may be for decoupling, if it is between Availability Zones it may be for\n          redundancy.\n        \n      \n        \n          Create data transfer modeling: After gathering all information, create a conceptual base data transfer modeling for multiple use cases and different workloads.\n        \n      \n     \n   \n\n  Resources\n\n      \n    \n      Related documents:\n    \n    \n       \n       \n       \n       \n       \n    \n        \n          AWS caching solutions\n        \n      \n        \n          AWS Pricing\n        \n      \n        \n          Amazon EC2 Pricing\n        \n      \n        \n          Amazon VPC pricing\n        \n      \n        \n          Understanding data transfer charges\n        \n      \n    \n      Related videos:\n    \n    \n       \n       \n    \n        \n          Monitoring and Optimizing Your Data Transfer Costs\n        \n      \n         \n          S3 Transfer Acceleration\n        \n      \n    \n      Related examples:\n    \n    \n       \n       \n    \n        \n          Overview of Data Transfer Costs for Common Architectures\n        \n      \n        AWS Prescriptive Guidance for Networking\n        \n      \n  \u003cdiv\u003e\u003cdiv\u003e\u003cdiv\u003e\u003cdiv id=\"js_error_message\"\u003e\u003cp\u003e\u003cimg src=\"https://d1ge0kk1l5kms0.cloudfront.net/images/G/01/webservices/console/warning.png\" alt=\"Warning\" /\u003e \u003cstrong\u003eJavascript is disabled or is unavailable in your browser.\u003c/strong\u003e\u003c/p\u003e\u003cp\u003eTo use the Amazon Web Services Documentation, Javascript must be enabled. Please refer to your browser's Help pages for instructions.\u003c/p\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003eDocument ConventionsCOST 8. How do you plan for data transfer charges?COST08-BP02 Select components to optimize data transfer\n  cost",
  "COST08-BP02 Select components to optimize data transfer\n  cost All components are selected, and architecture is designed to reduce data transfer costs.\n    This includes using components such as wide-area-network (WAN) optimization and\n    Multi-Availability Zone (AZ) configurations \n    Level of risk exposed if this best practice\n    is not established: Medium\n  \n\n  Implementation guidance\n    \n      Architecting for data transfer minimizes data transfer costs. This may involve using content delivery networks to locate data closer to users, or using dedicated network links from your premises to AWS. You can also use WAN optimization and application optimization to reduce the amount of data that is transferred between components. \n    \n    \n      When transferring data to or within the AWS Cloud, it is essential to know the destination based on varied use cases, the nature of the data, and the available network resources in order to select the right AWS services to optimize data transfer. AWS offers a range of data transfer services tailored for diverse data migration requirements. Select the right data storage and data transfer options based on the business needs within your organization. \n    \n    \n      When planning or reviewing your workload architecture, consider the following:\n    \n    \n       \n       \n       \n       \n       \n       \n    \n        \n          Use VPC endpoints within AWS: VPC endpoints allow for private connections between your VPC and supported AWS services. This allows you to avoid using the public internet, which can lead to data transfer costs.\n        \n      \n        \n          Use a NAT gateway: Use a NAT gateway so that instances in a private subnet can connect to the internet or to the services outside your VPC. Check whether the resources behind the NAT gateway that send the most traffic are in the same Availability Zone as the NAT gateway. If they are not, create new NAT gateways in the same Availability Zone as the resource to reduce cross-AZ data transfer charges.\n        \n      \n        \n          Use AWS Direct Connect AWS Direct Connect bypasses the public internet and establishes a direct, private connection between your on-premises network and AWS. This can be more cost-effective and consistent than transferring large volumes of data over the internet.\n        \n      \n        \n          Avoid transferring data across Regional boundaries: Data transfers between AWS Regions (from one Region to another) typically incur charges. It should be a very thoughtful decision to pursue a multi-Region path. For more detail, see Multi-Region scenarios.\n        \n      \n        \n          Monitor data transfer: Use Amazon CloudWatch and VPC flow logs to capture details about your data transfer and network usage. Analyze captured network traffic information in your VPCs, such as IP address or range going to and from network interfaces.\n        \n      \n        \n          Analyze your network usage: Use metering and reporting tools such as AWS Cost Explorer, CUDOS Dashboards, or CloudWatch to understand data transfer cost of your workload.  \n        \n      \n     \n      \n      Implementation steps\n    \n    \n       \n    \n        \n          Select components for data transfer: Using the data transfer modeling explained in COST08-BP01 Perform data transfer modeling, focus on where the largest data transfer costs are or where they would be if the workload usage changes. Look for alternative architectures or additional components that remove or reduce the need for data transfer (or lower its cost).\n        \n          \n     \n   \n\n  Resources\n\n    \n      Related best practices:\n    \n    \n       \n       \n    \n        \n          COST08-BP01 Perform data transfer modeling\n        \n      \n        \n          COST08-BP03 Implement services to reduce data transfer\n  costs\n        \n      \n    \n      Related documents:\n    \n    \n       \n       \n       \n    \n        \n          Cloud Data Migration\n        \n      \n        \n          AWS           caching solutions\n        \n      \n        \n          Deliver\n          content faster with Amazon CloudFront\n        \n      \n    \n      Related examples:\n    \n    \n       \n       \n       \n    \n        \n          Overview of Data Transfer Costs for Common Architectures\n        \n      \n        AWS Network Optimization Tips\n        \n      \n        \n          Optimize performance and reduce costs for network analytics with VPC Flow Logs in Apache Parquet format\n        \n      \n  \u003cdiv\u003e\u003cdiv\u003e\u003cdiv\u003e\u003cdiv id=\"js_error_message\"\u003e\u003cp\u003e\u003cimg src=\"https://d1ge0kk1l5kms0.cloudfront.net/images/G/01/webservices/console/warning.png\" alt=\"Warning\" /\u003e \u003cstrong\u003eJavascript is disabled or is unavailable in your browser.\u003c/strong\u003e\u003c/p\u003e\u003cp\u003eTo use the Amazon Web Services Documentation, Javascript must be enabled. Please refer to your browser's Help pages for instructions.\u003c/p\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003eDocument ConventionsCOST08-BP01 Perform data transfer modelingCOST08-BP03 Implement services to reduce data transfer\n  costs",
  "COST08-BP03 Implement services to reduce data transfer\n  costs\n    Implement services to reduce data transfer. For example, use edge\n    locations or content delivery networks (CDN) to deliver content to\n    end users, build caching layers in front of your application servers\n    or databases, and use dedicated network connections instead of VPN\n    for connectivity to the cloud.\n  \n    Level of risk exposed if this best practice\n    is not established: Medium\n  \n    \n    Implementation guidance\n    \n    \n    \n      There are various AWS services that can help you to optimize your\n      network data transfer usage. Depending on your workload components,\n      type, and cloud architecture, these services can assist you in\n      compression, caching, and sharing and distribution of your traffic\n      on the cloud.\n    \n    \n       \n       \n       \n       \n       \n       \n    \n        \n          Amazon CloudFront is a global content delivery\n          network that delivers data with low latency and high transfer\n          speeds. It caches data at edge locations across the world,\n          which reduces the load on your resources. By using CloudFront,\n          you can reduce the administrative effort in delivering content\n          to large numbers of users globally with minimum latency. The\n          security\n            savings bundle can help you to save up to 30% on your\n          CloudFront usage if you plan to grow your usage over time.\n        \n      \n        \n          AWS Direct Connect allows you to establish a\n          dedicated network connection to AWS. This can reduce network\n          costs, increase bandwidth, and provide a more consistent\n          network experience than internet-based connections.\n        \n      \n        \n          AWS VPN allows you to establish a secure and\n          private connection between your private network and the AWS\n          global network. It is ideal for small offices or business\n          partners because it provides simplified connectivity, and it\n          is a fully managed and elastic service.\n        \n      \n        \n          VPC\n            Endpoints allow connectivity between AWS\n          services over private networking and can be used to reduce\n          public data transfer and\n          NAT\n            gateway costs.\n          Gateway\n            VPC endpoints have no hourly charges, and\n          support Amazon S3 and Amazon DynamoDB.\n          Interface\n            VPC endpoints are provided by\n          AWS PrivateLink and have an hourly fee and per-GB usage cost.\n        \n      \n        \n          NAT\n            gateways provide built-in scaling and management\n          for reducing costs as opposed to a standalone NAT instance. Place\n          NAT gateways in the same Availability Zones as high traffic\n          instances and consider using VPC endpoints for the instances\n          that need to access Amazon DynamoDB or Amazon S3 to reduce the data\n          transfer and processing costs.\n        \n      \n        \n          Use AWS Snow Family devices which have computing resources to\n          collect and process data at the edge. AWS Snow Family devices\n          (Snowball Edge,\n          Snowball Edge\n          and\n          Snowmobile)\n          allow you to move petabytes of data to the AWS Cloud cost\n          effectively and offline.\n        \n      \n     \n      \n      Implementation steps\n      \n      \n      \n         \n      \n          \n            Implement services:\n            Select applicable AWS network services based on your\n            service workload type using the data transfer modeling and\n            reviewing VPC Flow Logs. Look at where the largest costs and\n            highest volume flows are. Review the AWS services and assess\n            whether there is a service that reduces or removes the\n            transfer, specifically networking and content delivery. Also\n            look for caching services where there is repeated access to\n            data or large amounts of data.\n          \n        \n     \n   \n    \n    Resources\n    \n    \n    \n      Related documents:\n    \n    \n       \n       \n       \n       \n       \n       \n    \n        \n          AWS Direct Connect\n        \n      \n        \n          AWS Explore Our\n            Products\n        \n      \n        \n          AWS           caching solutions\n        \n      \n        \n          Amazon CloudFront\n        \n      \n        \n          AWS Snow Family\n        \n      \n        \n          Amazon CloudFront Security Savings Bundle\n        \n      \n    \n      Related videos:\n    \n    \n       \n       \n       \n    \n        \n          Monitoring\n            and Optimizing Your Data Transfer Costs\n        \n      \n        \n          AWS           Cost Optimization Series: CloudFront\n        \n      \n        \n          How\n            can I reduce data transfer charges for my NAT gateway?\n        \n      \n    \n      Related examples:\n    \n    \n       \n       \n       \n       \n       \n       \n    \n        \n          How-to\n            chargeback shared services: An AWS Transit Gateway\n            example\n        \n      \n        \n          Understand\n            AWS data transfer details in depth from cost and usage report\n            using Athena query and QuickSight\n        \n      \n        \n          Overview\n            of Data Transfer Costs for Common Architectures\n        \n      \n        \n          Using\n            AWS Cost Explorer to analyze data transfer costs\n        \n      \n        \n          Cost-Optimizing\n            your AWS architectures by utilizing Amazon CloudFront\n            features\n        \n      \n        \n          How\n            can I reduce data transfer charges for my NAT gateway?\n        \n      \n  \u003cdiv\u003e\u003cdiv\u003e\u003cdiv\u003e\u003cdiv id=\"js_error_message\"\u003e\u003cp\u003e\u003cimg src=\"https://d1ge0kk1l5kms0.cloudfront.net/images/G/01/webservices/console/warning.png\" alt=\"Warning\" /\u003e \u003cstrong\u003eJavascript is disabled or is unavailable in your browser.\u003c/strong\u003e\u003c/p\u003e\u003cp\u003eTo use the Amazon Web Services Documentation, Javascript must be enabled. Please refer to your browser's Help pages for instructions.\u003c/p\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003eDocument ConventionsCOST08-BP02 Select components to optimize data transfer\n  costManage demand and supply resources",
  "COST09-BP01 Perform an analysis on the workload demand\n    Analyze the demand of the workload over time. Verify that the\n    analysis covers seasonal trends and accurately represents operating\n    conditions over the full workload lifetime. Analysis effort should\n    reflect the potential benefit, for example, time spent is\n    proportional to the workload cost.\n  \n    Level of risk exposed if this best practice\n    is not established: High\n  \n    \n    Implementation guidance\n    \n    \n    \n      Analyzing workload demand for cloud computing involves\n      understanding the patterns and characteristics of computing tasks\n      that are initiated in the cloud environment. This analysis helps\n      users optimize resource allocation, manage costs, and verify that\n      performance meets required levels.\n    \n    \n      Know the requirements of the workload. Your organization's\n      requirements should indicate the workload response times for\n      requests. The response time can be used to determine if the demand\n      is managed, or if the supply of resources should change to meet\n      the demand.\n    \n    \n      The analysis should include the predictability and repeatability\n      of the demand, the rate of change in demand, and the amount of\n      change in demand. Perform the analysis over a long enough period\n      to incorporate any seasonal variance, such as end-of-month\n      processing or holiday peaks.\n    \n    \n      Analysis effort should reflect the potential benefits of\n      implementing scaling. Look at the expected total cost of the\n      component and any increases or decreases in usage and cost over\n      the workload's lifetime.\n    \n    \n      The following are some key aspects to consider when performing\n      workload demand analysis for cloud computing:\n    \n    \n       \n       \n       \n       \n    \n        \n          Resource utilization and performance\n            metrics: Analyze how AWS resources are being used\n          over time. Determine peak and off-peak usage patterns to\n          optimize resource allocation and scaling strategies. Monitor\n          performance metrics such as response times, latency,\n          throughput, and error rates. These metrics help assess the\n          overall health and efficiency of the cloud infrastructure.\n        \n      \n        \n          User and application scaling\n            behaviour: Understand user behavior and how it\n          affects workload demand. Examining the patterns of user\n          traffic assists in enhancing the delivery of content and the\n          responsiveness of applications. Analyze how workloads scale\n          with increasing demand. Determine whether auto-scaling\n          parameters are configured correctly and effectively for\n          handling load fluctuations.\n        \n      \n        \n          Workload types: Identify\n          the different types of workloads running in the cloud, such as\n          batch processing, real-time data processing, web applications,\n          databases, or machine learning. Each type of workload may have\n          different resource requirements and performance profiles.\n        \n      \n        \n          Service-level agreements\n            (SLAs): Compare actual performance with SLAs to\n          ensure compliance and identify areas that need improvement.\n        \n      \n    \n      You can use\n      Amazon CloudWatch to collect and track metrics, monitor log files,\n      set alarms, and automatically react to changes in your AWS\n      resources. You can also use Amazon CloudWatch to gain system-wide\n      visibility into resource utilization, application performance, and\n      operational health.\n    \n    \n      With\n      AWS Trusted Advisor, you can provision your resources following\n      best practices to improve system performance and reliability,\n      increase security, and look for opportunities to save money. You\n      can also turn off non-production instances and use Amazon CloudWatch and Auto Scaling to match increases or reductions in\n      demand.\n    \n    \n      Finally, you can use\n      AWS Cost Explorer or\n      QuickSight with the AWS Cost and Usage Report (CUR) file or your application\n      logs to perform advanced analysis of workload demand.\n    \n    \n      Overall, a comprehensive workload demand analysis allows\n      organizations to make informed decisions about resource\n      provisioning, scaling, and optimization, leading to better\n      performance, cost efficiency, and user satisfaction.\n    \n     \n      \n      Implementation steps\n      \n      \n      \n         \n         \n      \n          \n            Analyze existing workload\n              data: Analyze data from the existing workload,\n            previous versions of the workload, or predicted usage\n            patterns. Use Amazon CloudWatch, log files and monitoring\n            data to gain insight on how workload was used. Analyze a\n            full cycle of the workload, and collect data for any\n            seasonal changes such as end-of-month or end-of-year events.\n            The effort reflected in the analysis should reflect the\n            workload characteristics. The largest effort should be\n            placed on high-value workloads that have the largest changes\n            in demand. The least effort should be placed on low-value\n            workloads that have minimal changes in demand.\n          \n        \n          \n            Forecast outside\n              influence: Meet with team members from across the\n            organization that can influence or change the demand in the\n            workload. Common teams would be sales, marketing, or\n            business development. Work with them to know the cycles they\n            operate within, and if there are any events that would\n            change the demand of the workload. Forecast the workload\n            demand with this data.\n          \n        \n     \n   \n    \n    Resources\n    \n    \n    \n      Related documents:\n    \n    \n       \n       \n       \n       \n       \n       \n       \n       \n    \n        \n          Amazon CloudWatch\n        \n      \n        \n          AWS Trusted Advisor\n        \n      \n        \n          AWS X-Ray\n        \n      \n        \n          AWS Auto Scaling\n        \n      \n        \n          AWS           Instance Scheduler\n        \n      \n        \n          Getting\n            started with Amazon SQS\n        \n      \n        \n          AWS Cost Explorer\n        \n      \n        \n          QuickSight\n        \n      \n    \n      Related examples:\n    \n    \n       \n       \n    \n        \n          Monitor,\n            Track and Analyze for cost optimization\n        \n      \n        \n          Searching\n            and analyzing logs in CloudWatch\n        \n      \n  \u003cdiv\u003e\u003cdiv\u003e\u003cdiv\u003e\u003cdiv id=\"js_error_message\"\u003e\u003cp\u003e\u003cimg src=\"https://d1ge0kk1l5kms0.cloudfront.net/images/G/01/webservices/console/warning.png\" alt=\"Warning\" /\u003e \u003cstrong\u003eJavascript is disabled or is unavailable in your browser.\u003c/strong\u003e\u003c/p\u003e\u003cp\u003eTo use the Amazon Web Services Documentation, Javascript must be enabled. Please refer to your browser's Help pages for instructions.\u003c/p\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003eDocument ConventionsCOST 9. How do you manage demand, and supply resources?COST09-BP02 Implement a buffer or throttle to manage\n  demand",
  "COST09-BP02 Implement a buffer or throttle to manage\n  demand\n    Buffering and throttling modify the demand on your workload,\n    smoothing out any peaks. Implement throttling when your clients\n    perform retries. Implement buffering to store the request and defer\n    processing until a later time. Verify that your throttles and\n    buffers are designed so clients receive a response in the required\n    time.\n  \n    Level of risk exposed if this best practice\n    is not established: Medium\n  \n\n  Implementation guidance\n    \n      Implementing a buffer or throttle is crucial in cloud computing in order to manage demand and reduce the provisioned capacity required for your workload. For optimal performance, it's essential to gauge the total demand, including peaks, the pace of change in requests, and the necessary response time. When clients have the ability to resend their requests, it becomes practical to apply throttling. Conversely, for clients lacking retry functionalities, the ideal approach is implementing a buffer solution. Such buffers streamline the influx of requests and optimize the interaction of applications with varied operational speeds. \n    \n    \n       \n        \n       \n       \n      Demand curve with two distinct peaks that require high provisioned capacity\n    \n    \n      Assume a workload with the demand curve shown in preceding image. This workload has two peaks, and to handle those peaks, the resource capacity as shown by orange line is provisioned. The resources and energy used for this workload are not indicated by the area under the demand curve, but the area under the provisioned capacity line, as provisioned capacity is needed to handle those two peaks. Flattening the workload demand curve can help you to reduce the provisioned capacity for a workload and reduce its environmental impact. To smooth out the peak, consider to implement throttling or buffering solution. \n    \n    \n      To understand them better, let’s explore throttling and buffering.\n    \n    \n      Throttling: If the source of the demand has retry\n      capability, then you can implement throttling. Throttling tells the source that if it cannot\n      service the request at the current time, it should try again later. The source waits for a\n      period of time, and then retries the request. Implementing throttling has the advantage of\n      limiting the maximum amount of resources and costs of the workload. In AWS, you can use\n        Amazon API Gateway to implement throttling.\n    \n    \n      Buffer based: A buffer-based approach uses producers (components that send messages to the queue), consumers (components that receive messages from the queue), and a queue (which holds messages) to store the messages. Messages are read by consumers and processed, allowing the messages to run at the rate that meets the consumers’ business requirements. By using a buffer-centric methodology, messages from producers are housed in queues or streams, ready to be accessed by consumers at a pace that aligns with their operational demands. \n    \n    In AWS, you can choose from multiple services to implement a buffering approach. Amazon Simple Queue Service(Amazon SQS) is a managed service that\n      provides queues that allow a single consumer to read individual messages. Amazon Kinesis provides a stream that allows many\n      consumers to read the same messages.\n    \n      Buffering and throttling can smooth out any peaks by modifying the demand on your workload. Use throttling when clients retry actions and use buffering to hold the request and process it later. When working with a buffer-based approach, architect your workload to service the request in the required time, verify that you are able to handle duplicate requests for work. Analyze the overall demand, rate of change, and required response time to right size the throttle or buffer required.\n    \n     \n      \n      Implementation steps\n    \n      \n    \n       \n       \n    \n         Analyze the client requirements: Analyze the client requests to determine if they are capable of performing retries. For clients that cannot perform retries, buffers need to be implemented. Analyze the overall demand, rate of change, and required response time to determine the size of throttle or buffer required.\n      \n         Implement a buffer or throttle: Implement a buffer\n          or throttle in the workload. A queue such as Amazon Simple Queue Service (Amazon SQS) can provide a buffer to\n          your workload components. Amazon API Gateway can provide throttling for your workload components. \n      \n     \n   \n\n  Resources\n\n    \n      Related best practices:\n    \n    \n       \n       \n    \n        \n          SUS02-BP06 Implement buffering or throttling to flatten the demand curve\n        \n      \n        \n          REL05-BP02 Throttle requests\n        \n      \n    \n      Related documents:\n    \n    \n       \n       \n       \n       \n       \n       \n    \n        \n          AWS Auto Scaling\n        \n      \n        \n          AWS           Instance Scheduler\n        \n      \n        \n          Amazon API Gateway\n        \n      \n        \n          Amazon Simple Queue Service\n        \n      \n        \n          Getting\n          started with Amazon SQS\n        \n      \n        \n          Amazon Kinesis\n        \n      \n    \n      Related videos:\n    \n    \n       \n    \n        \n          Choosing the Right Messaging Service for Your Distributed App\n        \n      \n    \n      Related examples:\n    \n    \n       \n       \n       \n       \n    \n        \n          Managing and monitoring API throttling in your workloads\n        \n      \n        \n          Throttling a tiered, multi-tenant REST API at scale using API Gateway\n        \n      \n        \n          Enabling Tiering and Throttling in a Multi-Tenant Amazon EKS SaaS Solution Using Amazon API Gateway\n        \n      \n        \n          Application integration Using Queues and Messages   \n        \n      \n  \u003cdiv\u003e\u003cdiv\u003e\u003cdiv\u003e\u003cdiv id=\"js_error_message\"\u003e\u003cp\u003e\u003cimg src=\"https://d1ge0kk1l5kms0.cloudfront.net/images/G/01/webservices/console/warning.png\" alt=\"Warning\" /\u003e \u003cstrong\u003eJavascript is disabled or is unavailable in your browser.\u003c/strong\u003e\u003c/p\u003e\u003cp\u003eTo use the Amazon Web Services Documentation, Javascript must be enabled. Please refer to your browser's Help pages for instructions.\u003c/p\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003eDocument ConventionsCOST09-BP01 Perform an analysis on the workload demandCOST09-BP03 Supply resources dynamically",
  "COST09-BP03 Supply resources dynamicallyResources are provisioned in a planned manner. This can be demand-based, such as through automatic scaling, or time-based, where demand is predictable and resources are provided based on time. These methods result in the least amount of over-provisioning or under-provisioning.\n    Level of risk exposed if this best practice\n    is not established: Low\n  \n\n  Implementation guidance\n    \n      There are several ways for AWS customers to increase the resources available to their applications and supply resources to meet the demand. One of these options is to use AWS Instance Scheduler, which automates the starting and stopping of Amazon Elastic Compute Cloud (Amazon EC2) and Amazon Relational Database Service (Amazon RDS) instances. The other option is to use AWS Auto Scaling, which allows you to automatically scale your computing resources based on the demand of your application or service. Supplying resources based on demand will allow you to pay for the resources you use only, reduce cost by launching resources when they are needed, and terminate them when they aren't.\n    \n    \n      AWS Instance Scheduler \n      allows you to configure the stop and start of your Amazon EC2 and Amazon RDS instances \n      at defined times so that you can meet the demand for the same resources within a consistent time \n      pattern such as every day user access Amazon EC2 instances at eight in the morning that they don’t need \n      after six at night. This solution helps reduce operational cost by stopping resources that are \n      not in use and starting them when they are needed. \n    \n    \n       \n        \n       \n       \n      Cost optimization with AWS Instance Scheduler.\n    \n    \n      \n    \n    You can also easily configure schedules for your Amazon EC2 instances across your accounts and Regions with a simple user interface (UI) using AWS Systems Manager Quick Setup. You can schedule Amazon EC2 or Amazon RDS instances with AWS Instance Scheduler and you can stop and start existing instances. However, you cannot stop and start instances which are part of your Auto Scaling group (ASG) or that manage services such as Amazon Redshift or Amazon OpenSearch Service. Auto Scaling groups have their own scheduling for the instances in the group and these instances are created. \n    AWS Auto Scaling helps you adjust your capacity to maintain steady, predictable performance at the lowest possible cost to meet changing demand. It is a fully managed and free service to scale the capacity of your application that integrates with Amazon EC2 instances and Spot Fleets, Amazon ECS, Amazon DynamoDB, and Amazon Aurora. Auto Scaling provides automatic resource discovery to help find resources in your workload that can be configured, it has built-in scaling strategies to optimize performance, costs, or a balance between the two, and provides predictive scaling to assist with regularly occurring spikes. \n    \n      There are multiple scaling options available to scale your Auto Scaling group:\n    \n    \n       \n       \n       \n       \n       \n    \n        \n          Maintain current instance levels at all times\n        \n      \n        \n          Scale manually\n        \n      \n        \n          Scale based on a schedule\n        \n      \n        \n          Scale based on demand\n        \n      \n        \n          Use predictive scaling\n        \n      \n    \n      Auto Scaling policies differ and can be categorized as dynamic and scheduled scaling policies. Dynamic policies are manual or dynamic scaling which, scheduled or predictive scaling. You can use scaling policies for dynamic, scheduled, and predictive scaling. You can also use metrics and alarms from Amazon CloudWatch to trigger scaling events for your workload. We recommend you use launch templates, which allow you to access the latest features and improvements. Not all Auto Scaling features are available when you use launch configurations. For example, you cannot create an Auto Scaling group that launches both Spot and On-Demand Instances or that specifies multiple instance types. You must use a launch template to configure these features. When using launch templates, we recommended you version each one. With versioning of launch templates, you can create a subset of the full set of parameters. Then, you can reuse it to create other versions of the same launch template. \n    \n    \n      You can use AWS Auto Scaling or incorporate scaling in your code with AWS APIs or SDKs. This reduces your overall workload costs by removing the operational cost from manually making changes to your environment, and changes can be performed much faster. This also matches your workload resourcing to your demand at any time. In order to follow this best practice and supply resources dynamically for your organization, you should understand horizontal and vertical scaling in the AWS Cloud, as well as the nature of the applications running on Amazon EC2 instances. It is better for your Cloud Financial Management team to work with technical teams to follow this best practice. \n    \n    \n      Elastic Load Balancing (Elastic Load Balancing) helps you scale by distributing demand across multiple resources. With using ASG and Elastic Load Balancing, you can manage incoming requests by optimally routing traffic so that no one instance is overwhelmed in an Auto Scaling group. The requests would be distributed among all the targets of a target group in a round-robin fashion without consideration for capacity or utilization.\n    \n    \n      Typical metrics can be standard Amazon EC2 metrics, such as CPU utilization, network throughput, and Elastic Load Balancing observed request and response latency. When possible, you should use a metric that is indicative of customer experience, typically a custom metric that might originate from application code within your workload. To elaborate how to meet the demand dynamically in this document, we will group Auto Scaling into two categories as demand-based and time-based supply models and deep dive into each. \n    \n    Demand-based supply: Take advantage of elasticity of the cloud to supply resources to meet changing demand by relying on near real-time demand state. For demand-based supply, use APIs or service features to programmatically vary the amount of cloud resources in your architecture. This allows you to scale components in your architecture and increase the number of resources during demand spikes to maintain performance and decrease capacity when demand subsides to reduce costs. \n    \n       \n        \n       \n       \n      Demand-based dynamic scaling policies\n    \n     \n    \n       \n       \n    \n        \n          Simple/Step Scaling: Monitors metrics and adds/removes instances as per steps defined by the customers manually.\n        \n      \n        \n          Target Tracking: Thermostat-like control mechanism that automatically adds or removes instances to maintain metrics at a customer defined target.\n        \n      \n    When architecting with a demand-based approach keep in mind two key considerations.\n      First, understand how quickly you must provision new resources. Second, understand that\n      the size of margin between supply and demand will shift. You must be ready to cope with\n      the rate of change in demand and also be ready for resource failures.\n    Time-based supply: A time-based approach aligns resource\n      capacity to demand that is predictable or well-defined by time. This approach is typically not\n      dependent upon utilization levels of the resources. A time-based approach ensures that\n      resources are available at the specific time they are required and can be provided without\n      any delays due to start-up procedures and system or consistency checks. Using a time-based\n      approach, you can provide additional resources or increase capacity during busy\n      periods.\n    \n       \n        \n       \n       \n      Time-based scaling policies\n    \n     \n    You can use scheduled or predictive auto scaling to implement a time-based approach. Workloads can be\n      scheduled to scale out or in at defined times (for example, the start of business hours), \n      making resources available when users arrive or demand increases. Predictive scaling uses \n      patterns to scale out while scheduled scaling uses pre-defined times to scale out. You can \n      also use attribute-based instance type selection (ABS) strategy in Auto Scaling groups, \n      which lets you express your instance requirements as a set of attributes, such as vCPU, \n      memory, and storage. This also allows you to automatically use newer generation instance \n      types when they are released and access a broader range of capacity with Amazon EC2 Spot Instances. \n      Amazon EC2 Fleet and Amazon EC2 Auto Scaling select and launch instances that fit the specified attributes, \n      removing the need to manually pick instance types.\n    \n    You can also leverage the AWS APIs\n        and SDKs and AWS CloudFormation\n      to automatically provision and decommission entire environments as you need them. This\n      approach is well suited for development or test environments that run only in defined business\n      hours or periods of time. You can use APIs to scale the size of resources within an environment (vertical scaling). For\n      example, you could scale up a production workload by changing the instance size or class.\n      This can be achieved by stopping and starting the instance and selecting the different\n      instance size or class. This technique can also be applied to other resources, such as Amazon EBS\n      Elastic Volumes, which can be modified to increase size, adjust performance (IOPS) or\n      change the volume type while in use.\n    When architecting with a time-based approach keep in mind two key considerations. First,\n      how consistent is the usage pattern? Second, what is the impact if the pattern changes? You\n      can increase the accuracy of predictions by monitoring your workloads and by using\n      business intelligence. If you see significant changes in the usage pattern, you can adjust the\n      times to ensure that coverage is provided.\n   \n    \n    Implementation steps\n    \n       \n       \n       \n    \n         Configure scheduled scaling: For predictable\n          changes in demand, time-based scaling can provide the correct number of resources in a\n          timely manner. It is also useful if resource creation and configuration is not fast enough\n          to respond to changes on demand. Using the workload analysis configure scheduled scaling\n          using AWS Auto Scaling. To configure time-based scheduling, you can use predictive scaling of scheduled \n          scaling to increase the number of Amazon EC2 instances in your Auto Scaling groups in advance according \n          to expected or predictable load changes.\n      \n        \n          Configure predictive scaling: Predictive scaling allows you \n          to increase the number of Amazon EC2 instances in your Auto Scaling group in advance of daily and \n          weekly patterns in traffic flows. If you have regular traffic spikes and applications that \n          take a long time to start, you should consider using predictive scaling. Predictive scaling \n          can help you scale faster by initializing capacity before projected load compared to dynamic \n          scaling alone, which is reactive in nature. For example, if users start using your workload \n          with the start of the business hours and don’t use after hours, then predictive scaling can \n          add capacity before the business hours which eliminates delay of dynamic scaling to react to \n          changing traffic.   \n        \n      \n         Configure dynamic automatic scaling: To configure scaling based\n          on active workload metrics, use Auto Scaling. Use the analysis and configure Auto Scaling to launch on the \n          correct resource levels, and verify that the workload scales in the required time. You can launch \n          and automatically scale a fleet of On-Demand Instances and Spot Instances within a single \n          Auto Scaling group. In addition to receiving discounts for using Spot Instances, you can use \n          Reserved Instances or a Savings Plan to receive discounted rates of the regular On-Demand \n          Instance pricing. All of these factors combined help you to optimize your cost savings for \n          Amazon EC2 instances and help you get the desired scale and performance for your application.\n      \n   \n\n  Resources\n\n      \n    \n      Related documents:\n    \n    \n       \n       \n       \n       \n       \n       \n       \n    \n        \n          AWS Auto Scaling\n        \n      \n        \n          AWS Instance Scheduler\n        \n      \n        \n          Scale the size of your Auto Scaling group\n        \n      \n        \n          Getting\n          Started with Amazon EC2 Auto Scaling\n        \n      \n        \n          Getting\n          started with Amazon SQS\n        \n      \n        \n          Scheduled\n          Scaling for Amazon EC2 Auto Scaling\n        \n      \n        \n          Predictive scaling for Amazon EC2 Auto Scaling\n        \n      \n    \n      Related videos:\n    \n    \n       \n       \n    \n        \n          Target Tracking Scaling Policies for Auto Scaling\n        \n      \n        AWS Instance Scheduler\n        \n      \n    \n      Related examples:\n    \n    \n       \n       \n       \n       \n    \n        \n          Attribute based Instance Type Selection for Auto Scaling for Amazon EC2 Fleet\n        \n      \n        \n          Optimizing Amazon Elastic Container Service for cost using scheduled scaling\n        \n      \n        \n          Predictive Scaling with Amazon EC2 Auto Scaling\n        \n      \n        \n          How do I use Instance Scheduler with AWS CloudFormation to schedule Amazon EC2 instances?\n        \n      \n  \u003cdiv\u003e\u003cdiv\u003e\u003cdiv\u003e\u003cdiv id=\"js_error_message\"\u003e\u003cp\u003e\u003cimg src=\"https://d1ge0kk1l5kms0.cloudfront.net/images/G/01/webservices/console/warning.png\" alt=\"Warning\" /\u003e \u003cstrong\u003eJavascript is disabled or is unavailable in your browser.\u003c/strong\u003e\u003c/p\u003e\u003cp\u003eTo use the Amazon Web Services Documentation, Javascript must be enabled. Please refer to your browser's Help pages for instructions.\u003c/p\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003eDocument ConventionsCOST09-BP02 Implement a buffer or throttle to manage\n  demandOptimize over time",
  "COST10-BP01 Develop a workload review process\n    Develop a process that defines the criteria and process for workload\n    review. The review effort should reflect potential benefit. For\n    example, core workloads or workloads with a value of over ten percent of the\n    bill are reviewed quarterly or every six months, while workloads below ten percent \n    are reviewed annually.\n  \n    Level of risk exposed if this best practice\n    is not established: High\n  \n\n  Implementation guidance\n\n    To have the most cost-efficient workload, you must regularly review the workload to know \n      if there are opportunities to implement new services, features, and components. To achieve \n      overall lower costs the process must be proportional to the potential amount of savings. \n      For example, workloads that are 50% of your overall spend should be reviewed more regularly, \n      and more thoroughly, than workloads that are five percent of your overall spend. Factor in \n      any external factors or volatility. If the workload services a specific geography or market \n      segment, and change in that area is predicted, more frequent reviews could lead to cost \n      savings. Another factor in review is the effort to implement changes. If there are significant \n      costs in testing and validating changes, reviews should be less frequent.   \n    Factor in the long-term cost of maintaining outdated and legacy, components and resources \n      and the inability to implement new features into them. The current cost of testing and validation \n      may exceed the proposed benefit. However, over time, the cost of making the change may significantly \n      increase as the gap between the workload and the current technologies increases, resulting in even \n      larger costs. For example, the cost of moving to a new programming language may not currently be \n      cost effective. However, in five years time, the cost of people skilled in that language may \n      increase, and due to workload growth, you would be moving an even larger system to the new language, \n      requiring even more effort than previously.   \n    Break down your workload into components, assign the cost of the component (an estimate is sufficient), \n      and then list the factors (for example, effort and external markets) next to each component. \n      Use these indicators to determine a review frequency for each workload. For example, you may \n      have webservers as a high cost, low change effort, and high external factors, resulting in \n      high frequency of review. A central database may be medium cost, high change effort, and \n      low external factors, resulting in a medium frequency of review.\n     \n    \n      Define a process to evaluate new services, design patterns, resource types, and configurations \n      to optimize your workload cost as they become available. Similar to performance pillar review \n      and reliability pillar review processes, identify, validate, and prioritize optimization and \n      improvement activities and issue remediation and incorporate this into your backlog.\n    \n    Implementation steps\n    \n       \n       \n    \n        \n          Define review frequency: Define how frequently the workload and \n          its components should be reviewed. Allocate time and resources to continual improvement and review \n          frequency to improve the efficiency and optimization of your workload. This is a combination of \n          factors and may differ from workload to workload within your organization and between components \n          in the workload. Common factors include the importance to the organization measured in terms of \n          revenue or brand, the total cost of running the workload (including operation and resource costs), \n          the complexity of the workload, how easy is it to implement a change, any software licensing \n          agreements, and if a change would incur significant increases in licensing costs due to punitive \n          licensing. Components can be defined functionally or technically, such as web servers and databases, \n          or compute and storage resources. Balance the factors accordingly and develop a period for the \n          workload and its components. You may decide to review the full workload every 18 months, review \n          the web servers every six months, the database every 12 months, compute and short-term storage \n          every six months, and long-term storage every 12 months.\n      \n         Define review thoroughness: Define how much effort is spent \n          on the review of the workload or workload components. Similar to the review frequency, this is a \n          balance of multiple factors. Evaluate and prioritize opportunities for improvement to focus efforts \n          where they provide the greatest benefits while estimating how much effort is required for these \n          activities. If the expected outcomes do not satisfy the goals, and required effort costs more, \n          then iterate using alternative courses of action. Your review processes should include dedicated \n          time and resources to make continuous incremental improvements possible. As an example, you may \n          decide to spend one week of analysis on the database component, one week of analysis for compute \n          resources, and four hours for storage reviews.\n      \n   \n\n  Resources\n\n      \n    \n      Related documents:\n    \n    \n       \n       \n       \n    \n        \n          AWS News\n          Blog\n        \n      \n        \n          Types of Cloud Computing\n          \n        \n      \n        \n          What's New with\n          AWS\n        \n      \n    \n      Related examples:\n    \n    \n       \n       \n    \n        AWS Support Proactive Services\n        \n      \n        \n          Regular workload reviews for SAP workloads\n        \n      \n  \u003cdiv\u003e\u003cdiv\u003e\u003cdiv\u003e\u003cdiv id=\"js_error_message\"\u003e\u003cp\u003e\u003cimg src=\"https://d1ge0kk1l5kms0.cloudfront.net/images/G/01/webservices/console/warning.png\" alt=\"Warning\" /\u003e \u003cstrong\u003eJavascript is disabled or is unavailable in your browser.\u003c/strong\u003e\u003c/p\u003e\u003cp\u003eTo use the Amazon Web Services Documentation, Javascript must be enabled. Please refer to your browser's Help pages for instructions.\u003c/p\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003eDocument ConventionsCOST 10. How do you evaluate new services?COST10-BP02 Review and analyze this workload regularly",
  "COST10-BP02 Review and analyze this workload regularlyExisting workloads are regularly reviewed based on each defined \n    process to find out if new services can be adopted, existing services \n    can be replaced, or workloads can be re-architected.\n    Level of risk exposed if this best practice\n    is not established: Medium\n  \n\n  Implementation guidance\n    AWS is constantly adding new features so you can experiment and innovate faster with the latest technology. AWS What's New details how AWS is doing this and provides a quick overview of AWS services, features, and Regional expansion announcements as they are released. You can dive deeper into the launches that have been announced and use them for your review and analyze of your existing workloads. To realize the benefits of new AWS services and features, you review on your workloads and implement new services and features as required. This means you may need to replace existing services you use for your workload, or modernize your workload to adopt these new AWS services. For example, you might review your workloads and replace the messaging component with Amazon Simple Email Service. This removes the cost of operating and maintaining a fleet of instances, while providing all the functionality at a reduced cost. \n    \n      To analyze your workload and highlight potential opportunities, you should consider not only new services but also new ways of building solutions. Review the This is My Architecture videos on AWS to learn about other customers’ architecture designs, their challenges and their solutions. Check the All-In series to find out real world applications of AWS services and customer stories. You can also watch the Back to Basics video series that explains, examines, and breaks down basic cloud architecture pattern best practices. Another source is How to Build This videos, which are designed to assist people with big ideas on how to bring their minimum viable product (MVP) to life using AWS services. It is a way for builders from all over the world who have a strong idea to gain architectural guidance from experienced AWS Solutions Architects. Finally, you can review the Getting Started resource materials, which has step by step tutorials.\n    \n    \n      Before starting your review process, follow your business’ requirements for the workload, security and data privacy requirements in order to use specific service or Region and performance requirements while following your agreed review process. \n    \n    Implementation steps\n      \n    \n       \n       \n    \n         Regularly review the workload: Using your defined\n          process, perform reviews with the frequency specified. Verify that you spend the correct\n          amount of effort on each component. This process would be similar to the initial design\n          process where you selected services for cost optimization. Analyze the services and the\n          benefits they would bring, this time factor in the cost of making the change, not just the\n          long-term benefits. \n      \n         Implement new services: If the outcome of the\n          analysis is to implement changes, first perform a baseline of the workload to know the\n          current cost for each output. Implement the changes, then perform an analysis to confirm\n          the new cost for each output. \n      \n   \n\n  Resources\n\n      \n    \n      Related documents:\n    \n    \n       \n       \n       \n       \n       \n    \n        \n          AWS News\n          Blog\n        \n      \n        \n          What's New with\n          AWS\n        \n      \n        AWS Documentation\n        \n      \n        AWS Getting Started\n        \n      \n        AWS General Resources\n        \n      \n    \n      Related videos:\n    \n    \n       \n       \n       \n       \n    \n        \n          AWS - This is My Architecture\n        \n      \n        \n          AWS - Back to Basics        \n      \n        \n          AWS - All-In series        \n      \n        \n          How to Build This        \n      \n  \u003cdiv\u003e\u003cdiv\u003e\u003cdiv\u003e\u003cdiv id=\"js_error_message\"\u003e\u003cp\u003e\u003cimg src=\"https://d1ge0kk1l5kms0.cloudfront.net/images/G/01/webservices/console/warning.png\" alt=\"Warning\" /\u003e \u003cstrong\u003eJavascript is disabled or is unavailable in your browser.\u003c/strong\u003e\u003c/p\u003e\u003cp\u003eTo use the Amazon Web Services Documentation, Javascript must be enabled. Please refer to your browser's Help pages for instructions.\u003c/p\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003eDocument ConventionsCOST10-BP01 Develop a workload review processCOST 11. How do you evaluate the cost of effort?",
  "COST11-BP01 Perform automation for operations\n    Evaluate the operational costs on the cloud, focusing on\n    quantifying the time and effort savings in administrative tasks,\n    deployments, mitigating the risk of human errors, compliance, and\n    other operations through automation. Assess the time and associated\n    costs required for operational efforts and implement automation for\n    administrative tasks to minimize manual effort wherever\n    feasible.\n  \n    Level of risk exposed if this best practice\n    is not established: Low\n  \n\n  Implementation guidance\n\n      \n    \n      Automating operations reduces the frequency of manual tasks,\n      improves efficiency, and benefits customers by delivering a\n      consistent and reliable experience when deploying, administering,\n      or operating workloads. You can free up infrastructure resources\n      from manual operational tasks and use them for higher value tasks\n      and innovations, which improves business value. Enterprises\n      require a proven, tested way to manage their workloads in the\n      cloud. That solution must be secure, fast, and cost effective,\n      with minimum risk and maximum reliability.\n    \n    \n      Start by prioritizing your operational activities based on\n      required effort by looking at overall operations cost. For\n      example, how long does it take to deploy new resources in the\n      cloud, make optimization changes to existing ones, or implement\n      necessary configurations? Look at the total cost of human actions\n      by factoring in cost of operations and management. Prioritize\n      automations for admin tasks to reduce the human effort.\n    \n    \n      Review effort should reflect the potential benefit. For example,\n      examine time spent performing tasks manually as opposed to\n      automatically. Prioritize automating repetitive, high value, time\n      consuming and complex activities. Activities that pose a high\n      value or high risk of human error are typically the better place\n      to start automating as the risk often poses an unwanted additional\n      operational cost (like operations team working extra hours).\n    \n    \n      Use automation tools like AWS Systems Manager or AWS Config to\n      streamline operations, compliance, monitoring, lifecycle, and\n      termination processes. With AWS services, tools, and third-party\n      products, you can customize the automations you implement to meet\n      your specific requirement. Following table shows some of the core\n      operation functions and capabilities you can achieve with AWS\n      services to automate administration and operation:\n    \n    \n       \n       \n       \n       \n       \n       \n       \n       \n       \n       \n    \n        \n          AWS Audit Manager: Continually audit your AWS usage to\n          simplify risk and compliance assessment\n        \n      \n        \n          AWS Backup: Centrally manage and automate data protection.\n        \n      \n        \n          AWS Config: Configure compute resources, asses, audit,\n          evaluate configurations and resource inventory.\n        \n      \n        \n          AWS CloudFormation: Launch highly available resources with\n          Infrastructure as Code.\n        \n      \n        \n          AWS CloudTrail: IT change management,\n          compliance, and control.\n        \n      \n        \n          Amazon EventBridge Schedule events and trigger AWS Lambda to\n          take action.\n        \n      \n        \n          AWS Lambda: Automate repetitive processes by triggering\n          them with events or by running them on a fixed schedule with\n          AWS EventBridge.\n        \n      \n        \n          AWS Systems Manager: Start and stop workloads, patch\n          operating systems, automate configuration, and ongoing\n          management.\n        \n      \n        \n          AWS Step Functions: Schedule jobs and automate workflows.\n        \n      \n        \n          AWS Service Catalog: Template consumption,\n          infrastructure as code with compliance and control.\n        \n      \n    \n      If you would like to adopt automations immediately with using AWS\n      products and service and if don't have skills in your\n      organization, reach out to\n      AWS Managed Services (AMS),\n      \n      \n      AWS       Professional Services, or\n      \n      \n      \n      AWS       Partners to increase adoption of automation and improve\n      your operational excellence in the cloud.\n    \n    \n      AWS Managed Services (AMS) is a service that operates AWS\n      infrastructure on behalf of enterprise customers and partners. It\n      provides a secure and compliant environment that you can deploy\n      your workloads onto. AMS uses enterprise cloud operating models\n      with automation to allow you to meet your organization\n      requirements, move into the cloud faster, and reduce your on-going\n      management costs.\n    \n    \n      AWS Professional Services can also help you achieve your desired\n      business outcomes and automate operations with AWS. They help\n      customers to deploy automated, robust, agile IT operations, and\n      governance capabilities optimized for the cloud. For detailed\n      monitoring examples and recommended best practices, see\n      Operational Excellence Pillar whitepaper.\n    \n     \n\n  Implementation steps\n\n      \n      \n         \n         \n         \n         \n         \n         \n      \n          \n            Build once and deploy\n            many: Use infrastructure-as-code such as\n            CloudFormation, AWS SDK, or AWS CLI to deploy once and use\n            many times for similar environments or for disaster recovery\n            scenarios. Tag while deploying to track your consumption as\n            defined in other best practices. Use\n            AWS Launch Wizard to reduce the time to deploy many\n            popular enterprise workloads. AWS Launch Wizard guides you\n            through the sizing, configuration, and deployment of\n            enterprise workloads following AWS best practices. You can\n            also use the\n            Service Catalog, which helps you create and manage\n            infrastructure-as-code approved templates for use on AWS so\n            anyone can discover approved, self-service cloud resources.\n          \n        \n          \n            Automate continuous\n            compliance: Consider automating assessment and\n            remediation of recorded configurations against predefined\n            standards. When you combine AWS Organizations with the\n            capabilities of AWS Config\n            and AWS CloudFormation, you can efficiently manage and\n            automate configuration compliance at scale for hundreds of\n            member accounts. You can review changes in configurations\n            and relationships between AWS resources and dive into the\n            history of a resource configuration.\n          \n        \n          \n            Automate monitoring tasks\n            AWS provides various tools that you can use to monitor\n            services. You can configure these tools to automate\n            monitoring tasks. Create and implement a monitoring plan\n            that collects monitoring data from all the parts in your\n            workload so that you can more easily debug a multi-point\n            failure if one occurs. For example, you can use the\n            automated monitoring tools to observe Amazon EC2 and report\n            back to you when something is wrong for system status\n            checks, instance status checks, and Amazon CloudWatch\n            alarms.\n          \n        \n          \n            Automate maintenance and\n            operations: Run routine operations automatically\n            without human intervention. Using AWS services and tools,\n            you can choose which AWS automations to implement and\n            customize for your specific requirements. For example, use\n            EC2 Image Builder for building, testing, and deployment\n            of virtual machine and container images for use on AWS or\n            on-premises or patching your EC2 instances with AWS SSM. If\n            your desired action cannot be done with AWS services or you\n            need more complex actions with filtering resources, then\n            automate your operations with using\n            AWS Command Line Interface (AWS CLI) or AWS SDK tools.\n            AWS CLI provides the ability to automate the entire process\n            of controlling and managing AWS services with scripts without\n            using the AWS Management Console. Select your preferred AWS SDKs to\n            interact with AWS services. For other code examples,\n            see AWS SDK Code\n            examples\n            repository.\n          \n        \n          \n            Create a continual lifecycle with\n            automations: It is important that you establish\n            and preserve mature lifecycle policies not only for\n            regulations or redundancy but also for cost optimization.\n            You can use AWS Backup to centrally manage and automate data\n            protection of data stores, such as your buckets, volumes,\n            databases, and file systems. You can also use Amazon Data Lifecycle Manager to automate the creation, retention, and\n            deletion of EBS snapshots and EBS-backed AMIs.\n          \n        \n          \n            Delete unnecessary\n            resources: It's quite common to accumulate unused\n            resources in sandbox or development AWS accounts. Developers\n            create and experiment with various services and resources as\n            part of the normal development cycle, and then they don't\n            delete those resources when they're no longer needed. Unused\n            resources can incur unnecessary and sometimes high costs for\n            the organization. Deleting these resources can reduce the\n            costs of operating these environments. Make sure your data\n            is not needed or backed up if you are not sure. You can use\n            AWS CloudFormation to clean up deployed stacks, which\n            automatically deletes most resources defined in the\n            template. Alternatively, you can create an automation for\n            the deletion of AWS resources using tools like\n            aws-nuke.\n          \n        \n     \n   \n\n  Resources\n\n      \n    \n      Related documents:\n    \n    \n       \n       \n       \n       \n       \n       \n       \n       \n    \n        \n          Modernizing\n          operations in the AWS Cloud\n        \n      \n        \n          AWS           Services for Automation\n        \n      \n        \n          Infrastructure\n          and automation\n        \n      \n        \n          AWS Systems Manager Automation\n        \n      \n        \n          Automated\n          and manual monitoring\n        \n      \n        \n          AWS           automations for SAP administration and operations\n        \n      \n        \n          AWS Managed Services\n        \n      \n        \n          AWS           Professional Services\n        \n      \n    \n      Related videos:\n    \n    \n       \n       \n       \n    \n        \n          Automate\n          Continuous Compliance at Scale in AWS\n        \n      \n        \n          AWS Backup Demo: Cross-Account \u0026 Cross-Region Backup\n        \n      \n        \n          Patching\n          for your Amazon EC2 Instances\n        \n      \n    \n      Related examples:\n    \n    \n       \n       \n       \n       \n       \n       \n    \n        \n          Reinventing\n          automated operations (Part I)\n        \n      \n        \n          Reinventing\n          automated operations (Part II)\n        \n      \n        \n          Automate\n          deletion of AWS resources by using aws-nuke\n        \n      \n        \n          Delete\n          unused Amazon EBS volumes by using AWS Config and AWS\n          SSM\n        \n      \n        \n          Automate\n          continuous compliance at scale in AWS\n        \n      \n        \n          IT\n          Automations with AWS Lambda\n        \n      \n  \u003cdiv\u003e\u003cdiv\u003e\u003cdiv\u003e\u003cdiv id=\"js_error_message\"\u003e\u003cp\u003e\u003cimg src=\"https://d1ge0kk1l5kms0.cloudfront.net/images/G/01/webservices/console/warning.png\" alt=\"Warning\" /\u003e \u003cstrong\u003eJavascript is disabled or is unavailable in your browser.\u003c/strong\u003e\u003c/p\u003e\u003cp\u003eTo use the Amazon Web Services Documentation, Javascript must be enabled. Please refer to your browser's Help pages for instructions.\u003c/p\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003eDocument ConventionsCOST 11. How do you evaluate the cost of effort?Sustainability",
  "SUS01-BP01 Choose Region based on both business requirements and sustainability goalsChoose a Region for your workload based on both your business requirements \n    and sustainability goals to optimize its KPIs, including performance, cost, \n    and carbon footprint.\n    Common anti-patterns:\n  \n     \n     \n  \n      \n        You select the workload’s Region based on your own location.\n      \n    \n      \n        You consolidate all workload resources into one geographic location.\n      \n    \n    Benefits of establishing this best practice: Placing a workload \n    close to Amazon renewable energy projects or Regions with low published carbon intensity can help \n    to lower the carbon footprint of a cloud workload.\n  \n    Level of risk exposed if this best practice\n    is not established: Medium\n  \n\n  Implementation guidance\n\n      \n    The AWS Cloud is a constantly expanding network of Regions and points of presence (PoP), \n      with a global network infrastructure linking them together. The choice of Region for your \n      workload significantly affects its KPIs, including performance, cost, and carbon footprint. \n      To effectively improve these KPIs, you should choose Regions for your workload based on \n      both your business requirements and sustainability goals.\n     \n      \n      Implementation steps\n    \n    \n       \n       \n    \n        \n          Shortlist potential Regions: Follow these steps to assess and shortlist potential Regions for your workload \n          based on your business requirements, including compliance, available features, \n          cost, and latency:\n        \n        \n           \n           \n           \n           \n        \n            \n              Confirm that these Regions are compliant, based on your required local regulations (for example, data sovereignty).\n            \n          \n            \n              Use the AWS Regional Services Lists to check if the Regions have the services and features you need to run your workload.\n            \n          \n            \n              Calculate the cost of the workload on each Region using the AWS Pricing Calculator.\n            \n          \n            \n              Test the network latency between your end user locations and each AWS Region.\n            \n          \n      \n        \n          Choose Regions: Choose Regions near Amazon renewable energy projects and Regions where the grid has a \n          published carbon intensity that is lower than other locations (or Regions).\n        \n        \n           \n           \n        \n            \n              Identify your relevant sustainability guidelines to track and compare year-to-year \n              carbon emissions based on Greenhouse Gas Protocol (market-based and location based methods).\n            \n          \n            \n              Choose region based on method you use to track carbon emissions. For more detail \n              on choosing a Region based on your sustainability guidelines, see\n              How to select a Region for your workload based on sustainability goals.\n            \n          \n      \n     \n   \n\n  Resources    \n    \n      Related documents:\n    \n    \n       \n       \n       \n       \n    \n        \n          Understanding your carbon emission estimations\n        \n      \n        \n          Amazon\n          Around the Globe\n        \n      \n        \n          Renewable\n          Energy Methodology\n        \n      \n        \n          What\n          to Consider when Selecting a Region for your Workloads\n        \n      \n    \n      Related videos:\n    \n    \n       \n       \n       \n       \n       \n    \n        AWS re:Invent 2023 - Sustainability innovation in AWS Global Infrastructure\n        \n      \n        AWS re:Invent 2023 - Sustainable architecture: Past, present, and future\n        \n      \n        AWS re:Invent 2022 - Delivering sustainable, high-performing architectures\n        \n      \n        \n          AWS re:Invent 2022 - Architecting sustainably and reducing your AWS carbon footprint\n        \n      \n        AWS re:Invent 2022 - Sustainability in AWS global infrastructure\n        \n      \n  \u003cdiv\u003e\u003cdiv\u003e\u003cdiv\u003e\u003cdiv id=\"js_error_message\"\u003e\u003cp\u003e\u003cimg src=\"https://d1ge0kk1l5kms0.cloudfront.net/images/G/01/webservices/console/warning.png\" alt=\"Warning\" /\u003e \u003cstrong\u003eJavascript is disabled or is unavailable in your browser.\u003c/strong\u003e\u003c/p\u003e\u003cp\u003eTo use the Amazon Web Services Documentation, Javascript must be enabled. Please refer to your browser's Help pages for instructions.\u003c/p\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003eDocument ConventionsSUS 1 How do you select Regions for your workload?Alignment to demand",
  "SUS02-BP01 Scale workload infrastructure dynamicallyUse elasticity of the cloud and scale your infrastructure dynamically to match supply of cloud \n    resources to demand and avoid overprovisioned capacity in your workload.Common anti-patterns: \n     \n     \n     \n  You do not scale your infrastructure with user load.You manually scale your infrastructure all the time.You leave increased capacity after a scaling event instead of scaling back down.\n    Benefits of establishing this best practice: Configuring and testing \n    workload elasticity help to efficiently match supply of cloud resources to demand and avoid overprovisioned \n    capacity. You can take advantage of elasticity in the cloud to automatically scale capacity during and \n    after demand spikes to make sure you are only using the right number of resources needed to meet your \n    business requirements.\n    Level of risk exposed if this best practice\n    is not established: Medium\n  \n\n  Implementation guidance\n\n  The cloud provides the flexibility to expand or reduce your resources dynamically through a variety of \n  mechanisms to meet changes in demand. Optimally matching supply to demand delivers the lowest environmental \n  impact for a workload.  \n\n    \n      Demand can be fixed or variable, requiring metrics and automation to make sure that management does \n      not become burdensome. Applications can scale vertically (up or down) by modifying the instance size, \n      horizontally (in or out) by modifying the number of instances, or a combination of both.\n    \n    \n      You can use a number of different approaches to match supply of resources with demand. \n    \n    \n       \n       \n       \n       \n    \n        \n          Target-tracking approach: Monitor your scaling metric and \n          automatically increase or decrease capacity as you need it.\n        \n      \n        \n          Predictive scaling: Scale in anticipation of daily and weekly trends.\n        \n      \n        \n          Schedule-based approach: Set your own scaling schedule according to \n          predictable load changes.\n        \n      \n        \n          Service scaling: Pick services (like serverless) that are natively \n          scaling by design or provide auto scaling as a feature.\n        \n      \n      \n        Identify periods of low or no utilization and scale resources to remove excess capacity and improve efficiency.\n      \n   \n      \n      Implementation steps\n    \n    \n       \n       \n       \n       \n       \n       \n    \n        Elasticity matches the supply of resources you have against the demand for those resources. \n          Instances, containers, and functions provide mechanisms for elasticity, either in combination with \n          automatic scaling or as a feature of the service. AWS provides a range of auto scaling mechanisms \n          to ensure that workloads can scale down quickly and easily during periods of low user load. Here \n          are some examples of auto scaling mechanisms:\n        \n              \n                Auto scaling mechanism\n                Where to use\n              \n            \n              \n                \n                  Amazon EC2 Auto Scaling\n                \n                \n                  Use to verify you have the correct number of Amazon EC2 instances available to \n                    handle the user load for your application.\n                  \n                \n              \n              \n                \n                  Application Auto Scaling\n                \n                \n                  Use to automatically scale the resources for individual AWS services beyond Amazon EC2, \n                    such as Lambda functions or Amazon Elastic Container Service (Amazon ECS) services.\n                  \n                \n              \n              \n                \n                  \n                    Kubernetes Cluster Autoscaler\n                  \n                \n                \n                  Use to automatically scale Kubernetes clusters on AWS.\n                \n              \n            \n      \n        \n          Scaling is often discussed related to compute services like Amazon EC2 instances or AWS Lambda \n          functions. Consider the configuration of non-compute services like Amazon DynamoDB read and write \n          capacity units or Amazon Kinesis Data Streams shards to match the demand.\n        \n      \n        \n          Verify that the metrics for scaling up or down are validated against the type of workload being deployed. \n          If you are deploying a video transcoding application, 100% CPU utilization is expected and should not be your primary metric. \n          You can use a customized metric (such as memory utilization) for your \n          scaling policy if required. To choose the right metrics, consider the following guidance for Amazon EC2:\n        \n        \n           \n           \n        \n            \n              The metric should be a valid utilization metric and describe how busy an instance is.\n            \n          \n            \n              The metric value must increase or decrease proportionally to the number of instances in the Auto Scaling group.\n            \n          \n      \n        \n          Use dynamic scaling instead of \n          manual scaling for your Auto Scaling group. \n          We also recommend that you use target \n            tracking scaling policies in your dynamic scaling. \n        \n      \n        \n           Verify that workload deployments can handle both scale-out and scale-in events. Create test scenarios for scale-in events to verify that the workload behaves as expected\n           and does not affect the user experience (like losing sticky sessions). You can use \n          Activity history to verify a scaling activity for an Auto Scaling group.\n        \n      \n        \n          Evaluate your workload for predictable patterns and proactively scale as you anticipate predicted and planned changes in demand. With predictive scaling, you can eliminate the need to overprovision capacity. For more detail, see \n          Predictive Scaling with Amazon EC2 Auto Scaling.\n        \n      \n   \n\n  Resources\n\n      \n    \n      Related documents:\n    \n    \n       \n       \n       \n       \n       \n       \n       \n       \n    \n        \n          Getting\n          Started with Amazon EC2 Auto Scaling\n        \n      \n        \n          Predictive\n          Scaling for EC2, Powered by Machine Learning\n        \n      \n        \n          Analyze\n          user behavior using Amazon OpenSearch Service, Amazon Data Firehose and Kibana\n        \n      \n        \n          What\n          is Amazon CloudWatch?\n        \n      \n        \n          Monitoring\n          DB load with Performance Insights on Amazon RDS\n        \n      \n        \n          Introducing Native Support for Predictive Scaling with Amazon EC2 Auto Scaling\n        \n      \n        \n          Introducing Karpenter - An Open-Source, High-Performance Kubernetes Cluster Autoscaler\n        \n      \n        \n          Deep Dive on Amazon ECS Cluster Auto Scaling\n        \n      \n    \n      Related videos:\n    \n    \n       \n       \n       \n       \n       \n       \n    \n        AWS re:Invent 2023 - Scaling on AWS for the first 10 million users\n        \n      \n        AWS re:Invent 2023 - Sustainable architecture: Past, present, and future\n        \n      \n        \n          AWS re:Invent 2022 - Build a cost-, energy-, and resource-efficient\n          compute environment\n        \n      \n        AWS re:Invent 2022 - Scaling containers from one user to millions\n        \n      \n        AWS re:Invent 2023 - Scaling FM inference to hundreds of models with Amazon SageMaker AI\n        \n      \n        AWS re:Invent 2023 - Harness the power of Karpenter to scale, optimize \u0026 upgrade Kubernetes\n        \n      \n    \n      Related examples:\n    \n    \n       \n    \n        \n          Autoscaling\n        \n      \n  \u003cdiv\u003e\u003cdiv\u003e\u003cdiv\u003e\u003cdiv id=\"js_error_message\"\u003e\u003cp\u003e\u003cimg src=\"https://d1ge0kk1l5kms0.cloudfront.net/images/G/01/webservices/console/warning.png\" alt=\"Warning\" /\u003e \u003cstrong\u003eJavascript is disabled or is unavailable in your browser.\u003c/strong\u003e\u003c/p\u003e\u003cp\u003eTo use the Amazon Web Services Documentation, Javascript must be enabled. Please refer to your browser's Help pages for instructions.\u003c/p\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003eDocument ConventionsSUS 2 How do you align cloud resources to your demand?SUS02-BP02 Align SLAs with sustainability goals",
  "SUS02-BP02 Align SLAs with sustainability goals\n    Review and optimize workload service-level agreements \n    (SLA) based on your sustainability goals to minimize the \n    resources required to support your workload while continuing \n    to meet business needs.\n  \n    Common anti-patterns:\n  \n     \n     \n     \n  \n      \n        Workload SLAs are unknown or ambiguous. \n      \n    \n      \n        You define your SLA just for availability and performance.\n      \n    \n      \n        You use the same design pattern (like Multi-AZ architecture) for all your workloads.\n      \n    \n    Benefits of establishing this best practice: Aligning \n    SLAs with sustainability goals leads to optimal resource usage while meeting business \n    needs.\n  \n    Level of risk exposed if this best practice\n    is not established: Low\n  \n\n  Implementation guidance\n    \n      SLAs define the level of service expected from a cloud workload, \n      such as response time, availability, and data retention. They \n      influence the architecture, resource usage, and environmental \n      impact of a cloud workload. At a regular cadence, review SLAs \n      and make trade-offs that significantly reduce resource usage \n      in exchange for acceptable decreases in service levels. \n    \n     \n      \n      Implementation steps\n      \n         \n         \n         \n         \n         \n         \n         \n         \n         \n      \n          \n            Understand sustainability goals: Identify sustainability goals in your organization, such as carbon reduction or improving resource utilization.\n          \n        \n          \n            Review SLAs: Evaluate your SLAs to assess if they support your business requirements. If you are exceeding SLAs, perform further review.\n          \n        \n          \n            Understand trade-offs: Understand the trade-offs across your workload’s complexity (like high volume of concurrent users), performance (like latency), and sustainability impact (like required resources). Typically, prioritizing two of the factors comes at the expense of the third.\n          \n        \n          \n            Adjust SLAs: Adjust your SLAs by making trade-offs that significantly reduce sustainability impacts in exchange for acceptable decreases in service levels.\n          \n          \n             \n             \n             \n          \n              \n                Sustainability and reliability: Highly available workloads tend to consume more resources.\n              \n            \n              \n                Sustainability and performance: Using more resources to boost performance could have a higher environmental impact.\n              \n            \n              \n                Sustainability and security: Overly secure workloads could have a higher environmental impact.\n              \n            \n        \n          \n            Define sustainability SLAs if possible: Include sustainability SLAs for your workload. For example, define a minimum utilization level as a sustainability SLA for your compute instances.\n          \n        \n          \n            Use efficient design patterns: Use design patterns such as microservices on AWS that prioritize business-critical functions and allow lower service levels (such as response time or recovery time objectives) for non-critical functions.\n          \n        \n          \n            Communicate and establish accountability: Share the SLAs with all relevant stakeholders, including your development team and your customers. Use reporting to track and monitor the SLAs. Assign accountability to meet the sustainability targets for your SLAs.\n          \n        \n          \n            Use incentives and rewards: Use incentives and rewards to achieve or exceed SLAs aligned with sustainability goals.\n           \n        \n          \n            Review and iterate: Regularly review and adjust your SLAs to make sure they are aligned with evolving sustainability and performance goals. \n          \n        \n     \n   \n\n  Resources\n\n      \n    \n      Related documents:\n    \n    \n       \n       \n    \n        \n          Understand resiliency patterns and trade-offs to architect efficiently in the cloud\n        \n      \n        \n          Importance\n          of Service Level Agreement for SaaS Providers\n        \n      \n    \n      Related videos:\n    \n    \n       \n       \n       \n       \n       \n    \n        AWS re:Invent 2023 - Capacity, availability, cost efficiency: Pick three\n        \n      \n        AWS re:Invent 2023 - Sustainable architecture: Past, present, and future\n        \n      \n        AWS re:Invent 2023 - Advanced integration patterns \u0026 trade-offs for loosely coupled systems\n        \n      \n        AWS re:Invent 2022 - Delivering sustainable, high-performing architectures\n        \n      \n        AWS re:Invent 2022 - Build a cost-, energy-, and resource-efficient compute environment\n        \n      \n  \u003cdiv\u003e\u003cdiv\u003e\u003cdiv\u003e\u003cdiv id=\"js_error_message\"\u003e\u003cp\u003e\u003cimg src=\"https://d1ge0kk1l5kms0.cloudfront.net/images/G/01/webservices/console/warning.png\" alt=\"Warning\" /\u003e \u003cstrong\u003eJavascript is disabled or is unavailable in your browser.\u003c/strong\u003e\u003c/p\u003e\u003cp\u003eTo use the Amazon Web Services Documentation, Javascript must be enabled. Please refer to your browser's Help pages for instructions.\u003c/p\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003eDocument ConventionsSUS02-BP01 Scale workload infrastructure dynamicallySUS02-BP03 Stop the creation and maintenance of unused\n  assets",
  "SUS02-BP03 Stop the creation and maintenance of unused\n  assetsDecommission unused assets in your workload to reduce \n    the number of cloud resources required to support your \n    demand and minimize waste.\n    Common anti-patterns:\n  \n     \n     \n  \n      \n        You do not analyze your application for assets that are redundant or no longer required.\n      \n    \n      \n        You do not remove assets that are redundant or no longer required.\n      \n    \n    Benefits of establishing this best practice: Removing \n    unused assets frees resources and improves the overall efficiency of the workload.\n  \n    Level of risk exposed if this best practice\n    is not established: Low\n  \n\n  Implementation guidance\n    \n      Unused assets consume cloud resources like storage space and compute power. By identifying and eliminating these assets, you can free up these resources, resulting in a more efficient cloud architecture. Perform regular analysis on application assets such as pre-compiled reports, datasets, static images, and asset access patterns to identify redundancy, underutilization, and potential decommission targets. Remove those redundant assets to reduce the resource waste in your workload.\n    \n     \n      \n      Implementation steps\n    \n    \n       \n       \n       \n       \n       \n       \n    \n        \n          Conduct an inventory: Conduct a comprehensive inventory to identify all assets within your workload.\n        \n      \n        \n          Analyze usage: Use continuous monitoring to identify static assets that are no longer required.\n        \n      \n        \n          Remove unused assets: Develop a plan to remove assets that are no longer required. \n        \n        \n           \n           \n           \n        \n            \n              Before removing any asset, evaluate the impact of removing it on the architecture.\n            \n          \n            \n              Consolidate overlapping generated assets to remove redundant processing.\n            \n          \n            \n              Update your applications to no longer produce and store assets that are not required.\n            \n          \n      \n        \n          Communicate with third parties: Instruct third parties to stop producing and storing assets managed on your behalf that are no longer required. Ask to consolidate redundant assets. \n        \n      \n        \n          Use lifecycle policies: Use lifecycle policies to automatically delete unused assets.\n        \n        \n           \n            \n        \n            \n              You can use Amazon S3 Lifecycle to manage your objects throughout their lifecycle.\n            \n          \n            \n              You can use Amazon Data Lifecycle Manager to automate the creation, retention, and deletion of Amazon EBS snapshots and Amazon EBS-backed AMIs.\n            \n          \n      \n        \n          Review and optimize: Regularly review your workload to identify and remove any unused assets.\n        \n      \n     \n      \n    \n   \n\n  Resources\n\n      \n    \n      Related documents:\n    \n    \n       \n       \n    \n        \n          Optimizing\n          your AWS Infrastructure for Sustainability, Part II:\n          Storage\n        \n      \n        \n          How do I terminate active resources that I no longer need on my AWS account?\n        \n      \n    \n      Related videos:\n    \n    \n       \n       \n       \n    \n        AWS re:Invent 2023 - Sustainable architecture: Past, present, and future\n        \n      \n        AWS re:Invent 2022 - Preserving and maximizing the value of digital media assets using Amazon S3\n        \n      \n        AWS re:Invent 2023 - Optimize costs in your multi-account environments\n        \n      \n  \u003cdiv\u003e\u003cdiv\u003e\u003cdiv\u003e\u003cdiv id=\"js_error_message\"\u003e\u003cp\u003e\u003cimg src=\"https://d1ge0kk1l5kms0.cloudfront.net/images/G/01/webservices/console/warning.png\" alt=\"Warning\" /\u003e \u003cstrong\u003eJavascript is disabled or is unavailable in your browser.\u003c/strong\u003e\u003c/p\u003e\u003cp\u003eTo use the Amazon Web Services Documentation, Javascript must be enabled. Please refer to your browser's Help pages for instructions.\u003c/p\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003eDocument ConventionsSUS02-BP02 Align SLAs with sustainability goalsSUS02-BP04 Optimize geographic placement of workloads based on their networking requirements",
  "SUS02-BP04 Optimize geographic placement of workloads based on their networking requirementsSelect cloud location and services for your workload that reduce the distance network \n    traffic must travel and decrease the total network resources required to support your workload.\n    \n      Common anti-patterns:\n    \n  \n     \n     \n     \n  \n      \n        You select the workload's Region based on your own location.\n      \n    \n      \n        You consolidate all workload resources into one geographic location.\n      \n    \n      \n        All traffic flows through your existing data centers.\n      \n    \n    Benefits of establishing this best practice: Placing a workload close \n    to its users provides the lowest latency while decreasing data movement across \n    the network and reducing environmental impact.\n  \n    Level of risk exposed if this best practice\n    is not established: Medium\n  \n\n  Implementation guidance\n    \n      The AWS Cloud infrastructure is built around location options such as Regions, Availability Zones, \n      placement groups, and edge locations such as AWS Outposts and AWS Local Zones. These location options \n      are responsible for maintaining connectivity between application components, cloud services, edge networks and on-premises data centers. \n    \n    \n      Analyze the network access patterns in your workload to identify how to use these cloud location options and reduce the distance network traffic must travel. \n    \n   \n    \n    Implementation steps\n    \n       \n       \n       \n       \n       \n       \n       \n    \n        \n          Analyze network access patterns in your workload to identify how users use your application. \n        \n        \n           \n           \n        \n            \n              Use monitoring tools, such as Amazon CloudWatch and AWS CloudTrail, to gather data on network activities.\n            \n          \n            \n              Analyze the data to identify the network access pattern.\n            \n          \n      \n        \n          Select the Regions for your workload deployment based on the following key elements:         \n        \n        \n           \n           \n           \n           \n        \n            \n              Your Sustainability goal: as explained in Region selection. \n          \n            \n              Where your data is located: For data-heavy applications (such as big data and machine learning), \n              application code should run as close to the data as possible. \n            \n          \n            \n              Where your users are located: For user-facing\n              applications, choose a Region (or Regions) close to your workload’s users.\n          \n            Other constraints: Consider constraints such as\n              cost and compliance as explained in What to Consider when Selecting a Region for your Workloads.\n          \n      \n         Use local caching or AWS\n            Caching Solutions for frequently used assets to improve performance, reduce\n          data movement, and lower environmental impact. \n        \n              \n                Service\n                When to use\n              \n            \n              \n                \n                  Amazon CloudFront\n                \n                \n                  Use to cache static content such as images, scripts, and videos, as well as dynamic content\n                    such as API responses or web applications.\n                \n              \n              \n                \n                  Amazon ElastiCache\n                \n                \n                  Use to cache content for web applications.\n                \n              \n              \n                \n                  DynamoDB Accelerator\n                \n                \n                  Use to add in-memory acceleration to your DynamoDB tables.\n                \n              \n            \n      \n         Use services that can help you run code closer to users of your workload:\n        \n              \n                Service\n                When to use\n              \n            \n              \n                \n                  Lambda@Edge\n                \n                \n                  Use for\n                    compute-heavy operations that are initiated when objects are not in the cache. \n                \n              \n              \n                \n                  Amazon CloudFront Functions\n                \n                \n                  Use for simple use cases like HTTP(s) request or response manipulations \n                    that can be initiated by short-lived functions.\n                \n              \n              \n                \n                  AWS IoT Greengrass\n                \n                \n                  Use to run local compute, messaging, and data caching for connected devices. \n                \n              \n            \n      \n        \n          Use connection pooling to allow for connection reuse and reduce required\n          resources.\n        \n      \n        \n          Use distributed data stores that don’t rely on persistent connections and synchronous updates for consistency to serve regional populations.\n        \n      \n        \n          Replace pre-provisioned static network capacity with shared dynamic capacity, and share the sustainability impact of network capacity with other subscribers.\n        \n      \n   \n\n  Resources\n\n      \n    \n      Related documents:\n    \n    \n       \n       \n       \n       \n       \n       \n       \n       \n       \n    \n        \n          Optimizing\n          your AWS Infrastructure for Sustainability, Part III:\n          Networking\n        \n      \n        \n          Amazon ElastiCache Documentation\n        \n      \n        \n          What\n          is Amazon CloudFront?\n        \n      \n        \n          Amazon CloudFront Key Features\n        \n      \n        AWS Global Infrastructure \n        \n      \n        AWS Local Zones and AWS Outposts, choosing the right technology for your edge workload\n        \n      \n        \n          Placement groups\n        \n      \n        AWS Local Zones\n        \n      \n        AWS Outposts\n      \n    \n      Related videos:\n    \n    \n       \n       \n       \n       \n       \n       \n       \n       \n       \n       \n       \n       \n    \n        \n          Demystifying data transfer on AWS\n        \n      \n        \n          Scaling network performance on next-gen Amazon EC2 instances\n        \n      \n        AWS Local Zones Explainer Video\n        \n      \n        AWS Outposts: Overview and How it Works\n        \n      \n        AWS re:Invent 2023 - A migration strategy for edge and on-premises workloads\n        \n      \n        AWS re:Invent 2021 - AWS Outposts: Bringing the AWS experience on premises\n        \n      \n        AWS re:Invent 2020 - AWS Wavelength: Run apps with ultra-low latency at 5G edge\n        \n      \n        AWS re:Invent 2022 - AWS Local Zones: Building applications for a distributed edge\n        \n      \n        AWS re:Invent 2021 - Building low-latency websites with Amazon CloudFront\n        \n      \n        AWS re:Invent 2022 - Improve performance and availability with AWS Global Accelerator\n      \n        AWS re:Invent 2022 - Build your global wide area network using AWS\n      \n        AWS re:Invent 2020: Global traffic management with Amazon Route 53\n        \n      \n    \n      Related examples:\n    \n    \n       \n       \n    \n        \n          AWS Networking\n            Workshops\n        \n      \n        \n          Architecting for sustainability - Minimize data movement across networks\n        \n      \n  \u003cdiv\u003e\u003cdiv\u003e\u003cdiv\u003e\u003cdiv id=\"js_error_message\"\u003e\u003cp\u003e\u003cimg src=\"https://d1ge0kk1l5kms0.cloudfront.net/images/G/01/webservices/console/warning.png\" alt=\"Warning\" /\u003e \u003cstrong\u003eJavascript is disabled or is unavailable in your browser.\u003c/strong\u003e\u003c/p\u003e\u003cp\u003eTo use the Amazon Web Services Documentation, Javascript must be enabled. Please refer to your browser's Help pages for instructions.\u003c/p\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003eDocument ConventionsSUS02-BP03 Stop the creation and maintenance of unused\n  assetsSUS02-BP05 Optimize team member resources for activities\n  performed",
  "SUS02-BP05 Optimize team member resources for activities\n  performedOptimize resources provided to team members to minimize the \n    environmental sustainability impact while supporting their needs. \n    Common anti-patterns:\n  \n     \n     \n  \n      \n        You ignore the impact of devices used by your team members on the overall efficiency of your cloud application.\n      \n    \n      \n        You manually manage and update resources used by team members.\n      \n    \n    Benefits of establishing this best practice: Optimizing \n    team member resources improves the overall efficiency of cloud-enabled applications.\n  \n    Level of risk exposed if this best practice\n    is not established: Low\n  \n\n  Implementation guidance\n    \n      Understand the resources your team members use to consume your services, their expected lifecycle, and the financial and sustainability impact. Implement strategies to optimize these resources. For example, perform complex operations, such as rendering and compilation, on highly utilized scalable infrastructure instead of on underutilized high-powered single-user systems.\n    \n     \n      \n      Implementation steps\n    \n      \n         \n         \n         \n         \n         \n         \n      \n          \n            Use energy-efficient workstations: Provide team members with energy-efficient workstations and peripherals. Use efficient power management features (like low power mode) in these devices to reduce their energy usage\n          \n        \n          \n            use virtualization: Use virtual desktops and application streaming to limit upgrade and device requirements.\n          \n        \n          \n            Encourage remote collaboration: Encourage team members to use remote collaboration tools such as Amazon Chime or AWS Wickr to reduce the need for travel and associated carbon emissions.\n          \n        \n          \n            Use energy-efficient software: Provide team members with energy-efficient software by removing or turning off unnecessary features and processes.\n          \n        \n          \n            Manage lifecycles: Evaluate the impact of processes and systems on your device lifecycle, and select solutions that minimize the requirement for device replacement while satisfying business requirements. Regularly maintain and update workstations or software to maintain and improve efficiency.\n          \n        \n          \n            Remote device management: Implement remote management for devices to reduce required business travel.\n          \n          \n             \n          \n              \n                AWS Systems Manager Fleet Manager is a unified user interface (UI) experience that helps you remotely manage your nodes running on AWS or on premises.\n              \n            \n        \n     \n   \n\n  Resources\n\n      \n    \n      Related documents:\n    \n    \n       \n       \n       \n       \n    \n        \n          What\n          is Amazon WorkSpaces?\n        \n      \n        \n          Cost Optimizer for Amazon WorkSpaces\n        \n      \n        \n          Amazon\n          AppStream 2.0 Documentation\n        \n      \n        \n          NICE\n          DCV\n        \n      \n    \n      Related videos:\n    \n    \n       \n    \n        \n          Managing cost for Amazon WorkSpaces on AWS\n        \n      \n  \u003cdiv\u003e\u003cdiv\u003e\u003cdiv\u003e\u003cdiv id=\"js_error_message\"\u003e\u003cp\u003e\u003cimg src=\"https://d1ge0kk1l5kms0.cloudfront.net/images/G/01/webservices/console/warning.png\" alt=\"Warning\" /\u003e \u003cstrong\u003eJavascript is disabled or is unavailable in your browser.\u003c/strong\u003e\u003c/p\u003e\u003cp\u003eTo use the Amazon Web Services Documentation, Javascript must be enabled. Please refer to your browser's Help pages for instructions.\u003c/p\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003eDocument ConventionsSUS02-BP04 Optimize geographic placement of workloads based on their networking requirementsSUS02-BP06 Implement buffering or throttling to flatten the demand curve",
  "SUS02-BP06 Implement buffering or throttling to flatten the demand curveBuffering and throttling flatten the demand curve and reduce the provisioned capacity required for your workload. \n    Common anti-patterns:\n  \n     \n     \n  \n      You process the client requests immediately while it is not needed.\n    \n      You do not analyze the requirements for client requests.\n    \n    Benefits of establishing this best practice: Flattening the demand curve reduce the required provisioned capacity for the workload. Reducing the provisioned capacity means less energy consumption and less environmental impact.\n  \n    Level of risk exposed if this best practice\n    is not established: Low\n  \n    Implementation guidance\n    \n      Flattening the workload demand curve can help you to reduce the provisioned capacity for a workload and reduce its environmental impact. Assume a workload with the demand curve shown in below figure. This workload has two peaks, and to handle those peaks, the resource capacity as shown by orange line is provisioned.  The resources and energy used for this workload is not indicated by the area under the demand curve, but the area under the provisioned capacity line, as provisioned capacity is needed to handle those two peaks.\n    \n    \n       \n        \n       \n       \n      Demand curve with two distinct peaks that require high provisioned capacity.\n    \n      \n    \n      You can use buffering or throttling to modify the demand curve and smooth out the peaks, which means less provisioned capacity and less energy consumed. Implement throttling when your clients can perform retries. Implement buffering to store the request and defer processing until a later time. \n    \n    \n       \n        \n       \n       \n      Throttling's effect on the demand curve and provisioned capacity.\n    \n      \n    \n      Implementation steps\n    \n    \n       \n       \n       \n       \n    \n        \n          Analyze the client requests to determine how to respond to them. Questions to consider include:\n        \n        \n           \n           \n        \n            \n              Can this request be processed asynchronously?\n            \n          \n            \n              Does the client have retry capability?\n            \n          \n      \n        \n          If the client has retry capability, then you can implement throttling, which tells the source that if it cannot service the request at the current time, it should try again later. \n        \n        \n           \n        \n            \n              You can use Amazon API Gateway to implement throttling. \n            \n          \n      \n        \n          For clients that cannot perform retries, a buffer needs to be implemented to flatten the demand curve. A buffer defers request processing, allowing applications that run at different rates to communicate effectively. A buffer-based approach uses a queue or a stream to accept messages from producers. Messages are read by consumers and processed, allowing the messages to run at the rate that meets the consumers’ business requirements. \n        \n        \n           \n           \n        \n            \n              Amazon Simple Queue Service (Amazon SQS) is a managed service that provides queues that allow a single consumer to read individual messages.\n            \n          \n            \n              Amazon Kinesis provides a stream that allows many consumers to read the same messages.\n            \n          \n      \n        \n          Analyze the overall demand, rate of change, and required response time to right size the throttle or buffer required.\n        \n      \n   \n    \n    Resources\n    \n      Related documents:\n    \n    \n       \n       \n       \n       \n       \n    \n        \n          Getting started with Amazon SQS\n        \n      \n        \n          Application integration Using Queues and Messages\n        \n      \n        \n          Managing and monitoring API throttling in your workloads\n        \n      \n        \n          Throttling a tiered, multi-tenant REST API at scale using API Gateway\n        \n      \n        \n          Application integration Using Queues and Messages\n        \n      \n    \n      Related videos:\n    \n    \n       \n       \n       \n    \n        AWS re:Invent 2022 - Application integration patterns for microservices\n        \n      \n        AWS re:Invent 2023 - Smart savings: Amazon EC2 cost-optimization strategies\n        \n      \n        AWS re:Invent 2023 - Advanced integration patterns \u0026 trade-offs for loosely coupled systems\n        \n      \n  \u003cdiv\u003e\u003cdiv\u003e\u003cdiv\u003e\u003cdiv id=\"js_error_message\"\u003e\u003cp\u003e\u003cimg src=\"https://d1ge0kk1l5kms0.cloudfront.net/images/G/01/webservices/console/warning.png\" alt=\"Warning\" /\u003e \u003cstrong\u003eJavascript is disabled or is unavailable in your browser.\u003c/strong\u003e\u003c/p\u003e\u003cp\u003eTo use the Amazon Web Services Documentation, Javascript must be enabled. Please refer to your browser's Help pages for instructions.\u003c/p\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003eDocument ConventionsSUS02-BP05 Optimize team member resources for activities\n  performedSoftware and architecture",
  "SUS03-BP01 Optimize software and architecture for asynchronous\n  and scheduled jobsUse efficient software and architecture patterns such as queue-driven to \n    maintain consistent high utilization of deployed resources.\n    Common anti-patterns:\n  \n     \n     \n  \n      \n        You overprovision the resources in your cloud workload to meet unforeseen spikes in demand.\n      \n    \n      \n        Your architecture does not decouple senders and receivers of asynchronous messages by a messaging component. \n      \n    \n    Benefits of establishing this best practice:\n  \n     \n     \n     \n  \n      \n        Efficient software and architecture patterns minimize the unused resources in your workload and improve the overall efficiency.\n      \n    \n      \n        You can scale the processing independently of the receiving of asynchronous messages.\n      \n    \n      \n        Through a messaging component, you have relaxed availability requirements that you can meet with fewer resources.\n      \n    \n    Level of risk exposed if this best practice\n    is not established: Medium\n  \n\n  Implementation guidance\n    \n      Use efficient architecture patterns such as event-driven architecture that result in \n      even utilization of components and minimize overprovisioning in your workload. Using \n      efficient architecture patterns minimizes idle resources from lack of use due to changes \n      in demand over time. \n    \n    \n      Understand the requirements of your workload components and adopt architecture patterns \n      that increase overall utilization of resources. Retire components that are no longer required.\n    \n    \n     \n      \n      Implementation steps\n     \n    \n       \n       \n       \n       \n       \n       \n    \n        \n          Analyze the demand for your workload to determine how to respond to those.\n        \n      \n        \n          For requests or jobs that don’t require synchronous responses, use queue-driven \n          architectures and auto scaling workers to maximize utilization. Here are some \n          examples of when you might consider queue-driven architecture:\n        \n        \n              \n                Queuing mechanism\n                Description\n              \n            \n              \n                \n                  AWS Batch job queues\n                \n                \n                  AWS Batch jobs are submitted to a job queue where \n                    they reside until they can be scheduled to run in a \n                    compute environment.\n                \n              \n              \n                \n                  Amazon Simple Queue Service and Amazon EC2 Spot Instances\n                \n                \n                  Pairing Amazon SQS and Spot Instances to build fault tolerant and efficient architecture.\n                \n              \n            \n      \n        \n          For requests or jobs that can be processed anytime, use scheduling mechanisms \n          to process jobs in batch for more efficiency. Here are some examples of scheduling \n          mechanisms on AWS: \n        \n        \n              \n                Scheduling mechanism\n                Description\n              \n            \n              \n                \n                  Amazon EventBridge Scheduler\n                \n                \n                  A capability from Amazon EventBridge that allows you to create, run, and manage scheduled tasks at scale.\n                \n              \n              \n                \n                  AWS Glue time-based schedule\n                \n                \n                  Define a time-based schedule for your crawlers and jobs in AWS Glue.\n                \n              \n              \n                \n                  Amazon Elastic Container Service (Amazon ECS) scheduled tasks\n                \n                \n                  Amazon ECS supports creating scheduled tasks. Scheduled tasks use Amazon EventBridge rules to run tasks either on a schedule or in a response to an EventBridge event.\n                \n              \n              \n                \n                  Instance Scheduler\n                \n                \n                  Configure start and stop schedules for your Amazon EC2 and Amazon Relational Database Service instances.\n                \n              \n            \n      \n        \n          If you use polling and webhooks mechanisms in your architecture, replace those with events. \n          Use event-driven architectures to build highly efficient workloads. \n        \n      \n        \n          Leverage serverless on AWS to eliminate over-provisioned infrastructure.\n        \n      \n        \n          Right size individual components of your architecture to prevent idling resources waiting for input.\n        \n        \n           \n           \n        \n            \n              You can use the Rightsizing Recommendations in AWS Cost Explorer or AWS Compute Optimizer to identify rightsizing opportunities.\n            \n          \n            \n              For more detail, see Right Sizing: Provisioning Instances to Match Workloads.\n            \n          \n      \n     \n   \n\n  Resources\n\n      \n    \n      Related documents:\n    \n    \n       \n       \n       \n       \n       \n       \n       \n       \n    \n        \n          What\n          is Amazon Simple Queue Service?\n        \n      \n        \n          What\n          is Amazon MQ?\n        \n      \n        \n          Scaling\n          based on Amazon SQS\n        \n      \n        \n          What\n          is AWS Step Functions?\n        \n      \n        \n          What\n          is AWS Lambda?\n        \n      \n        \n          Using\n          AWS Lambda with Amazon SQS\n        \n      \n        \n          What\n          is Amazon EventBridge?\n        \n      \n        \n          Managing Asynchronous Workflows with a REST API\n        \n      \n    \n      Related videos:\n    \n    \n       \n       \n       \n       \n       \n    \n        AWS re:Invent 2023 - Navigating the journey to serverless event-driven architecture\n        \n      \n        AWS re:Invent 2023 - Using serverless for event-driven architecture \u0026 domain-driven design\n        \n      \n        AWS re:Invent 2023 - Advanced event-driven patterns with Amazon EventBridge\n        \n      \n        AWS re:Invent 2023 - Sustainable architecture: Past, present, and future\n        \n      \n        \n          Asynchronous Message Patterns | AWS Events\n        \n      \n    \n      Related examples:\n    \n    \n       \n    \n        \n          Event-driven architecture with AWS Graviton Processors and Amazon EC2 Spot Instances\n        \n      \n  \u003cdiv\u003e\u003cdiv\u003e\u003cdiv\u003e\u003cdiv id=\"js_error_message\"\u003e\u003cp\u003e\u003cimg src=\"https://d1ge0kk1l5kms0.cloudfront.net/images/G/01/webservices/console/warning.png\" alt=\"Warning\" /\u003e \u003cstrong\u003eJavascript is disabled or is unavailable in your browser.\u003c/strong\u003e\u003c/p\u003e\u003cp\u003eTo use the Amazon Web Services Documentation, Javascript must be enabled. Please refer to your browser's Help pages for instructions.\u003c/p\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003eDocument ConventionsSUS 3 How do you take advantage of software and architecture patterns to\n                  support your sustainability goals?SUS03-BP02 Remove or refactor workload components with low or\n  no use",
  "SUS03-BP02 Remove or refactor workload components with low or\n  no useRemove components that are unused and no longer required, and refactor \n    components with little utilization to minimize waste in your workload.\n    Common anti-patterns:\n  \n     \n     \n  \n      \n        You do not regularly check the utilization level of individual components of your workload.\n      \n    \n      \n        You do not check and analyze recommendations from AWS rightsizing tools such as AWS Compute Optimizer.\n      \n    \n    Benefits of establishing this best practice: Removing unused components \n    minimizes waste and improves the overall efficiency of your cloud workload.\n  \n    Level of risk exposed if this best practice\n      is not established: Medium\n  \n\n  Implementation guidance\n    Unused or underutilized components in a cloud workload consume unnecessary compute, storage or network resources. Remove or refactor these components to directly reduce waste and improve the overall efficiency of a cloud workload. This is an iterative improvement process which can be initiated by changes in demand or the release of a new cloud service. For example, a significant drop in AWS Lambda function run time can be indicate a need to lower the memory size. Also, as AWS releases new services and features, the optimal services and architecture for your workload may change.\n    \n      Continually monitor workload activity and look for opportunities to improve the utilization level of individual components.  By removing idle components and performing rightsizing activities, you meet your business requirements with the fewest cloud resources.  \n    \n     \n      \n      Implementation steps\n    \n    \n       \n       \n       \n       \n       \n       \n    \n        \n          Inventory your AWS resourceds: Create an inventory of your AWS resources. In AWS, you can turn on AWS Resource Explorer to explore and organize your AWS resources. For more details, see AWS re:Invent 2022 - How to manage resources and applications at scale on AWS.\n        \n      \n        \n          Monitor utilization: Monitor and capture the utilization metrics for critical components of your workload (like CPU utilization, memory utilization, or network throughput in Amazon CloudWatch metrics). \n        \n      \n        \n          Identify unused components: Identify unused or under-utilized components in your architecture.\n        \n        \n           \n           \n        \n            \n              For stable workloads, check AWS rightsizing tools such as AWS Compute Optimizer at regular intervals to identify idle, unused, or underutilized components.\n            \n          \n            \n              For ephemeral workloads, evaluate utilization metrics to identify idle, unused, or underutilized components.\n            \n          \n      \n        \n          Remove unused components: Retire components and associated assets (like Amazon ECR images) that are no longer needed.\n        \n        \n           \n           \n        \n            \n              Automated Cleanup of Unused Images in Amazon ECR\n            \n          \n            \n              Delete unused Amazon Elastic Block Store (Amazon EBS) volumes by using AWS Config and AWS Systems Manager\n          \n      \n        \n          Refactor underutilized components: Refactor or consolidate underutilized components with other resources to improve utilization efficiency. For example, you can provision multiple small databases on a single Amazon RDS database instance instead of running databases on individual underutilized instances.\n        \n      \n        \n          Evaluate improvements: Understand the resources provisioned by your workload to complete a unit of work. Use this information to evaluate improvements achieved by removing or refactoring components.\n        \n        \n           \n           \n        \n            \n              Measure and track cloud efficiency with sustainability proxy metrics, Part I: What are proxy metrics?\n            \n          \n            \n              Measure and track cloud efficiency with sustainability proxy metrics, Part II: Establish a metrics pipeline\n            \n          \n      \n     \n   \n\n  Resources\n\n      \n    \n      Related documents:\n    \n    \n       \n       \n       \n       \n    \n        AWS Trusted Advisor\n      \n        \n          What\n          is Amazon CloudWatch?\n        \n      \n        \n          Right Sizing: Provisioning Instances to Match Workloads\n        \n      \n        \n          Optimizing your cost with Rightsizing Recommendations\n        \n      \n    \n      Related videos:\n    \n    \n       \n    \n        AWS re:Invent 2023 - Capacity, availability, cost efficiency: Pick three\n        \n      \n  \u003cdiv\u003e\u003cdiv\u003e\u003cdiv\u003e\u003cdiv id=\"js_error_message\"\u003e\u003cp\u003e\u003cimg src=\"https://d1ge0kk1l5kms0.cloudfront.net/images/G/01/webservices/console/warning.png\" alt=\"Warning\" /\u003e \u003cstrong\u003eJavascript is disabled or is unavailable in your browser.\u003c/strong\u003e\u003c/p\u003e\u003cp\u003eTo use the Amazon Web Services Documentation, Javascript must be enabled. Please refer to your browser's Help pages for instructions.\u003c/p\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003eDocument ConventionsSUS03-BP01 Optimize software and architecture for asynchronous\n  and scheduled jobsSUS03-BP03 Optimize areas of code that consume the most time or\n  resources",
  "SUS03-BP03 Optimize areas of code that consume the most time or\n  resourcesOptimize your code that runs within different components of your \n    architecture to minimize resource usage while maximizing performance.\n    Common anti-patterns:\n  \n     \n     \n     \n  \n      \n        You ignore optimizing your code for resource usage.\n      \n    \n      \n        You usually respond to performance issues by increasing the resources.\n      \n    \n      \n        Your code review and development process does not track performance changes.\n      \n    \n    Benefits of establishing this best practice: Using \n    efficient code minimizes resource usage and improves performance.\n  \n    Level of risk exposed if this best practice\n      is not established: Medium\n  \n\n  Implementation guidance\n    \n      It is crucial to examine every functional area, including the code for a cloud architected application, to optimize its resource usage and performance. Continually monitor your workload’s performance in build environments and production and identify opportunities to improve code snippets that have particularly high resource usage. Adopt a regular review process to identify bugs or anti-patterns within your code that use resources inefficiently. Leverage simple and efficient algorithms that produce the same results for your use case.\n    \n   \n    \n    Implementation steps\n    \n       \n       \n       \n       \n       \n       \n    \n        Use efficient programming language:\n          Use an efficient operating system and programming language for the workload. For details on energy efficient programming languages (including Rust), see Sustainability with Rust.\n        \n      \n        \n          Use an AI coding companion: Consider using an AI coding companion such as Amazon Q Developer to efficiently write code.\n        \n      \n        Automate code reviews:\n          While developing your workloads, adopt an automated code review process to improve quality and identify bugs and anti-patterns.\n        \n        \n           \n           \n           \n        \n            \n              Automate code reviews with Amazon CodeGuru Reviewer\n            \n          \n            \n              Detecting concurrency bugs with Amazon CodeGuru\n            \n          \n            \n              Raising code quality for Python applications using Amazon CodeGuru\n            \n          \n      \n        Use a code profiler:\n          Use a code profiler to identify the areas of code that use the most time or resources as targets for optimization.\n        \n        \n           \n           \n           \n        \n            \n              Reducing your organization's carbon footprint with Amazon CodeGuru Profiler\n            \n          \n            \n              Understanding memory usage in your Java application with Amazon CodeGuru Profiler\n            \n          \n            \n              Improving customer experience and reducing cost with Amazon CodeGuru Profiler\n            \n          \n      \n        \n          Monitor and optimize: Use continuous monitoring resources to identify components with high resource requirements or suboptimal configuration.\n        \n        \n           \n           \n        \n            \n              Replace computationally intensive algorithms with simpler and more efficient version that produce the same result.\n            \n          \n            \n              Remove unnecessary code such as sorting and formatting.\n            \n          \n      \n        \n          Use code refactoring or transformation: Explore the possibility of Amazon Q code transformation for application maintenance and upgrades.\n        \n        \n           \n           \n        \n            \n              Upgrade language versions with Amazon Q Code Transformation\n            \n          \n            AWS re:Invent 2023 - Automate app upgrades \u0026 maintenance using Amazon Q Code Transformation\n            \n          \n      \n   \n\n  Resources\n\n      \n    \n      Related documents:\n    \n    \n       \n       \n       \n    \n        \n          What\n          is Amazon CodeGuru Profiler?\n        \n      \n        \n          FPGA\n          instances\n        \n      \n        \n          The AWS SDKs\n          on Tools to Build on AWS\n        \n      \n    \n      Related videos:\n    \n    \n       \n       \n    \n         \n          Improve Code Efficiency Using Amazon CodeGuru Profiler\n        \n      \n        \n          Automate Code Reviews and Application Performance Recommendations with Amazon CodeGuru\n        \n      \n  \u003cdiv\u003e\u003cdiv\u003e\u003cdiv\u003e\u003cdiv id=\"js_error_message\"\u003e\u003cp\u003e\u003cimg src=\"https://d1ge0kk1l5kms0.cloudfront.net/images/G/01/webservices/console/warning.png\" alt=\"Warning\" /\u003e \u003cstrong\u003eJavascript is disabled or is unavailable in your browser.\u003c/strong\u003e\u003c/p\u003e\u003cp\u003eTo use the Amazon Web Services Documentation, Javascript must be enabled. Please refer to your browser's Help pages for instructions.\u003c/p\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003eDocument ConventionsSUS03-BP02 Remove or refactor workload components with low or\n  no useSUS03-BP04 Optimize impact on devices and\n  equipment",
  "SUS03-BP04 Optimize impact on devices and\n  equipmentUnderstand the devices and equipment used in your architecture and use strategies to reduce their usage. This can minimize the overall environmental impact of your cloud workload. \n    Common anti-patterns:\n  \n     \n     \n  \n      \n        You ignore the environmental impact of devices used by your customers.\n      \n    \n      \n        You manually manage and update resources used by customers.\n      \n    \n    Benefits of establishing this best practice: Implementing software patterns and features that are optimized for customer device can reduce the overall environmental impact of cloud workload.\n  \n    Level of risk exposed if this best practice\n      is not established: Medium\n  \n\n  Implementation guidance\n    \n      Implementing software patterns and features that are optimized for customer devices can reduce the environmental impact in several ways:\n    \n    \n       \n       \n       \n    \n        \n          Implementing new features that are backward compatible can reduce the number of hardware replacements.\n        \n      \n        \n          Optimizing an application to run efficiently on devices can help to reduce their energy consumption and extend their battery life (if they are powered by battery). \n        \n      \n        \n          Optimizing an application for devices can also reduce the data transfer over the network.\n        \n      \n    \n      Understand the devices and equipment used in your architecture, their expected lifecycle, and the impact of replacing those components. Implement software patterns and features that can help to minimize the device energy consumption, the need for customers to replace the device and also upgrade it manually. \n    \n     \n      \n      Implementation steps\n    \n    \n       \n       \n       \n       \n       \n       \n       \n    \n        Conduct an inventory:\n          Inventory the devices used in your architecture. Devices can be mobile, tablet, IOT devices, smart light, or even smart devices in a factory.\n        \n      \n        Use energy-efficient devices: Consider using energy-efficient devices in your architecture. Use power management configurations on devices to enter low power mode when not in use.\n        \n      \n        Run efficient applications:\n          Optimize the application running on the devices:\n        \n        \n           \n           \n           \n           \n           \n        \n            \n              Use strategies such as running tasks in the background to reduce their energy consumption.\n            \n          \n            \n              Account for network bandwidth and latency when building payloads, and implement capabilities that help your applications work well on low bandwidth, high latency links.\n            \n          \n            \n              Convert payloads and files into optimized formats required by devices. For example, you can use Amazon Elastic Transcoder or AWS Elemental MediaConvert to convert large, high quality digital media files into formats that users can play back on mobile devices, tablets, web browsers, and connected televisions.\n            \n          \n            \n              Perform computationally intense activities server-side (such as image rendering), or use application streaming to improve the user experience on older devices.\n            \n          \n            \n              Segment and paginate output, especially for interactive sessions, to manage payloads and limit local storage requirements.\n            \n          \n      \n        Engage suppliers:\n          Work with device suppliers who use sustainable materials and provide transparency in their supply chains and environmental certifications.\n        \n      \n        Use over-the-air (OTA) updates:\n          Use automated over-the-air (OTA) mechanism to deploy updates to one or more devices. \n        \n        \n           \n           \n        \n            \n              You can use a CI/CD pipeline to update mobile applications.\n            \n          \n            \n              You can use AWS IoT Device Management to remotely manage connected devices at scale.\n            \n          \n      \n        Use managed device farms:\n          To test new features and updates, use managed device farms with representative sets of hardware and iterate development to maximize the devices supported. For more details, see SUS06-BP05 Use managed device farms for testing.\n        \n      \n        Continue to monitor and improve:\n          Track the energy usage of devices to identify areas for improvement. Use new technologies or best practices to enhance environmental impacts of these devices.\n        \n      \n     \n   \n\n  Resources\n\n      \n    \n      Related documents:\n    \n    \n       \n       \n       \n       \n       \n    \n        \n          What\n          is AWS Device Farm?\n        \n      \n        \n          AppStream 2.0 Documentation\n        \n      \n        \n          NICE\n          DCV\n        \n      \n        \n          OTA tutorial for updating firmware on devices running FreeRTOS\n        \n      \n        \n          Optimizing Your IoT Devices for Environmental Sustainability\n        \n      \n    \n      Related videos:\n    \n    \n       \n    \n        AWS re:Invent 2023 - Improve your mobile and web app quality using AWS Device Farm\n      \n  \u003cdiv\u003e\u003cdiv\u003e\u003cdiv\u003e\u003cdiv id=\"js_error_message\"\u003e\u003cp\u003e\u003cimg src=\"https://d1ge0kk1l5kms0.cloudfront.net/images/G/01/webservices/console/warning.png\" alt=\"Warning\" /\u003e \u003cstrong\u003eJavascript is disabled or is unavailable in your browser.\u003c/strong\u003e\u003c/p\u003e\u003cp\u003eTo use the Amazon Web Services Documentation, Javascript must be enabled. Please refer to your browser's Help pages for instructions.\u003c/p\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003eDocument ConventionsSUS03-BP03 Optimize areas of code that consume the most time or\n  resourcesSUS03-BP05 Use software patterns and architectures that best\n  support data access and storage patterns",
  "SUS03-BP05 Use software patterns and architectures that best\n  support data access and storage patternsUnderstand how data is used within your workload, consumed by your users, transferred, and stored. \n    Use software patterns and architectures that best support data access and storage to minimize the compute, \n    networking, and storage resources required to support the workload.\n    Common anti-patterns:\n  \n     \n     \n     \n     \n  \n      \n        You assume that all workloads have similar data storage and access patterns.\n      \n    \n      \n        You only use one tier of storage, assuming all workloads fit within that tier.\n      \n    \n      \n        You assume that data access patterns will stay consistent over time.\n      \n    \n      \n        Your architecture supports a potential high data access burst, which results in the resources remaining idle most of the time.\n      \n    \n    Benefits of establishing this best practice: Selecting and optimizing your architecture based on data access and storage patterns will help decrease development complexity and increase overall utilization. Understanding when to use global tables, data partitioning, and caching will help you decrease operational overhead and scale based on your workload needs.\n  \n    Level of risk exposed if this best practice\n      is not established: Medium\n  \n\n  Implementation guidance\n    \n      To improve long-term workload sustainability, use architecture patterns that support data access and storage characteristics for your workload. These patterns help you efficiently retrieve and process data. For example, you can use modern data architecture on AWS with purpose-built services optimized for your unique analytics use cases. These architecture patterns allow for efficient data processing and reduce the resource usage.\n    \n     \n      \n      Implementation steps\n    \n    \n       \n       \n       \n       \n    \n        \n          Understand data characteristics: Analyze your data characteristics and access patterns to identify the correct configuration for your cloud resources. Key characteristics to consider include:\n        \n        \n           \n           \n           \n           \n        \n            \n              Data type: structured, semi-structured, unstructured \n            \n          \n            \n              Data growth: bounded, unbounded\n            \n          \n            \n              Data durability: persistent, ephemeral, transient\n            \n          \n            \n              Access patterns reads or writes, update frequency, spiky, or consistent\n            \n          \n      \n        \n          Use optimal architecture patterns: Use architecture patterns that best support data access and storage patterns.\n        \n        \n           \n           \n           \n        \n            \n              Patterns for enabling data persistence\n            \n          \n             \n              Let’s Architect! Modern data architectures\n            \n          \n            \n              Databases on AWS: The Right Tool for the Right Job\n            \n          \n      \n        \n          Use purpose-built services: Use technologies that are fit-for-purpose.\n        \n        \n           \n           \n           \n        \n            \n              Use technologies that work natively with compressed data.\n            \n            \n               \n               \n               \n            \n                \n                  Athena Compression Support file formats\n                \n              \n                \n                  Format Options for ETL Inputs and Outputs in AWS Glue\n              \n                \n                  Loading compressed data files from Amazon S3 with Amazon Redshift\n                \n              \n          \n            \n              Use purpose-built analytics services for data processing in your architecture. For detail on AWS purpose-built analytics services, see AWS re:Invent 2022 - Building modern data architectures on AWS.\n            \n          \n            \n              Use the database engine that best supports your dominant query pattern. Manage your database indexes for efficient querying. For further details, see AWS Databases and AWS re:Invent 2022 - Modernize apps with purpose-built databases.\n            \n          \n      \n        \n         Minimize data transfer: Select network protocols that reduce the amount of network capacity consumed in your architecture.\n        \n      \n     \n   \n\n  Resources\n\n      \n    \n      Related documents:\n    \n    \n       \n       \n       \n       \n       \n       \n       \n    \n        \n          COPY\n          from columnar data formats with Amazon Redshift\n        \n      \n        \n          Converting\n          Your Input Record Format in Firehose\n        \n      \n        \n          Improve\n          query performance on Amazon Athena by Converting to Columnar\n          Formats\n        \n      \n        \n          Monitoring\n          DB load with Performance Insights on Amazon Aurora\n        \n      \n        \n          Monitoring\n          DB load with Performance Insights on Amazon RDS\n        \n      \n        \n          Amazon S3 Intelligent-Tiering storage class\n        \n      \n        \n          Build a CQRS event store with Amazon DynamoDB\n        \n      \n    \n      Related videos:\n    \n    \n       \n       \n       \n       \n       \n       \n    \n        AWS re:Invent 2022 - Building data mesh architectures on AWS\n      \n        AWS re:Invent 2023 - Deep dive into Amazon Aurora and its innovations \n        \n      \n        AWS re:Invent 2023 - Improve Amazon EBS efficiency and be more cost-efficient\n        \n      \n        AWS re:Invent 2023 - Optimizing storage price and performance with Amazon S3\n        \n      \n        AWS re:Invent 2023 - Building and optimizing a data lake on Amazon S3\n        \n      \n        AWS re:Invent 2023 - Advanced event-driven patterns with Amazon EventBridge\n        \n      \n    \n      Related examples:\n    \n    \n       \n       \n       \n    \n        AWS Purpose Built Databases Workshop\n        \n      \n        AWS Modern Data Architecture Immersion Day\n        \n      \n        \n          Build a Data Mesh on AWS\n      \n  \u003cdiv\u003e\u003cdiv\u003e\u003cdiv\u003e\u003cdiv id=\"js_error_message\"\u003e\u003cp\u003e\u003cimg src=\"https://d1ge0kk1l5kms0.cloudfront.net/images/G/01/webservices/console/warning.png\" alt=\"Warning\" /\u003e \u003cstrong\u003eJavascript is disabled or is unavailable in your browser.\u003c/strong\u003e\u003c/p\u003e\u003cp\u003eTo use the Amazon Web Services Documentation, Javascript must be enabled. Please refer to your browser's Help pages for instructions.\u003c/p\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003eDocument ConventionsSUS03-BP04 Optimize impact on devices and\n  equipmentData",
  "SUS04-BP01 Implement a data classification policyClassify data to understand its criticality to business \n    outcomes and choose the right energy-efficient storage tier \n    to store the data.\n    Common anti-patterns:\n  \n     \n     \n  \n      \n        You do not identify data assets with similar characteristics \n        (such as sensitivity, business criticality, or regulatory \n        requirements) that are being processed or stored. \n      \n    \n      \n        You have not implemented a data catalog to inventory your data assets.\n      \n    \n    Benefits of establishing this best practice: Implementing \n    a data classification policy allows you to determine the most energy-efficient storage \n    tier for data.\n  \n    Level of risk exposed if this best practice\n      is not established: Medium\n  \n\n  Implementation guidance\n    \n      Data classification involves identifying the types of data \n      that are being processed and stored in an information system \n      owned or operated by an organization. It also involves making \n      a determination on the criticality of the data and the likely \n      impact of a data compromise, loss, or misuse. \n    \n    \n      Implement data classification policy by working backwards \n      from the contextual use of the data and creating a categorization \n      scheme that takes into account the level of criticality of a \n      given dataset to an organization’s operations.\n    \n     \n      \n      Implementation steps\n    \n    \n       \n       \n       \n       \n       \n       \n    \n        Perform data inventory:\n          Conduct an inventory of the various data types that exist for your workload. \n        \n      \n        Group data:\n          Determine criticality, confidentiality, integrity, and \n          availability of data based on risk to the organization. \n          Use these requirements to group data into one of the data \n          classification tiers that you adopt. As an example, see Four simple steps to classify your data and\n            secure your startup.\n        \n      \n        Define data classification levels and policies:\n          For each data group, define data classification level (for example, public or confidential) and handling policies. Tag data accordingly. For more detail on data classification categories, see Data Classification whitepaper.\n        \n      \n        Periodically review:\n          Periodically review and audit your environment for untagged and\n          unclassified data. Use automation to identify this data, and classify and tag the data\n          appropriately. As an example, see Data Catalog and crawlers in AWS Glue.\n        \n      \n        Establish a data catalog:\n          Establish a data catalog that provides \n          audit and governance capabilities. \n        \n      \n        Documentation:\n          Document data classification policies and handling procedures \n          for each data class.\n        \n      \n     \n   \n\n  Resources\n\n      \n    \n      Related documents:\n    \n    \n       \n       \n    \n        \n          Leveraging\n          AWS Cloud to Support Data Classification\n        \n      \n        \n          Tag\n          policies from AWS Organizations\n        \n      \n    \n      Related videos:\n    \n    \n       \n       \n    \n        AWS re:Invent 2022 - Enabling agility with data governance on AWS\n      \n        AWS re:Invent 2023 - Data protection and resilience with AWS storage\n        \n      \n  \u003cdiv\u003e\u003cdiv\u003e\u003cdiv\u003e\u003cdiv id=\"js_error_message\"\u003e\u003cp\u003e\u003cimg src=\"https://d1ge0kk1l5kms0.cloudfront.net/images/G/01/webservices/console/warning.png\" alt=\"Warning\" /\u003e \u003cstrong\u003eJavascript is disabled or is unavailable in your browser.\u003c/strong\u003e\u003c/p\u003e\u003cp\u003eTo use the Amazon Web Services Documentation, Javascript must be enabled. Please refer to your browser's Help pages for instructions.\u003c/p\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003eDocument ConventionsSUS 4 How do you take advantage of data management policies and patterns to support your sustainability goals?SUS04-BP02 Use technologies that support data access and\n  storage patterns",
  "SUS04-BP02 Use technologies that support data access and\n  storage patterns\n    Use storage technologies that best support how your \n    data is accessed and stored to minimize the resources \n    provisioned while supporting your workload.\n  \n    Common anti-patterns:\n  \n     \n     \n     \n  \n      \n        You assume that all workloads have similar data storage and access patterns.\n      \n    \n      \n        You only use one tier of storage, assuming all workloads fit within that tier.\n      \n    \n      \n        You assume that data access patterns will stay consistent over time.\n      \n    \n    Benefits of establishing this best practice: Selecting \n    and optimizing your storage technologies based on data access and storage patterns will \n    help you reduce the required cloud resources to meet your business needs and improve \n    the overall efficiency of cloud workload.\n  \n    Level of risk exposed if this best practice\n    is not established: Low\n  \n\n  Implementation guidance\n    \n      Select the storage solution that aligns best to your access \n      patterns, or consider changing your access patterns to align \n      with the storage solution to maximize performance efficiency. \n    \n     \n      \n      Implementation steps\n    \n    \n       \n       \n       \n       \n    \n        Evaluate data and access characteristics:\n          Evaluate your data characteristics and access pattern to \n          collect the key characteristics of your storage needs. \n          Key characteristics to consider include:\n        \n        \n           \n           \n           \n           \n        \n            \n              Data type: structured, semi-structured, \n              unstructured \n            \n          \n            \n              Data growth: bounded, unbounded \n            \n          \n            \n              Data durability: persistent, ephemeral, \n              transient \n            \n          \n            \n              Access patterns: reads or writes, \n              frequency, spiky, or consistent\n            \n          \n      \n        Choose the right storage technology:\n          Migrate data to the appropriate storage technology that supports \n          your data characteristics and access pattern. Here are some \n          examples of AWS storage technologies and their \n          key characteristics:\n        \n        \n              \n                Type\n                Technology\n                Key characteristics\n              \n            \n              \n                \n                  \n                    Object storage\n                  \n                \n                \n                  Amazon S3\n                \n                \n                  \n                    An object storage service with unlimited scalability, \n                    high availability, and multiple options for accessibility. \n                    Transferring and accessing objects in and out of Amazon S3 \n                    can use a service, such as \n                    Transfer Acceleration\n                    or Access \n                    Points, to support your location, security needs, and access \n                    patterns.\n                  \n                \n              \n              \n                \n                  \n                    Archiving storage\n                  \n                \n                \n                  Amazon S3 Glacier\n                \n                \n                  \n                    Storage class of Amazon S3 built for data-archiving. \n                  \n                \n              \n              \n                \n                  \n                    Shared file system\n                  \n                \n                \n                  Amazon Elastic File System (Amazon EFS)\n                \n                \n                  \n                    Mountable file system that can be accessed by multiple \n                    types of compute solutions. Amazon EFS automatically \n                    grows and shrinks storage and is performance-optimized \n                    to deliver consistent low latencies.\n                  \n                \n              \n              \n                \n                  \n                    Shared file system\n                  \n                \n                \n                  Amazon FSx\n                \n                \n                  \n                    Built on the latest AWS compute solutions to support \n                    four commonly used file systems: NetApp ONTAP, OpenZFS, \n                    Windows File Server, and Lustre. Amazon FSx \n                    latency, \n                    throughput, and IOPS vary per file system and should be \n                    considered when selecting the right file system for your \n                    workload needs.\n                  \n                \n              \n              \n                \n                  \n                    Block storage\n                  \n                \n                \n                  Amazon Elastic Block Store (Amazon EBS)\n                \n                \n                  \n                    Scalable, high-performance block-storage service \n                    designed for Amazon Elastic Compute Cloud (Amazon EC2). \n                    Amazon EBS includes SSD-backed storage for transactional, \n                    IOPS-intensive workloads and HDD-backed storage for \n                    throughput-intensive workloads.\n                  \n                \n              \n              \n                \n                  Relational database\n                \n                \n                  Amazon Aurora, Amazon RDS, Amazon Redshift\n                \n                \n                  Designed to support ACID (atomicity, consistency, isolation, durability) transactions and maintain referential integrity and strong data consistency. Many traditional applications, enterprise resource planning (ERP), customer relationship management (CRM), and ecommerce systems use relational databases to store their data.\n                \n              \n              \n                \n                  Key-value database\n                \n                \n                  Amazon DynamoDB\n                \n                \n                  Optimized for common access patterns, typically to store and retrieve large volumes of data. High-traffic web apps, ecommerce systems, and gaming applications are typical use-cases for key-value databases.\n                \n              \n            \n      \n        Automate storage allocation:\n          For storage systems that are a fixed size, such as \n          Amazon EBS or Amazon FSx, monitor the available \n          storage space and automate storage allocation on \n          reaching a threshold. You can leverage Amazon CloudWatch \n          to collect and analyze different metrics for \n          Amazon EBS and \n          Amazon FSx.\n        \n      \n        Choose the right storage class:\n          Choose the appropriate storage class for your data.\n        \n        \n           \n           \n        \n            \n              Amazon S3 storage classes can be configured at the object \n              level. A single bucket can contain objects stored \n              across all of the storage classes. \n            \n          \n            \n              You can use \n              Amazon S3 Lifecycle policies to automatically transition objects \n              between storage classes or remove data without any application changes. \n              In general, you have to make a trade-off between resource\n              efficiency, access latency, and reliability when considering \n              these storage mechanisms. \n            \n          \n      \n     \n   \n\n  Resources\n\n      \n    \n      Related documents:\n    \n    \n       \n       \n       \n       \n       \n       \n    \n        \n          Amazon EBS volume types\n        \n      \n        \n          Amazon EC2 instance store\n        \n      \n        \n          Amazon S3 Intelligent-Tiering\n        \n      \n        \n          Amazon EBS I/O Characteristics\n        \n      \n        \n          Using Amazon S3 storage classes\n        \n      \n        \n          What\n          is Amazon S3 Glacier?\n        \n      \n    \n      Related videos:\n    \n    \n       \n       \n       \n       \n       \n       \n       \n       \n    \n        AWS re:Invent 2023 - Improve Amazon EBS efficiency and be more cost-efficient\n        \n      \n        AWS re:Invent 2023 - Optimizing storage price and performance with Amazon S3\n        \n      \n        AWS re:Invent 2023 - Building and optimizing a data lake on Amazon S3\n        \n      \n        AWS re:Invent 2022 - Building modern data architectures on AWS\n      \n        AWS re:Invent 2022 - Modernize apps with purpose-built databases\n        \n      \n        AWS re:Invent 2022 - Building data mesh architectures on AWS\n      \n        AWS re:Invent 2023 - Deep dive into Amazon Aurora and its innovations \n        \n      \n        AWS re:Invent 2023 - Advanced data modeling with Amazon DynamoDB\n        \n      \n    \n      Related examples:\n    \n    \n       \n       \n       \n       \n       \n    \n        \n          Amazon S3 Examples\n        \n      \n        AWS Purpose Built Databases Workshop\n        \n      \n        \n          Databases for Developers\n        \n      \n        AWS Modern Data Architecture Immersion Day\n        \n      \n        \n          Build a Data Mesh on AWS\n      \n  \u003cdiv\u003e\u003cdiv\u003e\u003cdiv\u003e\u003cdiv id=\"js_error_message\"\u003e\u003cp\u003e\u003cimg src=\"https://d1ge0kk1l5kms0.cloudfront.net/images/G/01/webservices/console/warning.png\" alt=\"Warning\" /\u003e \u003cstrong\u003eJavascript is disabled or is unavailable in your browser.\u003c/strong\u003e\u003c/p\u003e\u003cp\u003eTo use the Amazon Web Services Documentation, Javascript must be enabled. Please refer to your browser's Help pages for instructions.\u003c/p\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003eDocument ConventionsSUS04-BP01 Implement a data classification policySUS04-BP03 Use policies to manage the lifecycle of your datasets",
  "SUS04-BP03 Use policies to manage the lifecycle of your datasetsManage the lifecycle of all of your data and automatically enforce \n    deletion to minimize the total storage required for your workload.\n    Common anti-patterns:\n  \n     \n     \n     \n  \n      \n        You manually delete data.\n      \n    \n      \n        You do not delete any of your workload data.\n      \n    \n      \n        You do not move data to more energy-efficient storage tiers based on its retention and access requirements.\n      \n    \n    Benefits of establishing this best practice: Using data lifecycle policies ensures efficient data access and retention in a workload.\n  \n    Level of risk exposed if this best practice\n      is not established: Medium\n  \n\n  Implementation guidance\n    \n      Datasets usually have different retention and access requirements during their lifecycle. \n      For example, your application may need frequent access to some datasets for a limited \n      period of time. After that, those datasets are infrequently accessed. To improve the efficiency of data storage and computation over time, implement lifecycle policies, which are rules that define how data is handled over time. \n    \n    \n      With lifecycle configuration rules, you can tell the specific storage service to transition a dataset to more energy-efficient storage tiers, archive it, or delete it. This practice minimizes active data storage and retrieval, which leads to lower energy consumption. In addition, practices such as archiving or deleting obsolete data support regulatory compliance and data governance. \n    \n     \n      \n      Implementation steps\n    \n    \n       \n       \n       \n       \n       \n    \n        \n          Use data classification: Classify datasets in your workload.\n        \n      \n        \n          Define handling rules: Define handling procedures for each data class.\n        \n      \n        \n          Enable automation: Set automated lifecycle policies to enforce lifecycle rules. \n          Here are some examples of how to set up automated lifecycle policies \n          for different AWS storage services:\n        \n        \n              \n                Storage service\n                How to set automated lifecycle policies\n              \n            \n              \n                \n                  Amazon S3\n                \n                \n                  You can use Amazon S3 Lifecycle to manage your objects throughout their lifecycle. \n                    If your access patterns are unknown, changing, or unpredictable, you can use Amazon S3\n                    Intelligent-Tiering, which monitors access patterns and automatically moves objects that \n                    have not been accessed to lower-cost access tiers. You can leverage Amazon S3 Storage Lens \n                    metrics to identify optimization opportunities and gaps in lifecycle management.\n                  \n                \n              \n              \n                \n                  Amazon Elastic Block Store\n                \n                \n                  You can use Amazon Data Lifecycle Manager to automate the creation, \n                    retention, and deletion of Amazon EBS snapshots and Amazon EBS-backed AMIs.\n                \n              \n              \n                \n                  Amazon Elastic File System\n                \n                \n                  Amazon EFS lifecycle management automatically manages file storage for your file systems.\n                \n              \n              \n                \n                  Amazon Elastic Container Registry\n                \n                \n                  Amazon ECR lifecycle policies automate the cleanup of your \n                    container images by expiring images based on age or count.\n                \n              \n              \n                \n                  AWS Elemental MediaStore\n                \n                \n                  You can use an object lifecycle policy that governs how long objects should be stored in the MediaStore container.\n                \n              \n            \n      \n        \n          Delete unused assets: Delete unused volumes, snapshots, and data that is out of its retention period. \n          Use native service features like Amazon DynamoDB Time To Live or Amazon CloudWatch \n          log retention for deletion. \n        \n      \n        \n          Aggregate and compress: Aggregate and compress data where applicable based on lifecycle rules.\n        \n      \n     \n   \n\n  Resources\n\n      \n    \n      Related documents:\n    \n    \n       \n       \n    \n        \n          Optimize your Amazon S3 Lifecycle rules with Amazon S3 Storage Class Analysis\n        \n      \n        \n          Evaluating\n          Resources with AWS Config Rules\n        \n      \n    \n      Related videos:\n    \n    \n       \n       \n       \n       \n    \n        AWS re:Invent 2021 - Amazon S3 Lifecycle best practices to optimize your storage spend\n        \n      \n        AWS re:Invent 2023 - Optimizing storage price and performance with Amazon S3\n        \n      \n        \n          Simplify Your Data Lifecycle and Optimize Storage Costs With Amazon S3 Lifecycle\n        \n      \n        \n          Reduce Your Storage Costs Using Amazon S3 Storage Lens\n        \n      \n  \u003cdiv\u003e\u003cdiv\u003e\u003cdiv\u003e\u003cdiv id=\"js_error_message\"\u003e\u003cp\u003e\u003cimg src=\"https://d1ge0kk1l5kms0.cloudfront.net/images/G/01/webservices/console/warning.png\" alt=\"Warning\" /\u003e \u003cstrong\u003eJavascript is disabled or is unavailable in your browser.\u003c/strong\u003e\u003c/p\u003e\u003cp\u003eTo use the Amazon Web Services Documentation, Javascript must be enabled. Please refer to your browser's Help pages for instructions.\u003c/p\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003eDocument ConventionsSUS04-BP02 Use technologies that support data access and\n  storage patternsSUS04-BP04 Use elasticity and automation to expand block storage or file system",
  "SUS04-BP04 Use elasticity and automation to expand block storage or file systemUse elasticity and automation to expand block storage or file system as data grows to minimize the total provisioned storage.\n    Common anti-patterns:\n  \n     \n     \n     \n  \n      \n        You procure large block storage or file system for future need.\n      \n    \n      \n        You overprovision the input and output operations per second (IOPS) of your file system.\n      \n    \n      \n        You do not monitor the utilization of your data volumes.\n      \n    \n    Benefits of establishing this best practice: Minimizing over-provisioning for storage system reduces the idle resources and improves the overall efficiency of your workload.\n  \n    Level of risk exposed if this best practice\n      is not established: Medium\n  \n\n  Implementation guidance\n    \n      Create block storage and file systems with size allocation, throughput, and latency that are appropriate for your workload. Use elasticity and automation to expand block storage or file system as data grows without having to over-provision these storage services.\n    \n     \n      \n      Implementation steps\n    \n    \n       \n       \n       \n       \n       \n       \n       \n    \n        \n          For fixed size storage like Amazon EBS, verify that you are monitoring the amount of storage used versus the overall storage size and create automation, if possible, to increase the storage size when reaching a threshold.\n        \n      \n        \n          Use elastic volumes and managed block data services to automate allocation of additional storage as your persistent data grows. As an example, you can use Amazon EBS Elastic Volumes to change volume size, volume type, or adjust the performance of your Amazon EBS volumes.\n        \n      \n        \n          Choose the right storage class, performance mode, and throughput mode for your file system to address your business need, not exceeding that.\n        \n        \n           \n           \n        \n            \n              Amazon EFS performance\n            \n          \n            \n              Amazon EBS volume performance on Linux instances\n            \n          \n      \n        \n          Set target levels of utilization for your data volumes, and resize volumes outside of expected ranges.\n        \n      \n        \n          Right size read-only volumes to fit the data.\n        \n      \n        \n          Migrate data to object stores to avoid provisioning the excess capacity from fixed volume sizes on block storage.\n        \n      \n        \n          Regularly review elastic volumes and file systems to terminate idle volumes and shrink over-provisioned resources to fit the current data size.\n        \n      \n     \n   \n\n  Resources\n\n      \n    \n      Related documents:\n    \n    \n       \n       \n       \n       \n    \n        \n          Extend the file system after resizing an EBS volume\n        \n      \n        \n          Modify a volume using Amazon EBS Elastic Volumes\n        \n      \n        \n          Amazon FSx Documentation\n        \n      \n        \n          What\n          is Amazon Elastic File System?\n        \n      \n    \n      Related videos:\n    \n    \n       \n       \n       \n    \n        \n          Deep Dive on Amazon EBS Elastic Volumes\n        \n      \n        \n          Amazon EBS and Snapshot Optimization Strategies for Better Performance and Cost Savings\n        \n      \n        \n          Optimizing Amazon EFS for cost and performance, using best practices\n        \n      \n  \u003cdiv\u003e\u003cdiv\u003e\u003cdiv\u003e\u003cdiv id=\"js_error_message\"\u003e\u003cp\u003e\u003cimg src=\"https://d1ge0kk1l5kms0.cloudfront.net/images/G/01/webservices/console/warning.png\" alt=\"Warning\" /\u003e \u003cstrong\u003eJavascript is disabled or is unavailable in your browser.\u003c/strong\u003e\u003c/p\u003e\u003cp\u003eTo use the Amazon Web Services Documentation, Javascript must be enabled. Please refer to your browser's Help pages for instructions.\u003c/p\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003eDocument ConventionsSUS04-BP03 Use policies to manage the lifecycle of your datasetsSUS04-BP05 Remove unneeded or redundant data",
  "SUS04-BP05 Remove unneeded or redundant dataRemove unneeded or redundant data to minimize the storage resources required to store your\n    datasets. \n    Common anti-patterns:\n  \n     \n     \n     \n     \n     \n  \n       You duplicate data that can be easily obtained or recreated. \n    \n       You back up all data without considering its criticality. \n    \n       You only delete data irregularly, on operational events, or not at all. \n    \n       You store data redundantly irrespective of the storage service's durability. \n    \n       You turn on Amazon S3 versioning without any business justification. \n    \n    Benefits of establishing this best practice: Removing unneeded\n    data reduces the storage size required for your workload and the workload environmental impact. \n    Level of risk exposed if this best practice is not established:\n    Medium \n\n    Implementation guidance\n    \n      When you remove unneeded and redundant datasets, you can reduce storage cost and environmental footprint. This practice may also make computing more efficient, as compute resources only process important data instead of unneeded data. Automate the deletion of unneeded data. Use technologies that deduplicate data at the file and block level. Use service features for native data replication and redundancy.\n    \n     \n      \n      Implementation steps\n    \n    \n       \n       \n       \n       \n       \n       \n       \n       \n       \n       \n    \n         \n          Evaluate public datasets: Evaluate if you can avoid storing data by using existing publicly available datasets\n          in AWS Data Exchange and Open Data on AWS. \n      \n        \n          De-deplicate data: Use mechanisms that can deduplicate data at the block and object level. Here are some\n          examples of how to deduplicate data on AWS: \n        \n              \n                Storage service\n                Deduplication mechanism\n              \n            \n              \n                \n                  Amazon S3\n                \n                \n                  Use AWS Lake Formation FindMatches to find matching records across a dataset\n                    (including ones without identifiers) by using the new FindMatches ML\n                    Transform.\n                \n              \n              \n                \n                  Amazon FSx\n                \n                \n                  Use data deduplication\n                    on Amazon FSx for Windows.\n                \n              \n              \n                \n                  Amazon Elastic Block Store snapshots\n                \n                \n                  Snapshots are incremental backups, which means that only the blocks on the\n                    device that have changed after your most recent snapshot are saved.\n                \n              \n            \n      \n         \n          Use lifecycle policies: Use lifecycle policies to automate unneeded data deletion. Use native service features like Amazon DynamoDB Time To Live,\n            Amazon S3 Lifecycle, or Amazon CloudWatch log\n            retention for deletion. \n      \n        \n          Use data virtualization: Use data virtualization capabilities on AWS to maintain data at its source and\n          avoid data duplication. \n        \n           \n           \n        \n            \n              Cloud Native Data\n                Virtualization on AWS\n            \n          \n            \n              Optimize Data Pattern Using Amazon Redshift Data Sharing\n            \n          \n      \n        \n          Use incremental backup: Use backup technology that can make incremental backups. \n      \n         \n          Use native durability: Leverage the durability of Amazon S3 and replication of\n            Amazon EBS to meet your durability goals instead of self-managed technologies (such\n          as a redundant array of independent disks (RAID)). \n      \n         \n          Use efficient logging: Centralize log and trace data, deduplicate identical log entries, and establish\n          mechanisms to tune verbosity when needed. \n      \n         \n          Use efficient caching: Pre-populate caches only where justified. \n      \n         Establish cache monitoring and automation to resize the cache accordingly. \n      \n         \n          Remove old version assets: Remove out-of-date deployments and assets from object stores and edge caches when\n          pushing new versions of your workload. \n      \n     \n   \n\n    Resources\n\n    \n      Related documents:\n    \n    \n       \n       \n       \n       \n       \n       \n       \n       \n    \n        \n          Change log data retention in CloudWatch Logs\n        \n      \n        \n          Data\n            deduplication on Amazon FSx for Windows File Server\n        \n      \n        \n          Features of\n            Amazon FSx for ONTAP including data deduplication\n        \n      \n        \n          Invalidating Files on Amazon CloudFront\n        \n      \n        \n          Using AWS Backup to back up\n            and restore Amazon EFS file systems\n        \n      \n        \n          What is\n            Amazon CloudWatch Logs?\n        \n      \n        \n          Working with\n            backups on Amazon RDS\n        \n      \n        \n          Integrate and deduplicate datasets using AWS Lake Formation\n        \n      \n    \n      Related videos:\n    \n    \n       \n    \n        \n          Amazon Redshift Data Sharing Use\n            Cases\n        \n      \n    \n      Related examples:\n    \n    \n       \n    \n        \n          How do\n            I analyze my Amazon S3 server access logs using Amazon Athena?\n        \n      \n  \u003cdiv\u003e\u003cdiv\u003e\u003cdiv\u003e\u003cdiv id=\"js_error_message\"\u003e\u003cp\u003e\u003cimg src=\"https://d1ge0kk1l5kms0.cloudfront.net/images/G/01/webservices/console/warning.png\" alt=\"Warning\" /\u003e \u003cstrong\u003eJavascript is disabled or is unavailable in your browser.\u003c/strong\u003e\u003c/p\u003e\u003cp\u003eTo use the Amazon Web Services Documentation, Javascript must be enabled. Please refer to your browser's Help pages for instructions.\u003c/p\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003eDocument ConventionsSUS04-BP04 Use elasticity and automation to expand block storage or file systemSUS04-BP06 Use shared file systems or storage to access common data",
  "SUS04-BP06 Use shared file systems or storage to access common\n      dataAdopt shared file systems or storage to avoid data duplication and allow for more efficient\n    infrastructure for your workload. \n    Common anti-patterns:\n  \n     \n     \n     \n  \n       You provision storage for each individual client. \n    \n       You do not detach data volume from inactive clients. \n    \n       You do not provide access to storage across platforms and systems. \n    \n    Benefits of establishing this best practice: Using shared file\n    systems or storage allows for sharing data to one or more consumers without having to copy the\n    data. This helps to reduce the storage resources required for the workload. \n    Level of risk exposed if this best practice is not established:\n    Medium \n\n    Implementation guidance\n     If you have multiple users or applications accessing the same datasets, using shared\n      storage technology is crucial to use efficient infrastructure for your workload. Shared\n      storage technology provides a central location to store and manage datasets and avoid data\n      duplication. It also enforces consistency of the data across different systems. Moreover,\n      shared storage technology allows for more efficient use of compute power, as multiple compute\n      resources can access and process data at the same time in parallel. \n     Fetch data from these shared storage services only as needed and detach unused volumes to\n      free up resources. \n     \n      \n      Implementation steps\n    \n\n    \n       \n       \n       \n       \n    \n         \n          Use shared storage: Migrate data to shared storage when the data has multiple consumers. Here are some\n          examples of shared storage technology on AWS: \n        \n              \n                Storage option\n                When to use\n              \n            \n              \n                \n                  Amazon EBS\n                      Multi-Attach\n                \n                \n                  Amazon EBS Multi-Attach allows you to attach a single Provisioned IOPS SSD (io1\n                    or io2) volume to multiple instances that are in the same Availability\n                    Zone.\n                \n              \n              \n                \n                  Amazon EFS\n                \n                \n                  See When to Choose\n                      Amazon EFS.\n                \n              \n              \n                \n                  Amazon FSx\n                \n                \n                  See Choosing an Amazon FSx\n                      File System.\n                \n              \n              \n                \n                  Amazon S3\n                \n                \n                  Applications that do not require a file system structure and are designed to\n                    work with object storage can use Amazon S3 as a massively scalable, durable, low-cost\n                    object storage solution.\n                \n              \n            \n      \n        \n          Fetch data as needed: Copy data to or fetch data from shared file systems only as needed. As an example, you\n          can create an Amazon FSx for Lustre file system backed by Amazon S3 and only load the subset of data\n          required for processing jobs to Amazon FSx.\n      \n        \n          Delete unneeded data: Delete data as appropriate for your usage patterns as outlined in SUS04-BP03 Use policies to manage the lifecycle of your datasets.\n      \n        \n          Detach inactive clients: Detach volumes from clients that are not actively using them. \n      \n     \n   \n\n    Resources\n\n\n    \n      Related documents:\n    \n    \n       \n       \n       \n       \n    \n         Linking your file system\n            to an Amazon S3 bucket \n      \n        \n            Using Amazon EFS for AWS Lambda in your serverless applications \n      \n         Amazon EFS Intelligent-Tiering Optimizes Costs for Workloads with Changing Access Patterns\n          \n      \n         Using\n            Amazon FSx with your on-premises data repository \n      \n    \n      related videos:\n    \n    \n       \n       \n       \n    \n         Storage cost optimization\n            with Amazon EFS \n      \n        AWS re:Invent 2023 - What's\n            new with AWS file storage\n      \n        AWS re:Invent 2023 - File\n            storage for builders and data scientists on Amazon Elastic File System\n      \n  \u003cdiv\u003e\u003cdiv\u003e\u003cdiv\u003e\u003cdiv id=\"js_error_message\"\u003e\u003cp\u003e\u003cimg src=\"https://d1ge0kk1l5kms0.cloudfront.net/images/G/01/webservices/console/warning.png\" alt=\"Warning\" /\u003e \u003cstrong\u003eJavascript is disabled or is unavailable in your browser.\u003c/strong\u003e\u003c/p\u003e\u003cp\u003eTo use the Amazon Web Services Documentation, Javascript must be enabled. Please refer to your browser's Help pages for instructions.\u003c/p\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003eDocument ConventionsSUS04-BP05 Remove unneeded or redundant dataSUS04-BP07 Minimize data movement across networks",
  "SUS04-BP07 Minimize data movement across networksUse shared file systems or object storage to access common data and minimize the total\n    networking resources required to support data movement for your workload.\n    Common anti-patterns:\n  \n     \n     \n  \n       You store all data in the same AWS Region independent of where the data users are.\n      \n    \n       You do not optimize data size and format before moving it over the network. \n    \n    Benefits of establishing this best practice: Optimizing data\n    movement across the network reduces the total networking resources required for the workload and\n    lowers its environmental impact. \n    Level of risk exposed if this best practice is not established:\n    Medium \n\n    Implementation guidance\n     Moving data around your organization requires compute, networking, and storage resources.\n      Use techniques to minimize data movement and improve the overall efficiency of your workload.\n    \n   \n\n    Implementation steps\n    \n       \n       \n       \n       \n       \n    \n        \n          Use proximity: Consider proximity to the data or users as a decision factor when selecting a Region for your workload. \n      \n         \n          Partition services: Partition Regionally-consumed services so that their Region-specific data is stored\n          within the Region where it is consumed. \n      \n         \n          Use efficient file formats: Use efficient file formats (such as Parquet or ORC) and compress data before you move\n          it over the network. \n      \n         \n          Minimize data movement: Don't move unused data. Some examples that can help you avoid moving unused data: \n        \n           \n           \n           \n           \n        \n             Reduce API responses to only relevant data. \n          \n             Aggregate data where detailed (record-level information is not required). \n          \n             See Well-Architected Lab - Optimize Data Pattern Using Amazon Redshift Data Sharing.\n            \n          \n             Consider Cross-account data\n                sharing in AWS Lake Formation. \n          \n      \n        \n          Use edge services: Use services that can help you run code closer to users of your workload. \n        \n              \n                Service\n                When to use\n              \n            \n              \n                \n                  Lambda@Edge\n                \n                \n                  Use for compute-heavy operations that are run when objects are not in the\n                    cache.\n                \n              \n              \n                \n                  CloudFront\n                      Functions\n                \n                \n                  Use for simple use cases such as HTTP(s) request/response manipulations that\n                    can be initiated by short-lived functions.\n                \n              \n              \n                \n                  AWS IoT Greengrass\n                \n                \n                  Run local compute, messaging, and data caching for connected devices.\n                \n              \n            \n      \n   \n\n    Resources\n\n\n    \n      Related documents:\n    \n    \n       \n       \n       \n       \n       \n       \n       \n    \n        \n          Optimizing your AWS Infrastructure for Sustainability, Part III: Networking\n        \n      \n        \n          AWS Global\n            Infrastructure\n        \n      \n        \n          Amazon CloudFront Key Features including the\n            CloudFront Global Edge Network\n        \n      \n        \n          Compressing HTTP requests in Amazon OpenSearch Service\n        \n      \n        \n          Intermediate data compression with Amazon EMR\n        \n      \n        \n          Loading\n            compressed data files from Amazon S3 into Amazon Redshift\n        \n      \n        \n          Serving compressed\n            files with Amazon CloudFront\n        \n      \n    \n      Related videos:\n    \n    \n       \n    \n         Demystifying data transfer\n            on AWS\n      \n  \u003cdiv\u003e\u003cdiv\u003e\u003cdiv\u003e\u003cdiv id=\"js_error_message\"\u003e\u003cp\u003e\u003cimg src=\"https://d1ge0kk1l5kms0.cloudfront.net/images/G/01/webservices/console/warning.png\" alt=\"Warning\" /\u003e \u003cstrong\u003eJavascript is disabled or is unavailable in your browser.\u003c/strong\u003e\u003c/p\u003e\u003cp\u003eTo use the Amazon Web Services Documentation, Javascript must be enabled. Please refer to your browser's Help pages for instructions.\u003c/p\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003eDocument ConventionsSUS04-BP06 Use shared file systems or storage to access common dataSUS04-BP08 Back up data only when difficult to recreate",
  "SUS04-BP08 Back up data only when difficult to\n      recreateAvoid backing up data that has no business value to minimize storage resources requirements\n    for your workload. \n    Common anti-patterns:\n  \n     \n     \n  \n       You do not have a backup strategy for your data. \n    \n       You back up data that can be easily recreated. \n    \n    Benefits of establishing this best practice: Avoiding back-up\n    of non-critical data reduces the required storage resources for the workload and lowers its\n    environmental impact. \n    Level of risk exposed if this best practice is not established:\n    Medium \n\n    Implementation guidance\n     Avoiding the back up of unnecessary data can help lower cost and reduce the storage\n      resources used by the workload. Only back up data that has business value or is needed to\n      satisfy compliance requirements. Examine backup policies and exclude ephemeral storage that\n      doesn’t provide value in a recovery scenario. \n     \n      \n      Implementation steps\n    \n    \n       \n       \n       \n    \n        \n          Classify data: Implement data classification policy as outlined in SUS04-BP01 Implement a data classification policy. \n      \n         \n          Design a backup strategy: Use the criticality of your data classification and design backup strategy based on\n          your recovery time objective (RTO) and recovery point objective (RPO). Avoid backing\n          up non-critical data. \n        \n           \n           \n           \n        \n             Exclude data that can be easily recreated. \n          \n             Exclude ephemeral data from your backups. \n          \n             Exclude local copies of data, unless the time required to restore that data from\n              a common location exceeds your service-level agreements (SLAs). \n          \n      \n         \n          Use automated backup: Use an automated solution or managed service to back up business-critical data. \n        \n           \n           \n        \n            \n              AWS Backup is a fully-managed service that makes it easy to centralize and\n              automate data protection across AWS services, in the cloud, and on premises. For\n              hands-on guidance on how to create automated backups using AWS Backup, see Well-Architected Labs - Testing Backup and Restore of Data. \n          \n            \n              Automate backups and optimize backup costs for Amazon EFS using AWS Backup.\n            \n          \n      \n     \n   \n\n    Resources\n    \n      Related best practices:\n    \n    \n       \n       \n       \n    \n        REL09-BP01 Identify and back up all data that needs to be backed up, or reproduce the\n            data from sources\n      \n        REL09-BP03 Perform data backup automatically\n      \n        REL13-BP02 Use defined recovery strategies to meet the recovery\n          objectives\n      \n\n\n    \n      Related documents:\n    \n    \n       \n       \n       \n       \n       \n       \n       \n       \n    \n        \n          Using AWS Backup to back up\n            and restore Amazon EFS file systems\n        \n      \n        \n          Amazon EBS\n            snapshots\n        \n      \n        \n          Working with\n            backups on Amazon Relational Database Service\n        \n      \n         APN\n            Partner: partners that can help with backup \n      \n        AWS Marketplace: products that can be used for backup \n      \n         Backing Up\n            Amazon EFS \n      \n         Backing\n            Up Amazon FSx for Windows File Server \n      \n         Backup\n            and Restore for Amazon ElastiCache (Redis OSS) \n      \n    \n      Related videos:\n    \n    \n       \n       \n       \n    \n        AWS re:Invent 2023 -\n            Backup and disaster recovery strategies for increased resilience\n      \n        AWS re:Invent 2023 - What's\n            new with AWS Backup\n      \n        AWS re:Invent 2021 -\n            Backup, disaster recovery, and ransomware protection with AWS\n      \n  \u003cdiv\u003e\u003cdiv\u003e\u003cdiv\u003e\u003cdiv id=\"js_error_message\"\u003e\u003cp\u003e\u003cimg src=\"https://d1ge0kk1l5kms0.cloudfront.net/images/G/01/webservices/console/warning.png\" alt=\"Warning\" /\u003e \u003cstrong\u003eJavascript is disabled or is unavailable in your browser.\u003c/strong\u003e\u003c/p\u003e\u003cp\u003eTo use the Amazon Web Services Documentation, Javascript must be enabled. Please refer to your browser's Help pages for instructions.\u003c/p\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003eDocument ConventionsSUS04-BP07 Minimize data movement across networksHardware and services",
  "SUS05-BP01 Use the minimum amount of hardware to meet your\n      needsUse the minimum amount of hardware for your workload to efficiently meet your business\n    needs.\n    Common anti-patterns:\n  \n     \n     \n     \n     \n  \n       You do not monitor resource utilization. \n    \n       You have resources with a low utilization level in your architecture. \n    \n       You do not review the utilization of static hardware to determine if it should be\n        resized. \n    \n       You do not set hardware utilization goals for your compute infrastructure based on\n        business KPIs. \n    \n    Benefits of establishing this best practice: Rightsizing your\n    cloud resources helps to reduce a workload’s environmental impact, save money, and maintain\n    performance benchmarks. \n    Level of risk exposed if this best practice is not established:\n    Medium \n\n    Implementation guidance\n     Optimally select the total number of hardware required for your workload to improve its\n      overall efficiency. The AWS Cloud provides the flexibility to expand or reduce the number of\n      resources dynamically through a variety of mechanisms, such as AWS Auto Scaling, and meet changes in demand. It also provides APIs and SDKs that allow resources to be\n      modified with minimal effort. Use these capabilities to make frequent changes to your workload\n      implementations. Additionally, use rightsizing guidelines from AWS tools to efficiently\n      operate your cloud resource and meet your business needs. \n    \n      Implementation steps\n    \n    \n       \n       \n       \n       \n       \n       \n    \n        \n          Choose the instances type: Choose the right instances\n          type to best fit your needs. To learn about how to choose Amazon Elastic Compute Cloud instances and use\n          mechanisms such as attribute-based instance selection, see the following: \n        \n           \n           \n           \n        \n             How do\n                I choose the appropriate Amazon EC2 instance type for my workload? \n          \n             Attribute-based instance type selection for Amazon EC2 Fleet. \n          \n             Create\n                an Auto Scaling group using attribute-based instance type selection. \n          \n      \n        Scale: Use small increments to scale variable\n          workloads.\n      \n        Use multiple compute purchase options: Balance\n          instance flexibility, scalability, and cost savings with multiple compute purchase\n          options.\n        \n           \n           \n           \n        \n            \n              Amazon EC2 On-Demand Instances are best suited for new, stateful, and spiky\n              workloads which can’t be instance type, location, or time flexible. \n          \n            \n              Amazon EC2 Spot Instances are a great way to supplement the other options for\n              applications that are fault tolerant and flexible. \n          \n             Leverage Compute\n                Savings Plans for steady state workloads that allow flexibility if your\n              needs (like AZ, Region, instance families, or instance types) change. \n          \n      \n        Use instance and Availability Zone diversity: Maximize\n          application availability and take advantage of excess capacity by diversifying your\n          instances and Availability Zones. \n      \n        Rightsize instances: Use the rightsizing\n          recommendations from AWS tools to make adjustments on your workload. For more\n          information, see Optimizing your cost with Rightsizing Recommendations and Right\n            Sizing: Provisioning Instances to Match Workloads\n        \n           \n        \n            Use rightsizing recommendations in AWS Cost Explorer or AWS Compute Optimizer to identify rightsizing\n              opportunities.\n          \n      \n        Negotiate service-level agreements (SLAs): Negotiate\n          SLAs that permit temporarily reducing capacity while automation deploys replacement\n          resources.\n      \n   \n\n    Resources\n\n    \n      Related documents:\n    \n    \n       \n       \n       \n       \n       \n    \n         Optimizing your AWS Infrastructure for Sustainability, Part I: Compute\n          \n      \n         Attirbute based Instance Type Selection for Auto Scaling for Amazon EC2 Fleet \n      \n        AWS Compute Optimizer Documentation\n          \n      \n        \n          Operating Lambda:\n            Performance optimization\n        \n      \n        \n          Auto Scaling\n            Documentation\n        \n      \n    \n      Related videos:\n    \n    \n       \n       \n       \n       \n    \n        AWS re:Invent 2023 - What's\n            new with Amazon EC2\n      \n        AWS re:Invent 2023 - Smart\n            savings: Amazon Elastic Compute Cloud cost-optimization strategies\n      \n        AWS re:Invent 2022 -\n            Optimizing Amazon Elastic Kubernetes Service for performance and cost on AWS\n      \n        AWS re:Invent 2023 -\n            Sustainable compute: reducing costs and carbon emissions with AWS\n      \n  \u003cdiv\u003e\u003cdiv\u003e\u003cdiv\u003e\u003cdiv id=\"js_error_message\"\u003e\u003cp\u003e\u003cimg src=\"https://d1ge0kk1l5kms0.cloudfront.net/images/G/01/webservices/console/warning.png\" alt=\"Warning\" /\u003e \u003cstrong\u003eJavascript is disabled or is unavailable in your browser.\u003c/strong\u003e\u003c/p\u003e\u003cp\u003eTo use the Amazon Web Services Documentation, Javascript must be enabled. Please refer to your browser's Help pages for instructions.\u003c/p\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003eDocument ConventionsSUS 5 How do you select and use cloud hardware and services in your architecture to support your sustainability goals?SUS05-BP02 Use instance types with the least impact",
  "SUS05-BP02 Use instance types with the least\n      impactContinually monitor and use new instance types to take advantage of energy efficiency\n    improvements.\n    Common anti-patterns:\n  \n     \n     \n     \n     \n     \n     \n  \n       You are only using one family of instances. \n    \n       You are only using x86 instances. \n    \n       You specify one instance type in your Amazon EC2 Auto Scaling configuration. \n    \n       You use AWS instances in a manner that they were not designed for (for example, you\n        use compute-optimized instances for a memory-intensive workload). \n    \n       You do not evaluate new instance types regularly. \n    \n       You do not check recommendations from AWS rightsizing tools such as AWS Compute Optimizer.\n      \n    \n    Benefits of establishing this best practice: By using\n    energy-efficient and right-sized instances, you are able to greatly reduce the environmental\n    impact and cost of your workload. \n    Level of risk exposed if this best practice is not established:\n    Medium \n\n    Implementation guidance\n     Using efficient instances in cloud workload is crucial for lower resource usage and\n      cost-effectiveness. Continually monitor the release of new instance types and take advantage\n      of energy efficiency improvements, including those instance types designed to support specific\n      workloads such as machine learning training and inference, and video transcoding. \n   \n\n    Implementation steps\n    \n       \n       \n       \n    \n        \n          Learn and explore instance types: Find instance types\n          that can lower your workload's environmental impact. \n        \n           \n           \n           \n        \n             Subscribe to What's New with AWS to\n              stay up-to-date with the latest AWS technologies and instances. \n          \n             Learn about different AWS instance types. \n          \n             Learn about AWS Graviton-based instances which offer the best performance per\n              watt of energy use in Amazon EC2 by watching re:Invent 2020 - Deep dive on\n                AWS Graviton2 processor-powered Amazon EC2 instances and Deep dive\n                into AWS Graviton3 and Amazon EC2 C7g instances. \n          \n      \n        \n          Use instance types with the least impact: Plan and\n          transition your workload to instance types with the least impact. \n        \n           \n           \n           \n           \n           \n           \n           \n           \n           \n        \n             Define a process to evaluate new features or instances for your workload. Take\n              advantage of agility in the cloud to quickly test how new instance types can improve\n              your workload environmental sustainability. Use proxy metrics to measure how many\n              resources it takes you to complete a unit of work. \n          \n             If possible, modify your workload to work with different numbers of vCPUs and\n              different amounts of memory to maximize your choice of instance type. \n          \n             Consider transitioning your workload to Graviton-based instances to improve the\n              performance efficiency of your workload. For more information on moving workloads to\n              AWS Graviton, see AWS\n                Graviton Fast Start and Considerations when transitioning workloads to AWS Graviton-based Amazon Elastic Compute Cloud\n                instances. \n          \n             Consider selecting the AWS Graviton option in your usage of AWS managed services.\n            \n          \n             Migrate your workload to Regions that offer instances with the least\n              sustainability impact and still meet your business requirements. \n          \n             For machine learning workloads, take advantage of purpose-built hardware that is\n              specific to your workload such as AWS Trainium, AWS Inferentia, and Amazon EC2 DL1. AWS Inferentia\n              instances such as Inf2 instances offer up to 50% better performance per watt over\n              comparable Amazon EC2 instances. \n          \n             Use Amazon SageMaker AI Inference\n                Recommender to right size ML inference endpoint. \n          \n             For spiky workloads (workloads with infrequent requirements for additional\n              capacity), use burstable\n                performance instances.\n            \n          \n             For stateless and fault-tolerant workloads, use Amazon EC2 Spot Instances\n              to increase overall utilization of the cloud, and reduce the sustainability impact of\n              unused resources. \n          \n      \n        Operate and optimize: Operate and optimize your\n          workload instance.\n        \n           \n           \n        \n             For ephemeral workloads, evaluate instance Amazon CloudWatch metrics such as CPUUtilization to identify\n              if the instance is idle or under-utilized. \n          \n             For stable workloads, check AWS rightsizing tools such as AWS Compute Optimizer at regular intervals to\n              identify opportunities to optimize and right-size the instances. For further examples and recommendations, see the following labs:\n            \n               \n               \n               \n            \n                 Well-Architected Lab - Rightsizing Recommendations \n              \n                 Well-Architected Lab - Rightsizing with Compute Optimizer \n              \n                 Well-Architected Lab - Optimize Hardware Patterns and Observice Sustainability\n                    KPIs \n              \n          \n      \n   \n\n    Resources\n\n\n    \n      Related documents:\n    \n    \n       \n       \n       \n       \n       \n       \n       \n       \n       \n    \n        \n          Optimizing your AWS Infrastructure for Sustainability, Part I: Compute\n        \n      \n        \n          AWS Graviton\n        \n      \n        \n          Amazon EC2 DL1\n        \n      \n        \n          Amazon EC2 Capacity\n            Reservation Fleets\n        \n      \n        \n          Amazon EC2 Spot\n            Fleet\n        \n      \n        \n          Functions: Lambda\n            Function Configuration\n        \n      \n        \n            Attribute-based instance type selection for Amazon EC2 Fleet \n      \n        Building Sustainable, Efficient, and Cost-Optimized Applications on\n          AWS\n      \n         How the Contino Sustainability Dashboard Helps Customers Optimize Their Carbon\n            Footprint \n      \n    \n      Related videos:\n    \n    \n       \n       \n       \n       \n       \n       \n    \n        \n          AWS re:Invent 2023 - AWS\n            Graviton: The best price performance for your AWS workloads\n        \n      \n        \n          AWS re:Invent 2023 - New\n            Amazon Elastic Compute Cloud generative AI capabilities in AWS Management Console\n        \n      \n        \n          AWS re:Invent 2023 = What's new\n            with Amazon Elastic Compute Cloud\n        \n      \n        \n          AWS re:Invent 2023 - Smart\n            savings: Amazon Elastic Compute Cloud cost-optimization strategies\n        \n      \n        \n          AWS\n            re:Invent 2021 - Deep dive into AWS Graviton3 and Amazon EC2 C7g instances\n        \n      \n        AWS re:Invent 2022 - Build\n            a cost-, energy-, and resource-efficient compute environment \n      \n    \n      Related examples:\n    \n    \n       \n    \n        \n            Solution: Guidance for Optimizing Deep Learning Workloads for Sustainability on AWS\n      \n  \u003cdiv\u003e\u003cdiv\u003e\u003cdiv\u003e\u003cdiv id=\"js_error_message\"\u003e\u003cp\u003e\u003cimg src=\"https://d1ge0kk1l5kms0.cloudfront.net/images/G/01/webservices/console/warning.png\" alt=\"Warning\" /\u003e \u003cstrong\u003eJavascript is disabled or is unavailable in your browser.\u003c/strong\u003e\u003c/p\u003e\u003cp\u003eTo use the Amazon Web Services Documentation, Javascript must be enabled. Please refer to your browser's Help pages for instructions.\u003c/p\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003eDocument ConventionsSUS05-BP01 Use the minimum amount of hardware to meet your needsSUS05-BP03 Use managed services",
  "SUS05-BP03 Use managed servicesUse managed services to operate more efficiently in the cloud.\n    Common anti-patterns:\n  \n     \n     \n     \n  \n       You use Amazon EC2 instances with low utilization to run your applications. \n    \n       Your in-house team only manages the workload, without time to focus on innovation or\n        simplifications. \n    \n       You deploy and maintain technologies for tasks that can run more efficiently on managed\n        services. \n    \n    Benefits of establishing this best practice:\n  \n     \n     \n  \n       Using managed services shifts the responsibility to AWS, which has insights across\n        millions of customers that can help drive new innovations and efficiencies. \n    \n       Managed service distributes the environmental impact of the service across many users\n        because of the multi-tenet control planes. \n    \n    Level of risk exposed if this best practice is not established:\n    Medium \n\n    Implementation guidance\n    Managed services shift responsibility to AWS for maintaining high utilization and\n      sustainability optimization of the deployed hardware. Managed services also remove the\n      operational and administrative burden of maintaining a service, which allows your team to have\n      more time and focus on innovation. \n     Review your workload to identify the components that can be replaced by AWS managed\n      services. For example, Amazon RDS, Amazon Redshift, and Amazon ElastiCache provide a managed database service. Amazon Athena, Amazon EMR, and Amazon OpenSearch Service\n      provide a managed analytics service. \n    \n      Implementation steps\n    \n    \n       \n       \n       \n       \n       \n       \n    \n        Inventory your workload: Inventory your workload for\n          services and components. \n      \n        Identify candidates: Assess and identify components\n          that can be replaced by managed services. Here are some examples of when you might\n          consider using a managed service: \n        \n              \n                Task\n                What to use on AWS\n              \n            \n              \n                \n                  Hosting a database\n                \n                \n                   Use managed Amazon Relational Database Service (Amazon RDS)\n                    instances instead of maintaining your own Amazon RDS instances on Amazon Elastic Compute Cloud (Amazon EC2). \n                \n              \n              \n                \n                  Hosting a container workload\n                \n                \n                  Use AWS Fargate, instead of\n                    implementing your own container infrastructure.\n                \n              \n              \n                \n                  Hosting web apps\n                \n                \n                  Use AWS Amplify\n                      Hosting as fully managed CI/CD and hosting service for static websites\n                    and server-side rendered web apps.\n                \n              \n            \n      \n        Create a migration plan: Identify dependencies and\n          create a migrations plan. Update runbooks and playbooks accordingly. \n        \n           \n        \n            The AWS Application Discovery Service\n              automatically collects and presents detailed information about application\n              dependencies and utilization to help you make more informed decisions as you plan your\n              migration \n          \n      \n        Perform tests Test the service before migrating to\n          the managed service. \n      \n        Replace self-hosted services: Use your migration plan\n          to replace self-hosted services with managed service. \n      \n        Monitor and adjust: Continually monitor the service\n          after the migration is complete to make adjustments as required and optimize the service.\n        \n      \n   \n\n    Resources\n\n    \n      Related documents:\n    \n    \n       \n       \n       \n       \n       \n    \n        AWS Cloud Products \n      \n        AWS Total Cost of Ownership (TCO) Calculator\n          \n      \n        \n          Amazon DocumentDB\n        \n      \n        \n          Amazon Elastic Kubernetes Service (EKS)\n        \n      \n        \n          Amazon Managed Streaming for Apache Kafka (Amazon MSK)\n        \n      \n    \n      Related videos:\n    \n    \n       \n       \n    \n        AWS re:Invent 2021 - Cloud\n            operations at scale with AWS Managed Services\n      \n        AWS re:Invent 2023 - Best\n            practices for operating on AWS\n      \n  \u003cdiv\u003e\u003cdiv\u003e\u003cdiv\u003e\u003cdiv id=\"js_error_message\"\u003e\u003cp\u003e\u003cimg src=\"https://d1ge0kk1l5kms0.cloudfront.net/images/G/01/webservices/console/warning.png\" alt=\"Warning\" /\u003e \u003cstrong\u003eJavascript is disabled or is unavailable in your browser.\u003c/strong\u003e\u003c/p\u003e\u003cp\u003eTo use the Amazon Web Services Documentation, Javascript must be enabled. Please refer to your browser's Help pages for instructions.\u003c/p\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003eDocument ConventionsSUS05-BP02 Use instance types with the least impactSUS05-BP04 Optimize your use of hardware-based compute accelerators",
  "SUS05-BP04 Optimize your use of hardware-based compute\n      acceleratorsOptimize your use of accelerated computing instances to reduce the physical infrastructure\n    demands of your workload.\n    Common anti-patterns:\n  \n     \n     \n     \n  \n       You are not monitoring GPU usage. \n    \n       You are using a general-purpose instance for workload while a purpose-built instance\n        can deliver higher performance, lower cost, and better performance per watt. \n    \n       You are using hardware-based compute accelerators for tasks where they’re more\n        efficient using CPU-based alternatives. \n    \n    Benefits of establishing this best practice: By optimizing the\n    use of hardware-based accelerators, you can reduce the physical-infrastructure demands of your\n    workload. \n    Level of risk exposed if this best practice is not established:\n    Medium \n\n    Implementation guidance\n     If you require high processing capability, you can benefit from using accelerated\n      computing instances, which provide access to hardware-based compute accelerators such as\n      graphics processing units (GPUs) and field programmable gate arrays (FPGAs). These hardware\n      accelerators perform certain functions like graphic processing or data pattern matching more\n      efficiently than CPU-based alternatives. Many accelerated workloads, such as rendering,\n      transcoding, and machine learning, are highly variable in terms of resource usage. Only run\n      this hardware for the time needed, and decommission them with automation when not required to\n      minimize resources consumed. \n   \n\n    Implementation steps\n    \n       \n       \n       \n       \n       \n       \n    \n         \n          Explore compute accelerators: Identify which accelerated computing\n            instances can address your requirements. \n      \n         \n          Use purpose-built hardware: For machine learning workloads, take advantage of purpose-built hardware that is\n          specific to your workload, such as AWS Trainium, AWS Inferentia, and Amazon EC2 DL1. AWS Inferentia instances such as Inf2\n          instances offer up to 50%\n            better performance per watt over comparable Amazon EC2 instances. \n      \n         \n          Monitor usage metrics: Collect usage metric for your accelerated computing instances. For example, you can\n          use CloudWatch agent to collect metrics such as utilization_gpu and\n            utilization_memory for your GPUs as shown in Collect NVIDIA\n            GPU metrics with Amazon CloudWatch. \n      \n         \n          Rightsize: Optimize the code, network operation, and settings of hardware accelerators to make\n          sure that underlying hardware is fully utilized. \n        \n           \n           \n           \n        \n            \n              Optimize\n                GPU settings\n            \n          \n            \n              GPU\n                Monitoring and Optimization in the Deep Learning AMI\n            \n          \n            \n              Optimizing I/O for GPU performance tuning of deep learning training in\n                Amazon SageMaker AI\n            \n          \n      \n         \n          Keep up to date: Use the latest high performant libraries and GPU drivers. \n      \n         \n          Release unneeded instances: Use automation to release GPU instances when not in use. \n      \n   \n\n    Resources\n\n\n    \n      Related documents:\n    \n    \n       \n       \n       \n       \n       \n    \n        \n          Accelerated\n            Computing\n        \n      \n         Let's Architect!\n            Architecting with custom chips and accelerators \n      \n         How do I\n            choose the appropriate Amazon EC2 instance type for my workload? \n      \n        \n          Amazon EC2 VT1 Instances\n        \n      \n         Choose the best AI accelerator and model compilation for computer vision inference\n            with Amazon SageMaker AI \n      \n    \n      Related videos:\n    \n    \n       \n       \n       \n       \n       \n       \n\n    \n        AWS re:Invent 2021 - How\n            to select Amazon EC2 GPU instances for deep learning \n      \n        \n          AWS Online Tech Talks -\n            Deploying Cost-Effective Deep Learning Inference\n        \n      \n        AWS re:Invent 2023 -\n            Cutting-edge AI with AWS and NVIDIA\n      \n        AWS re:Invent 2022 - [NEW\n            LAUNCH!] Introducing AWS Inferentia2-based Amazon EC2 Inf2 instances\n      \n        AWS re:Invent 2022 -\n            Accelerate deep learning and innovate faster with AWS Trainium\n      \n        AWS re:Invent 2022 - Deep\n            learning on AWS with NVIDIA: From training to deployment\n      \n  \u003cdiv\u003e\u003cdiv\u003e\u003cdiv\u003e\u003cdiv id=\"js_error_message\"\u003e\u003cp\u003e\u003cimg src=\"https://d1ge0kk1l5kms0.cloudfront.net/images/G/01/webservices/console/warning.png\" alt=\"Warning\" /\u003e \u003cstrong\u003eJavascript is disabled or is unavailable in your browser.\u003c/strong\u003e\u003c/p\u003e\u003cp\u003eTo use the Amazon Web Services Documentation, Javascript must be enabled. Please refer to your browser's Help pages for instructions.\u003c/p\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003eDocument ConventionsSUS05-BP03 Use managed servicesProcess and culture",
  "SUS06-BP01 Communicate and cascade your sustainability\n  goals\n    Technology is a key enabler of sustainability. IT teams play a\n    crucial role in driving meaningful change towards your\n    organization's sustainability goals. These teams should clearly\n    understand the company's sustainability targets and work to\n    communicate and cascade those priorities across its operations.\n  \n    Common anti-patterns:\n  \n     \n     \n     \n     \n  \n      \n        You don't know your organization's sustainability goals and how\n        they apply to your team.\n      \n    \n      \n        You have insufficient awareness and training about the\n        environmental impact of cloud workloads.\n      \n    \n      \n        You are unsure about the specific areas to prioritize.\n      \n    \n      \n        You do not involve your employees and customers in your\n        sustainability initiatives.\n      \n    \n    Benefits of establishing this best\n    practice: From optimization of infrastructure and systems\n    to use of innovative technologies, IT teams can reduce the\n    organization's carbon emissions and minimize resource consumption.\n    Communication of sustainability goals can provide the ability for IT\n    teams continuously improve and adapt to evolving sustainability\n    challenges. Additionally, these sustainable optimizations often\n    translate to cost savings as well, which strengthens the business\n    case.\n  \n    Level of risk exposed if this best practice\n    is not established: Medium\n  \n\n  Implementation guidance\n\n      \n    \n      The primary sustainability goals for IT teams should be to\n      optimize systems and solutions to increase resource efficiency and\n      minimize the organization's carbon footprint and overall\n      environmental impact. Shared services and initiatives like\n      training programs and operational dashboards can support\n      organizations as they optimize IT operations and build solutions\n      that can help significantly reduce the carbon footprint. The cloud\n      presents an opportunity not only to move physical infrastructure\n      and energy procurement responsibilities to the shared\n      responsibility of the cloud provider but also to continuously\n      optimize the resource efficiency of cloud-based services.\n    \n    \n      When teams use the cloud's inherent efficiency and shared\n      responsibility model, they can drive meaningful reductions in the\n      organization's environmental impact. This, in turn, can contribute\n      to the organization's overall sustainability goals and demonstrate\n      the value of these teams as strategic partners in the journey\n      towards a more sustainable future.\n    \n     \n\n  Implementation steps\n\n      \n      \n         \n         \n         \n         \n         \n         \n         \n      \n          \n            Define goals and\n            objectives: Establish well-defined goals for your IT program. This involves getting input from\n            responsible stakeholders from different departments such as\n            IT, sustainability, and finance. These teams should define\n            measurable goals that align with your organization's\n            sustainability goals, including areas such carbon reduction\n            and resource optimization.\n          \n        \n          \n            Understand the carbon accounting\n            boundaries of your business: Understand how\n            carbon accounting methods like the Greenhouse Gas (GHG)\n            Protocol relate to your workloads in the cloud\n            (for more detail, see Cloud\n            sustainability).\n          \n        \n          \n            Use cloud solutions for carbon\n            accounting: Use cloud solutions such as\n            carbon\n            accounting solutions on AWS to track scope one, two,\n            and three for GHG emissions across your operations,\n            portfolios, and value chains. With these solutions,\n            organizations can streamline GHG emission data acquisition,\n            simplify reporting, and derive insights to inform their\n            climate strategies.\n          \n        \n          \n            Monitor the carbon footprint of your\n            IT portfolio: Track and report carbon emissions\n            of your IT systems. Use the\n            AWS             Customer Carbon Footprint Tool to track, measure,\n            review, and forecast the carbon emissions generated from\n            your AWS usage.\n          \n        \n          \n            Communicate resource usage through\n            proxy metrics to your teams: Track and report on\n            your\n            resource\n            usage through proxy metrics. In the on-demand pricing\n            models of the cloud, resource usage is related to cost,\n            which is a generally-understandable metric. At a minimum,\n            use cost as a proxy metric to communicate the resource usage\n            and improvements by each team.\n          \n          \n             \n             \n             \n          \n              \n                Enable hourly granularity in\n                your Cost Explorer and create a\n                Cost\n                and Usage Report (CUR): The CUR\n                provides daily or hourly usage granularity, rates,\n                costs, and usage attributes for all AWS services. Use\n                the\n                Cloud Intelligence Dashboards and its\n                Sustainability Proxy Metrics Dashboard as a starting\n                point for the processing and visualization of cost and\n                usage based data. For more detail, see the following:\n              \n            \n              \n                Measure\n                and track cloud efficiency with sustainability proxy\n                metrics, Part I: What are proxy metrics?\n              \n            \n              \n                Measure\n                and track cloud efficiency with sustainability proxy\n                metrics, Part II: Establish a metrics pipeline\n              \n            \n        \n          \n            Continuously optimize and\n            evaluate: Use an\n            improvement\n            process to continuously optimize your IT systems,\n            including cloud workload for efficiency and sustainability.\n            Monitor carbon footprint before and after implementation of\n            optimization strategy. Use the reduction in carbon footprint\n            to assess the effectiveness.\n          \n        \n          \n            Foster a sustainability\n            culture: Use training programs (like\n            AWS             Skill Builder) to educate your employees about\n            sustainability. Engage them in sustainability initiatives.\n            Share and celebrate their success stories. Use incentives to\n            award them if they achieve sustainability targets.\n          \n        \n     \n   \n\n  Resources\n\n      \n    \n      Related documents:\n    \n    \n       \n    \n        \n          Understanding\n          your carbon emission estimations\n        \n      \n    \n      Related videos:\n    \n    \n       \n       \n       \n       \n       \n       \n    \n        \n          AWS           re:Invent 2023 - Accelerate data-driven circular economy\n          initiatives with AWS\n        \n      \n        \n          AWS           re:Invent 2023 - Sustainability innovation in AWS Global\n          Infrastructure \n        \n      \n        \n          AWS           re:Invent 2023 - Sustainable architecture: Past, present, and\n          future \n        \n      \n        \n          AWS           re:Invent 2022 - Delivering sustainable, high-performing\n          architectures \n        \n      \n        \n          AWS           re:Invent 2022 - Architecting sustainably and reducing your\n          AWS carbon footprint\n        \n      \n        \n          AWS           re:Invent 2022 - Sustainability in AWS global\n          infrastructure \n        \n      \n    \n      Related examples: \n    \n    \n       \n    \n        \n          Well-Architected\n            Lab - Turning cost \u0026 usage reports into efficiency\n            reports      \n        \n      \n    \n      Related trainings:\n    \n    \n       \n       \n       \n    \n        \n          Sustainability\n          Transformation on AWS\n        \n      \n        \n          SimuLearn\n          - Sustainability Reporting\n        \n      \n        \n          Decarbonization\n          with AWS\n        \n      \n  \u003cdiv\u003e\u003cdiv\u003e\u003cdiv\u003e\u003cdiv id=\"js_error_message\"\u003e\u003cp\u003e\u003cimg src=\"https://d1ge0kk1l5kms0.cloudfront.net/images/G/01/webservices/console/warning.png\" alt=\"Warning\" /\u003e \u003cstrong\u003eJavascript is disabled or is unavailable in your browser.\u003c/strong\u003e\u003c/p\u003e\u003cp\u003eTo use the Amazon Web Services Documentation, Javascript must be enabled. Please refer to your browser's Help pages for instructions.\u003c/p\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003eDocument ConventionsSUS 6 How do your organizational processes support your sustainability goals?SUS06-BP02 Adopt methods that can rapidly introduce sustainability\n      improvements",
  "SUS06-BP02 Adopt methods that can rapidly introduce\n      sustainability improvements\n    Adopt methods and processes to validate potential improvements,\nminimize testing costs, and deliver small improvements.\n  \n    Common anti-patterns:\n  \n     \n     \n     \n  \n      \n        Reviewing your application for sustainability is a task done\n        only once at the beginning of a project.\n      \n    \n      \n        Your workload has become stale, as the release process is too\n        cumbersome to introduce minor changes for resource efficiency.\n      \n    \n      \n        You do not have mechanisms to improve your workload for\n        sustainability.\n      \n    \n    Benefits of establishing this best\n      practice: By establishing a process to introduce and\n    track sustainability improvements, you will be able to continually\n    adopt new features and capabilities, remove issues, and improve\n    workload efficiency.\n  \n    Level of risk exposed if this best practice\n      is not established: Medium\n  \n    \n    Implementation guidance\n    \n    \n    \n      Test and validate potential sustainability improvements before\n      deploying them to production. Account for the cost of testing when\n      calculating potential future benefit of an improvement. Develop\n      low cost testing methods to deliver small improvements.\n    \n     \n      \n      Implementation steps\n    \n    \n       \n       \n       \n       \n       \n       \n       \n    \n        \n          Understand and communicate your\n            organizational sustainability goals: Understand\n          your organizational sustainability goals, such carbon\n          reduction or water stewardship. Translate these goals into\n          sustainability requirements for your cloud workloads.\n          Communicate these requirements to key stakeholders.\n        \n      \n        \n          Add sustainability requirements to\n            your backlog: Add requirements for sustainability\n          improvement to your development backlog.\n        \n      \n        \n          Iterate and improve: Use an\n          iterative\n            improvement process to identify, evaluate, prioritize,\n          test, and deploy these improvements.\n        \n      \n        \n          Test using minimum viable product\n            (MVP): Develop and test potential improvements\n          using the minimum viable representative components to reduce\n          the cost and environmental impact of testing.\n        \n      \n        \n          Streamline the process:\n          Continually improve and streamline your development processes.\n          As an example, Automate your software delivery process using\n          continuous integration and delivery (CI/CD) pipelines to test\n          and deploy potential improvements to reduce the level of\n          effort and limit errors caused by manual processes.\n        \n      \n        \n          Training and awareness: Run\n          training programs for your team members to educate them about\n          sustainability and how their activities impact your\n          organizational sustainability goals.\n        \n      \n        \n          Assess and adjust:\n          Continually assess the impact of improvements and make\n          adjustments as needed.\n        \n      \n     \n   \n    \n    Resources\n    \n    \n    \n      Related documents:\n    \n    \n       \n    \n        \n          AWS           enables sustainability solutions\n        \n      \n    \n      Related videos:\n    \n    \n       \n       \n       \n       \n       \n    \n        \n          AWS           re:Invent 2023 - Sustainable architecture: Past, present, and\n            future\n        \n      \n        \n          AWS           re:Invent 2022 - Delivering sustainable, high-performing\n            architectures\n        \n      \n        \n          AWS           re:Invent 2022 - Architecting sustainably and reducing your\n            AWS carbon footprint\n        \n      \n        \n          AWS           re:Invent 2022 - Sustainability in AWS global\n            infrastructure\n        \n      \n        \n          AWS           re:Invent 2023 - What's new with AWS observability and\n            operations\n        \n      \n  \u003cdiv\u003e\u003cdiv\u003e\u003cdiv\u003e\u003cdiv id=\"js_error_message\"\u003e\u003cp\u003e\u003cimg src=\"https://d1ge0kk1l5kms0.cloudfront.net/images/G/01/webservices/console/warning.png\" alt=\"Warning\" /\u003e \u003cstrong\u003eJavascript is disabled or is unavailable in your browser.\u003c/strong\u003e\u003c/p\u003e\u003cp\u003eTo use the Amazon Web Services Documentation, Javascript must be enabled. Please refer to your browser's Help pages for instructions.\u003c/p\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003eDocument ConventionsSUS06-BP01 Communicate and cascade your sustainability\n  goalsSUS06-BP03 Keep your workload up-to-date",
  "SUS06-BP03 Keep your workload up-to-date\nKeep your workload up-to-date to adopt efficient features, remove\nissues, and improve the overall efficiency of your workload.\n\n    Common anti-patterns:\n  \n     \n     \n  \n      \n        You assume your current architecture is static and will not be\n        updated over time.\n      \n    \n      \n        You do not have any systems or a regular cadence to evaluate if\n        updated software and packages are compatible with your workload.\n      \n    \n    Benefits of establishing this best\n      practice: By establishing a process to keep your workload\n    up to date, you can adopt new features and capabilities, resolve\n    issues, and improve workload efficiency.\n  \n    Level of risk exposed if this best practice\n      is not established: Low\n  \n    \n    Implementation guidance\n    \n    \n    \n      Up to date operating systems, runtimes, middlewares, libraries,\n      and applications can improve workload efficiency and make it\n      easier to adopt more efficient technologies. Up to date software\n      might also include features to measure the sustainability impact\n      of your workload more accurately, as vendors deliver features to\n      meet their own sustainability goals. Adopt a regular cadence to\n      keep your workload up to date with the latest features and\n      releases.\n    \n     \n      \n      Implementation steps\n    \n    \n       \n       \n       \n    \n        \n          Define a process: Use a\n          process and schedule to evaluate new features or instances for\n          your workload. Take advantage of agility in the cloud to\n          quickly test how new features can improve your workload to:\n        \n        \n           \n           \n           \n           \n        \n            \n              Reduce sustainability impacts.\n            \n          \n            \n              Gain performance efficiencies.\n            \n          \n            \n              Remove barriers for a planned improvement.\n            \n          \n            \n              Improve your ability to measure and manage sustainability\n              impacts.\n            \n          \n      \n        \n          Conduct an inventory:\n          Inventory your workload software and architecture and identify\n          components that need to be updated.\n        \n        \n           \n        \n            \n              You can use\n              AWS Systems Manager Inventory to collect operating\n              system (OS), application, and instance metadata from your\n              Amazon EC2 instances and quickly understand which\n              instances are running the software and configurations\n              required by your software policy and which instances need\n              to be updated.\n            \n          \n      \n        \n          Learn the update procedure:\n          Understand how to update the components of your workload.\n        \n      \n    \n          \n            \n              Workload component\n            \n            \n              How to update\n            \n          \n        \n          \n            \n              Machine images\n            \n            \n              Use\n              EC2 Image Builder to manage updates to\n              Amazon\n                Machine Images (AMIs) for Linux or Windows server\n              images.\n            \n          \n          \n            \n              Container images\n            \n            \n              Use\n              Amazon Elastic Container Registry (Amazon ECR) with your\n              existing pipeline to\n              manage\n                Amazon Elastic Container Service (Amazon ECS)\n                images.\n            \n          \n          \n            \n              AWS Lambda\n            \n            \n              AWS Lambda includes\n              version\n                management features.\n            \n          \n        \n    \n       \n    \n        \n          Use automation: Automate\n          updates to reduce the level of effort to deploy new features\n          and limit errors caused by manual processes.\n        \n        \n           \n           \n        \n            \n              You can use\n              CI/CD\n              to automatically update AMIs, container images, and other\n              artifacts related to your cloud application.\n            \n          \n            \n              You can use tools such as\n              AWS Systems Manager Patch Manager to automate the\n              process of system updates, and schedule the activity using\n              AWS Systems Manager Maintenance Windows.\n            \n          \n      \n     \n   \n    \n    Resources\n    \n    \n    \n      Related documents:\n    \n    \n       \n       \n       \n    \n        \n          AWS           Architecture Center\n        \n      \n        \n          What's\n            New with AWS\n        \n      \n        \n          AWS           Developer Tools\n        \n      \n    \n      Related videos:\n    \n    \n       \n       \n    \n        \n          AWS           re:Invent 2022 - Optimize your AWS workloads with\n            best-practice guidance\n        \n      \n        \n          All\n            Things Patch: AWS Systems Manager\n        \n      \n  \u003cdiv\u003e\u003cdiv\u003e\u003cdiv\u003e\u003cdiv id=\"js_error_message\"\u003e\u003cp\u003e\u003cimg src=\"https://d1ge0kk1l5kms0.cloudfront.net/images/G/01/webservices/console/warning.png\" alt=\"Warning\" /\u003e \u003cstrong\u003eJavascript is disabled or is unavailable in your browser.\u003c/strong\u003e\u003c/p\u003e\u003cp\u003eTo use the Amazon Web Services Documentation, Javascript must be enabled. Please refer to your browser's Help pages for instructions.\u003c/p\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003eDocument ConventionsSUS06-BP02 Adopt methods that can rapidly introduce sustainability\n      improvementsSUS06-BP04 Increase utilization of build environments",
  "SUS06-BP04 Increase utilization of build environments\n    Increase the utilization of resources to develop, test, and build\n    your workloads.\n  \n    Common anti-patterns:\n  \n     \n     \n     \n  \n      \n        You manually provision or terminate your build environments.\n      \n    \n      \n        You keep your build environments running independent of test,\n        build, or release activities (for example, running an\n        environment outside of the working hours of your development\n        team members).\n      \n    \n      \n        You over-provision resources for your build environments.\n      \n    \n    Benefits of establishing this best\n      practice: By increasing the utilization of build\n    environments, you can improve the overall efficiency of your cloud\n    workload while allocating the resources to builders to develop,\n    test, and build efficiently.\n  \n    Level of risk exposed if this best practice\n      is not established: Low\n  \n    \n    Implementation guidance\n    \n    \n    \n      Use automation and infrastructure-as-code to bring build\n      environments up when needed and take them down when not used. A\n      common pattern is to schedule periods of availability that\n      coincide with the working hours of your development team members.\n      Your test environments should closely resemble the production\n      configuration. However, look for opportunities to use instance\n      types with burst capacity, Amazon EC2 Spot Instances, automatic\n      scaling database services, containers, and serverless technologies\n      to align development and test capacity with use. Limit data volume\n      to just meet the test requirements. If using production data in\n      test, explore possibilities of sharing data from production and\n      not moving data across.\n    \n    \n      Implementation steps\n    \n    \n       \n       \n       \n    \n        \n          Use infrastructure as code:\n          Use infrastructure as code to provision your build\n          environments.\n        \n      \n        \n          Use automation: Use\n          automation to manage the lifecycle of your development and\n          test environments and maximize the efficiency of your build\n          resources.\n        \n      \n        \n          Maximize utilization: Use\n          strategies to maximize the utilization of development and test\n          environments.\n        \n        \n           \n           \n           \n           \n           \n           \n        \n            \n              Use minimum viable representative environments to develop\n              and test potential improvements.\n            \n          \n            \n              Use serverless technologies if possible.\n            \n          \n            \n              Use On-Demand Instances to supplement your developer\n              devices.\n            \n          \n            \n              Use instance types with burst capacity, Spot Instances,\n              and other technologies to align build capacity with use.\n            \n          \n            \n              Adopt native cloud services for secure instance shell\n              access rather than deploying fleets of bastion hosts.\n            \n          \n            \n              Automatically scale your build resources depending on your\n              build jobs.\n            \n          \n      \n   \n    \n    Resources\n    \n    \n    \n      Related documents:\n    \n    \n       \n       \n       \n       \n       \n    \n        \n          AWS Systems Manager Session Manager\n        \n      \n        \n          Amazon EC2 Burstable performance instances\n        \n      \n        \n          What\n            is AWS CloudFormation?\n        \n      \n        \n          What\n            is AWS CodeBuild?\n        \n      \n        \n          Instance\n            Scheduler on AWS\n        \n      \n    \n      Related videos:\n    \n    \n       \n    \n        \n          AWS           re:Invent 2023 - Continuous integration and delivery for\n            AWS\n        \n      \n  \u003cdiv\u003e\u003cdiv\u003e\u003cdiv\u003e\u003cdiv id=\"js_error_message\"\u003e\u003cp\u003e\u003cimg src=\"https://d1ge0kk1l5kms0.cloudfront.net/images/G/01/webservices/console/warning.png\" alt=\"Warning\" /\u003e \u003cstrong\u003eJavascript is disabled or is unavailable in your browser.\u003c/strong\u003e\u003c/p\u003e\u003cp\u003eTo use the Amazon Web Services Documentation, Javascript must be enabled. Please refer to your browser's Help pages for instructions.\u003c/p\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003eDocument ConventionsSUS06-BP03 Keep your workload up-to-dateSUS06-BP05 Use managed device farms for testing",
  "SUS06-BP05 Use managed device farms for testing\n    Use managed device farms to efficiently test a new feature on a\n    representative set of hardware.\n  \n    Common anti-patterns:\n  \n     \n     \n  \n      \n        You manually test and deploy your application on individual\n        physical devices.\n      \n    \n      \n        You do not use app testing service to test and interact with\n        your apps (for example, Android, iOS, and web apps) on real,\n        physical devices.\n      \n    \n    Benefits of establishing this best\n      practice: Using managed device farms for testing\n    cloud-enabled applications provides a number of benefits:\n  \n     \n     \n     \n  \n      \n        They include more efficient features to test application on wide\n        range of devices.\n      \n    \n      \n        They eliminate the need for in-house infrastructure for testing.\n      \n    \n      \n        They offer diverse device types, including older and less\n        popular hardware, which eliminates the need for unnecessary\n        device upgrades.\n      \n    \n    Level of risk exposed if this best practice\n      is not established: Low\n  \n    \n    Implementation guidance\n    \n    \n    \n      Using Managed device farms can help you to streamline the testing\n      process for new features on a representative set of hardware.\n      Managed device farms offer diverse device types including older,\n      less popular hardware, and avoid customer sustainability impact\n      from unnecessary device upgrades.\n    \n     \n      \n      Implementation steps\n    \n    \n       \n       \n       \n       \n    \n        \n          Define testing\n            requirements: Define your testing requirements and\n          plan (like test type, operating systems, and test schedule).\n        \n        \n           \n        \n            \n              You can use\n              Amazon CloudWatch RUM to collect and analyze client-side\n              data and shape your testing plan.\n            \n          \n      \n        \n          Select a managed device\n            farm: Select a managed device farm that can support\n          your testing requirements. For example, you can use\n          AWS Device Farm to test and understand the impact of your\n          changes on a representative set of hardware.\n        \n      \n        \n          Use automation: Use\n          automation and continuous integration/continuous deployment\n          (CI/CD) to schedule and run your tests.\n        \n        \n           \n           \n        \n            \n              Integrating\n                AWS Device Farm with your CI/CD pipeline to run\n                cross-browser Selenium tests\n            \n          \n            \n              Building\n                and testing iOS and iPadOS apps with AWS DevOps and mobile\n                services\n            \n          \n      \n        \n          Review and adjust:\n          Continually review your testing results and make necessary\n          improvements.\n        \n      \n     \n   \n    \n    Resources\n    \n    \n    \n      Related documents:\n    \n    \n       \n       \n    \n        \n          AWS Device Farm\n            device list\n        \n      \n        \n          Viewing\n            the CloudWatch RUM dashboard\n        \n      \n    \n      Related videos:\n    \n    \n       \n       \n    \n        \n          AWS           re:Invent 2023 - Improve your mobile and web app quality using\n            AWS Device Farm\n        \n      \n        \n          AWS           re:Invent 2021 - Optimize applications through end user\n            insights with Amazon CloudWatch RUM\n        \n      \n    \n      Related examples:\n    \n    \n       \n       \n       \n    \n        \n          AWS Device Farm Sample App for Android\n        \n      \n        \n          AWS Device Farm Sample App for iOS\n        \n      \n        \n          Appium\n            Web tests for AWS Device Farm\n        \n      \n  \u003cdiv\u003e\u003cdiv\u003e\u003cdiv\u003e\u003cdiv id=\"js_error_message\"\u003e\u003cp\u003e\u003cimg src=\"https://d1ge0kk1l5kms0.cloudfront.net/images/G/01/webservices/console/warning.png\" alt=\"Warning\" /\u003e \u003cstrong\u003eJavascript is disabled or is unavailable in your browser.\u003c/strong\u003e\u003c/p\u003e\u003cp\u003eTo use the Amazon Web Services Documentation, Javascript must be enabled. Please refer to your browser's Help pages for instructions.\u003c/p\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003eDocument ConventionsSUS06-BP04 Increase utilization of build environmentsNotices"
]